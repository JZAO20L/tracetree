{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 溯源树-web搜索版\n",
    "1. 接收一个论文的链接、或pdf、或论文名，然后：\n",
    "    * 如果是pdf，直接使用pypdf解析\n",
    "    * 如果是链接，获取其pdf版本，\n",
    "    * 如果是论文名，使用google scholar搜索，获取其pdf链接，然后使用pypdf解析\n",
    "2. LLM解析论文，获取{title，publication_date，abstract，……，category，references[article_1,...]} \n",
    "3. 在谷歌学术上搜索所有引用论文的论文名，回到1.的接收论文名的情况，对这些论文也进行解析，然后基于一定的方法（比如论文引用数、摘要的相似度）选取其中的top k，这种做法可以只对摘要进行，因为嵌入模型处理长度有上限"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 依赖安装\n",
    "! pip install pydantic langchain langchain_community langchain_core PyPDF2 anytree arxiv requests matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# 加载 .env 文件\n",
    "load_dotenv()\n",
    "\n",
    "# 尝试获取环境变量并打印出来\n",
    "dashscope_api_key = os.getenv(\"DASHSCOPE_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.由论文名获取论文内容\n",
    "- pdf解析  \n",
    "- 论文名搜索-测试api调用时是否有websearch能力？--没有\n",
    "- 没有的话就得结合工具了，看一下能不能直接获取论文内容"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过arxiv，根据论文名搜索论文，返回：1.网页上的论文信息 2.论文原文"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arxiv\n",
    "import requests\n",
    "from PyPDF2 import PdfReader\n",
    "from io import BytesIO\n",
    "\n",
    "def search_paper_info(paper_title):\n",
    "    \"\"\"\n",
    "    搜索arXiv论文并返回其元数据信息。\n",
    "    \n",
    "    参数:\n",
    "        paper_title (str): 要搜索的论文标题\n",
    "        \n",
    "    返回:\n",
    "        info (str): 论文标题、摘要等元数据（若未找到则返回错误信息）\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 使用arxiv.py库搜索论文\n",
    "        search = arxiv.Search(\n",
    "            query=paper_title,\n",
    "            max_results=1,\n",
    "            sort_by=arxiv.SortCriterion.Relevance\n",
    "        )\n",
    "        client = arxiv.Client()\n",
    "        results = list(client.results(search))\n",
    "        \n",
    "        if not results:\n",
    "            return \"未找到匹配论文\"\n",
    "        \n",
    "        paper = results[0]\n",
    "        \n",
    "        # 构建元数据字典\n",
    "        info = {\n",
    "            \"title\": paper.title,\n",
    "            \"abstract\": paper.summary\n",
    "        }\n",
    "        return info\n",
    "\n",
    "    except arxiv.ArxivError as e:\n",
    "        return f\"arXiv搜索失败: {str(e)}\"\n",
    "\n",
    "def search_paper_text(paper_title):\n",
    "    \"\"\"\n",
    "    下载并解析arXiv论文的PDF文本内容。\n",
    "    \n",
    "    参数:\n",
    "        paper_title (str): 要搜索的论文标题\n",
    "        \n",
    "    返回:\n",
    "        text (str|None): 论文PDF文本内容（解析失败或未找到时返回None）\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 使用arxiv.py库搜索论文\n",
    "        search = arxiv.Search(\n",
    "            query=paper_title,\n",
    "            max_results=1,\n",
    "            sort_by=arxiv.SortCriterion.Relevance\n",
    "        )\n",
    "        client = arxiv.Client()\n",
    "        results = list(client.results(search))\n",
    "        \n",
    "        if not results:\n",
    "            return \"未找到匹配论文\", None\n",
    "        \n",
    "        paper = results[0]\n",
    "        \n",
    "        # 下载并解析PDF\n",
    "        pdf_url = paper.pdf_url\n",
    "        \n",
    "        \n",
    "        # 下载PDF文件\n",
    "        response = requests.get(pdf_url, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # 解析PDF内容\n",
    "        with BytesIO(response.content) as pdf_file:\n",
    "            reader = PdfReader(pdf_file)\n",
    "            text = \"\\n\".join(page.extract_text() for page in reader.pages)\n",
    "        \n",
    "        return text.strip() if text else None\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        return f\"PDF下载失败: {str(e)}\"\n",
    "    except Exception as e:\n",
    "        return f\"PDF解析失败: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试\n",
    "# paper_title=\"HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paper_info= search_paper_info(paper_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(paper_info)\n",
    "# print(paper_info[\"abstract\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paper_text = search_paper_text(paper_title) \n",
    "# print(paper_text[:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.解析论文LLM\n",
    "和之前差不多，解析论文信息：题目、发布时间、摘要、引用文献；  \n",
    "改用LangChain的结构化输出处理格式问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Tongyi\n",
    "from langchain.prompts import PromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "\n",
    "class PaperInfo(BaseModel):\n",
    "    \"\"\"\n",
    "    定义论文信息的结构化输出格式。\n",
    "    \"\"\"\n",
    "    title: str = Field(description=\"Title of the paper\")\n",
    "    abstract: str = Field(description=\"Abstract of the paper\")\n",
    "    references: list[str] = Field(description=\"List of titles from the 'references' section\")\n",
    "\n",
    "class ParseLLM:\n",
    "    def __init__(self, model_name, api_key):\n",
    "        self.llm = Tongyi(model=model_name, api_key=api_key)\n",
    "        \n",
    "        # 使用 PydanticOutputParser 来解析结构化输出\n",
    "        self.output_parser = PydanticOutputParser(pydantic_object=PaperInfo)\n",
    "        \n",
    "        # 解析引用，对应建树方法2\n",
    "        self.parse_prompt_template = PromptTemplate(template=\"\"\"\n",
    "            <document>{document}</document>\n",
    "            \n",
    "            <task>\n",
    "            Extract the following information from the document above and return it in JSON format:\n",
    "            - title: Title of the paper\n",
    "            - abstract: Abstract of the paper\n",
    "            - references: List of titles from the \"references\" section (exact strings as they appear). Only extract titles, do not include other information.\n",
    "            </task>\n",
    "            \n",
    "            {format_instructions}\n",
    "        \"\"\", input_variables=[\"document\"], partial_variables={\"format_instructions\": self.output_parser.get_format_instructions()})\n",
    "\n",
    "    def parse_paper(self, paper):\n",
    "        try:\n",
    "            # 格式化提示模板\n",
    "            prompt = self.parse_prompt_template.format(document=paper)\n",
    "            \n",
    "            # 调用模型并获取响应\n",
    "            response = self.llm.invoke(prompt)\n",
    "            \n",
    "            # 使用 PydanticOutputParser 解析响应为结构化数据\n",
    "            structured_output = self.output_parser.parse(response)\n",
    "            return structured_output\n",
    "        \n",
    "        except Exception as e:\n",
    "            # 捕获其他异常（如 LLM 拒绝回答、网络错误等）\n",
    "            raise RuntimeError(f\"Error occurred while processing the paper: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试\n",
    "# 初始化解析器\n",
    "# parser =ParseLLM(model_name=\"qwen-turbo\",api_key=dashscope_api_key)\n",
    " \n",
    "# result = parser.parse_paper(paper_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Title:\", result.title)\n",
    "# print(\"Abstract:\", result.abstract)\n",
    "# print(\"References:\", result.references)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.进行建树\n",
    "def parse_and_build_trace_tree(root_paper_title, max_layers, parse_llm, embedding, output_path, top_k):  \n",
    "维护一个待处理节点队列nodes_to_process[paper_title:str]，进行以下建树操作：  \n",
    "1. 获取根节点文献内容，使用search_paper_text获取paper_text\n",
    "2. 根据当前处理节点的引用，对引用列表中的论文，使用search_paper_info获取paper_info\n",
    "3. 使用嵌入模型，对引用论文的摘要向量化，选择摘要相关性top k且之前未出现在溯源树中的引用论文作为当前处理论文的子节点，并将选择的论文的paper_title加入待处理节点队列\n",
    "4. 重复步骤1-3，直至溯源树达到要求的层次\n",
    "使用anytree建树，建完树后使用save_tree_to_json(root,output_path)函数，将树结构保存到output_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "辅助函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from anytree import Node, RenderTree\n",
    "    \n",
    "# 打印树结构\n",
    "def print_tree(root_node):\n",
    "    for pre, fill, node in RenderTree(root_node):\n",
    "        print(f\"{pre}{node.name}\")\n",
    "        \n",
    "# 将树转换为dict\n",
    "def tree_to_dict(node):\n",
    "    \"\"\"\n",
    "    将 anytree 节点及其子节点递归转换为字典。\n",
    "    \n",
    "    参数:0\n",
    "        node: anytree 节点。\n",
    "        \n",
    "    返回:\n",
    "        表示树结构的字典。\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"name\": node.name,\n",
    "        \"abstract\": node.abstract, \n",
    "        \"children\": [tree_to_dict(child) for child in node.children]\n",
    "    }\n",
    "\n",
    "# 将树保存为json\n",
    "def save_tree_to_json(root, file_path):\n",
    "    \"\"\"\n",
    "    将 anytree 树保存为 JSON 文件。\n",
    "    \n",
    "    参数:\n",
    "        root: 树的根节点。\n",
    "        file_path: 输出 JSON 文件的路径。\n",
    "    \"\"\"\n",
    "    # 确保文件路径的目录存在\n",
    "    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "    \n",
    "    # 将树转换为字典\n",
    "    tree_dict = tree_to_dict(root)\n",
    "    \n",
    "    # 写入 JSON 文件\n",
    "    with open(file_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(tree_dict, f, ensure_ascii=False, indent=4)\n",
    "    \n",
    "    print(f\"树已成功保存到: {file_path}\")\n",
    "        \n",
    "# 将字典转换为树\n",
    "def dict_to_tree(tree_dict, parent=None):\n",
    "    \"\"\"\n",
    "    将字典形式的树结构递归转换为 anytree 节点。\n",
    "    \n",
    "    参数:\n",
    "        tree_dict: 表示树结构的字典。\n",
    "        parent: 当前节点的父节点（用于递归构建子节点）。\n",
    "        \n",
    "    返回:\n",
    "        构建完成的 anytree 节点。\n",
    "    \"\"\"\n",
    "    # 创建当前节点\n",
    "    current_node = Node(tree_dict[\"name\"], parent=parent, data=tree_dict.get(\"data\"))\n",
    "    \n",
    "    # 递归构建子节点\n",
    "    for child_dict in tree_dict.get(\"children\", []):\n",
    "        dict_to_tree(child_dict, parent=current_node)\n",
    "    \n",
    "    return current_node\n",
    "\n",
    "# 从json加载树\n",
    "def load_tree_from_json(file_path):\n",
    "    \"\"\"\n",
    "    从 JSON 文件中加载树结构并返回根节点。\n",
    "    \n",
    "    参数:\n",
    "        file_path: 输入 JSON 文件的路径。\n",
    "        \n",
    "    返回:\n",
    "        树的根节点。\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        tree_dict = json.load(f)\n",
    "    \n",
    "    # 从字典重建树\n",
    "    root = dict_to_tree(tree_dict)\n",
    "    return root"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "建树，额外说明：  \n",
    "- references_limit  \n",
    "    - 设references_limit主要是为了速度，以及有的论文引用数量太大（遇到过一个九百多的），\n",
    "    - 当references_limit=10时，生成一个节点需要1min左右，包括获取论文信息（5~10s/篇）、获取原文并解析论文引用（20s左右）、建向量库和选取topk（20s左右）的时间开销；\n",
    "    - 大部分论文的引用论文数量在30左右，设为30应该已经足够；实测30篇论文获取信息大约需要2min，建节点大约需要3min\n",
    "- 实际使用时需要需要平衡topk和max_layers，个人感觉max_layers超过三层基本没有必要，以三层为例，topk选取3~5应该比较适宜\n",
    "\n",
    "后续用于research时，看看能不能并行加速"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from anytree import Node, RenderTree\n",
    "from collections import deque\n",
    "from typing import List, Tuple\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "from tqdm import tqdm\n",
    "\n",
    "def build_trace_tree(\n",
    "    root_paper_title: str,\n",
    "    max_layers: int,\n",
    "    parse_llm,\n",
    "    embedding,\n",
    "    output_path: str,\n",
    "    top_k: int,\n",
    "    references_limit:int\n",
    "):\n",
    "    \"\"\"\n",
    "    使用 anytree 构建溯源树，并将树结构保存为 JSON 文件。\n",
    "    \n",
    "    参数:\n",
    "        root_paper_title (str): 根节点论文标题\n",
    "        max_layers (int): 溯源树的最大层数\n",
    "        parse_llm: 用于解析论文内容的 LLM 工具\n",
    "        embedding: 嵌入模型，用于计算摘要向量的相关性\n",
    "        output_path (str): 输出溯源树的文件路径\n",
    "        top_k (int): 每层选择的最相关引用论文数量\n",
    "        references_limit (int): 每篇论文中引用文献的最大数量\n",
    "    \n",
    "    返回:\n",
    "        None: 将溯源树保存到指定路径\n",
    "    \"\"\"\n",
    "    # 初始化溯源树和待处理队列\n",
    "    nodes_to_process = deque()  # 待处理节点队列\n",
    "    visited_titles = set()  # 已访问的论文标题集合，避免重复处理\n",
    "    # 初始化内存向量数据库\n",
    "    vector_store = InMemoryVectorStore(embedding)\n",
    "    \n",
    "    # 获取根节点论文内容并初始化溯源树\n",
    "    try:\n",
    "        root_paper_info = search_paper_info(root_paper_title)\n",
    "        if \"未找到匹配论文\" in root_paper_info:\n",
    "            raise ValueError(\"未找到根节点论文\")\n",
    "        \n",
    "        root_paper_text = search_paper_text(root_paper_title)\n",
    "        if not root_paper_text:\n",
    "            raise ValueError(\"无法获取根节点论文内容\")\n",
    "        \n",
    "        # 创建根节点，节点中保存论文名和论文摘要\n",
    "        root_node = Node(root_paper_info[\"title\"], abstract=root_paper_info[\"abstract\"])\n",
    "        nodes_to_process.append((root_node, 0))  # (当前节点, 当前层数)\n",
    "        visited_titles.add(root_paper_info[\"title\"])    # 节点标题添加到已访问集合\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"初始化根节点失败: {e}\")\n",
    "        return\n",
    "\n",
    "    # 构建溯源树\n",
    "    while nodes_to_process:\n",
    "        current_node, current_layer = nodes_to_process.popleft()\n",
    "\n",
    "        # 如果达到最大层数，停止处理\n",
    "        if current_layer >= max_layers:\n",
    "            break\n",
    "        \n",
    "        # 提示当前处理论文的信息\n",
    "        print(f\"\\n当前节点论文: {current_node.name}\")\n",
    "\n",
    "        # 获取当前论文的引用列表\n",
    "        # 先获取原文，然后用LLM解析出引用文献列表\n",
    "        try:\n",
    "            paper_text=search_paper_text(current_node.name)\n",
    "            references = parse_llm.parse_paper(paper_text).references\n",
    "            if not references:\n",
    "                continue\n",
    "        except Exception as e:\n",
    "            print(f\"解析引用失败: {e}\")\n",
    "            continue\n",
    "        \n",
    "        # 截取前references_limit个引用\n",
    "        if len(references) > references_limit:\n",
    "            references = references[:references_limit]\n",
    "\n",
    "        # 对引用论文进行信息提取和筛选\n",
    "        for ref_title in tqdm(references,desc=\"处理引用论文信息\", unit=\"paper\"):\n",
    "            # 获取引用论文的元数据信息\n",
    "            ref_info = search_paper_info(ref_title)\n",
    "                        \n",
    "            if \"未找到匹配论文\" in ref_info:\n",
    "                continue  # 跳过无效引用\n",
    "            \n",
    "            if ref_info[\"title\"] in visited_titles:\n",
    "                continue  # 跳过已访问的论文\n",
    "\n",
    "            # 将引用论文的标题和摘要添加到内存向量数据库,注意使用arxiv上获得的论文信息，避免LLM解析时出现的任何格式和大小写问题\n",
    "            doc=Document(page_content=ref_info[\"abstract\"], metadata={\"title\": ref_info[\"title\"]})\n",
    "            vector_store.add_documents([doc])\n",
    "\n",
    "        # 选择相关性最高的 top_k 引用论文\n",
    "        selected_docs = vector_store.similarity_search(current_node.abstract, k=top_k)\n",
    "\n",
    "        # 对召回且未加入树中的论文创建节点，更新溯源树，添加子节点到队列，添加论文标题到已访问集合\n",
    "        for doc in selected_docs:\n",
    "            if doc.metadata[\"title\"] not in visited_titles:\n",
    "                child_node = Node(doc.metadata[\"title\"], parent=current_node, abstract=doc.page_content)\n",
    "                nodes_to_process.append((child_node, current_layer + 1))\n",
    "                visited_titles.add(doc.metadata[\"title\"])\n",
    "                \n",
    "        # 清空vectore，最简单的方法就是重新初始化一个\n",
    "        vector_store = InMemoryVectorStore(embedding)\n",
    "        \n",
    "\n",
    "    # 打印溯源树结构（可选）\n",
    "    print(\"\\n溯源树构建完成，打印树结构:\")\n",
    "    # print(RenderTree(root_node))\n",
    "    print_tree(root_node)\n",
    "\n",
    "    # # 保存溯源树到 JSON 文件\n",
    "    save_tree_to_json(root_node, output_path)\n",
    "    \n",
    "    return root_node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding\n",
    "from langchain_community.embeddings import DashScopeEmbeddings\n",
    "\n",
    "parse_llm=ParseLLM(model_name=\"qwen-turbo\",api_key=dashscope_api_key)\n",
    "\n",
    "embeddings = DashScopeEmbeddings(\n",
    "    model=\"text-embedding-v3\", dashscope_api_key=dashscope_api_key\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建树示例\n",
    "root_paper_title=\"PoisonedRAG\"\n",
    "output_path=\"output/tree/\"+root_paper_title+\".json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root=build_trace_tree(\n",
    "    root_paper_title=root_paper_title,\n",
    "    max_layers=3,\n",
    "    parse_llm=parse_llm,\n",
    "    embedding=embeddings,\n",
    "    output_path=output_path,\n",
    "    top_k=3,\n",
    "    references_limit=30\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建树后的写入、写出json文件测试\n",
    "print_tree(root)\n",
    "\n",
    "# tree_path=\"output/tree/trace_tree01.json\"\n",
    "# save_tree_to_json(root,tree_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tree_path=\"output/tree/trace_tree01.json\"\n",
    "# root=load_tree_from_json(tree_path)\n",
    "# print_tree(root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.绘制溯源树,转为图像"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用Graphviz，需要先去官网下载：https://www.graphviz.org/download/，  \n",
    "添加Graphviz/bin的路径到环境变量path，并且重启程序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from anytree import Node, RenderTree\n",
    "from anytree.exporter import DotExporter\n",
    "from graphviz import Source\n",
    "import os\n",
    "import textwrap\n",
    "\n",
    "def draw_tree_to_image(root, filename=\"tree.png\", format=\"png\", output_path=\".\", max_text_width=20):\n",
    "    \"\"\"\n",
    "    使用 graphviz 将 anytree 格式的树绘制为图像文件，节点颜色按层次轮替。\n",
    "    \n",
    "    参数:\n",
    "        root (Node): 树的根节点，类型为 anytree.Node。\n",
    "        filename (str): 导出的图像文件名，默认为 \"tree.png\"。\n",
    "        format (str): 图像格式，默认为 \"png\"，可选值包括 \"png\", \"svg\", \"pdf\" 等。\n",
    "        output_path (str): 输出目录，默认为当前目录 \".\"。\n",
    "        max_text_width (int): 每行最大字符数，默认为 20。\n",
    "        \n",
    "    输出:\n",
    "        生成指定格式的树形图，并保存到指定目录中。\n",
    "    \"\"\"\n",
    "    if not root:\n",
    "        print(\"树为空\")\n",
    "        return\n",
    "\n",
    "    # 确保输出目录存在\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "    # 构造完整的文件路径\n",
    "    full_path = os.path.join(output_path, filename)\n",
    "\n",
    "    # 计算每个节点的颜色映射\n",
    "    node_color_map = calculate_depth_colors(root)\n",
    "\n",
    "    # 动态计算节点大小和间距\n",
    "    def calculate_node_size_and_spacing():\n",
    "        # 获取所有节点\n",
    "        all_nodes = [node for _, _, node in RenderTree(root)]\n",
    "        max_name_length = max(len(node.name) for node in all_nodes)\n",
    "        max_children = max(len(node.children) for node in all_nodes) or 1\n",
    "        node_width = max(0.03, 0.005 * max_name_length)  # 减小节点宽度\n",
    "        ranksep = max(0.05, 0.01 * max_name_length)      # 层间距随名称长度调整\n",
    "        nodesep = max(0.05, 0.02 * max_children)        # 同层节点间距随子节点数量调整\n",
    "        return node_width, ranksep, nodesep\n",
    "\n",
    "    node_width, ranksep, nodesep = calculate_node_size_and_spacing()\n",
    "\n",
    "    # 自动换行函数\n",
    "    def wrap_text(text, max_width):\n",
    "        \"\"\"\n",
    "        将文字内容按指定宽度自动换行，并确保内容符合 DOT 格式要求。\n",
    "        \n",
    "        参数:\n",
    "            text (str): 原始文字内容。\n",
    "            max_width (int): 每行最大字符数。\n",
    "            \n",
    "        返回:\n",
    "            str: 插入换行符后的文字内容，已转义为符合 DOT 格式的字符串。\n",
    "        \"\"\"\n",
    "        wrapped_lines = textwrap.wrap(text, width=max_width)\n",
    "        wrapped_text = \"\\n\".join(wrapped_lines)\n",
    "        return json.dumps(wrapped_text)  # 使用 json.dumps 自动转义特殊字符\n",
    "\n",
    "    # 使用 DotExporter 导出 DOT 格式\n",
    "    dot_exporter = DotExporter(\n",
    "        root,\n",
    "        nodeattrfunc=lambda node: (\n",
    "            f'label={wrap_text(node.name, max_text_width)} '\n",
    "            f'shape=box style=filled fillcolor=\"{node_color_map[node]}\" '\n",
    "            f'width={node_width} height={node_width} fontsize=14'\n",
    "        ),\n",
    "        edgeattrfunc=lambda parent, child: 'color=\"#E8F3E8\" style=\"bold\"',\n",
    "        options=[\n",
    "            \"rankdir=TB\",                # 设置从上到下布局\n",
    "            f\"ranksep={ranksep}\",        # 设置层间距\n",
    "            f\"nodesep={nodesep}\",        # 设置同层节点间距\n",
    "            \"splines=curved\"             # 设置边为曲线\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # 导出 DOT 格式数据\n",
    "    dot_data = \"\".join(dot_exporter)\n",
    "    # print(\"Generated DOT data:\")  # 调试输出\n",
    "    # print(dot_data)  # 打印生成的 DOT 数据以便检查\n",
    "\n",
    "    # 使用 graphviz 渲染为图像文件\n",
    "    graph = Source(dot_data)\n",
    "    rendered_file = graph.render(filename=full_path, format=format, cleanup=True)\n",
    "    print(f\"树已导出为图像文件: {rendered_file}\")\n",
    "\n",
    "def calculate_depth_colors(root):\n",
    "    \"\"\"\n",
    "    计算树中每个节点的深度，并返回一个颜色映射字典。\n",
    "    \n",
    "    参数:\n",
    "        root (Node): 树的根节点。\n",
    "        \n",
    "    返回:\n",
    "        dict: 节点到颜色的映射（如 {node: \"#006400\"}）。\n",
    "    \"\"\"\n",
    "    # 定义三种逐渐变浅的绿色\n",
    "    green_colors = [\n",
    "        \"#32CD32\",  # 中绿\n",
    "        \"#98FB98\",  # 浅绿\n",
    "        \"#F0FFF0\",  # 淡绿\n",
    "    ]\n",
    "\n",
    "    # 使用广度优先搜索（BFS）计算深度并生成颜色映射\n",
    "    node_color_map = {}\n",
    "    queue = [(root, 0)]  # (当前节点, 当前深度)\n",
    "    while queue:\n",
    "        node, depth = queue.pop(0)\n",
    "        color_index = depth % len(green_colors)  # 根据深度选择颜色\n",
    "        node_color_map[node] = green_colors[color_index]\n",
    "        for child in node.children:\n",
    "            queue.append((child, depth + 1))\n",
    "\n",
    "    return node_color_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 示例用法\n",
    "output_directory = \"output/charts/\" # 指定输出路径\n",
    "draw_tree_to_image(root, filename=\"tree03\", output_path=output_directory,max_text_width=20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
