{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 溯源树-web搜索版\n",
    "1. 接收一个论文的链接、或pdf、或论文名，然后：\n",
    "    * 如果是pdf，直接使用pypdf解析\n",
    "    * 如果是链接，获取其pdf版本，\n",
    "    * 如果是论文名，使用google scholar搜索，获取其pdf链接，然后使用pypdf解析\n",
    "2. LLM解析论文，获取{title，publication_date，abstract，……，category，references[article_1,...]} \n",
    "3. 在谷歌学术上搜索所有引用论文的论文名，回到1.的接收论文名的情况，对这些论文也进行解析，然后基于一定的方法（比如论文引用数、摘要的相似度）选取其中的top k，这种做法可以只对摘要进行，因为嵌入模型处理长度有上限"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.由论文名获取论文内容\n",
    "- pdf解析  \n",
    "- 论文名搜索-测试api调用时是否有websearch能力？--没有\n",
    "- 没有的话就得结合工具了，看一下能不能直接获取论文内容"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: pydantic in c:\\users\\25859\\conda3\\envs\\rag\\lib\\site-packages (2.10.5)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\25859\\conda3\\envs\\rag\\lib\\site-packages (from pydantic) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\25859\\conda3\\envs\\rag\\lib\\site-packages (from pydantic) (2.27.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in c:\\users\\25859\\conda3\\envs\\rag\\lib\\site-packages (from pydantic) (4.12.2)\n"
     ]
    }
   ],
   "source": [
    "# 依赖安装\n",
    "! pip install pydantic\n",
    "! pip install langchain\n",
    "! pip install langchain_community\n",
    "! pip install PyPDF2\n",
    "! pip install anytree\n",
    "! pip install langchain_core\n",
    "! pip install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# 加载 .env 文件\n",
    "load_dotenv()\n",
    "\n",
    "# 尝试获取环境变量并打印出来\n",
    "dashscope_api_key = os.getenv(\"DASHSCOPE_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Tongyi\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过arxiv，根据论文名搜索论文原文"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from PyPDF2 import PdfReader\n",
    "from io import BytesIO\n",
    "import re\n",
    "from xml.etree import ElementTree as ET\n",
    "\n",
    "\n",
    "def search_paper(paper_title):\n",
    "    \"\"\"\n",
    "    搜索论文并返回其文本内容。\n",
    "    \n",
    "    参数:\n",
    "        paper_title (str): 论文标题\n",
    "        \n",
    "    返回:\n",
    "        str: 论文的文本内容（如果成功解析），否则返回 None\n",
    "    \"\"\"\n",
    "    def clean_title(title):\n",
    "        # 移除非字母数字字符，并转换为小写\n",
    "        return re.sub(r'[^\\w\\s\\-]', '', title).strip().lower()\n",
    "\n",
    "    paper_title = clean_title(paper_title)\n",
    "    \n",
    "    # Step 1: 在arXiv上搜索论文并获取PDF链接\n",
    "    url = \"http://export.arxiv.org/api/query\"\n",
    "    \n",
    "    # 参数设置：搜索标题中包含的关键词\n",
    "    params = {\n",
    "        'search_query': f'ti:\"{paper_title}\"',  # ti: 表示按标题搜索\n",
    "        'start': 0,\n",
    "        'max_results': 1  # 只返回第一个结果\n",
    "    }\n",
    "    \n",
    "    # 寻找最早的论文\n",
    "    params['sort_by'] = 'submittedDate'\n",
    "    params['sort_order'] = 'ascending'\n",
    "    \n",
    "    response = requests.get(url, params=params)\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        print(f\"请求失败，状态码: {response.status_code}\")\n",
    "        return None\n",
    "    \n",
    "    # 解析arXiv返回的XML数据\n",
    "    root = ET.fromstring(response.content)\n",
    "    namespace = {'atom': 'http://www.w3.org/2005/Atom'}\n",
    "    entry = root.find('atom:entry', namespace)\n",
    "    \n",
    "    if entry is None:\n",
    "        print(\"未找到相关论文\")\n",
    "        return None\n",
    "    \n",
    "    # 提取论文的PDF链接\n",
    "    pdf_link = entry.find('atom:id', namespace).text.replace('abs', 'pdf') + '.pdf'\n",
    "    \n",
    "    # Step 2: 使用PyPDF2解析PDF文件（不下载PDF）\n",
    "    response = requests.get(pdf_link)\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        print(f\"无法获取PDF内容，状态码: {response.status_code}\")\n",
    "        return None\n",
    "    \n",
    "    # 将二进制内容转换为BytesIO对象\n",
    "    pdf_content = BytesIO(response.content)\n",
    "    \n",
    "    # 使用PyPDF2解析PDF内容\n",
    "    reader = PdfReader(pdf_content)\n",
    "    number_of_pages = len(reader.pages)\n",
    "    text = \"\"\n",
    "    \n",
    "    for page in range(number_of_pages):\n",
    "        page_text = reader.pages[page].extract_text()\n",
    "        if page_text:\n",
    "            text += page_text\n",
    "    \n",
    "    if text.strip():\n",
    "        return text\n",
    "    else:\n",
    "        print(\"无法提取论文文本内容\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试\n",
    "# paper_title = input(\"请输入论文标题: \")\n",
    "    \n",
    "# paper_text = search_paper(paper_title)\n",
    "    \n",
    "# if paper_text:\n",
    "#     print(\"\\n论文内容解析如下:\\n\")\n",
    "#     print(paper_text[:500])  # 打印前500个字符作为示例\n",
    "# else:\n",
    "#     print(\"未能获取论文文本\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO：查找成功率较低，之后探索如何解析为容易查找到的格式、查找时应该使用什么样的param\n",
    "# paper_text = search_paper(\"HotpotQA\")\n",
    "# paper_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.解析论文LLM\n",
    "和之前差不多，解析论文信息：题目、发布时间、摘要、引用文献；  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Tongyi\n",
    "from langchain.prompts import PromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "\n",
    "class PaperInfo(BaseModel):\n",
    "    \"\"\"\n",
    "    定义论文信息的结构化输出格式。\n",
    "    \"\"\"\n",
    "    title: str = Field(description=\"Title of the paper\")\n",
    "    abstract: str = Field(description=\"Abstract of the paper\")\n",
    "    references: list[str] = Field(description=\"List of titles from the 'references' section\")\n",
    "\n",
    "class ParseLLM:\n",
    "    def __init__(self, model_name, api_key):\n",
    "        self.llm = Tongyi(model=model_name, api_key=api_key)\n",
    "        \n",
    "        # 使用 PydanticOutputParser 来解析结构化输出\n",
    "        self.output_parser = PydanticOutputParser(pydantic_object=PaperInfo)\n",
    "        \n",
    "        # 解析引用，对应建树方法2\n",
    "        self.parse_prompt_template = PromptTemplate(template=\"\"\"\n",
    "            <document>{document}</document>\n",
    "            \n",
    "            Extract the following information from the document above and return it in JSON format:\n",
    "            - title: Title of the paper\n",
    "            - abstract: Abstract of the paper\n",
    "            - references: List of titles from the \"references\" section (exact strings as they appear). Only extract titles, do not include other information.\n",
    "\n",
    "            {format_instructions}\n",
    "        \"\"\", input_variables=[\"document\"], partial_variables={\"format_instructions\": self.output_parser.get_format_instructions()})\n",
    "\n",
    "    def response(self, prompt):\n",
    "        try:\n",
    "            # 尝试调用模型并获取响应\n",
    "            response = self.llm.invoke(prompt)\n",
    "            if not response or not isinstance(response, str):\n",
    "                raise ValueError(\"Invalid response received from the model.\")\n",
    "            return response\n",
    "        except Exception as e:\n",
    "            # 捕获异常并抛出自定义错误信息\n",
    "            raise RuntimeError(f\"Error occurred while invoking the model: {str(e)}\")\n",
    "\n",
    "    def parse_paper(self, paper):\n",
    "        try:\n",
    "            # 格式化提示模板\n",
    "            prompt = self.parse_prompt_template.format(document=paper)\n",
    "            # 获取模型响应\n",
    "            response = self.response(prompt)\n",
    "            \n",
    "            # 使用 PydanticOutputParser 解析响应为结构化数据\n",
    "            structured_output = self.output_parser.parse(response)\n",
    "            return structured_output\n",
    "        except Exception as e:\n",
    "            # 捕获异常并抛出自定义错误信息\n",
    "            raise RuntimeError(f\"Error occurred while parsing the paper: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Attention Is All You Need\n",
      "Abstract: We propose a new architecture called Transformer...\n",
      "References: ['Neural Machine Translation by Jointly Learning to Align and Translate', 'ImageNet Classification with Deep Convolutional Neural Networks']\n"
     ]
    }
   ],
   "source": [
    "# 测试\n",
    "# 初始化解析器\n",
    "# parser =ParseLLM(model_name=\"qwen-turbo\",api_key=dashscope_api_key)\n",
    "\n",
    "# # 输入论文内容\n",
    "# paper_content = \"\"\"\n",
    "# <title>Attention Is All You Need</title>\n",
    "# <abstract>We propose a new architecture called Transformer...</abstract>\n",
    "# <references>\n",
    "# - Neural Machine Translation by Jointly Learning to Align and Translate\n",
    "# - ImageNet Classification with Deep Convolutional Neural Networks\n",
    "# </references>\n",
    "# \"\"\"\n",
    "\n",
    "# try:\n",
    "#     # 解析论文\n",
    "#     result = parser.parse_paper(paper_content)\n",
    "    \n",
    "#     # 输出结果\n",
    "#     print(\"Title:\", result.title)\n",
    "#     print(\"Abstract:\", result.abstract)\n",
    "#     print(\"References:\", result.references)\n",
    "# except RuntimeError as e:\n",
    "#     print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.进行建树\n",
    "1. 获取根节点文献内容，解析，获取摘要和引用文献名\n",
    "2. 获取引用文献内容，解析论文获取摘要\n",
    "3. 使用嵌入模型对摘要向量化，选择摘要相关性top k，且之前未出现在溯源树中论文的作为子节点\n",
    "4. 然后对子节点继续进行步骤1-3，直至溯源树达到要求的层次"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "辅助函数\n",
    "- TODO 改用LangGraph处理格式问题、解析为统一的类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from anytree import Node, RenderTree\n",
    "\n",
    "# 将模型返回的带有 Markdown 格式代码块的 JSON 字符串解析为 Python 字典\n",
    "def parse_to_json(response):\n",
    "    \"\"\"\n",
    "    解析模型返回的带有 Markdown 格式代码块的 JSON 字符串。\n",
    "    \n",
    "    :param response: 模型返回的字符串，例如：\n",
    "                     ```json\\n{\\n  \"论文题目\": \"INJEC AGENT...\",\\n  \"发布时间\": \"2024-08-04\",\\n  \"摘要\": \"Recent work...\"\\n}\\n```\n",
    "    :return: 解析后的 Python 字典\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Step 1: 去掉 Markdown 格式的代码块标记\n",
    "        if response.startswith(\"```json\") and response.endswith(\"```\"):\n",
    "            # 去掉开头的 ```json 和结尾的 ```\n",
    "            response = response[len(\"```json\"):-len(\"```\")].strip()\n",
    "        \n",
    "        # Step 2: 将剩余的字符串解析为 JSON\n",
    "        parsed_data = json.loads(response)\n",
    "        return parsed_data\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"JSON 解析失败: {e}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"发生错误: {e}\")\n",
    "        return None\n",
    "    \n",
    "# 打印树结构\n",
    "def print_tree(root_node):\n",
    "    for pre, fill, node in RenderTree(root_node):\n",
    "        print(f\"{pre}{node.name}\")\n",
    "        \n",
    "# 将树转换为dict\n",
    "def tree_to_dict(node):\n",
    "    \"\"\"\n",
    "    将 anytree 节点及其子节点递归转换为字典。\n",
    "    \n",
    "    参数:0\n",
    "        node: anytree 节点。\n",
    "        \n",
    "    返回:\n",
    "        表示树结构的字典。\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"name\": node.name,\n",
    "        \"data\": node.data,  # 假设每个节点有 data 属性\n",
    "        \"children\": [tree_to_dict(child) for child in node.children]\n",
    "    }\n",
    "\n",
    "# 将树保存为json\n",
    "def save_tree_to_json(root, file_path):\n",
    "    \"\"\"\n",
    "    将 anytree 树保存为 JSON 文件。\n",
    "    \n",
    "    参数:\n",
    "        root: 树的根节点。\n",
    "        file_path: 输出 JSON 文件的路径。\n",
    "    \"\"\"\n",
    "    # 确保文件路径的目录存在\n",
    "    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "    \n",
    "    # 将树转换为字典\n",
    "    tree_dict = tree_to_dict(root)\n",
    "    \n",
    "    # 写入 JSON 文件\n",
    "    with open(file_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(tree_dict, f, ensure_ascii=False, indent=4)\n",
    "    \n",
    "    print(f\"树已成功保存到: {file_path}\")\n",
    "        \n",
    "# 将字典转换为树\n",
    "def dict_to_tree(tree_dict, parent=None):\n",
    "    \"\"\"\n",
    "    将字典形式的树结构递归转换为 anytree 节点。\n",
    "    \n",
    "    参数:\n",
    "        tree_dict: 表示树结构的字典。\n",
    "        parent: 当前节点的父节点（用于递归构建子节点）。\n",
    "        \n",
    "    返回:\n",
    "        构建完成的 anytree 节点。\n",
    "    \"\"\"\n",
    "    # 创建当前节点\n",
    "    current_node = Node(tree_dict[\"name\"], parent=parent, data=tree_dict.get(\"data\"))\n",
    "    \n",
    "    # 递归构建子节点\n",
    "    for child_dict in tree_dict.get(\"children\", []):\n",
    "        dict_to_tree(child_dict, parent=current_node)\n",
    "    \n",
    "    return current_node\n",
    "\n",
    "# 从json加载树\n",
    "def load_tree_from_json(file_path):\n",
    "    \"\"\"\n",
    "    从 JSON 文件中加载树结构并返回根节点。\n",
    "    \n",
    "    参数:\n",
    "        file_path: 输入 JSON 文件的路径。\n",
    "        \n",
    "    返回:\n",
    "        树的根节点。\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        tree_dict = json.load(f)\n",
    "    \n",
    "    # 从字典重建树\n",
    "    root = dict_to_tree(tree_dict)\n",
    "    return root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from anytree import Node, RenderTree\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "from langchain_core.documents import Document  # 导入 Document 类\n",
    "from tqdm import tqdm  # 用于显示进度条\n",
    "\n",
    "# 建树\n",
    "def parse_and_build_trace_tree(root_paper_title, max_layers, parse_llm, embedding, output_path, top_k):\n",
    "    \"\"\"\n",
    "    构建溯源树。\n",
    "    \n",
    "    参数:\n",
    "        root_paper_title (str): 根论文标题。\n",
    "        max_layers (int): 最大层数。\n",
    "        parse_llm: 用于解析论文内容的 LLM 解析器。\n",
    "        embedding: 用于向量化文本的嵌入模型。\n",
    "        output_path (str): 输出 JSON 文件路径。\n",
    "        top_k (int): 每层选择的相似论文数量。\n",
    "    \"\"\"\n",
    "    print(f\"开始构建溯源树，根论文: {root_paper_title}\")\n",
    "\n",
    "    # Step 1：处理根论文\n",
    "    paper_text = search_paper(root_paper_title)\n",
    "    if not paper_text:\n",
    "        print(f\"无法获取论文: {root_paper_title}\")\n",
    "        return\n",
    "\n",
    "    paper_info = parse_llm.parse_paper(paper_text)\n",
    "    json_paper_info = parse_to_json(paper_info)\n",
    "\n",
    "    # 创建根节点\n",
    "    root_node = Node(json_paper_info[\"title\"], data=json_paper_info)\n",
    "\n",
    "    # 哈希表统计当前已经在树中的论文\n",
    "    in_tree_papers_set = set()\n",
    "    in_tree_papers_set.add(root_paper_title)\n",
    "\n",
    "    # 初始化队列，将根论文加入队列\n",
    "    queue = [(root_node, 0)]  # (当前节点, 当前层数)\n",
    "\n",
    "    # 初始化内存向量数据库\n",
    "    vector_store = InMemoryVectorStore(embedding)\n",
    "\n",
    "    # Step 2：使用队列按层次进行建树\n",
    "    total_nodes_processed = 0  # 统计处理的节点总数\n",
    "    with tqdm(total=max_layers, desc=\"构建溯源树\", unit=\"层\") as pbar:\n",
    "        while queue:\n",
    "            current_node, current_layer = queue.pop(0)\n",
    "\n",
    "            # 如果达到最大层数，停止扩展\n",
    "            if current_layer >= max_layers:\n",
    "                break\n",
    "\n",
    "            print(f\"\\n正在处理第 {current_layer + 1} 层，当前节点: {current_node.name}\")\n",
    "\n",
    "            # 获取当前论文的引用论文内容\n",
    "            references = current_node.data.get(\"references\", [])\n",
    "            if not references:\n",
    "                print(\"当前节点没有引用论文，跳过...\")\n",
    "                continue\n",
    "\n",
    "            print(f\"正在解析 {len(references)} 篇引用论文...\")\n",
    "\n",
    "            # 解析引用论文的内容并提取摘要\n",
    "            reference_docs = []\n",
    "            for ref_title in references:\n",
    "                print(f\"正在获取引用论文: {ref_title}\")\n",
    "                ref_text = search_paper(ref_title)\n",
    "                if not ref_text:\n",
    "                    continue\n",
    "\n",
    "                ref_info = parse_llm.parse_paper(ref_text)\n",
    "                ref_json_info = parse_to_json(ref_info)\n",
    "\n",
    "                # 将引用论文的摘要和题目转为 Document 类型，添加到 vector_store 中\n",
    "                if ref_json_info is None:\n",
    "                    continue\n",
    "                \n",
    "                doc = Document(page_content=ref_json_info[\"abstract\"], metadata={\"title\": ref_json_info[\"title\"]})\n",
    "                vector_store.add_documents([doc])\n",
    "                reference_docs.append(doc)\n",
    "\n",
    "            # 使用当前论文的摘要作为匹配文本，在 vector_store 中进行相似度匹配\n",
    "            current_abstract = current_node.data[\"abstract\"]\n",
    "            similar_docs = vector_store.similarity_search(current_abstract, k=top_k)\n",
    "\n",
    "            # 筛选未出现在树中的论文\n",
    "            for doc in similar_docs:\n",
    "                title = doc.metadata[\"title\"]\n",
    "                if title not in in_tree_papers_set:\n",
    "                    # 获取论文内容\n",
    "                    paper_text = search_paper(title)\n",
    "                    # 解析论文内容\n",
    "                    try:\n",
    "                        paper_info = parse_llm.parse_paper(paper_text)\n",
    "                        json_paper_info = parse_to_json(paper_info)  # 转换为 JSON 格式\n",
    "                    except Exception as e:\n",
    "                        print(f\"解析论文失败: {title}，错误信息: {e}\")\n",
    "                        continue\n",
    "                    \n",
    "                    # 创建新节点并加入树中\n",
    "                    new_node = Node(title, parent=current_node, data=json_paper_info)\n",
    "                    in_tree_papers_set.add(title)\n",
    "\n",
    "                    # 将新节点加入队列\n",
    "                    queue.append((new_node, current_layer + 1))\n",
    "\n",
    "\n",
    "            # 处理完一个节点，清空 vector_store\n",
    "            vector_store = InMemoryVectorStore(embedding)\n",
    "\n",
    "            # 更新进度条\n",
    "            total_nodes_processed += 1\n",
    "            pbar.update(1)\n",
    "\n",
    "    # Step 3：后处理，打印、存储\n",
    "    print(\"\\n溯源树构建完成！最终结构如下：\")\n",
    "    print_tree(root_node)\n",
    "\n",
    "    # 存储\n",
    "    save_tree_to_json(root_node, output_path)\n",
    "    print(f\"溯源树已保存为 JSON 文件: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding\n",
    "from langchain_community.embeddings import DashScopeEmbeddings\n",
    "# a simple, in memory vector store\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "\n",
    "parse_llm=ParseLLM(model_name=\"qwen-turbo\",api_key=dashscope_api_key)\n",
    "\n",
    "embeddings = DashScopeEmbeddings(\n",
    "    model=\"text-embedding-v3\", dashscope_api_key=dashscope_api_key\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 进行测试\n",
    "parse_and_build_trace_tree(\n",
    "    root_paper_title=\"LightRAG\",\n",
    "    max_layers=3,\n",
    "    parse_llm=parse_llm,\n",
    "    embedding=embeddings,\n",
    "    output_path=\"output/trace_tree.json\",\n",
    "    top_k=3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.绘制溯源树\n",
    "尽量进行美化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
