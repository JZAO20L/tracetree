{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 溯源树构建\n",
    "1. 使用LLM解析论文，处理成节点；节点belike：论文，论文信息（题目，时间，tag/主题），引用论文中和主题相关的论文\n",
    "2. 使用节点进行建树，根=选择一篇作为核心，根据其引用论文进行溯源，类似beam search，每次溯源到一个节点，选择其topk相关的引用节点，从根节点开始迭代进行n次，形成n层的检索树\n",
    "3. 输出为树结构，打印  \n",
    "\n",
    "基于本地处理，整体过程可以用检索实现，收集到的论文作为知识库，处理节点时，可以在知识库中进行检索，找到的topk作为当前节点的溯源节点；  \n",
    "知识库中的每条文档只存放论文、时间、摘要、相关引用即可，正文可以省略；  \n",
    "所以可以分为基于引用关系的溯源，和基于时间&主题的溯源，前者是按照上面说的建立引用树，后者是利用知识库+检索找出主题相关且更早的工作；  \n",
    "可以考虑二者混合，形成混合检索系统"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "获取apikey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get Dashscope key\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# 加载 .env 文件\n",
    "load_dotenv()\n",
    "\n",
    "# 尝试获取环境变量并打印出来\n",
    "dashscope_api_key = os.getenv(\"DASHSCOPE_API_KEY\")\n",
    "# print(f\"DASHSCOPE_API_KEY: {dashscope_api_key}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.论文解析\n",
    "解析论文，处理成dict，每个对象中包括：{\"id\",\"name\",\"publish_time（处理成一个数如20250311）\",\"abstract\"}  \n",
    "遍历文件夹articles中的论文pdf，顺序设置id，使用pdf解析器，解析出name、publish_time、abstract,   \n",
    "//决定还是不加key_words了，后面直接让LLM对abstract进行比较，否则两次使用LLM，精度不好保证   \n",
    "将得到的dict存储成json文件，方便后续处理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Collecting PyPDF2\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/8e/5e/c86a5643653825d3c913719e788e41386bee415c2b87b4f955432f2de6b2/pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
      "Installing collected packages: PyPDF2\n",
      "Successfully installed PyPDF2-3.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# pip install PyPDF2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "尝试使用字符串解析，结果不太行；  \n",
    "还是使用LLM进行提取吧；  \n",
    "测试了一下网页工具上是可以直接提取时间的，那就还是按原来的计划提取时间+题目+摘要；  \n",
    "以下是测试代码"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对文件夹中的论文进行批量处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Tongyi\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "class ParseLLM:\n",
    "    def __init__(self,model_name,key):\n",
    "        self.llm=Tongyi(model=model_name,api_key=key)\n",
    "        \n",
    "        # 解析摘要，对应建树方法1\n",
    "        # self.parse_prompt_template=PromptTemplate(template=\"\"\"\n",
    "        #     <document>{document}</document>\n",
    "        #         Extract the following information from the document above and return it in JSON format:\n",
    "        #         - Title of the paper\n",
    "        #         - publication date, use'%Y-%m-%d' fromat\n",
    "        #         - Abstract\n",
    "                \n",
    "        #         Output example:\n",
    "        #         {{\n",
    "        #             \"title\": \"The title of the paper\",\n",
    "        #             \"publication_date\": \"2025-03-13\",\n",
    "        #             \"abstract\": \"The abstract of the paper\"\n",
    "        #         }}\n",
    "        #     \"\"\")\n",
    "        \n",
    "        # 解析引用，对应建树方法2\n",
    "        # self.parse_prompt_template = PromptTemplate(template=\"\"\"\n",
    "        #     <document>{document}</document>\n",
    "            \n",
    "        #     Extract the following information from the document above and return it in JSON format:\n",
    "        #     - Title of the paper\n",
    "        #     - Publication date (use '%Y-%m-%d' format)\n",
    "        #     - References: List of titles from the \"References\" section (exact strings as they appear)\n",
    "\n",
    "        #     Output example:\n",
    "        #     {{\n",
    "        #         \"title\": \"Attention Is All You Need\",\n",
    "        #         \"publication_date\": \"2017-12-01\",\n",
    "        #         \"references\": [\n",
    "        #             \"Neural Machine Translation by Jointly Learning to Align and Translate\",\n",
    "        #             \"ImageNet Classification with Deep Convolutional Neural Networks\"\n",
    "        #         ]\n",
    "        #     }}\n",
    "        # \"\"\")\n",
    "        \n",
    "        # 解析类别，对应建树方法3\n",
    "        self.parse_prompt_template = PromptTemplate(template=\"\"\"\n",
    "            <document>{document}</document>\n",
    "            \n",
    "            Extract the following information from the document above and return it in JSON format:\n",
    "            - Title of the paper\n",
    "            - Publication date, use '%Y-%m-%d' format\n",
    "            - Tags of the paper: Choose one or more of the following tags based on the abstract. If multiple tags apply, include all relevant ones:\n",
    "                * \"jailbreak\": Papers discussing methods to bypass the safety mechanisms of language models [[1]].\n",
    "                * \"alignment\": Papers focused on ensuring language models behave ethically and align with human values [[2]].\n",
    "                * \"hallucination\": Papers addressing issues where language models generate incorrect or nonsensical content [[3]].\n",
    "                * \"prompt-injection\": Papers exploring techniques to manipulate language models through crafted inputs [[4]].\n",
    "            \n",
    "            Output example:\n",
    "            {{\n",
    "                \"title\": \"The title of the paper\",\n",
    "                \"publication_date\": \"2025-03-13\",\n",
    "                \"tags\": [\"alignment\", \"hallucination\"]  // Can be one or more tags\n",
    "            }}\n",
    "        \"\"\")\n",
    "        \n",
    "    \n",
    "    def response(self,prompt):\n",
    "        response=self.llm.invoke(prompt)\n",
    "        return response\n",
    "    \n",
    "    def parse_paper(self,paper):\n",
    "        prompt=self.parse_prompt_template.format(document=paper)\n",
    "        response=self.response(prompt)\n",
    "        return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from PyPDF2 import PdfReader\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Step 1: 读取指定文件夹中的所有 PDF 文件内容\n",
    "def read_papers_from_folder(folder_path):\n",
    "    papers = []\n",
    "    \n",
    "    # 获取所有 PDF 文件路径\n",
    "    pdf_files = [filename for filename in os.listdir(folder_path) if filename.endswith(\".pdf\")]\n",
    "    \n",
    "    # 使用 tqdm 包裹 pdf_files 列表，显示进度条\n",
    "    for filename in tqdm(pdf_files, desc=\"Reading PDFs\", unit=\"file\"):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        try:\n",
    "            reader = PdfReader(file_path)\n",
    "            paper_content = \"\"\n",
    "            \n",
    "            # 遍历每一页并提取文本\n",
    "            for page in reader.pages:\n",
    "                paper_content += page.extract_text()\n",
    "            \n",
    "            papers.append(paper_content)\n",
    "        \n",
    "        except Exception as e:\n",
    "            # 捕获异常并打印错误信息\n",
    "            tqdm.write(f\"Error reading {filename}: {e}\")  # 使用 tqdm.write 避免干扰进度条\n",
    "    \n",
    "    return papers\n",
    "\n",
    "# 辅助函数：去掉多余字符，将输出转为json格式\n",
    "def parse_to_json(response):\n",
    "    \"\"\"\n",
    "    解析模型返回的带有 Markdown 格式代码块的 JSON 字符串。\n",
    "    \n",
    "    :param response: 模型返回的字符串，例如：\n",
    "                     ```json\\n{\\n  \"论文题目\": \"INJEC AGENT...\",\\n  \"发布时间\": \"2024-08-04\",\\n  \"摘要\": \"Recent work...\"\\n}\\n```\n",
    "    :return: 解析后的 Python 字典\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Step 1: 去掉 Markdown 格式的代码块标记\n",
    "        if response.startswith(\"```json\") and response.endswith(\"```\"):\n",
    "            # 去掉开头的 ```json 和结尾的 ```\n",
    "            response = response[len(\"```json\"):-len(\"```\")].strip()\n",
    "        \n",
    "        # Step 2: 将剩余的字符串解析为 JSON\n",
    "        parsed_data = json.loads(response)\n",
    "        return parsed_data\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"JSON 解析失败: {e}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"发生错误: {e}\")\n",
    "        return None\n",
    "    \n",
    "# Step 2: 使用 ParseLLM 处理每篇论文，生成解析后的信息列表\n",
    "def process_papers_with_parse_llm(papers, model_name, api_key=None):\n",
    "    parse_llm = ParseLLM(model_name=model_name, key=api_key)\n",
    "    paper_infos = []\n",
    "    for i, paper in enumerate(tqdm(papers,desc=\"Processing papers\", unit=\"paper\")):\n",
    "        # print(f\"Processing paper {i + 1}/{len(papers)}...\")\n",
    "        try:\n",
    "            response = parse_llm.parse_paper(paper)\n",
    "            parsed_info = parse_to_json(response)  # 将 LLM 的响应解析为字典\n",
    "            parsed_info_with_index={\"index\": i + 1, **parsed_info}\n",
    "            paper_infos.append(parsed_info_with_index)\n",
    "        except Exception as e:\n",
    "            tqdm.write(f\"Error parsing paper {i + 1}: {e}\") \n",
    "    return paper_infos\n",
    "\n",
    "# Step 3: 将解析后的信息写入 JSON 文件\n",
    "def save_paper_infos_to_json(paper_infos, output_file_path):\n",
    "    with open(output_file_path, 'w') as f:\n",
    "        json.dump(paper_infos, f, ensure_ascii=False, indent=4)\n",
    "    print(f\"Paper infos have been saved to {output_file_path}\")\n",
    "\n",
    "# 解析\n",
    "def parse(folder_path, output_file_path, model_name, api_key=None):\n",
    "    # Step 1: 读取 PDF 文件\n",
    "    papers = read_papers_from_folder(folder_path)\n",
    "    if not papers:\n",
    "        print(\"No PDF files found in the specified folder.\")\n",
    "        return\n",
    "\n",
    "    # Step 2: 使用 ParseLLM 解析论文\n",
    "    paper_infos = process_papers_with_parse_llm(papers, model_name, api_key)\n",
    "\n",
    "    # Step 3: 将解析结果写入 JSON 文件\n",
    "    save_paper_infos_to_json(paper_infos, output_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 配置参数\n",
    "folder_path = \"../data\"  # 替换为你的 PDF 文件夹路径\n",
    "# folder_path=\"../EDK/Agent.Data/PDF\"\n",
    "output_file_path = \"output/paper_infos5.json\"  # 替换为你想保存的 JSON 文件路径"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以分步执行子函数，或者直接执行封装的parse函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading PDFs:  30%|███       | 112/369 [01:41<03:38,  1.17file/s]FloatObject (b'0.000000000000-14210855') invalid; use 0.0 instead\n",
      "Reading PDFs: 100%|██████████| 369/369 [06:32<00:00,  1.06s/file]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'SafeAgentBench: A Benchmark for Safe Task Planning of\\nEmbodied LLM Agents\\nSheng Yin1*Xianghe Pang1*Yuanzhuo Ding1Menglan Chen1\\nYutong Bi1Yichen Xiong1Wenhao Huang1\\nZhen Xiang2Jing Shao3Siheng Chen1†\\n1Shanghai Jiao Tong University2University of Georgia3Shanghai AI Laboratory\\n1{Yin.sheng011224,xianghep,ssansjhicvc,vevive,biyutong\\ncc_eason,1579515851,sihengc}@sjtu.edu.cn;2zhen.xiang.lance@gmail.com;\\n3shaojing@pjlab.org.cn\\nAbstract\\nWith the integration of large language models (LLMs), em-\\nbodied age'"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papers=read_papers_from_folder(folder_path)\n",
    "papers[0][:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'```json\\n{\\n    \"title\": \"SafeAgentBench: A Benchmark for Safe Task Planning of Embodied LLM Agents\",\\n    \"publication_date\": \"2025-03-10\",\\n    \"tags\": [\"alignment\"]\\n}\\n```'"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse_llm = ParseLLM(\"qwen-turbo\", dashscope_api_key)\n",
    "response=parse_llm.parse_paper(papers[0])\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing papers:  33%|███▎      | 121/369 [07:09<14:53,  3.60s/paper]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error parsing paper 121: status_code: 400 \n",
      " code: DataInspectionFailed \n",
      " message: Input data may contain inappropriate content.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing papers:  38%|███▊      | 139/369 [08:11<12:41,  3.31s/paper]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error parsing paper 139: status_code: 400 \n",
      " code: DataInspectionFailed \n",
      " message: Input data may contain inappropriate content.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing papers:  40%|███▉      | 146/369 [08:35<13:11,  3.55s/paper]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error parsing paper 146: status_code: 400 \n",
      " code: DataInspectionFailed \n",
      " message: Input data may contain inappropriate content.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing papers:  58%|█████▊    | 213/369 [13:37<09:06,  3.50s/paper]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error parsing paper 213: status_code: 400 \n",
      " code: DataInspectionFailed \n",
      " message: Input data may contain inappropriate content.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing papers:  60%|█████▉    | 220/369 [14:01<08:32,  3.44s/paper]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error parsing paper 220: status_code: 400 \n",
      " code: DataInspectionFailed \n",
      " message: Input data may contain inappropriate content.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing papers:  71%|███████   | 262/369 [16:48<05:58,  3.35s/paper]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error parsing paper 262: status_code: 400 \n",
      " code: DataInspectionFailed \n",
      " message: Input data may contain inappropriate content.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing papers:  73%|███████▎  | 270/369 [17:16<05:20,  3.23s/paper]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error parsing paper 270: status_code: 400 \n",
      " code: DataInspectionFailed \n",
      " message: Input data may contain inappropriate content.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing papers:  75%|███████▍  | 276/369 [17:42<05:57,  3.84s/paper]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error parsing paper 276: status_code: 400 \n",
      " code: DataInspectionFailed \n",
      " message: Input data may contain inappropriate content.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing papers:  75%|███████▌  | 277/369 [17:48<06:55,  4.52s/paper]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error parsing paper 277: status_code: 400 \n",
      " code: DataInspectionFailed \n",
      " message: Input data may contain inappropriate content.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing papers:  77%|███████▋  | 283/369 [18:12<05:06,  3.56s/paper]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error parsing paper 283: status_code: 400 \n",
      " code: DataInspectionFailed \n",
      " message: Input data may contain inappropriate content.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing papers:  78%|███████▊  | 287/369 [18:26<04:32,  3.33s/paper]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error parsing paper 287: status_code: 400 \n",
      " code: DataInspectionFailed \n",
      " message: Input data may contain inappropriate content.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing papers:  86%|████████▌ | 317/369 [20:16<03:08,  3.62s/paper]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error parsing paper 317: status_code: 400 \n",
      " code: DataInspectionFailed \n",
      " message: Input data may contain inappropriate content.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing papers:  93%|█████████▎| 342/369 [21:45<01:36,  3.58s/paper]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error parsing paper 342: status_code: 400 \n",
      " code: DataInspectionFailed \n",
      " message: Input data may contain inappropriate content.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing papers:  95%|█████████▌| 352/369 [22:21<00:56,  3.31s/paper]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON 解析失败: Expecting value: line 1 column 1 (char 0)\n",
      "Error parsing paper 352: 'NoneType' object is not a mapping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing papers: 100%|██████████| 369/369 [23:19<00:00,  3.79s/paper]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'index': 1,\n",
       " 'title': 'SafeAgentBench: A Benchmark for Safe Task Planning of Embodied LLM Agents',\n",
       " 'publication_date': '2025-03-10',\n",
       " 'tags': ['alignment']}"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papers_infos=process_papers_with_parse_llm(papers,\"qwen-turbo\",dashscope_api_key)\n",
    "papers_infos[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paper infos have been saved to output/paper_infos5.json\n"
     ]
    }
   ],
   "source": [
    "save_paper_infos_to_json(papers_infos, output_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "执行解析函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse(folder_path, output_file_path, \"qwen-turbo\", dashscope_api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.建树\n",
    "使用anytree，可以在建树完成后直接画出树状图；   \n",
    "每次建树时，从前面建立的论文库中选取publish time更早且keywords有重叠的；\n",
    "\n",
    "build_treace_tree(json_file_path,layers)\n",
    "1. 读取json_file_path指定的json文件，将论文信息读入列表；\n",
    "2. 选取一个论文作为根结点，这里选时间最晚的、最新的论文；\n",
    "3. 执行以下操作：对于每个待处理节点，如果存在时间更早且主题相关的论文，则将相关且更早的论文作为anytree中的子节点，并且添加到待处理节点中；重复以上操作直到树的层数达到layers\n",
    "\n",
    "方法局限性：复杂度n方，并且调用大模型次数过多——当论文数量很多时，建树还是应该交给更简单的算法，比如：  \n",
    "1. 基于摘要的嵌入向量相似度；\n",
    "2. 在解析阶段，对文章进行分类，建树阶段基于类别进行溯源；\n",
    "\n",
    "最后还是使用基于引用关系进行建树了——所以绕一大圈是为了什么……（证明了其他方法的不可行性）  \n",
    "基本流程和初版一致，把is_related改为判断是否在引用文献中就行；  \n",
    "可能会导致建的树也很稀疏  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.0辅助函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 下载anytree\n",
    "# pip install anytree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将所有键转为小写——当LLM指令遵循做的比较差、出现不符合要求的输出时可能需要使用；"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def convert_keys_to_lowercase(data):\n",
    "    \"\"\"\n",
    "    递归地将 JSON 数据中的所有键转换为小写。\n",
    "    \n",
    "    参数:\n",
    "        data: JSON 数据（可以是字典、列表或其他类型）。\n",
    "        \n",
    "    返回:\n",
    "        转换后的数据，其中所有字典的键都变为小写。\n",
    "    \"\"\"\n",
    "    if isinstance(data, dict):  # 如果是字典\n",
    "        return {k.lower(): convert_keys_to_lowercase(v) for k, v in data.items()}\n",
    "    elif isinstance(data, list):  # 如果是列表\n",
    "        return [convert_keys_to_lowercase(item) for item in data]\n",
    "    else:  # 其他类型（如字符串、数字等），直接返回\n",
    "        return data\n",
    "\n",
    "def overwrite_json_file(file_path):\n",
    "    \"\"\"\n",
    "    读取 JSON 文件，将所有键转换为小写，并直接覆盖原文件。\n",
    "    \n",
    "    参数:\n",
    "        file_path: JSON 文件的路径。\n",
    "    \"\"\"\n",
    "    # Step 1: 读取 JSON 文件\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    # Step 2: 转换所有键为小写\n",
    "    converted_data = convert_keys_to_lowercase(data)\n",
    "    \n",
    "    # Step 3: 将转换后的数据写回原文件\n",
    "    with open(file_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(converted_data, f, ensure_ascii=False, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "处理完成！已将所有键转换为小写并覆盖原文件。\n"
     ]
    }
   ],
   "source": [
    "info_path = \"output/paper_infos3.json\"  # 输入文件路径\n",
    "\n",
    "overwrite_json_file(info_path)\n",
    "print(\"处理完成！已将所有键转换为小写并覆盖原文件。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 打印树结构\n",
    "def print_tree(root_node):\n",
    "    for pre, fill, node in RenderTree(root_node):\n",
    "        print(f\"{pre}{node.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 基于摘要相关性建树"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Tongyi\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "class JudgeLLM:\n",
    "    def __init__(self,model_name,key):\n",
    "        self.llm=Tongyi(model=model_name,api_key=key)\n",
    "        self.prompt_template=PromptTemplate(template=\"\"\"\n",
    "            You are an expert academic assistant tasked with determining whether two research papers are strongly related. Below are the details of the task:\n",
    "\n",
    "            1. Input the abstracts of two research papers:\n",
    "            - Abstract 1: {abstract1}\n",
    "            - Abstract 2: {abstract2}\n",
    "\n",
    "            2. Evaluation Criteria:\n",
    "            - The two papers must address the same topic.\n",
    "            - The two papers must share multiple keywords (e.g., research methods, technical terms, core concepts, etc.).\n",
    "\n",
    "            3. Output:\n",
    "            - If the two papers are strongly related, return `True`.\n",
    "            - If the two papers are not strongly related, return `False`.\n",
    "\n",
    "            Carefully analyze the content of the two abstracts, extract their themes and keywords, and draw a conclusion based on the criteria above.\n",
    "            \"\"\")\n",
    "    \n",
    "    def response(self,prompt):\n",
    "        response=self.llm.invoke(prompt)\n",
    "        return response\n",
    "    \n",
    "    def is_related(self,abstract1,abstract2):\n",
    "        prompt=self.prompt_template.format(abstract1=abstract1,abstract2=abstract2)\n",
    "        response=self.response(prompt)\n",
    "        if \"True\" in response:\n",
    "            return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from anytree import Node, RenderTree\n",
    "from datetime import datetime\n",
    "\n",
    "# 使用LLM判断相关性进行建树\n",
    "def build_relate_trace_tree(json_file_path, layers, model_name, api_key, root_num=None):\n",
    "    \"\"\"\n",
    "    构建论文的追溯树。\n",
    "    \n",
    "    :param json_file_path: JSON 文件路径，包含论文信息\n",
    "    :param layers: 树的最大层数\n",
    "    :param model_name: 使用的模型名称\n",
    "    :param api_key: API 密钥\n",
    "    :return: 根节点（anytree.Node）\n",
    "    \"\"\"\n",
    "    # 初始化 JudgeLLM\n",
    "    judge_llm = JudgeLLM(model_name, api_key)\n",
    "    \n",
    "    # Step 1: 读取 JSON 文件，将论文信息读入列表\n",
    "    with open(json_file_path, 'r', encoding='utf-8') as f:\n",
    "        papers = json.load(f)\n",
    "\n",
    "    # Step 2: 建根节点\n",
    "    papers = [paper for paper in papers if \"title\" in paper and \"publication_date\" in paper and \"references\" in paper]\n",
    "    papers.sort(key=lambda x: datetime.strptime(x[\"publication_date\"], \"%Y-%m-%d\"), reverse=True)\n",
    "    root_paper = papers[root_num]\n",
    "    root_node = Node(root_paper[\"title\"], data=root_paper)\n",
    "    \n",
    "    # 全局计数器\n",
    "    progress_counter = 0\n",
    "    total_papers = len(papers) - 1  # 总任务量（去掉根节点）\n",
    "\n",
    "\n",
    "    # Step 3: 构建追溯树\n",
    "    def add_children(parent_node, remaining_papers, current_layer):\n",
    "        nonlocal progress_counter  # 使用非局部变量更新计数器\n",
    "        \n",
    "        if current_layer >= layers or not remaining_papers:\n",
    "            return\n",
    "\n",
    "        parent_paper = parent_node.data\n",
    "        parent_title = parent_paper[\"title\"]\n",
    "        parent_date = datetime.strptime(parent_paper[\"publication_date\"], \"%Y-%m-%d\")\n",
    "\n",
    "        # 遍历剩余论文的副本\n",
    "        for paper in remaining_papers:\n",
    "            paper_date = datetime.strptime(paper[\"publication_date\"], \"%Y-%m-%d\")\n",
    "            paper_title = paper[\"title\"]\n",
    "\n",
    "            # 检查是否满足条件：时间更早且主题相关\n",
    "            if (paper_date < parent_date and \n",
    "                judge_llm.is_related(parent_paper[\"abstract\"], paper[\"abstract\"])):\n",
    "                \n",
    "                # 创建子节点\n",
    "                child_node = Node(paper_title, parent=parent_node, data=paper)\n",
    "                \n",
    "                # 更新计数器和进度条\n",
    "                progress_counter += 1\n",
    "                print(f\"{progress_counter}/{total_papers} papers processed.\")\n",
    "                \n",
    "                # 从 remaining_papers 中移除已添加的论文\n",
    "                remaining_papers.remove(paper)\n",
    "                \n",
    "                # 递归添加子节点\n",
    "                add_children(child_node, remaining_papers, current_layer + 1)\n",
    "\n",
    "    # Step 4: 开始构建树\n",
    "    papers.remove(root_paper) \n",
    "    remaining_papers = papers # 剩余论文（去掉根节点）\n",
    "    add_children(root_node, remaining_papers, current_layer=1)\n",
    "\n",
    "    # Step 5: 返回根节点\n",
    "    return root_node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 基于引用关系建树"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Collecting fuzzywuzzy\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/43/ff/74f23998ad2f93b945c0309f825be92e04e0348e062026998b5eefef4c33/fuzzywuzzy-0.18.0-py2.py3-none-any.whl (18 kB)\n",
      "Installing collected packages: fuzzywuzzy\n",
      "Successfully installed fuzzywuzzy-0.18.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# pip install fuzzywuzzy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "from anytree import Node, RenderTree\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from datetime import datetime\n",
    "from fuzzywuzzy import fuzz\n",
    "from collections import deque\n",
    "\n",
    "def build_refer_trace_tree(info_file_path, layers, root_num=None):\n",
    "    \"\"\"\n",
    "    构建论文的追溯树。\n",
    "    \n",
    "    :param info_file_path: JSON 文件路径，包含论文信息\n",
    "    :param layers: 树的最大层数\n",
    "    :param root_num: 根节点的序号，默认为 None，越小说明文章越新\n",
    "    :return: 根节点（anytree.Node）\n",
    "    \"\"\"\n",
    "    # Step 1: 读取 JSON 文件，将论文信息读入列表\n",
    "    with open(info_file_path, 'r', encoding='utf-8') as f:\n",
    "        papers = json.load(f)\n",
    "\n",
    "    # Step 2: 建根节点\n",
    "    papers = [paper for paper in papers if \"title\" in paper and \"publication_date\" in paper and \"references\" in paper]\n",
    "    papers.sort(key=lambda x: datetime.strptime(x[\"publication_date\"], \"%Y-%m-%d\"), reverse=True)\n",
    "    root_paper = papers[root_num]\n",
    "    root_node = Node(root_paper[\"title\"], data=root_paper)\n",
    "\n",
    "    # 全局计数器\n",
    "    progress_counter = 0\n",
    "    total_papers = len(papers) - 1  # 总任务量（去掉根节点）\n",
    "\n",
    "    # 辅助函数，检查引用关系\n",
    "    def is_refered(paper1, paper2):\n",
    "        if \"references\" not in paper1 or not paper1[\"references\"]:\n",
    "            return False\n",
    "        for ref_title in paper1[\"references\"]:\n",
    "            if fuzz.ratio(ref_title.lower(), paper2[\"title\"].lower()) >= 90:\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "    # Step 3: 构建追溯树\n",
    "    # def add_children(parent_node, remaining_papers, current_layer):\n",
    "    #     nonlocal progress_counter  # 使用非局部变量更新计数器\n",
    "\n",
    "    #     if current_layer > layers or not remaining_papers:\n",
    "    #         return\n",
    "\n",
    "    #     parent_paper = parent_node.data\n",
    "    #     parent_title = parent_paper[\"title\"]\n",
    "\n",
    "    #     # 遍历剩余论文\n",
    "    #     for paper in remaining_papers:  \n",
    "    #         paper_title = paper[\"title\"]\n",
    "\n",
    "    #         # 检查是否满足条件：有引用关系\n",
    "    #         if is_refered(parent_paper, paper):\n",
    "    #             # 创建子节点\n",
    "    #             child_node = Node(paper_title, parent=parent_node, data=paper)\n",
    "                \n",
    "    #             # 更新计数器和进度条\n",
    "    #             progress_counter += 1\n",
    "    #             print(f\"{progress_counter}/{total_papers} papers processed.\")\n",
    "                \n",
    "    #             # 从剩余论文中移除已添加的论文\n",
    "    #             remaining_papers.remove(paper)\n",
    "                \n",
    "    #             # 递归添加子节点\n",
    "    #             add_children(child_node, remaining_papers, current_layer + 1)\n",
    "\n",
    "\n",
    "    # Step 3: 构建追溯树（广度优先搜索）\n",
    "    def add_children(root_node, remaining_papers, layers):\n",
    "        # 初始化计数器和队列\n",
    "        progress_counter = 0\n",
    "        total_papers = len(remaining_papers)\n",
    "        queue = deque([(root_node, 1)])  # 队列元素为 (当前节点, 当前层数)\n",
    "\n",
    "        while queue:\n",
    "            parent_node, current_layer = queue.popleft()  # 取出队列头部的节点\n",
    "            if current_layer > layers:  # 如果超过最大层数，停止扩展\n",
    "                continue\n",
    "\n",
    "            parent_paper = parent_node.data\n",
    "            parent_title = parent_paper[\"title\"]\n",
    "\n",
    "            # 遍历剩余论文\n",
    "            for paper in remaining_papers:\n",
    "                paper_title = paper[\"title\"]\n",
    "\n",
    "                # 检查是否满足条件：有引用关系\n",
    "                if is_refered(parent_paper, paper):\n",
    "                    # 创建子节点\n",
    "                    child_node = Node(paper_title, parent=parent_node, data=paper)\n",
    "\n",
    "                    # 更新计数器并打印进度\n",
    "                    progress_counter += 1\n",
    "                    print(f\"{progress_counter}/{total_papers} papers processed.\")\n",
    "\n",
    "                    # 从剩余论文中移除已添加的论文\n",
    "                    remaining_papers.remove(paper)\n",
    "\n",
    "                    # 将子节点加入队列，准备后续扩展\n",
    "                    queue.append((child_node, current_layer + 1))\n",
    "\n",
    "    # Step 4: 开始构建树\n",
    "    papers.remove(root_paper)\n",
    "    remaining_papers = papers\n",
    "    # print(remaining_papers)\n",
    "    add_children(root_node, remaining_papers, layers)\n",
    "\n",
    "    # Step 5: 返回根节点\n",
    "    return root_node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from anytree import Node, RenderTree\n",
    "from datetime import datetime\n",
    "\n",
    "# 根据论文tag进行建树\n",
    "def build_tag_trace_tree(info_file_path, layers, root_index=None):\n",
    "    \"\"\"\n",
    "    构建论文的追溯树。\n",
    "    \n",
    "    :param info_file_path: JSON 文件路径，包含论文信息\n",
    "    :param layers: 树的最大层数\n",
    "    :param root_index: 根节点的索引，默认为 None\n",
    "    :return: 根节点（anytree.Node）\n",
    "    \"\"\"\n",
    "    # Step 1: 读取 JSON 文件，将论文信息读入列表\n",
    "    with open(info_file_path, 'r', encoding='utf-8') as f:\n",
    "        papers = json.load(f)\n",
    "\n",
    "    # Step 2: 建根节点\n",
    "    if root_index is not None:\n",
    "        root_paper = papers[root_index]\n",
    "    else:\n",
    "        # 过滤掉缺少必要字段的论文\n",
    "        papers = [paper for paper in papers if \"title\" in paper and \"publication_date\" in paper and \"references\" in paper]\n",
    "        papers.sort(key=lambda x: datetime.strptime(x[\"publication_date\"], \"%Y-%m-%d\"), reverse=True)\n",
    "        root_paper = papers[0]  \n",
    "        \n",
    "    root_node = Node(root_paper[\"title\"], data=root_paper)\n",
    "\n",
    "    # 辅助函数，检查引用关系\n",
    "    def has_same_tag(paper1, paper2):\n",
    "         # 确保两篇论文都有 tags 字段\n",
    "        if \"tags\" not in paper1 or \"tags\" not in paper2:\n",
    "            return False\n",
    "\n",
    "        # 将 tags 转换为集合\n",
    "        tags1 = set(paper1[\"tags\"])\n",
    "        tags2 = set(paper2[\"tags\"])\n",
    "\n",
    "        # 求交集并判断是否非空\n",
    "        common_tags = tags1 & tags2  # 或者使用 tags1.intersection(tags2)\n",
    "        return bool(common_tags)  # 如果交集非空，返回 True；否则返回 False\n",
    "    \n",
    "    # Step 3: 构建追溯树（广度优先搜索）\n",
    "    def add_children(root_node, remaining_papers, layers):\n",
    "        # 初始化计数器和队列\n",
    "        progress_counter = 0\n",
    "        total_papers = len(remaining_papers)\n",
    "        queue = deque([(root_node, 1)])  # 队列元素为 (当前节点, 当前层数)\n",
    "\n",
    "        while queue:\n",
    "            parent_node, current_layer = queue.popleft()  # 取出队列头部的节点\n",
    "            if current_layer > layers:  # 如果超过最大层数，停止扩展\n",
    "                continue\n",
    "\n",
    "            parent_paper = parent_node.data\n",
    "            parent_title = parent_paper[\"title\"]\n",
    "            parent_date = datetime.strptime(parent_paper[\"publication_date\"], \"%Y-%m-%d\")\n",
    "\n",
    "            # 遍历剩余论文\n",
    "            for paper in remaining_papers: \n",
    "                paper_title = paper[\"title\"]\n",
    "                paper_date = datetime.strptime(paper[\"publication_date\"], \"%Y-%m-%d\")\n",
    "\n",
    "                # 检查是否满足条件：有引用关系\n",
    "                if has_same_tag(parent_paper, paper) and parent_date > paper_date:\n",
    "                    # 创建子节点\n",
    "                    child_node = Node(paper_title, parent=parent_node, data=paper)\n",
    "\n",
    "                    # 更新计数器并打印进度\n",
    "                    progress_counter += 1\n",
    "                    print(f\"{progress_counter}/{total_papers} papers processed.\")\n",
    "\n",
    "                    # 从剩余论文中移除已添加的论文\n",
    "                    remaining_papers.remove(paper)\n",
    "\n",
    "                    # 将子节点加入队列，准备后续扩展\n",
    "                    queue.append((child_node, current_layer + 1))\n",
    "\n",
    "    # Step 4: 开始构建树\n",
    "    papers.remove(root_paper) \n",
    "    remaining_papers = papers # 剩余论文（去掉根节点）\n",
    "    add_children(root_node, remaining_papers, layers)\n",
    "\n",
    "    # Step 5: 返回根节点\n",
    "    return root_node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 执行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/354 papers processed.\n",
      "2/354 papers processed.\n",
      "3/354 papers processed.\n",
      "4/354 papers processed.\n",
      "5/354 papers processed.\n",
      "6/354 papers processed.\n",
      "7/354 papers processed.\n",
      "8/354 papers processed.\n",
      "9/354 papers processed.\n",
      "10/354 papers processed.\n",
      "11/354 papers processed.\n",
      "12/354 papers processed.\n",
      "13/354 papers processed.\n",
      "14/354 papers processed.\n",
      "15/354 papers processed.\n",
      "16/354 papers processed.\n",
      "17/354 papers processed.\n",
      "18/354 papers processed.\n",
      "19/354 papers processed.\n",
      "20/354 papers processed.\n",
      "21/354 papers processed.\n",
      "22/354 papers processed.\n",
      "23/354 papers processed.\n",
      "24/354 papers processed.\n",
      "25/354 papers processed.\n",
      "26/354 papers processed.\n",
      "27/354 papers processed.\n",
      "28/354 papers processed.\n",
      "29/354 papers processed.\n",
      "30/354 papers processed.\n",
      "31/354 papers processed.\n",
      "32/354 papers processed.\n",
      "33/354 papers processed.\n",
      "34/354 papers processed.\n",
      "35/354 papers processed.\n",
      "36/354 papers processed.\n",
      "37/354 papers processed.\n",
      "38/354 papers processed.\n",
      "39/354 papers processed.\n",
      "40/354 papers processed.\n",
      "41/354 papers processed.\n",
      "42/354 papers processed.\n",
      "43/354 papers processed.\n",
      "44/354 papers processed.\n",
      "45/354 papers processed.\n",
      "46/354 papers processed.\n",
      "47/354 papers processed.\n",
      "48/354 papers processed.\n",
      "49/354 papers processed.\n",
      "50/354 papers processed.\n",
      "51/354 papers processed.\n",
      "52/354 papers processed.\n",
      "53/354 papers processed.\n",
      "54/354 papers processed.\n",
      "55/354 papers processed.\n",
      "56/354 papers processed.\n",
      "57/354 papers processed.\n",
      "58/354 papers processed.\n",
      "59/354 papers processed.\n",
      "60/354 papers processed.\n",
      "61/354 papers processed.\n",
      "62/354 papers processed.\n",
      "63/354 papers processed.\n",
      "64/354 papers processed.\n",
      "65/354 papers processed.\n",
      "66/354 papers processed.\n",
      "67/354 papers processed.\n",
      "68/354 papers processed.\n",
      "69/354 papers processed.\n",
      "70/354 papers processed.\n",
      "71/354 papers processed.\n",
      "72/354 papers processed.\n",
      "73/354 papers processed.\n",
      "74/354 papers processed.\n",
      "75/354 papers processed.\n",
      "76/354 papers processed.\n",
      "77/354 papers processed.\n",
      "78/354 papers processed.\n",
      "79/354 papers processed.\n",
      "80/354 papers processed.\n",
      "81/354 papers processed.\n",
      "82/354 papers processed.\n",
      "83/354 papers processed.\n",
      "84/354 papers processed.\n",
      "85/354 papers processed.\n",
      "86/354 papers processed.\n",
      "87/354 papers processed.\n",
      "88/354 papers processed.\n",
      "89/354 papers processed.\n",
      "90/354 papers processed.\n",
      "91/354 papers processed.\n",
      "92/354 papers processed.\n",
      "93/354 papers processed.\n",
      "94/354 papers processed.\n",
      "95/354 papers processed.\n",
      "96/354 papers processed.\n",
      "97/354 papers processed.\n",
      "98/354 papers processed.\n",
      "99/354 papers processed.\n",
      "100/354 papers processed.\n",
      "101/354 papers processed.\n",
      "102/354 papers processed.\n",
      "103/354 papers processed.\n",
      "104/354 papers processed.\n",
      "105/354 papers processed.\n",
      "106/354 papers processed.\n",
      "107/354 papers processed.\n",
      "108/354 papers processed.\n",
      "109/354 papers processed.\n",
      "110/354 papers processed.\n",
      "111/354 papers processed.\n",
      "112/354 papers processed.\n",
      "113/354 papers processed.\n",
      "114/354 papers processed.\n",
      "115/354 papers processed.\n",
      "116/354 papers processed.\n",
      "117/354 papers processed.\n",
      "118/354 papers processed.\n",
      "119/354 papers processed.\n",
      "120/354 papers processed.\n",
      "121/354 papers processed.\n",
      "122/354 papers processed.\n",
      "123/354 papers processed.\n",
      "124/354 papers processed.\n",
      "125/354 papers processed.\n",
      "126/354 papers processed.\n",
      "127/354 papers processed.\n",
      "128/354 papers processed.\n",
      "129/354 papers processed.\n",
      "130/354 papers processed.\n",
      "131/354 papers processed.\n",
      "132/354 papers processed.\n",
      "133/354 papers processed.\n",
      "134/354 papers processed.\n",
      "135/354 papers processed.\n",
      "136/354 papers processed.\n",
      "137/354 papers processed.\n",
      "138/354 papers processed.\n",
      "139/354 papers processed.\n",
      "140/354 papers processed.\n",
      "141/354 papers processed.\n",
      "142/354 papers processed.\n",
      "143/354 papers processed.\n",
      "144/354 papers processed.\n",
      "145/354 papers processed.\n",
      "146/354 papers processed.\n",
      "147/354 papers processed.\n",
      "148/354 papers processed.\n",
      "149/354 papers processed.\n",
      "150/354 papers processed.\n",
      "151/354 papers processed.\n",
      "152/354 papers processed.\n",
      "153/354 papers processed.\n",
      "154/354 papers processed.\n",
      "155/354 papers processed.\n",
      "156/354 papers processed.\n",
      "157/354 papers processed.\n",
      "158/354 papers processed.\n",
      "159/354 papers processed.\n",
      "A Causal Explainable Guardrails for Large Language Models\n",
      "├── Advancing Adversarial Suffix Transfer Learning on Aligned Large Language Models\n",
      "│   ├── A Jailbroken GenAI Model Can Cause Substantial Harm: GenAI-powered Applications are Vulnerable to PromptWares\n",
      "│   ├── AdaPPA: Adaptive Position Pre-Fill Jailbreak Attack Approach Targeting LLMs\n",
      "│   ├── An Analysis of Recent Advances in Deepfake Image Detection in an Evolving Threat Landscape\n",
      "│   ├── Applying Pre-trained Multilingual BERT in Embeddings for Improved Malicious Prompt Injection Attacks Detection\n",
      "│   ├── Automatic Jailbreaking of the Text-to-Image Generative AI Systems\n",
      "│   ├── Can Large Language Models Automatically Jailbreak GPT-4V?\n",
      "│   ├── Can LLMs Deeply Detect Complex Malicious Queries? A Framework for Jailbreaking via Obfuscating Intent\n",
      "│   ├── Chain of Attack: a Semantic-Driven Contextual Multi-Turn attacker for LLM\n",
      "│   ├── Continuous Embedding Attacks via Clipped Inputs in Jailbreaking Large Language Models\n",
      "│   ├── Dataset and Lessons Learned from the 2024 SaTML LLM Capture-the-Flag Competition\n",
      "│   ├── Emerging Vulnerabilities in Frontier Models: Multi-Turn Jailbreak Attacks\n",
      "│   ├── Empirical Analysis of Large Vision-Language Models against Goal Hijacking via Visual Prompt Injection\n",
      "│   ├── FLIPATTACK: Jailbreak LLMs Via Flipping\n",
      "│   ├── GenTel-Safe: A Unified Benchmark and Shielding Framework for Defending Against Prompt Injection Attacks\n",
      "│   ├── h4rm3l: A Dynamic Benchmark of Composable Jailbreak Attacks for LLM Safety Assessment\n",
      "│   ├── Hacc-Man: An Arcade Game for Jailbreaking LLMs\n",
      "│   ├── Jailbreak Vision Language Models via Bi-Modal Adversarial Prompt\n",
      "│   ├── Jailbreaking Prompt Attack: A Controllable Adversarial Attack against Diffusion Models\n",
      "│   ├── Jailbreaking Text-to-Image Models with LLM-Based Agents\n",
      "│   ├── Knowledge-to-Jailbreak: One Knowledge Point Worth One Attack\n",
      "│   ├── Lockpicking LLMs: A Logit-Based Jailbreak Using Token-level Manipulation\n",
      "│   ├── Medical MLLM is Vulnerable: Cross-Modality Jailbreak and Mismatched Attacks on Medical Multimodal Large Language Models\n",
      "│   ├── MoJE: Mixture of Jailbreak Experts, Naive Tabular Classifiers as Guard for Prompt Attacks\n",
      "│   ├── Poisoned LangChain: Jailbreak LLMs by LangChain\n",
      "│   ├── Trust No AI: Prompt Injection Along The CIA Security Triad\n",
      "│   ├── PROMPT FUZZ: Harnessing Fuzzing Techniques for Robust Testing of Prompt Injection in LLMs\n",
      "│   ├── RedAgent: Red Teaming Large Language Models with Context-aware Autonomous Language Agent\n",
      "│   ├── Systematically Analyzing Prompt Injection Vulnerabilities in Diverse LLM Architectures\n",
      "│   ├── Unleashing Worms and Extracting Data: Escalating the Outcome of Attacks against RAG-based Inference in Scale and Severity Using Jailbreaking\n",
      "│   ├── Unlocking Adversarial Suffix Optimization Without Affirmative Phrases: Efficient Black-box Jailbreaking via LLM as Optimizer\n",
      "│   ├── Virtual Context: Enhancing Jailbreak Attacks with Special Token Injection\n",
      "│   ├── Visual-RolePlay: Universal Jailbreak Attack on MultiModal Large Language Models via Role-playing Image Character\n",
      "│   ├── VLMGuard: Defending VLMs against Malicious Prompts via Unlabeled Data\n",
      "│   ├── Voice Jailbreak Attacks Against GPT-4o\n",
      "│   ├── PromptShield: Deployable Detection for Prompt Injection Attacks\n",
      "│   └── 'Do as I say not as I do': A Semi-Automated Approach for Jailbreak Prompt Attack against Multimodal LLMs\n",
      "├── Adversarial Search Engine Optimization for Large Language Models\n",
      "├── Adversarial Tuning: Defending Against Jailbreak Attacks for LLMs\n",
      "├── Adversaries Can Misuse Combinations of Safe Models\n",
      "├── AdvPrompter: Fast Adaptive Adversarial Prompting for LLMs\n",
      "├── AEGIS: Online Adaptive AI Content Safety Moderation with Ensemble of LLM Experts\n",
      "├── AI Risk Management Should Incorporate Both Safety and Security\n",
      "├── ALERT: A Comprehensive Benchmark for Assessing Large Language Models’ Safety through Red Teaming\n",
      "├── Aligners: Decoupling LLMs and Alignment\n",
      "├── Antidote: Post-fine-tuning Safety Alignment for Large Language Models against Harmful Fine-tuning\n",
      "├── Are AI-Generated Text Detectors Robust to Adversarial Perturbations?\n",
      "├── Are PPO-ed Language Models Hackable?\n",
      "├── ARGS: Alignment as Reward-Guided Search\n",
      "├── Arondight: Red Teaming Large Vision Language Models with Auto-generated Multi-modal Jailbreak Prompts\n",
      "├── A safety realignment framework via subspace-oriented model fusion for large language models\n",
      "├── AutoJailbreak: Exploring Jailbreak Attacks and Defenses through a Dependency Lens\n",
      "├── Badllama 3: removing safety finetuning from Llama 3 in minutes\n",
      "├── BELLS: A Framework Towards Future Proof Benchmarks for the Evaluation of LLM Safeguards\n",
      "├── Benchmarking Llama2, Mistral, Gemma and GPT for Factuality, Toxicity, Bias and Propensity for Hallucinations\n",
      "├── Beyond Imitation: Leveraging Fine-Grained Quality Signals for Alignment\n",
      "├── Beyond Reverse KL: Generalizing Direct Preference Optimization with Diverse Divergence Constraints\n",
      "├── Moralized” Multi-Step Jailbreak Prompts: Black-Box Testing of Guardrails in Large Language Models for Verbal Attacks\n",
      "├── BOOSTER: TACKLING HARMFUL FINE-TUNING FOR LARGE LANGUAGE MODELS VIA ATTENUATING HARMFUL PERTURBATION\n",
      "├── CAN A LARGE LANGUAGE MODEL BE A GASLIGHTER ?\n",
      "│   ├── HARNESSING TASK OVERLOAD FOR SCALABLE JAILBREAK ATTACKS ON LARGE LANGUAGE MODELS\n",
      "│   └── PROMPT INFECTION : LLM- TO-LLM PROMPT INJECTION WITHIN MULTI-AGENT SYSTEMS\n",
      "├── Can Editing LLMs Inject Harm?\n",
      "├── CANLLM-Generated Misinformation Be Detected?\n",
      "├── CAS: A PROBABILITY-BASED APPROACH FOR UNIVERSAL CONDITION ALIGNMENT SCORE\n",
      "├── CHAIN -OF-JAILBREAK ATTACK FOR IMAGE GENERATION MODELS VIA EDITING STEP BY STEP\n",
      "├── Competition Report: Finding Universal Jailbreak Backdoors in Aligned LLMs\n",
      "├── Covert Malicious Finetuning: Challenges in Safeguarding LLM Adaptation\n",
      "├── CPPO: Continual Learning for Reinforcement Learning with Human Feedback\n",
      "├── Cross-Modal Safety Alignment: Is textual unlearning all you need?\n",
      "├── Cross-modality Information Check for Detecting Jailbreaking in Multimodal Large Language Models\n",
      "│   ├── F2A: An Innovative Approach for Prompt Injection by Utilizing Feign Security Detection Agents\n",
      "│   └── GPT-4 Jailbreaks Itself with Near-Perfect Success Using Self-Explanation\n",
      "├── Cross-Task Defense: Instruction-Tuning LLMs for Content Safety\n",
      "├── Decoupled Alignment for Robust Plug-and-Play Adaptation\n",
      "├── Defending Large Language Models Against Jailbreak Attacks via Layer-specific Editing\n",
      "├── Defending LLMs against Jailbreaking Attacks via Backtranslation\n",
      "├── Defensive Prompt Patch: A Robust and Interpretable Defense of LLMs against Jailbreak Attacks\n",
      "├── Derail Yourself: Multi-Turn LLM Jailbreak Attack Through Self-Discovered Clues\n",
      "├── Does Refusal Training in LLMs Generalize to the Past Tense?\n",
      "├── Don’t Say No: Jailbreaking LLM by Suppressing Refusal\n",
      "├── EEG-Defender: Defending against Jailbreak through Early Exit Generation of Large Language Models\n",
      "├── Emerging Safety Attack and Defense in Federated Instruction Tuning of Large Language Models\n",
      "├── Ensemble Jailbreak on Large Language Models\n",
      "├── Eraser: Jailbreaking Defense in Large Language Models via Unlearning Harmful Knowledge\n",
      "├── Event-Radar: Event-driven Multi-View Learning for Multimodal Fake News Detection\n",
      "├── FINE-TUNING ALIGNED LANGUAGE MODELS COMPROMISES SAFETY, EVEN WHEN USERS DO NOT INTEND TO!\n",
      "├── Fine-Tuning, Quantization, and LLMs: Navigating Unintended Outcomes\n",
      "├── FLASK: Fine-Grained Language Model Evaluation Based on Alignment Skill Sets\n",
      "├── Explain LLM Safety through Intermediate Hidden States\n",
      "├── Defending against Jailbreak Attacks with Hidden State Filtering\n",
      "├── Image-to-Text Logic Jailbreak: Your Imagination can Help You Do Anything\n",
      "├── Improved Techniques for Optimization-Based Jailbreaking on Large Language Models\n",
      "├── Improving Alignment and Robustness with Circuit Breakers\n",
      "├── Investigating the Influence of Prompt-Specific Shortcuts in AI Generated Text Detection\n",
      "├── Jailbreak Attacks and Defenses Against Large Language Models: A Survey\n",
      "├── Jailbreak Open-Sourced Large Language Models via Enforced Decoding\n",
      "├── Jailbreak Paradox : The Achilles’ Heel of LLMs\n",
      "├── JAILBREAKING LEADING SAFETY-ALIGNED LLMs WITH SIMPLE ADAPTIVE ATTACKS\n",
      "├── JailbreakLens: Visual Analysis of Jailbreak Attacks Against Large Language Models\n",
      "├── JailbreakZoo: Survey, Landscapes, and Horizons in Jailbreaking Large Language and Vision-Language Models\n",
      "├── LeCov: Multi-level Testing Criteria for Large Language Models\n",
      "├── LEVERAGING BIASES IN LARGE LANGUAGE MODELS: “BIAS-KNN” FOR EFFECTIVE FEW-SHOT LEARNING\n",
      "├── Multi-Turn Human Jailbreaks Are Not Robust to LLM Defenses Yet\n",
      "├── LLM Detectors Still Fall Short of Real World: Case of LLM-Generated Short News-Like Posts\n",
      "├── MAGE: Machine-generated Text Detection in the Wild\n",
      "├── Many-shot Jailbreaking\n",
      "├── MemeCraft: Contextual and Stance-Driven Multimodal Meme Generation\n",
      "├── Merging Improves Self-Critique Against Jailbreak Attacks\n",
      "├── Mitigating Text Toxicity with Counterfactual Generation\n",
      "├── MLLMG UARD: A Multi-dimensional Safety Evaluation Suite for Multimodal Large Language Models\n",
      "├── Model Merging and Safety Alignment: One Bad Model Spoils the Bunch\n",
      "├── Moderating Illicit Online Image Promotion for Unsafe User-Generated Content Games Using Large Vision-Language Models\n",
      "├── Moderating New Waves of Online Hate with Chain-of-Thought Reasoning in Large Language Models\n",
      "├── Moderator: Moderating Text-to-Image Diffusion Models through Fine-grained Context-based Policies\n",
      "├── MoGU: A Framework for Enhancing Safety of Open-Sourced LLMs While Preserving Their Usability\n",
      "├── MOSSBench: Is Your Multimodal Language Model Oversensitive to Safe Queries?\n",
      "├── Multimodal Pragmatic Jailbreak on Text-to-image Models\n",
      "├── MULTI TRUST: A Comprehensive Benchmark Towards Trustworthy Multimodal Large Language Models\n",
      "├── Optimization-based Prompt Injection Attack to LLM-as-a-Judge\n",
      "├── PathSeeker: Exploring LLM Security Vulnerabilities with a Reinforcement Learning-Based Jailbreak Approach\n",
      "├── PolygloToxicityPrompts: Multilingual Evaluation of Neural Toxic Degeneration in Large Language Models\n",
      "├── Preemptive Answer “Attacks” on Chain-of-Thought Reasoning\n",
      "├── Prefix Guidance: A Steering Wheel for Large Language Models to Defend Against Jailbreak Attacks\n",
      "├── PrimeGuard: Safe and Helpful LLMs through Tuning-Free Routing\n",
      "├── Pro-Woman, Anti-Man? Identifying Gender Bias in Stance Detection\n",
      "├── Probing the Safety Response Boundary of Large Language Models via Unsafe Decoding Path Generation\n",
      "├── Prompt Injection Attacks on Large Language Models in Oncology\n",
      "├── Protecting Your LLMs with Information Bottleneck\n",
      "├── Rag 'n Roll: An End-to-End Evaluation of Indirect Prompt Manipulations in LLM-based Application Frameworks\n",
      "├── RAID: A Shared Benchmark for Robust Evaluation of Machine-Generated Text Detectors\n",
      "├── RAIDAR: GENERATIVE AI DETECTION VIA REWRITING\n",
      "├── Read Over the Lines: Attacking LLMs and Toxicity Detection Systems with ASCII Art to Mask Profanity\n",
      "├── REDQUEEN : Safeguarding Large Language Models against Concealed Multi-Turn Jailbreaking\n",
      "├── Rethinking How to Evaluate Language Model Jailbreak\n",
      "├── Robustifying Safety-Aligned Large Language Models through Clean Data Curation\n",
      "├── S-Eval: Automatic and Adaptive Test Generation for Benchmarking Safety Evaluation of Large Language Models\n",
      "├── Safety Alignment for Vision Language Models\n",
      "├── Safety Alignment Should Be Made More Than Just a Few Tokens Deep\n",
      "├── Safetywashing: Do AI Safety Benchmarks Actually Measure Safety Progress?\n",
      "├── Synthetic Alignment data Generation for Safety Evaluation and Red Teaming (SAGE-RT or SAGE)\n",
      "├── SANDWICH ATTACK: MULTI-LANGUAGE MIXTURE ADAPTIVE ATTACK ON LLMs\n",
      "├── SoK: Prompt Hacking of Large Language Models\n",
      "├── STEER DIFF: S TEERING TOWARDS SAFE TEXT-TO-IMAGE DIFFUSION MODELS\n",
      "├── SUPERFICIAL SAFETY ALIGNMENT HYPOTHESIS\n",
      "├── Supporting Human Raters with the Detection of Harmful Content using Large Language Models\n",
      "├── System-Level Defense against Indirect Prompt Injection Attacks: An Information Flow Control Perspective\n",
      "├── The Better Angels of Machine Personality: How Personality Relates to LLM Safety\n",
      "├── The Instruction Hierarchy: Training LLMs to Prioritize Privileged Instructions\n",
      "├── Towards Understanding Unsafe Video Generation\n",
      "├── TRAINING SOCIALLY ALIGNED LANGUAGE MODELS INSIMULATED HUMAN SOCIETY\n",
      "├── Unbridled Icarus: A Survey of the Potential Perils of Image Inputs in Multimodal Large Language Model Security\n",
      "├── Uncertainty-Guided Modal Rebalance for Hateful Memes Detection\n",
      "├── Universal Adversarial Triggers Are Not Universal\n",
      "├── UnsafeBench: Benchmarking Image Safety Classifiers on Real-World and AI-Generated Images\n",
      "├── Unveiling the Implicit Toxicity in Large Language Models\n",
      "├── WILDTEAMING at Scale: From In-the-Wild Jailbreaks to (Adversarially) Safer Language Models\n",
      "├── YOUKNOW WHAT I’MSAYING : JAILBREAK ATTACK VIA IMPLICIT REFERENCE\n",
      "└── LLM-Virus: Evolutionary Jailbreak Attack on Large Language Models\n"
     ]
    }
   ],
   "source": [
    "# 示例调用\n",
    "\n",
    "info_file_path = \"output/paper_infos5.json\"  # 替换为你的 JSON 文件路径\n",
    "layers = 10  # 树的最大层数\n",
    "paper_num=1\n",
    "\n",
    "\n",
    "# 构建追溯树\n",
    "# root = build_relate_trace_tree(info_file_path, layers,paper_num)\n",
    "# root = build_refer_trace_tree(info_file_path, layers,paper_num)\n",
    "root = build_tag_trace_tree(info_file_path, layers,paper_num)\n",
    "\n",
    "# 打印树结构\n",
    "print_tree(root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 后续处理：将建好的树存入json文件、从json文件读取并打印树\n",
    "将树结构存储到json文件中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from anytree import NodeMixin\n",
    "\n",
    "def tree_to_dict(node):\n",
    "    \"\"\"\n",
    "    将 anytree 节点及其子节点递归转换为字典。\n",
    "    \n",
    "    参数:\n",
    "        node: anytree 节点。\n",
    "        \n",
    "    返回:\n",
    "        表示树结构的字典。\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"name\": node.name,\n",
    "        \"data\": node.data,  # 假设每个节点有 data 属性\n",
    "        \"children\": [tree_to_dict(child) for child in node.children]\n",
    "    }\n",
    "\n",
    "def save_tree_to_json(root, file_path):\n",
    "    \"\"\"\n",
    "    将 anytree 树保存为 JSON 文件。\n",
    "    \n",
    "    参数:\n",
    "        root: 树的根节点。\n",
    "        file_path: 输出 JSON 文件的路径。\n",
    "    \"\"\"\n",
    "    tree_dict = tree_to_dict(root)\n",
    "    with open(file_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(tree_dict, f, ensure_ascii=False, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "树结构已保存到 output/tree_structure6.json\n"
     ]
    }
   ],
   "source": [
    "# 示例调用\n",
    "save_file_path = \"output/tree_structure6.json\"\n",
    "save_tree_to_json(root, save_file_path)\n",
    "print(f\"树结构已保存到 {save_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将json文件中的树重建并打印"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from anytree import Node, RenderTree\n",
    "\n",
    "def dict_to_tree(tree_dict, parent=None):\n",
    "    \"\"\"\n",
    "    将字典形式的树结构递归转换为 anytree 节点。\n",
    "    \n",
    "    参数:\n",
    "        tree_dict: 表示树结构的字典。\n",
    "        parent: 当前节点的父节点（用于递归构建子节点）。\n",
    "        \n",
    "    返回:\n",
    "        构建完成的 anytree 节点。\n",
    "    \"\"\"\n",
    "    # 创建当前节点\n",
    "    current_node = Node(tree_dict[\"name\"], parent=parent, data=tree_dict.get(\"data\"))\n",
    "    \n",
    "    # 递归构建子节点\n",
    "    for child_dict in tree_dict.get(\"children\", []):\n",
    "        dict_to_tree(child_dict, parent=current_node)\n",
    "    \n",
    "    return current_node\n",
    "\n",
    "def load_tree_from_json(file_path):\n",
    "    \"\"\"\n",
    "    从 JSON 文件中加载树结构并返回根节点。\n",
    "    \n",
    "    参数:\n",
    "        file_path: 输入 JSON 文件的路径。\n",
    "        \n",
    "    返回:\n",
    "        树的根节点。\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        tree_dict = json.load(f)\n",
    "    \n",
    "    # 从字典重建树\n",
    "    root = dict_to_tree(tree_dict)\n",
    "    return root\n",
    "\n",
    "def print_tree(root):\n",
    "    \"\"\"\n",
    "    打印树的层次结构。\n",
    "    \n",
    "    参数:\n",
    "        root: 树的根节点。\n",
    "    \"\"\"\n",
    "    for pre, _, node in RenderTree(root):\n",
    "        print(f\"{pre}{node.name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "树结构如下：\n",
      "A Causal Explainable Guardrails for Large Language Models\n",
      "├── Advancing Adversarial Suffix Transfer Learning on Aligned Large Language Models\n",
      "│   ├── A Jailbroken GenAI Model Can Cause Substantial Harm: GenAI-powered Applications are Vulnerable to PromptWares\n",
      "│   ├── AdaPPA: Adaptive Position Pre-Fill Jailbreak Attack Approach Targeting LLMs\n",
      "│   ├── An Analysis of Recent Advances in Deepfake Image Detection in an Evolving Threat Landscape\n",
      "│   ├── Applying Pre-trained Multilingual BERT in Embeddings for Improved Malicious Prompt Injection Attacks Detection\n",
      "│   ├── Automatic Jailbreaking of the Text-to-Image Generative AI Systems\n",
      "│   ├── Can Large Language Models Automatically Jailbreak GPT-4V?\n",
      "│   ├── Can LLMs Deeply Detect Complex Malicious Queries? A Framework for Jailbreaking via Obfuscating Intent\n",
      "│   ├── Chain of Attack: a Semantic-Driven Contextual Multi-Turn attacker for LLM\n",
      "│   ├── Continuous Embedding Attacks via Clipped Inputs in Jailbreaking Large Language Models\n",
      "│   ├── Dataset and Lessons Learned from the 2024 SaTML LLM Capture-the-Flag Competition\n",
      "│   ├── Emerging Vulnerabilities in Frontier Models: Multi-Turn Jailbreak Attacks\n",
      "│   ├── Empirical Analysis of Large Vision-Language Models against Goal Hijacking via Visual Prompt Injection\n",
      "│   ├── FLIPATTACK: Jailbreak LLMs Via Flipping\n",
      "│   ├── GenTel-Safe: A Unified Benchmark and Shielding Framework for Defending Against Prompt Injection Attacks\n",
      "│   ├── h4rm3l: A Dynamic Benchmark of Composable Jailbreak Attacks for LLM Safety Assessment\n",
      "│   ├── Hacc-Man: An Arcade Game for Jailbreaking LLMs\n",
      "│   ├── Jailbreak Vision Language Models via Bi-Modal Adversarial Prompt\n",
      "│   ├── Jailbreaking Prompt Attack: A Controllable Adversarial Attack against Diffusion Models\n",
      "│   ├── Jailbreaking Text-to-Image Models with LLM-Based Agents\n",
      "│   ├── Knowledge-to-Jailbreak: One Knowledge Point Worth One Attack\n",
      "│   ├── Lockpicking LLMs: A Logit-Based Jailbreak Using Token-level Manipulation\n",
      "│   ├── Medical MLLM is Vulnerable: Cross-Modality Jailbreak and Mismatched Attacks on Medical Multimodal Large Language Models\n",
      "│   ├── MoJE: Mixture of Jailbreak Experts, Naive Tabular Classifiers as Guard for Prompt Attacks\n",
      "│   ├── Poisoned LangChain: Jailbreak LLMs by LangChain\n",
      "│   ├── Trust No AI: Prompt Injection Along The CIA Security Triad\n",
      "│   ├── PROMPT FUZZ: Harnessing Fuzzing Techniques for Robust Testing of Prompt Injection in LLMs\n",
      "│   ├── RedAgent: Red Teaming Large Language Models with Context-aware Autonomous Language Agent\n",
      "│   ├── Systematically Analyzing Prompt Injection Vulnerabilities in Diverse LLM Architectures\n",
      "│   ├── Unleashing Worms and Extracting Data: Escalating the Outcome of Attacks against RAG-based Inference in Scale and Severity Using Jailbreaking\n",
      "│   ├── Unlocking Adversarial Suffix Optimization Without Affirmative Phrases: Efficient Black-box Jailbreaking via LLM as Optimizer\n",
      "│   ├── Virtual Context: Enhancing Jailbreak Attacks with Special Token Injection\n",
      "│   ├── Visual-RolePlay: Universal Jailbreak Attack on MultiModal Large Language Models via Role-playing Image Character\n",
      "│   ├── VLMGuard: Defending VLMs against Malicious Prompts via Unlabeled Data\n",
      "│   ├── Voice Jailbreak Attacks Against GPT-4o\n",
      "│   ├── PromptShield: Deployable Detection for Prompt Injection Attacks\n",
      "│   └── 'Do as I say not as I do': A Semi-Automated Approach for Jailbreak Prompt Attack against Multimodal LLMs\n",
      "├── Adversarial Search Engine Optimization for Large Language Models\n",
      "├── Adversarial Tuning: Defending Against Jailbreak Attacks for LLMs\n",
      "├── Adversaries Can Misuse Combinations of Safe Models\n",
      "├── AdvPrompter: Fast Adaptive Adversarial Prompting for LLMs\n",
      "├── AEGIS: Online Adaptive AI Content Safety Moderation with Ensemble of LLM Experts\n",
      "├── AI Risk Management Should Incorporate Both Safety and Security\n",
      "├── ALERT: A Comprehensive Benchmark for Assessing Large Language Models’ Safety through Red Teaming\n",
      "├── Aligners: Decoupling LLMs and Alignment\n",
      "├── Antidote: Post-fine-tuning Safety Alignment for Large Language Models against Harmful Fine-tuning\n",
      "├── Are AI-Generated Text Detectors Robust to Adversarial Perturbations?\n",
      "├── Are PPO-ed Language Models Hackable?\n",
      "├── ARGS: Alignment as Reward-Guided Search\n",
      "├── Arondight: Red Teaming Large Vision Language Models with Auto-generated Multi-modal Jailbreak Prompts\n",
      "├── A safety realignment framework via subspace-oriented model fusion for large language models\n",
      "├── AutoJailbreak: Exploring Jailbreak Attacks and Defenses through a Dependency Lens\n",
      "├── Badllama 3: removing safety finetuning from Llama 3 in minutes\n",
      "├── BELLS: A Framework Towards Future Proof Benchmarks for the Evaluation of LLM Safeguards\n",
      "├── Benchmarking Llama2, Mistral, Gemma and GPT for Factuality, Toxicity, Bias and Propensity for Hallucinations\n",
      "├── Beyond Imitation: Leveraging Fine-Grained Quality Signals for Alignment\n",
      "├── Beyond Reverse KL: Generalizing Direct Preference Optimization with Diverse Divergence Constraints\n",
      "├── Moralized” Multi-Step Jailbreak Prompts: Black-Box Testing of Guardrails in Large Language Models for Verbal Attacks\n",
      "├── BOOSTER: TACKLING HARMFUL FINE-TUNING FOR LARGE LANGUAGE MODELS VIA ATTENUATING HARMFUL PERTURBATION\n",
      "├── CAN A LARGE LANGUAGE MODEL BE A GASLIGHTER ?\n",
      "│   ├── HARNESSING TASK OVERLOAD FOR SCALABLE JAILBREAK ATTACKS ON LARGE LANGUAGE MODELS\n",
      "│   └── PROMPT INFECTION : LLM- TO-LLM PROMPT INJECTION WITHIN MULTI-AGENT SYSTEMS\n",
      "├── Can Editing LLMs Inject Harm?\n",
      "├── CANLLM-Generated Misinformation Be Detected?\n",
      "├── CAS: A PROBABILITY-BASED APPROACH FOR UNIVERSAL CONDITION ALIGNMENT SCORE\n",
      "├── CHAIN -OF-JAILBREAK ATTACK FOR IMAGE GENERATION MODELS VIA EDITING STEP BY STEP\n",
      "├── Competition Report: Finding Universal Jailbreak Backdoors in Aligned LLMs\n",
      "├── Covert Malicious Finetuning: Challenges in Safeguarding LLM Adaptation\n",
      "├── CPPO: Continual Learning for Reinforcement Learning with Human Feedback\n",
      "├── Cross-Modal Safety Alignment: Is textual unlearning all you need?\n",
      "├── Cross-modality Information Check for Detecting Jailbreaking in Multimodal Large Language Models\n",
      "│   ├── F2A: An Innovative Approach for Prompt Injection by Utilizing Feign Security Detection Agents\n",
      "│   └── GPT-4 Jailbreaks Itself with Near-Perfect Success Using Self-Explanation\n",
      "├── Cross-Task Defense: Instruction-Tuning LLMs for Content Safety\n",
      "├── Decoupled Alignment for Robust Plug-and-Play Adaptation\n",
      "├── Defending Large Language Models Against Jailbreak Attacks via Layer-specific Editing\n",
      "├── Defending LLMs against Jailbreaking Attacks via Backtranslation\n",
      "├── Defensive Prompt Patch: A Robust and Interpretable Defense of LLMs against Jailbreak Attacks\n",
      "├── Derail Yourself: Multi-Turn LLM Jailbreak Attack Through Self-Discovered Clues\n",
      "├── Does Refusal Training in LLMs Generalize to the Past Tense?\n",
      "├── Don’t Say No: Jailbreaking LLM by Suppressing Refusal\n",
      "├── EEG-Defender: Defending against Jailbreak through Early Exit Generation of Large Language Models\n",
      "├── Emerging Safety Attack and Defense in Federated Instruction Tuning of Large Language Models\n",
      "├── Ensemble Jailbreak on Large Language Models\n",
      "├── Eraser: Jailbreaking Defense in Large Language Models via Unlearning Harmful Knowledge\n",
      "├── Event-Radar: Event-driven Multi-View Learning for Multimodal Fake News Detection\n",
      "├── FINE-TUNING ALIGNED LANGUAGE MODELS COMPROMISES SAFETY, EVEN WHEN USERS DO NOT INTEND TO!\n",
      "├── Fine-Tuning, Quantization, and LLMs: Navigating Unintended Outcomes\n",
      "├── FLASK: Fine-Grained Language Model Evaluation Based on Alignment Skill Sets\n",
      "├── Explain LLM Safety through Intermediate Hidden States\n",
      "├── Defending against Jailbreak Attacks with Hidden State Filtering\n",
      "├── Image-to-Text Logic Jailbreak: Your Imagination can Help You Do Anything\n",
      "├── Improved Techniques for Optimization-Based Jailbreaking on Large Language Models\n",
      "├── Improving Alignment and Robustness with Circuit Breakers\n",
      "├── Investigating the Influence of Prompt-Specific Shortcuts in AI Generated Text Detection\n",
      "├── Jailbreak Attacks and Defenses Against Large Language Models: A Survey\n",
      "├── Jailbreak Open-Sourced Large Language Models via Enforced Decoding\n",
      "├── Jailbreak Paradox : The Achilles’ Heel of LLMs\n",
      "├── JAILBREAKING LEADING SAFETY-ALIGNED LLMs WITH SIMPLE ADAPTIVE ATTACKS\n",
      "├── JailbreakLens: Visual Analysis of Jailbreak Attacks Against Large Language Models\n",
      "├── JailbreakZoo: Survey, Landscapes, and Horizons in Jailbreaking Large Language and Vision-Language Models\n",
      "├── LeCov: Multi-level Testing Criteria for Large Language Models\n",
      "├── LEVERAGING BIASES IN LARGE LANGUAGE MODELS: “BIAS-KNN” FOR EFFECTIVE FEW-SHOT LEARNING\n",
      "├── Multi-Turn Human Jailbreaks Are Not Robust to LLM Defenses Yet\n",
      "├── LLM Detectors Still Fall Short of Real World: Case of LLM-Generated Short News-Like Posts\n",
      "├── MAGE: Machine-generated Text Detection in the Wild\n",
      "├── Many-shot Jailbreaking\n",
      "├── MemeCraft: Contextual and Stance-Driven Multimodal Meme Generation\n",
      "├── Merging Improves Self-Critique Against Jailbreak Attacks\n",
      "├── Mitigating Text Toxicity with Counterfactual Generation\n",
      "├── MLLMG UARD: A Multi-dimensional Safety Evaluation Suite for Multimodal Large Language Models\n",
      "├── Model Merging and Safety Alignment: One Bad Model Spoils the Bunch\n",
      "├── Moderating Illicit Online Image Promotion for Unsafe User-Generated Content Games Using Large Vision-Language Models\n",
      "├── Moderating New Waves of Online Hate with Chain-of-Thought Reasoning in Large Language Models\n",
      "├── Moderator: Moderating Text-to-Image Diffusion Models through Fine-grained Context-based Policies\n",
      "├── MoGU: A Framework for Enhancing Safety of Open-Sourced LLMs While Preserving Their Usability\n",
      "├── MOSSBench: Is Your Multimodal Language Model Oversensitive to Safe Queries?\n",
      "├── Multimodal Pragmatic Jailbreak on Text-to-image Models\n",
      "├── MULTI TRUST: A Comprehensive Benchmark Towards Trustworthy Multimodal Large Language Models\n",
      "├── Optimization-based Prompt Injection Attack to LLM-as-a-Judge\n",
      "├── PathSeeker: Exploring LLM Security Vulnerabilities with a Reinforcement Learning-Based Jailbreak Approach\n",
      "├── PolygloToxicityPrompts: Multilingual Evaluation of Neural Toxic Degeneration in Large Language Models\n",
      "├── Preemptive Answer “Attacks” on Chain-of-Thought Reasoning\n",
      "├── Prefix Guidance: A Steering Wheel for Large Language Models to Defend Against Jailbreak Attacks\n",
      "├── PrimeGuard: Safe and Helpful LLMs through Tuning-Free Routing\n",
      "├── Pro-Woman, Anti-Man? Identifying Gender Bias in Stance Detection\n",
      "├── Probing the Safety Response Boundary of Large Language Models via Unsafe Decoding Path Generation\n",
      "├── Prompt Injection Attacks on Large Language Models in Oncology\n",
      "├── Protecting Your LLMs with Information Bottleneck\n",
      "├── Rag 'n Roll: An End-to-End Evaluation of Indirect Prompt Manipulations in LLM-based Application Frameworks\n",
      "├── RAID: A Shared Benchmark for Robust Evaluation of Machine-Generated Text Detectors\n",
      "├── RAIDAR: GENERATIVE AI DETECTION VIA REWRITING\n",
      "├── Read Over the Lines: Attacking LLMs and Toxicity Detection Systems with ASCII Art to Mask Profanity\n",
      "├── REDQUEEN : Safeguarding Large Language Models against Concealed Multi-Turn Jailbreaking\n",
      "├── Rethinking How to Evaluate Language Model Jailbreak\n",
      "├── Robustifying Safety-Aligned Large Language Models through Clean Data Curation\n",
      "├── S-Eval: Automatic and Adaptive Test Generation for Benchmarking Safety Evaluation of Large Language Models\n",
      "├── Safety Alignment for Vision Language Models\n",
      "├── Safety Alignment Should Be Made More Than Just a Few Tokens Deep\n",
      "├── Safetywashing: Do AI Safety Benchmarks Actually Measure Safety Progress?\n",
      "├── Synthetic Alignment data Generation for Safety Evaluation and Red Teaming (SAGE-RT or SAGE)\n",
      "├── SANDWICH ATTACK: MULTI-LANGUAGE MIXTURE ADAPTIVE ATTACK ON LLMs\n",
      "├── SoK: Prompt Hacking of Large Language Models\n",
      "├── STEER DIFF: S TEERING TOWARDS SAFE TEXT-TO-IMAGE DIFFUSION MODELS\n",
      "├── SUPERFICIAL SAFETY ALIGNMENT HYPOTHESIS\n",
      "├── Supporting Human Raters with the Detection of Harmful Content using Large Language Models\n",
      "├── System-Level Defense against Indirect Prompt Injection Attacks: An Information Flow Control Perspective\n",
      "├── The Better Angels of Machine Personality: How Personality Relates to LLM Safety\n",
      "├── The Instruction Hierarchy: Training LLMs to Prioritize Privileged Instructions\n",
      "├── Towards Understanding Unsafe Video Generation\n",
      "├── TRAINING SOCIALLY ALIGNED LANGUAGE MODELS INSIMULATED HUMAN SOCIETY\n",
      "├── Unbridled Icarus: A Survey of the Potential Perils of Image Inputs in Multimodal Large Language Model Security\n",
      "├── Uncertainty-Guided Modal Rebalance for Hateful Memes Detection\n",
      "├── Universal Adversarial Triggers Are Not Universal\n",
      "├── UnsafeBench: Benchmarking Image Safety Classifiers on Real-World and AI-Generated Images\n",
      "├── Unveiling the Implicit Toxicity in Large Language Models\n",
      "├── WILDTEAMING at Scale: From In-the-Wild Jailbreaks to (Adversarially) Safer Language Models\n",
      "├── YOUKNOW WHAT I’MSAYING : JAILBREAK ATTACK VIA IMPLICIT REFERENCE\n",
      "└── LLM-Virus: Evolutionary Jailbreak Attack on Large Language Models\n"
     ]
    }
   ],
   "source": [
    "# 加载树结构\n",
    "root = load_tree_from_json(save_file_path)\n",
    "\n",
    "# 打印树结构\n",
    "print(\"树结构如下：\")\n",
    "print_tree(root)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
