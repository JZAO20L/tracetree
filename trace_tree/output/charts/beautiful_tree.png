digraph tree {    rankdir=TB    splines=curved    "PoisonedRAG: Knowledge Corruption Attacks to Retrieval-Augmented Generation of Large Language Models" [label="PoisonedRAG: Knowledge Corruption Attacks to Retrieval-Augmented Generation of Large Language Models\nLarge language models (LLMs) have achieved remarkable success due to their
exceptional generative capabilities. Despite their success, they also have
inherent limitations such as a lack of up-to-date knowledge and hallucination.
Retrieval-Augmented Generation (RAG) is a state-of-the-art technique to
mitigate these limitations. The key idea of RAG is to ground the answer
generation of an LLM on external knowledge retrieved from a knowledge database.
Existing studies mainly focus on improving the accuracy or efficiency of RAG,
leaving its security largely unexplored. We aim to bridge the gap in this work.
We find that the knowledge database in a RAG system introduces a new and
practical attack surface. Based on this attack surface, we propose PoisonedRAG,
the first knowledge corruption attack to RAG, where an attacker could inject a
few malicious texts into the knowledge database of a RAG system to induce an
LLM to generate an attacker-chosen target answer for an attacker-chosen target
question. We formulate knowledge corruption attacks as an optimization problem,
whose solution is a set of malicious texts. Depending on the background
knowledge (e.g., black-box and white-box settings) of an attacker on a RAG
system, we propose two solutions to solve the optimization problem,
respectively. Our results show PoisonedRAG could achieve a 90% attack success
rate when injecting five malicious texts for each target question into a
knowledge database with millions of texts. We also evaluate several defenses
and our results show they are insufficient to defend against PoisonedRAG,
highlighting the need for new defenses." shape=box style=filled fillcolor="#006400"];    "A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions" [label="A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions\nThe emergence of large language models (LLMs) has marked a significant
breakthrough in natural language processing (NLP), fueling a paradigm shift in
information acquisition. Nevertheless, LLMs are prone to hallucination,
generating plausible yet nonfactual content. This phenomenon raises significant
concerns over the reliability of LLMs in real-world information retrieval (IR)
systems and has attracted intensive research to detect and mitigate such
hallucinations. Given the open-ended general-purpose attributes inherent to
LLMs, LLM hallucinations present distinct challenges that diverge from prior
task-specific models. This divergence highlights the urgency for a nuanced
understanding and comprehensive overview of recent advances in LLM
hallucinations. In this survey, we begin with an innovative taxonomy of
hallucination in the era of LLM and then delve into the factors contributing to
hallucinations. Subsequently, we present a thorough overview of hallucination
detection methods and benchmarks. Our discussion then transfers to
representative methodologies for mitigating LLM hallucinations. Additionally,
we delve into the current limitations faced by retrieval-augmented LLMs in
combating hallucinations, offering insights for developing more robust IR
systems. Finally, we highlight the promising research directions on LLM
hallucinations, including hallucination in large vision-language models and
understanding of knowledge boundaries in LLM hallucinations." shape=box style=filled fillcolor="#32CD32"];    "Adaptive Chameleon or Stubborn Sloth: Revealing the Behavior of Large Language Models in Knowledge Conflicts" [label="Adaptive Chameleon or Stubborn Sloth: Revealing the Behavior of Large Language Models in Knowledge Conflicts\nBy providing external information to large language models (LLMs), tool
augmentation (including retrieval augmentation) has emerged as a promising
solution for addressing the limitations of LLMs' static parametric memory.
However, how receptive are LLMs to such external evidence, especially when the
evidence conflicts with their parametric memory? We present the first
comprehensive and controlled investigation into the behavior of LLMs when
encountering knowledge conflicts. We propose a systematic framework to elicit
high-quality parametric memory from LLMs and construct the corresponding
counter-memory, which enables us to conduct a series of controlled experiments.
Our investigation reveals seemingly contradicting behaviors of LLMs. On the one
hand, different from prior wisdom, we find that LLMs can be highly receptive to
external evidence even when that conflicts with their parametric memory, given
that the external evidence is coherent and convincing. On the other hand, LLMs
also demonstrate a strong confirmation bias when the external evidence contains
some information that is consistent with their parametric memory, despite being
presented with conflicting evidence at the same time. These results pose
important implications that are worth careful consideration for the further
development and deployment of tool- and retrieval-augmented LLMs. Resources are
available at https://github.com/OSU-NLP-Group/LLM-Knowledge-Conflict." shape=box style=filled fillcolor="#98FB98"];    "Rich Knowledge Sources Bring Complex Knowledge Conflicts: Recalibrating Models to Reflect Conflicting Evidence" [label="Rich Knowledge Sources Bring Complex Knowledge Conflicts: Recalibrating Models to Reflect Conflicting Evidence\nQuestion answering models can use rich knowledge sources -- up to one hundred
retrieved passages and parametric knowledge in the large-scale language model
(LM). Prior work assumes information in such knowledge sources is consistent
with each other, paying little attention to how models blend information stored
in their LM parameters with that from retrieved evidence documents. In this
paper, we simulate knowledge conflicts (i.e., where parametric knowledge
suggests one answer and different passages suggest different answers) and
examine model behaviors. We find retrieval performance heavily impacts which
sources models rely on, and current models mostly rely on non-parametric
knowledge in their best-performing settings. We discover a troubling trend that
contradictions among knowledge sources affect model confidence only marginally.
To address this issue, we present a new calibration study, where models are
discouraged from presenting any single answer when presented with multiple
conflicting answer candidates in retrieved evidences." shape=box style=filled fillcolor="#006400"];    "Quantifying Memorization Across Neural Language Models" [label="Quantifying Memorization Across Neural Language Models\nLarge language models (LMs) have been shown to memorize parts of their
training data, and when prompted appropriately, they will emit the memorized
training data verbatim. This is undesirable because memorization violates
privacy (exposing user data), degrades utility (repeated easy-to-memorize text
is often low quality), and hurts fairness (some texts are memorized over
others).
  We describe three log-linear relationships that quantify the degree to which
LMs emit memorized training data. Memorization significantly grows as we
increase (1) the capacity of a model, (2) the number of times an example has
been duplicated, and (3) the number of tokens of context used to prompt the
model. Surprisingly, we find the situation becomes more complicated when
generalizing these results across model families. On the whole, we find that
memorization in LMs is more prevalent than previously believed and will likely
get worse as models continues to scale, at least without active mitigations." shape=box style=filled fillcolor="#006400"];    "Multilingual Knowledge Editing with Language-Agnostic Factual Neurons" [label="Multilingual Knowledge Editing with Language-Agnostic Factual Neurons\nMultilingual knowledge editing (MKE) aims to simultaneously update factual
knowledge across multiple languages within large language models (LLMs).
Previous research indicates that the same knowledge across different languages
within LLMs exhibits a degree of shareability. However, most existing MKE
methods overlook the connections of the same knowledge between different
languages, resulting in knowledge conflicts and limited edit performance. To
address this issue, we first investigate how LLMs process multilingual factual
knowledge and discover that the same factual knowledge in different languages
generally activates a shared set of neurons, which we call language-agnostic
factual neurons (LAFNs). These neurons represent the same factual knowledge
shared across languages and imply the semantic connections among multilingual
knowledge. Inspired by this finding, we propose a new MKE method by Locating
and Updating Language-Agnostic Factual Neurons (LU-LAFNs) to edit multilingual
knowledge simultaneously, which avoids knowledge conflicts and thus improves
edit performance. Experimental results on Bi-ZsRE and MzsRE benchmarks
demonstrate that our method achieves the best edit performance, indicating the
effectiveness and importance of modeling the semantic connections among
multilingual knowledge." shape=box style=filled fillcolor="#006400"];    "Knowledge Neurons in Pretrained Transformers" [label="Knowledge Neurons in Pretrained Transformers\nLarge-scale pretrained language models are surprisingly good at recalling
factual knowledge presented in the training corpus. In this paper, we present
preliminary studies on how factual knowledge is stored in pretrained
Transformers by introducing the concept of knowledge neurons. Specifically, we
examine the fill-in-the-blank cloze task for BERT. Given a relational fact, we
propose a knowledge attribution method to identify the neurons that express the
fact. We find that the activation of such knowledge neurons is positively
correlated to the expression of their corresponding facts. In our case studies,
we attempt to leverage knowledge neurons to edit (such as update, and erase)
specific factual knowledge without fine-tuning. Our results shed light on
understanding the storage of knowledge within pretrained Transformers. The code
is available at https://github.com/Hunter-DDM/knowledge-neurons." shape=box style=filled fillcolor="#006400"];    "Extracting Training Data from Large Language Models" [label="Extracting Training Data from Large Language Models\nIt has become common to publish large (billion parameter) language models
that have been trained on private datasets. This paper demonstrates that in
such settings, an adversary can perform a training data extraction attack to
recover individual training examples by querying the language model.
  We demonstrate our attack on GPT-2, a language model trained on scrapes of
the public Internet, and are able to extract hundreds of verbatim text
sequences from the model's training data. These extracted examples include
(public) personally identifiable information (names, phone numbers, and email
addresses), IRC conversations, code, and 128-bit UUIDs. Our attack is possible
even though each of the above sequences are included in just one document in
the training data.
  We comprehensively evaluate our extraction attack to understand the factors
that contribute to its success. Worryingly, we find that larger models are more
vulnerable than smaller models. We conclude by drawing lessons and discussing
possible safeguards for training large language models." shape=box style=filled fillcolor="#006400"];    "Textbooks Are All You Need II: phi-1.5 technical report" [label="Textbooks Are All You Need II: phi-1.5 technical report\nWe continue the investigation into the power of smaller Transformer-based
language models as initiated by \textbf{TinyStories} -- a 10 million parameter
model that can produce coherent English -- and the follow-up work on
\textbf{phi-1}, a 1.3 billion parameter model with Python coding performance
close to the state-of-the-art. The latter work proposed to use existing Large
Language Models (LLMs) to generate ``textbook quality" data as a way to enhance
the learning process compared to traditional web data. We follow the
``Textbooks Are All You Need" approach, focusing this time on common sense
reasoning in natural language, and create a new 1.3 billion parameter model
named \textbf{phi-1.5}, with performance on natural language tasks comparable
to models 5x larger, and surpassing most non-frontier LLMs on more complex
reasoning tasks such as grade-school mathematics and basic coding. More
generally, \textbf{phi-1.5} exhibits many of the traits of much larger LLMs,
both good -- such as the ability to ``think step by step" or perform some
rudimentary in-context learning -- and bad, including hallucinations and the
potential for toxic and biased generations -- encouragingly though, we are
seeing improvement on that front thanks to the absence of web data. We
open-source \textbf{phi-1.5} to promote further research on these urgent
topics." shape=box style=filled fillcolor="#98FB98"];    "Self-Organizing Machine Translation: Example-Driven Induction of Transfer Functions" [label="Self-Organizing Machine Translation: Example-Driven Induction of Transfer Functions\nWith the advent of faster computers, the notion of doing machine translation
from a huge stored database of translation examples is no longer unreasonable.
This paper describes an attempt to merge the Example-Based Machine Translation
(EBMT) approach with psycholinguistic principles. A new formalism for context-
free grammars, called *marker-normal form*, is demonstrated and used to
describe language data in a way compatible with psycholinguistic theories. By
embedding this formalism in a standard multivariate optimization framework, a
system can be built that infers correct transfer functions for a set of
bilingual sentence pairs and then uses those functions to translate novel
sentences. The validity of this line of reasoning has been tested in the
development of a system called METLA-1. This system has been used to infer
English->French and English->Urdu transfer functions from small corpora. The
results of those experiments are examined, both in engineering terms as well as
in more linguistic terms. In general, the results of these experiments were
psycho- logically and linguistically well-grounded while still achieving a
respectable level of success when compared against a similar prototype using
Hidden Markov Models." shape=box style=filled fillcolor="#006400"];    "Sparks of Artificial General Intelligence: Early experiments with GPT-4" [label="Sparks of Artificial General Intelligence: Early experiments with GPT-4\nArtificial intelligence (AI) researchers have been developing and refining
large language models (LLMs) that exhibit remarkable capabilities across a
variety of domains and tasks, challenging our understanding of learning and
cognition. The latest model developed by OpenAI, GPT-4, was trained using an
unprecedented scale of compute and data. In this paper, we report on our
investigation of an early version of GPT-4, when it was still in active
development by OpenAI. We contend that (this early version of) GPT-4 is part of
a new cohort of LLMs (along with ChatGPT and Google's PaLM for example) that
exhibit more general intelligence than previous AI models. We discuss the
rising capabilities and implications of these models. We demonstrate that,
beyond its mastery of language, GPT-4 can solve novel and difficult tasks that
span mathematics, coding, vision, medicine, law, psychology and more, without
needing any special prompting. Moreover, in all of these tasks, GPT-4's
performance is strikingly close to human-level performance, and often vastly
surpasses prior models such as ChatGPT. Given the breadth and depth of GPT-4's
capabilities, we believe that it could reasonably be viewed as an early (yet
still incomplete) version of an artificial general intelligence (AGI) system.
In our exploration of GPT-4, we put special emphasis on discovering its
limitations, and we discuss the challenges ahead for advancing towards deeper
and more comprehensive versions of AGI, including the possible need for
pursuing a new paradigm that moves beyond next-word prediction. We conclude
with reflections on societal influences of the recent technological leap and
future research directions." shape=box style=filled fillcolor="#006400"];    "Language Modeling for Code-Switching: Evaluation, Integration of Monolingual Data, and Discriminative Training" [label="Language Modeling for Code-Switching: Evaluation, Integration of Monolingual Data, and Discriminative Training\nWe focus on the problem of language modeling for code-switched language, in
the context of automatic speech recognition (ASR). Language modeling for
code-switched language is challenging for (at least) three reasons: (1) lack of
available large-scale code-switched data for training; (2) lack of a replicable
evaluation setup that is ASR directed yet isolates language modeling
performance from the other intricacies of the ASR system; and (3) the reliance
on generative modeling. We tackle these three issues: we propose an
ASR-motivated evaluation setup which is decoupled from an ASR system and the
choice of vocabulary, and provide an evaluation dataset for English-Spanish
code-switching. This setup lends itself to a discriminative training approach,
which we demonstrate to work better than generative language modeling. Finally,
we explore a variety of training protocols and verify the effectiveness of
training with large amounts of monolingual data followed by fine-tuning with
small amounts of code-switched data, for both the generative and discriminative
cases." shape=box style=filled fillcolor="#006400"];    "BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions" [label="BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions\nIn this paper we study yes/no questions that are naturally occurring ---
meaning that they are generated in unprompted and unconstrained settings. We
build a reading comprehension dataset, BoolQ, of such questions, and show that
they are unexpectedly challenging. They often query for complex, non-factoid
information, and require difficult entailment-like inference to solve. We also
explore the effectiveness of a range of transfer learning baselines. We find
that transferring from entailment data is more effective than transferring from
paraphrase or extractive QA data, and that it, surprisingly, continues to be
very beneficial even when starting from massive pre-trained language models
such as BERT. Our best method trains BERT on MultiNLI and then re-trains it on
our train set. It achieves 80.4% accuracy compared to 90% accuracy of human
annotators (and 62% majority-baseline), leaving a significant gap for future
work." shape=box style=filled fillcolor="#006400"];    "Learning ASR pathways: A sparse multilingual ASR model" [label="Learning ASR pathways: A sparse multilingual ASR model\nNeural network pruning compresses automatic speech recognition (ASR) models
effectively. However, in multilingual ASR, language-agnostic pruning may lead
to severe performance drops on some languages because language-agnostic pruning
masks may not fit all languages and discard important language-specific
parameters. In this work, we present ASR pathways, a sparse multilingual ASR
model that activates language-specific sub-networks ("pathways"), such that the
parameters for each language are learned explicitly. With the overlapping
sub-networks, the shared parameters can also enable knowledge transfer for
lower-resource languages via joint multilingual training. We propose a novel
algorithm to learn ASR pathways, and evaluate the proposed method on 4
languages with a streaming RNN-T model. Our proposed ASR pathways outperform
both dense models and a language-agnostically pruned model, and provide better
performance on low-resource languages compared to the monolingual sparse
models." shape=box style=filled fillcolor="#006400"];    "Massive Editing for Large Language Models via Meta Learning" [label="Massive Editing for Large Language Models via Meta Learning\nWhile large language models (LLMs) have enabled learning knowledge from the
pre-training corpora, the acquired knowledge may be fundamentally incorrect or
outdated over time, which necessitates rectifying the knowledge of the language
model (LM) after the training. A promising approach involves employing a
hyper-network to generate parameter shift, whereas existing hyper-networks
suffer from inferior scalability in synchronous editing operation amount. To
mitigate the problem, we propose the MAssive Language Model Editing Network
(MALMEN), which formulates the parameter shift aggregation as the least square
problem, subsequently updating the LM parameters using the normal equation. To
accommodate editing multiple facts simultaneously with limited memory budgets,
we separate the computation on the hyper-network and LM, enabling arbitrary
batch size on both neural networks. Our method is evaluated by editing up to
thousands of facts on LMs with different architectures, i.e., BERT-base, GPT-2,
T5-XL (2.8B), and GPT-J (6B), across various knowledge-intensive NLP tasks,
i.e., closed book fact-checking and question answering. Remarkably, MALMEN is
capable of editing hundreds of times more facts than strong baselines with the
identical hyper-network architecture and outperforms editor specifically
designed for GPT. Our code is available at
https://github.com/ChenmienTan/malmen." shape=box style=filled fillcolor="#98FB98"];    "Does Localization Inform Editing? Surprising Differences in Causality-Based Localization vs. Knowledge Editing in Language Models" [label="Does Localization Inform Editing? Surprising Differences in Causality-Based Localization vs. Knowledge Editing in Language Models\nLanguage models learn a great quantity of factual information during
pretraining, and recent work localizes this information to specific model
weights like mid-layer MLP weights. In this paper, we find that we can change
how a fact is stored in a model by editing weights that are in a different
location than where existing methods suggest that the fact is stored. This is
surprising because we would expect that localizing facts to specific model
parameters would tell us where to manipulate knowledge in models, and this
assumption has motivated past work on model editing methods. Specifically, we
show that localization conclusions from representation denoising (also known as
Causal Tracing) do not provide any insight into which model MLP layer would be
best to edit in order to override an existing stored fact with a new one. This
finding raises questions about how past work relies on Causal Tracing to select
which model layers to edit. Next, we consider several variants of the editing
problem, including erasing and amplifying facts. For one of our editing
problems, editing performance does relate to localization results from
representation denoising, but we find that which layer we edit is a far better
predictor of performance. Our results suggest, counterintuitively, that better
mechanistic understanding of how pretrained language models work may not always
translate to insights about how to best change their behavior. Our code is
available at https://github.com/google/belief-localization" shape=box style=filled fillcolor="#006400"];    "Commonsense Knowledge Editing Based on Free-Text in LLMs" [label="Commonsense Knowledge Editing Based on Free-Text in LLMs\nKnowledge editing technology is crucial for maintaining the accuracy and
timeliness of large language models (LLMs) . However, the setting of this task
overlooks a significant portion of commonsense knowledge based on free-text in
the real world, characterized by broad knowledge scope, long content and non
instantiation. The editing objects of previous methods (e.g., MEMIT) were
single token or entity, which were not suitable for commonsense knowledge in
free-text form. To address the aforementioned challenges, we conducted
experiments from two perspectives: knowledge localization and knowledge
editing. Firstly, we introduced Knowledge Localization for Free-Text(KLFT)
method, revealing the challenges associated with the distribution of
commonsense knowledge in MLP and Attention layers, as well as in decentralized
distribution. Next, we propose a Dynamics-aware Editing Method(DEM), which
utilizes a Dynamics-aware Module to locate the parameter positions
corresponding to commonsense knowledge, and uses Knowledge Editing Module to
update knowledge. The DEM method fully explores the potential of the MLP and
Attention layers, and successfully edits commonsense knowledge based on
free-text. The experimental results indicate that the DEM can achieve excellent
editing performance." shape=box style=filled fillcolor="#006400"];    "Empirical Study on Updating Key-Value Memories in Transformer Feed-forward Layers" [label="Empirical Study on Updating Key-Value Memories in Transformer Feed-forward Layers\nThe feed-forward networks (FFNs) in transformers are recognized as a group of
key-value neural memories to restore abstract high-level knowledge. In this
work, we conduct an empirical ablation study on updating keys (the 1st layer in
the FFNs layer) or values (the 2nd layer in the FFNs layer). We compare those
two methods in various knowledge editing and fine-tuning tasks of large
language models to draw insights to understand FFNs further. Code is available
at $\href{https://github.com/qiuzh20/Tuning-keys-v.s.-values}{this\,repo}$." shape=box style=filled fillcolor="#006400"];    "Can bidirectional encoder become the ultimate winner for downstream applications of foundation models?" [label="Can bidirectional encoder become the ultimate winner for downstream applications of foundation models?\nOver the past few decades, Artificial Intelligence(AI) has progressed from
the initial machine learning stage to the deep learning stage, and now to the
stage of foundational models. Foundational models have the characteristics of
pre-training, transfer learning, and self-supervised learning, and pre-trained
models can be fine-tuned and applied to various downstream tasks. Under the
framework of foundational models, models such as Bidirectional Encoder
Representations from Transformers(BERT) and Generative Pre-trained
Transformer(GPT) have greatly advanced the development of natural language
processing(NLP), especially the emergence of many models based on BERT. BERT
broke through the limitation of only using one-way methods for language
modeling in pre-training by using a masked language model. It can capture
bidirectional context information to predict the masked words in the sequence,
this can improve the feature extraction ability of the model. This makes the
model very useful for downstream tasks, especially for specialized
applications. The model using the bidirectional encoder can better understand
the domain knowledge and be better applied to these downstream tasks. So we
hope to help understand how this technology has evolved and improved model
performance in various natural language processing tasks under the background
of foundational models and reveal its importance in capturing context
information and improving the model's performance on downstream tasks. This
article analyzes one-way and bidirectional models based on GPT and BERT and
compares their differences based on the purpose of the model. It also briefly
analyzes BERT and the improvements of some models based on BERT. The model's
performance on the Stanford Question Answering Dataset(SQuAD) and General
Language Understanding Evaluation(GLUE) was compared." shape=box style=filled fillcolor="#006400"];    "Hypernetwork Dismantling via Deep Reinforcement Learning" [label="Hypernetwork Dismantling via Deep Reinforcement Learning\nNetwork dismantling aims to degrade the connectivity of a network by removing
an optimal set of nodes. It has been widely adopted in many real-world
applications such as epidemic control and rumor containment. However,
conventional methods usually focus on simple network modeling with only
pairwise interactions, while group-wise interactions modeled by hypernetwork
are ubiquitous and critical. In this work, we formulate the hypernetwork
dismantling problem as a node sequence decision problem and propose a deep
reinforcement learning (DRL)-based hypernetwork dismantling framework. Besides,
we design a novel inductive hypernetwork embedding method to ensure the
transferability to various real-world hypernetworks. Our framework first
generates small-scale synthetic hypernetworks and embeds the nodes and
hypernetworks into a low dimensional vector space to represent the action and
state space in DRL, respectively. Then trial-and-error dismantling tasks are
conducted by an agent on these synthetic hypernetworks, and the dismantling
strategy is continuously optimized. Finally, the well-optimized strategy is
applied to real-world hypernetwork dismantling tasks. Experimental results on
five real-world hypernetworks demonstrate the effectiveness of our proposed
framework." shape=box style=filled fillcolor="#006400"];    "Evaluating the Factual Consistency of Large Language Models Through News Summarization" [label="Evaluating the Factual Consistency of Large Language Models Through News Summarization\nWhile large language models (LLMs) have proven to be effective on a large
variety of tasks, they are also known to hallucinate information. To measure
whether an LLM prefers factually consistent continuations of its input, we
propose a new benchmark called FIB(Factual Inconsistency Benchmark) that
focuses on the task of summarization. Specifically, our benchmark involves
comparing the scores an LLM assigns to a factually consistent versus a
factually inconsistent summary for an input news article. For factually
consistent summaries, we use human-written reference summaries that we manually
verify as factually consistent. To generate summaries that are factually
inconsistent, we generate summaries from a suite of summarization models that
we have manually annotated as factually inconsistent. A model's factual
consistency is then measured according to its accuracy, i.e.\ the proportion of
documents where it assigns a higher score to the factually consistent summary.
To validate the usefulness of FIB, we evaluate 23 large language models ranging
from 1B to 176B parameters from six different model families including BLOOM
and OPT. We find that existing LLMs generally assign a higher score to
factually consistent summaries than to factually inconsistent summaries.
However, if the factually inconsistent summaries occur verbatim in the
document, then LLMs assign a higher score to these factually inconsistent
summaries than factually consistent summaries. We validate design choices in
our benchmark including the scoring method and source of distractor summaries.
Our code and benchmark data can be found at https://github.com/r-three/fib." shape=box style=filled fillcolor="#98FB98"];    "FEQA: A Question Answering Evaluation Framework for Faithfulness Assessment in Abstractive Summarization" [label="FEQA: A Question Answering Evaluation Framework for Faithfulness Assessment in Abstractive Summarization\nNeural abstractive summarization models are prone to generate content
inconsistent with the source document, i.e. unfaithful. Existing automatic
metrics do not capture such mistakes effectively. We tackle the problem of
evaluating faithfulness of a generated summary given its source document. We
first collected human annotations of faithfulness for outputs from numerous
models on two datasets. We find that current models exhibit a trade-off between
abstractiveness and faithfulness: outputs with less word overlap with the
source document are more likely to be unfaithful. Next, we propose an automatic
question answering (QA) based metric for faithfulness, FEQA, which leverages
recent advances in reading comprehension. Given question-answer pairs generated
from the summary, a QA model extracts answers from the document; non-matched
answers indicate unfaithful information in the summary. Among metrics based on
word overlap, embedding similarity, and learned language understanding models,
our QA-based metric has significantly higher correlation with human
faithfulness scores, especially on highly abstractive summaries." shape=box style=filled fillcolor="#006400"];    "Faithful to the Original: Fact Aware Neural Abstractive Summarization" [label="Faithful to the Original: Fact Aware Neural Abstractive Summarization\nUnlike extractive summarization, abstractive summarization has to fuse
different parts of the source text, which inclines to create fake facts. Our
preliminary study reveals nearly 30% of the outputs from a state-of-the-art
neural summarization system suffer from this problem. While previous
abstractive summarization approaches usually focus on the improvement of
informativeness, we argue that faithfulness is also a vital prerequisite for a
practical abstractive summarization system. To avoid generating fake facts in a
summary, we leverage open information extraction and dependency parse
technologies to extract actual fact descriptions from the source text. The
dual-attention sequence-to-sequence framework is then proposed to force the
generation conditioned on both the source text and the extracted fact
descriptions. Experiments on the Gigaword benchmark dataset demonstrate that
our model can greatly reduce fake summaries by 80%. Notably, the fact
descriptions also bring significant improvement on informativeness since they
often condense the meaning of the source text." shape=box style=filled fillcolor="#006400"];    "Scaling Instruction-Finetuned Language Models" [label="Scaling Instruction-Finetuned Language Models\nFinetuning language models on a collection of datasets phrased as
instructions has been shown to improve model performance and generalization to
unseen tasks. In this paper we explore instruction finetuning with a particular
focus on (1) scaling the number of tasks, (2) scaling the model size, and (3)
finetuning on chain-of-thought data. We find that instruction finetuning with
the above aspects dramatically improves performance on a variety of model
classes (PaLM, T5, U-PaLM), prompting setups (zero-shot, few-shot, CoT), and
evaluation benchmarks (MMLU, BBH, TyDiQA, MGSM, open-ended generation). For
instance, Flan-PaLM 540B instruction-finetuned on 1.8K tasks outperforms PALM
540B by a large margin (+9.4% on average). Flan-PaLM 540B achieves
state-of-the-art performance on several benchmarks, such as 75.2% on five-shot
MMLU. We also publicly release Flan-T5 checkpoints, which achieve strong
few-shot performance even compared to much larger models, such as PaLM 62B.
Overall, instruction finetuning is a general method for improving the
performance and usability of pretrained language models." shape=box style=filled fillcolor="#006400"];    "MultiFC: A Real-World Multi-Domain Dataset for Evidence-Based Fact Checking of Claims" [label="MultiFC: A Real-World Multi-Domain Dataset for Evidence-Based Fact Checking of Claims\nWe contribute the largest publicly available dataset of naturally occurring
factual claims for the purpose of automatic claim verification. It is collected
from 26 fact checking websites in English, paired with textual sources and rich
metadata, and labelled for veracity by human expert journalists. We present an
in-depth analysis of the dataset, highlighting characteristics and challenges.
Further, we present results for automatic veracity prediction, both with
established baselines and with a novel method for joint ranking of evidence
pages and predicting veracity that outperforms all baselines. Significant
performance increases are achieved by encoding evidence, and by modelling
metadata. Our best-performing model achieves a Macro F1 of 49.2%, showing that
this is a challenging testbed for claim veracity prediction." shape=box style=filled fillcolor="#006400"];    "Fast Abstractive Summarization with Reinforce-Selected Sentence Rewriting" [label="Fast Abstractive Summarization with Reinforce-Selected Sentence Rewriting\nInspired by how humans summarize long documents, we propose an accurate and
fast summarization model that first selects salient sentences and then rewrites
them abstractively (i.e., compresses and paraphrases) to generate a concise
overall summary. We use a novel sentence-level policy gradient method to bridge
the non-differentiable computation between these two neural networks in a
hierarchical way, while maintaining language fluency. Empirically, we achieve
the new state-of-the-art on all metrics (including human evaluation) on the
CNN/Daily Mail dataset, as well as significantly higher abstractiveness scores.
Moreover, by first operating at the sentence-level and then the word-level, we
enable parallel decoding of our neural generative model that results in
substantially faster (10-20x) inference speed as well as 4x faster training
convergence than previous long-paragraph encoder-decoder models. We also
demonstrate the generalization of our model on the test-only DUC-2002 dataset,
where we achieve higher scores than a state-of-the-art model." shape=box style=filled fillcolor="#006400"];    "On Context Utilization in Summarization with Large Language Models" [label="On Context Utilization in Summarization with Large Language Models\nLarge language models (LLMs) excel in abstractive summarization tasks,
delivering fluent and pertinent summaries. Recent advancements have extended
their capabilities to handle long-input contexts, exceeding 100k tokens.
However, in question answering, language models exhibit uneven utilization of
their input context. They tend to favor the initial and final segments,
resulting in a U-shaped performance pattern concerning where the answer is
located within the input. This bias raises concerns, particularly in
summarization where crucial content may be dispersed throughout the source
document(s). Besides, in summarization, mapping facts from the source to the
summary is not trivial as salient content is usually re-phrased. In this paper,
we conduct the first comprehensive study on context utilization and position
bias in summarization. Our analysis encompasses 6 LLMs, 10 datasets, and 5
evaluation metrics. We introduce a new evaluation benchmark called MiddleSum on
the which we benchmark two alternative inference methods to alleviate position
bias: hierarchical summarization and incremental summarization. Our code and
data can be found here: https://github.com/ntunlp/MiddleSum." shape=box style=filled fillcolor="#98FB98"];    "Revisiting Zero-Shot Abstractive Summarization in the Era of Large Language Models from the Perspective of Position Bias" [label="Revisiting Zero-Shot Abstractive Summarization in the Era of Large Language Models from the Perspective of Position Bias\nWe characterize and study zero-shot abstractive summarization in Large
Language Models (LLMs) by measuring position bias, which we propose as a
general formulation of the more restrictive lead bias phenomenon studied
previously in the literature. Position bias captures the tendency of a model
unfairly prioritizing information from certain parts of the input text over
others, leading to undesirable behavior. Through numerous experiments on four
diverse real-world datasets, we study position bias in multiple LLM models such
as GPT 3.5-Turbo, Llama-2, and Dolly-v2, as well as state-of-the-art pretrained
encoder-decoder abstractive summarization models such as Pegasus and BART. Our
findings lead to novel insights and discussion on performance and position bias
of models for zero-shot summarization tasks." shape=box style=filled fillcolor="#006400"];    "Source Code Summarization in the Era of Large Language Models" [label="Source Code Summarization in the Era of Large Language Models\nTo support software developers in understanding and maintaining programs,
various automatic (source) code summarization techniques have been proposed to
generate a concise natural language summary (i.e., comment) for a given code
snippet. Recently, the emergence of large language models (LLMs) has led to a
great boost in the performance of code-related tasks. In this paper, we
undertake a systematic and comprehensive study on code summarization in the era
of LLMs, which covers multiple aspects involved in the workflow of LLM-based
code summarization. Specifically, we begin by examining prevalent automated
evaluation methods for assessing the quality of summaries generated by LLMs and
find that the results of the GPT-4 evaluation method are most closely aligned
with human evaluation. Then, we explore the effectiveness of five prompting
techniques (zero-shot, few-shot, chain-of-thought, critique, and expert) in
adapting LLMs to code summarization tasks. Contrary to expectations, advanced
prompting techniques may not outperform simple zero-shot prompting. Next, we
investigate the impact of LLMs' model settings (including top\_p and
temperature parameters) on the quality of generated summaries. We find the
impact of the two parameters on summary quality varies by the base LLM and
programming language, but their impacts are similar. Moreover, we canvass LLMs'
abilities to summarize code snippets in distinct types of programming
languages. The results reveal that LLMs perform suboptimally when summarizing
code written in logic programming languages compared to other language types.
Finally, we unexpectedly find that CodeLlama-Instruct with 7B parameters can
outperform advanced GPT-4 in generating summaries describing code
implementation details and asserting code properties. We hope that our findings
can provide a comprehensive understanding of code summarization in the era of
LLMs." shape=box style=filled fillcolor="#006400"];    "When Benchmarks are Targets: Revealing the Sensitivity of Large Language Model Leaderboards" [label="When Benchmarks are Targets: Revealing the Sensitivity of Large Language Model Leaderboards\nLarge Language Model (LLM) leaderboards based on benchmark rankings are
regularly used to guide practitioners in model selection. Often, the published
leaderboard rankings are taken at face value - we show this is a (potentially
costly) mistake. Under existing leaderboards, the relative performance of LLMs
is highly sensitive to (often minute) details. We show that for popular
multiple-choice question benchmarks (e.g., MMLU), minor perturbations to the
benchmark, such as changing the order of choices or the method of answer
selection, result in changes in rankings up to 8 positions. We explain this
phenomenon by conducting systematic experiments over three broad categories of
benchmark perturbations and identifying the sources of this behavior. Our
analysis results in several best-practice recommendations, including the
advantage of a hybrid scoring method for answer selection. Our study highlights
the dangers of relying on simple benchmark evaluations and charts the path for
more robust evaluation schemes on the existing benchmarks. The code for this
paper is available at
https://github.com/National-Center-for-AI-Saudi-Arabia/lm-evaluation-harness." shape=box style=filled fillcolor="#006400"];    "MovieSum: An Abstractive Summarization Dataset for Movie Screenplays" [label="MovieSum: An Abstractive Summarization Dataset for Movie Screenplays\nMovie screenplay summarization is challenging, as it requires an
understanding of long input contexts and various elements unique to movies.
Large language models have shown significant advancements in document
summarization, but they often struggle with processing long input contexts.
Furthermore, while television transcripts have received attention in recent
studies, movie screenplay summarization remains underexplored. To stimulate
research in this area, we present a new dataset, MovieSum, for abstractive
summarization of movie screenplays. This dataset comprises 2200 movie
screenplays accompanied by their Wikipedia plot summaries. We manually
formatted the movie screenplays to represent their structural elements.
Compared to existing datasets, MovieSum possesses several distinctive features:
(1) It includes movie screenplays, which are longer than scripts of TV
episodes. (2) It is twice the size of previous movie screenplay datasets. (3)
It provides metadata with IMDb IDs to facilitate access to additional external
knowledge. We also show the results of recently released large language models
applied to summarization on our dataset to provide a detailed baseline." shape=box style=filled fillcolor="#006400"];    "Extending Context Window of Large Language Models via Positional Interpolation" [label="Extending Context Window of Large Language Models via Positional Interpolation\nWe present Position Interpolation (PI) that extends the context window sizes
of RoPE-based pretrained LLMs such as LLaMA models to up to 32768 with minimal
fine-tuning (within 1000 steps), while demonstrating strong empirical results
on various tasks that require long context, including passkey retrieval,
language modeling, and long document summarization from LLaMA 7B to 65B.
Meanwhile, the extended model by Position Interpolation preserve quality
relatively well on tasks within its original context window. To achieve this
goal, Position Interpolation linearly down-scales the input position indices to
match the original context window size, rather than extrapolating beyond the
trained context length which may lead to catastrophically high attention scores
that completely ruin the self-attention mechanism. Our theoretical study shows
that the upper bound of interpolation is at least $\sim 600 \times$ smaller
than that of extrapolation, further demonstrating its stability. Models
extended via Position Interpolation retain its original architecture and can
reuse most pre-existing optimization and infrastructure." shape=box style=filled fillcolor="#006400"];    "Making LLMs Worth Every Penny: Resource-Limited Text Classification in Banking" [label="Making LLMs Worth Every Penny: Resource-Limited Text Classification in Banking\nStandard Full-Data classifiers in NLP demand thousands of labeled examples,
which is impractical in data-limited domains. Few-shot methods offer an
alternative, utilizing contrastive learning techniques that can be effective
with as little as 20 examples per class. Similarly, Large Language Models
(LLMs) like GPT-4 can perform effectively with just 1-5 examples per class.
However, the performance-cost trade-offs of these methods remain underexplored,
a critical concern for budget-limited organizations. Our work addresses this
gap by studying the aforementioned approaches over the Banking77 financial
intent detection dataset, including the evaluation of cutting-edge LLMs by
OpenAI, Cohere, and Anthropic in a comprehensive set of few-shot scenarios. We
complete the picture with two additional methods: first, a cost-effective
querying method for LLMs based on retrieval-augmented generation (RAG), able to
reduce operational costs multiple times compared to classic few-shot
approaches, and second, a data augmentation method using GPT-4, able to improve
performance in data-limited scenarios. Finally, to inspire future research, we
provide a human expert's curated subset of Banking77, along with extensive
error analysis." shape=box style=filled fillcolor="#32CD32"];    "Efficient Intent Detection with Dual Sentence Encoders" [label="Efficient Intent Detection with Dual Sentence Encoders\nBuilding conversational systems in new domains and with added functionality
requires resource-efficient models that work under low-data regimes (i.e., in
few-shot setups). Motivated by these requirements, we introduce intent
detection methods backed by pretrained dual sentence encoders such as USE and
ConveRT. We demonstrate the usefulness and wide applicability of the proposed
intent detectors, showing that: 1) they outperform intent detectors based on
fine-tuning the full BERT-Large model or using BERT as a fixed black-box
encoder on three diverse intent detection data sets; 2) the gains are
especially pronounced in few-shot setups (i.e., with only 10 or 30 annotated
examples per intent); 3) our intent detectors can be trained in a matter of
minutes on a single CPU; and 4) they are stable across different hyperparameter
settings. In hope of facilitating and democratizing research focused on
intention detection, we release our code, as well as a new challenging
single-domain intent detection dataset comprising 13,083 annotated examples
over 77 intents." shape=box style=filled fillcolor="#98FB98"];    "The Daunting Dilemma with Sentence Encoders: Success on Standard Benchmarks, Failure in Capturing Basic Semantic Properties" [label="The Daunting Dilemma with Sentence Encoders: Success on Standard Benchmarks, Failure in Capturing Basic Semantic Properties\nIn this paper, we adopted a retrospective approach to examine and compare
five existing popular sentence encoders, i.e., Sentence-BERT, Universal
Sentence Encoder (USE), LASER, InferSent, and Doc2vec, in terms of their
performance on downstream tasks versus their capability to capture basic
semantic properties. Initially, we evaluated all five sentence encoders on the
popular SentEval benchmark and found that multiple sentence encoders perform
quite well on a variety of popular downstream tasks. However, being unable to
find a single winner in all cases, we designed further experiments to gain a
deeper understanding of their behavior. Specifically, we proposed four semantic
evaluation criteria, i.e., Paraphrasing, Synonym Replacement, Antonym
Replacement, and Sentence Jumbling, and evaluated the same five sentence
encoders using these criteria. We found that the Sentence-Bert and USE models
pass the paraphrasing criterion, with SBERT being the superior between the two.
LASER dominates in the case of the synonym replacement criterion.
Interestingly, all the sentence encoders failed the antonym replacement and
jumbling criteria. These results suggest that although these popular sentence
encoders perform quite well on the SentEval benchmark, they still struggle to
capture some basic semantic properties, thus, posing a daunting dilemma in NLP
research." shape=box style=filled fillcolor="#006400"];    "Conversational Contextual Cues: The Case of Personalization and History for Response Ranking" [label="Conversational Contextual Cues: The Case of Personalization and History for Response Ranking\nWe investigate the task of modeling open-domain, multi-turn, unstructured,
multi-participant, conversational dialogue. We specifically study the effect of
incorporating different elements of the conversation. Unlike previous efforts,
which focused on modeling messages and responses, we extend the modeling to
long context and participant's history. Our system does not rely on handwritten
rules or engineered features; instead, we train deep neural networks on a large
conversational dataset. In particular, we exploit the structure of Reddit
comments and posts to extract 2.1 billion messages and 133 million
conversations. We evaluate our models on the task of predicting the next
response in a conversation, and we find that modeling both context and
participants improves prediction accuracy." shape=box style=filled fillcolor="#006400"];    "Snips Voice Platform: an embedded Spoken Language Understanding system for private-by-design voice interfaces" [label="Snips Voice Platform: an embedded Spoken Language Understanding system for private-by-design voice interfaces\nThis paper presents the machine learning architecture of the Snips Voice
Platform, a software solution to perform Spoken Language Understanding on
microprocessors typical of IoT devices. The embedded inference is fast and
accurate while enforcing privacy by design, as no personal user data is ever
collected. Focusing on Automatic Speech Recognition and Natural Language
Understanding, we detail our approach to training high-performance Machine
Learning models that are small enough to run in real-time on small devices.
Additionally, we describe a data generation procedure that provides sufficient,
high-quality training data without compromising user privacy." shape=box style=filled fillcolor="#006400"];    "Domain adaptation for sequence labeling using hidden Markov models" [label="Domain adaptation for sequence labeling using hidden Markov models\nMost natural language processing systems based on machine learning are not
robust to domain shift. For example, a state-of-the-art syntactic dependency
parser trained on Wall Street Journal sentences has an absolute drop in
performance of more than ten points when tested on textual data from the Web.
An efficient solution to make these methods more robust to domain shift is to
first learn a word representation using large amounts of unlabeled data from
both domains, and then use this representation as features in a supervised
learning algorithm. In this paper, we propose to use hidden Markov models to
learn word representations for part-of-speech tagging. In particular, we study
the influence of using data from the source, the target or both domains to
learn the representation and the different ways to represent words using an
HMM." shape=box style=filled fillcolor="#006400"];    "Transfer Fine-Tuning: A BERT Case Study" [label="Transfer Fine-Tuning: A BERT Case Study\nA semantic equivalence assessment is defined as a task that assesses semantic
equivalence in a sentence pair by binary judgment (i.e., paraphrase
identification) or grading (i.e., semantic textual similarity measurement). It
constitutes a set of tasks crucial for research on natural language
understanding. Recently, BERT realized a breakthrough in sentence
representation learning (Devlin et al., 2019), which is broadly transferable to
various NLP tasks. While BERT's performance improves by increasing its model
size, the required computational power is an obstacle preventing practical
applications from adopting the technology. Herein, we propose to inject phrasal
paraphrase relations into BERT in order to generate suitable representations
for semantic equivalence assessment instead of increasing the model size.
Experiments on standard natural language understanding tasks confirm that our
method effectively improves a smaller BERT model while maintaining the model
size. The generated model exhibits superior performance compared to a larger
BERT model on semantic equivalence assessment tasks. Furthermore, it achieves
larger performance gains on tasks with limited training datasets for
fine-tuning, which is a property desirable for transfer learning." shape=box style=filled fillcolor="#006400"];    "An Empirical Study on Large-Scale Multi-Label Text Classification Including Few and Zero-Shot Labels" [label="An Empirical Study on Large-Scale Multi-Label Text Classification Including Few and Zero-Shot Labels\nLarge-scale Multi-label Text Classification (LMTC) has a wide range of
Natural Language Processing (NLP) applications and presents interesting
challenges. First, not all labels are well represented in the training set, due
to the very large label set and the skewed label distributions of LMTC
datasets. Also, label hierarchies and differences in human labelling guidelines
may affect graph-aware annotation proximity. Finally, the label hierarchies are
periodically updated, requiring LMTC models capable of zero-shot
generalization. Current state-of-the-art LMTC models employ Label-Wise
Attention Networks (LWANs), which (1) typically treat LMTC as flat multi-label
classification; (2) may use the label hierarchy to improve zero-shot learning,
although this practice is vastly understudied; and (3) have not been combined
with pre-trained Transformers (e.g. BERT), which have led to state-of-the-art
results in several NLP benchmarks. Here, for the first time, we empirically
evaluate a battery of LMTC methods from vanilla LWANs to hierarchical
classification approaches and transfer learning, on frequent, few, and
zero-shot learning on three datasets from different domains. We show that
hierarchical methods based on Probabilistic Label Trees (PLTs) outperform
LWANs. Furthermore, we show that Transformer-based approaches outperform the
state-of-the-art in two of the datasets, and we propose a new state-of-the-art
method which combines BERT with LWANs. Finally, we propose new models that
leverage the label hierarchy to improve few and zero-shot learning, considering
on each dataset a graph-aware annotation proximity measure that we introduce." shape=box style=filled fillcolor="#98FB98"];    "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer" [label="Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\nTransfer learning, where a model is first pre-trained on a data-rich task
before being fine-tuned on a downstream task, has emerged as a powerful
technique in natural language processing (NLP). The effectiveness of transfer
learning has given rise to a diversity of approaches, methodology, and
practice. In this paper, we explore the landscape of transfer learning
techniques for NLP by introducing a unified framework that converts all
text-based language problems into a text-to-text format. Our systematic study
compares pre-training objectives, architectures, unlabeled data sets, transfer
approaches, and other factors on dozens of language understanding tasks. By
combining the insights from our exploration with scale and our new ``Colossal
Clean Crawled Corpus'', we achieve state-of-the-art results on many benchmarks
covering summarization, question answering, text classification, and more. To
facilitate future work on transfer learning for NLP, we release our data set,
pre-trained models, and code." shape=box style=filled fillcolor="#006400"];    "A Primer in BERTology: What we know about how BERT works" [label="A Primer in BERTology: What we know about how BERT works\nTransformer-based models have pushed state of the art in many areas of NLP,
but our understanding of what is behind their success is still limited. This
paper is the first survey of over 150 studies of the popular BERT model. We
review the current state of knowledge about how BERT works, what kind of
information it learns and how it is represented, common modifications to its
training objectives and architecture, the overparameterization issue and
approaches to compression. We then outline directions for future research." shape=box style=filled fillcolor="#006400"];    "Publicly Available Clinical BERT Embeddings" [label="Publicly Available Clinical BERT Embeddings\nContextual word embedding models such as ELMo (Peters et al., 2018) and BERT
(Devlin et al., 2018) have dramatically improved performance for many natural
language processing (NLP) tasks in recent months. However, these models have
been minimally explored on specialty corpora, such as clinical text; moreover,
in the clinical domain, no publicly-available pre-trained BERT models yet
exist. In this work, we address this need by exploring and releasing BERT
models for clinical text: one for generic clinical text and another for
discharge summaries specifically. We demonstrate that using a domain-specific
model yields performance improvements on three common clinical NLP tasks as
compared to nonspecific embeddings. These domain-specific models are not as
performant on two clinical de-identification tasks, and argue that this is a
natural consequence of the differences between de-identified source text and
synthetically non de-identified task text." shape=box style=filled fillcolor="#006400"];    "SciBERT: A Pretrained Language Model for Scientific Text" [label="SciBERT: A Pretrained Language Model for Scientific Text\nObtaining large-scale annotated data for NLP tasks in the scientific domain
is challenging and expensive. We release SciBERT, a pretrained language model
based on BERT (Devlin et al., 2018) to address the lack of high-quality,
large-scale labeled scientific data. SciBERT leverages unsupervised pretraining
on a large multi-domain corpus of scientific publications to improve
performance on downstream scientific NLP tasks. We evaluate on a suite of tasks
including sequence tagging, sentence classification and dependency parsing,
with datasets from a variety of scientific domains. We demonstrate
statistically significant improvements over BERT and achieve new
state-of-the-art results on several of these tasks. The code and pretrained
models are available at https://github.com/allenai/scibert/." shape=box style=filled fillcolor="#006400"];    "RoBERTa: A Robustly Optimized BERT Pretraining Approach" [label="RoBERTa: A Robustly Optimized BERT Pretraining Approach\nLanguage model pretraining has led to significant performance gains but
careful comparison between different approaches is challenging. Training is
computationally expensive, often done on private datasets of different sizes,
and, as we will show, hyperparameter choices have significant impact on the
final results. We present a replication study of BERT pretraining (Devlin et
al., 2019) that carefully measures the impact of many key hyperparameters and
training data size. We find that BERT was significantly undertrained, and can
match or exceed the performance of every model published after it. Our best
model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results
highlight the importance of previously overlooked design choices, and raise
questions about the source of recently reported improvements. We release our
models and code." shape=box style=filled fillcolor="#006400"];    "Fine-Tuning Pretrained Language Models: Weight Initializations, Data Orders, and Early Stopping" [label="Fine-Tuning Pretrained Language Models: Weight Initializations, Data Orders, and Early Stopping\nFine-tuning pretrained contextual word embedding models to supervised
downstream tasks has become commonplace in natural language processing. This
process, however, is often brittle: even with the same hyperparameter values,
distinct random seeds can lead to substantially different results. To better
understand this phenomenon, we experiment with four datasets from the GLUE
benchmark, fine-tuning BERT hundreds of times on each while varying only the
random seeds. We find substantial performance increases compared to previously
reported results, and we quantify how the performance of the best-found model
varies as a function of the number of fine-tuning trials. Further, we examine
two factors influenced by the choice of random seed: weight initialization and
training data order. We find that both contribute comparably to the variance of
out-of-sample performance, and that some weight initializations perform well
across all tasks explored. On small datasets, we observe that many fine-tuning
trials diverge part of the way through training, and we offer best practices
for practitioners to stop training less promising runs early. We publicly
release all of our experimental data, including training and validation scores
for 2,100 trials, to encourage further analysis of training dynamics during
fine-tuning." shape=box style=filled fillcolor="#98FB98"];    "Sentence Encoders on STILTs: Supplementary Training on Intermediate Labeled-data Tasks" [label="Sentence Encoders on STILTs: Supplementary Training on Intermediate Labeled-data Tasks\nPretraining sentence encoders with language modeling and related unsupervised
tasks has recently been shown to be very effective for language understanding
tasks. By supplementing language model-style pretraining with further training
on data-rich supervised tasks, such as natural language inference, we obtain
additional performance improvements on the GLUE benchmark. Applying
supplementary training on BERT (Devlin et al., 2018), we attain a GLUE score of
81.8---the state of the art (as of 02/24/2019) and a 1.4 point improvement over
BERT. We also observe reduced variance across random restarts in this setting.
Our approach yields similar improvements when applied to ELMo (Peters et al.,
2018a) and Radford et al. (2018)'s model. In addition, the benefits of
supplementary training are particularly pronounced in data-constrained regimes,
as we show in experiments with artificially limited training data." shape=box style=filled fillcolor="#006400"];    "Passage Re-ranking with BERT" [label="Passage Re-ranking with BERT\nRecently, neural models pretrained on a language modeling task, such as ELMo
(Peters et al., 2017), OpenAI GPT (Radford et al., 2018), and BERT (Devlin et
al., 2018), have achieved impressive results on various natural language
processing tasks such as question-answering and natural language inference. In
this paper, we describe a simple re-implementation of BERT for query-based
passage re-ranking. Our system is the state of the art on the TREC-CAR dataset
and the top entry in the leaderboard of the MS MARCO passage retrieval task,
outperforming the previous state of the art by 27% (relative) in MRR@10. The
code to reproduce our results is available at
https://github.com/nyu-dl/dl4marco-bert" shape=box style=filled fillcolor="#006400"];    "MiniLM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers" [label="MiniLM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers\nPre-trained language models (e.g., BERT (Devlin et al., 2018) and its
variants) have achieved remarkable success in varieties of NLP tasks. However,
these models usually consist of hundreds of millions of parameters which brings
challenges for fine-tuning and online serving in real-life applications due to
latency and capacity constraints. In this work, we present a simple and
effective approach to compress large Transformer (Vaswani et al., 2017) based
pre-trained models, termed as deep self-attention distillation. The small model
(student) is trained by deeply mimicking the self-attention module, which plays
a vital role in Transformer networks, of the large model (teacher).
Specifically, we propose distilling the self-attention module of the last
Transformer layer of the teacher, which is effective and flexible for the
student. Furthermore, we introduce the scaled dot-product between values in the
self-attention module as the new deep self-attention knowledge, in addition to
the attention distributions (i.e., the scaled dot-product of queries and keys)
that have been used in existing works. Moreover, we show that introducing a
teacher assistant (Mirzadeh et al., 2019) also helps the distillation of large
pre-trained Transformer models. Experimental results demonstrate that our
monolingual model outperforms state-of-the-art baselines in different parameter
size of student models. In particular, it retains more than 99% accuracy on
SQuAD 2.0 and several GLUE benchmark tasks using 50% of the Transformer
parameters and computations of the teacher model. We also obtain competitive
results in applying deep self-attention distillation to multilingual
pre-trained models." shape=box style=filled fillcolor="#006400"];    "Combining Autoregressive and Autoencoder Language Models for Text Classification" [label="Combining Autoregressive and Autoencoder Language Models for Text Classification\nThis paper presents CAALM-TC (Combining Autoregressive and Autoencoder
Language Models for Text Classification), a novel method that enhances text
classification by integrating autoregressive and autoencoder language models.
Autoregressive large language models such as Open AI's GPT, Meta's Llama or
Microsoft's Phi offer promising prospects for content analysis practitioners,
but they generally underperform supervised BERT based models for text
classification. CAALM leverages autoregressive models to generate contextual
information based on input texts, which is then combined with the original text
and fed into an autoencoder model for classification. This hybrid approach
capitalizes on the extensive contextual knowledge of autoregressive models and
the efficient classification capabilities of autoencoders. Experimental results
on four benchmark datasets demonstrate that CAALM consistently outperforms
existing methods, particularly in tasks with smaller datasets and more abstract
classification objectives. The findings indicate that CAALM offers a scalable
and effective solution for automated content analysis in social science
research that minimizes sample size requirements." shape=box style=filled fillcolor="#98FB98"];    "Weak Human Preference Supervision For Deep Reinforcement Learning" [label="Weak Human Preference Supervision For Deep Reinforcement Learning\nThe current reward learning from human preferences could be used to resolve
complex reinforcement learning (RL) tasks without access to a reward function
by defining a single fixed preference between pairs of trajectory segments.
However, the judgement of preferences between trajectories is not dynamic and
still requires human input over thousands of iterations. In this study, we
proposed a weak human preference supervision framework, for which we developed
a human preference scaling model that naturally reflects the human perception
of the degree of weak choices between trajectories and established a
human-demonstration estimator via supervised learning to generate the predicted
preferences for reducing the number of human inputs. The proposed weak human
preference supervision framework can effectively solve complex RL tasks and
achieve higher cumulative rewards in simulated robot locomotion -- MuJoCo games
-- relative to the single fixed human preferences. Furthermore, our established
human-demonstration estimator requires human feedback only for less than 0.01\%
of the agent's interactions with the environment and significantly reduces the
cost of human inputs by up to 30\% compared with the existing approaches. To
present the flexibility of our approach, we released a video
(https://youtu.be/jQPe1OILT0M) showing comparisons of the behaviours of agents
trained on different types of human input. We believe that our naturally
inspired human preferences with weakly supervised learning are beneficial for
precise reward learning and can be applied to state-of-the-art RL systems, such
as human-autonomy teaming systems." shape=box style=filled fillcolor="#98FB98"];    "Emergence of Human-comparable Balancing Behaviors by Deep Reinforcement Learning" [label="Emergence of Human-comparable Balancing Behaviors by Deep Reinforcement Learning\nThis paper presents a hierarchical framework based on deep reinforcement
learning that learns a diversity of policies for humanoid balance control.
Conventional zero moment point based controllers perform limited actions during
under-actuation, whereas the proposed framework can perform human-like
balancing behaviors such as active push-off of ankles. The learning is done
through the design of an explainable reward based on physical constraints. The
simulated results are presented and analyzed. The successful emergence of
human-like behaviors through deep reinforcement learning proves the feasibility
of using an AI-based approach for learning humanoid balancing control in a
unified framework." shape=box style=filled fillcolor="#006400"];    "Experimental Evaluation of Human Motion Prediction: Toward Safe and Efficient Human Robot Collaboration" [label="Experimental Evaluation of Human Motion Prediction: Toward Safe and Efficient Human Robot Collaboration\nHuman motion prediction is non-trivial in modern industrial settings.
Accurate prediction of human motion can not only improve efficiency in human
robot collaboration, but also enhance human safety in close proximity to
robots. Among existing prediction models, the parameterization and
identification methods of those models vary. It remains unclear what is the
necessary parameterization of a prediction model, whether online adaptation of
the model is necessary, and whether prediction can help improve safety and
efficiency during human robot collaboration. These problems result from the
difficulty to quantitatively evaluate various prediction models in a
closed-loop fashion in real human-robot interaction settings. This paper
develops a method to evaluate the closed-loop performance of different
prediction models. In particular, we compare models with different
parameterizations and models with or without online parameter adaptation.
Extensive experiments were conducted on a human robot collaboration platform.
The experimental results demonstrated that human motion prediction
significantly enhanced the collaboration efficiency and human safety. Adaptable
prediction models that were parameterized by neural networks achieved the best
performance." shape=box style=filled fillcolor="#006400"];    "Active Hierarchical Imitation and Reinforcement Learning" [label="Active Hierarchical Imitation and Reinforcement Learning\nHumans can leverage hierarchical structures to split a task into sub-tasks
and solve problems efficiently. Both imitation and reinforcement learning or a
combination of them with hierarchical structures have been proven to be an
efficient way for robots to learn complex tasks with sparse rewards. However,
in the previous work of hierarchical imitation and reinforcement learning, the
tested environments are in relatively simple 2D games, and the action spaces
are discrete. Furthermore, many imitation learning works focusing on improving
the policies learned from the expert polices that are hard-coded or trained by
reinforcement learning algorithms, rather than human experts. In the scenarios
of human-robot interaction, humans can be required to provide demonstrations to
teach the robot, so it is crucial to improve the learning efficiency to reduce
expert efforts, and know human's perception about the learning/training
process. In this project, we explored different imitation learning algorithms
and designed active learning algorithms upon the hierarchical imitation and
reinforcement learning framework we have developed. We performed an experiment
where five participants were asked to guide a randomly initialized agent to a
random goal in a maze. Our experimental results showed that using DAgger and
reward-based active learning method can achieve better performance while saving
more human efforts physically and mentally during the training process." shape=box style=filled fillcolor="#006400"];    "Reinforcement Learning from Hierarchical Critics" [label="Reinforcement Learning from Hierarchical Critics\nIn this study, we investigate the use of global information to speed up the
learning process and increase the cumulative rewards of reinforcement learning
(RL) in competition tasks. Within the actor-critic RL, we introduce multiple
cooperative critics from two levels of the hierarchy and propose a
reinforcement learning from hierarchical critics (RLHC) algorithm. In our
approach, each agent receives value information from local and global critics
regarding a competition task and accesses multiple cooperative critics in a
top-down hierarchy. Thus, each agent not only receives low-level details but
also considers coordination from higher levels, thereby obtaining global
information to improve the training performance. Then, we test the proposed
RLHC algorithm against the benchmark algorithm, proximal policy optimisation
(PPO), for two experimental scenarios performed in a Unity environment
consisting of tennis and soccer agents' competitions. The results showed that
RLHC outperforms the benchmark on both competition tasks." shape=box style=filled fillcolor="#006400"];    "Differential Variable Speed Limits Control for Freeway Recurrent Bottlenecks via Deep Reinforcement learning" [label="Differential Variable Speed Limits Control for Freeway Recurrent Bottlenecks via Deep Reinforcement learning\nVariable speed limits (VSL) control is a flexible way to improve traffic
condition,increase safety and reduce emission. There is an emerging trend of
using reinforcement learning technique for VSL control and recent studies have
shown promising results. Currently, deep learning is enabling reinforcement
learning to develope autonomous control agents for problems that were
previously intractable. In this paper, we propose a more effective deep
reinforcement learning (DRL) model for differential variable speed limits
(DVSL) control, in which the dynamic and different speed limits among lanes can
be imposed. The proposed DRL models use a novel actor-critic architecture which
can learn a large number of discrete speed limits in a continues action space.
Different reward signals, e.g. total travel time, bottleneck speed, emergency
braking, and vehicular emission are used to train the DVSL controller, and
comparison between these reward signals are conducted. We test proposed DRL
baased DVSL controllers on a simulated freeway recurrent bottleneck. Results
show that the efficiency, safety and emissions can be improved by the proposed
method. We also show some interesting findings through the visulization of the
control policies generated from DRL models." shape=box style=filled fillcolor="#006400"];    "PaLM 2 Technical Report" [label="PaLM 2 Technical Report\nWe introduce PaLM 2, a new state-of-the-art language model that has better
multilingual and reasoning capabilities and is more compute-efficient than its
predecessor PaLM. PaLM 2 is a Transformer-based model trained using a mixture
of objectives. Through extensive evaluations on English and multilingual
language, and reasoning tasks, we demonstrate that PaLM 2 has significantly
improved quality on downstream tasks across different model sizes, while
simultaneously exhibiting faster and more efficient inference compared to PaLM.
This improved efficiency enables broader deployment while also allowing the
model to respond faster, for a more natural pace of interaction. PaLM 2
demonstrates robust reasoning capabilities exemplified by large improvements
over PaLM on BIG-Bench and other reasoning tasks. PaLM 2 exhibits stable
performance on a suite of responsible AI evaluations, and enables
inference-time control over toxicity without additional overhead or impact on
other capabilities. Overall, PaLM 2 achieves state-of-the-art performance
across a diverse set of tasks and capabilities.
  When discussing the PaLM 2 family, it is important to distinguish between
pre-trained models (of various sizes), fine-tuned variants of these models, and
the user-facing products that use these models. In particular, user-facing
products typically include additional pre- and post-processing steps.
Additionally, the underlying models may evolve over time. Therefore, one should
not expect the performance of user-facing products to exactly match the results
reported in this report." shape=box style=filled fillcolor="#32CD32"];    "The False Promise of Imitating Proprietary LLMs" [label="The False Promise of Imitating Proprietary LLMs\nAn emerging method to cheaply improve a weaker language model is to finetune
it on outputs from a stronger model, such as a proprietary system like ChatGPT
(e.g., Alpaca, Self-Instruct, and others). This approach looks to cheaply
imitate the proprietary model's capabilities using a weaker open-source model.
In this work, we critically analyze this approach. We first finetune a series
of LMs that imitate ChatGPT using varying base model sizes (1.5B--13B), data
sources, and imitation data amounts (0.3M--150M tokens). We then evaluate the
models using crowd raters and canonical NLP benchmarks. Initially, we were
surprised by the output quality of our imitation models -- they appear far
better at following instructions, and crowd workers rate their outputs as
competitive with ChatGPT. However, when conducting more targeted automatic
evaluations, we find that imitation models close little to none of the gap from
the base LM to ChatGPT on tasks that are not heavily supported in the imitation
data. We show that these performance discrepancies may slip past human raters
because imitation models are adept at mimicking ChatGPT's style but not its
factuality. Overall, we conclude that model imitation is a false promise: there
exists a substantial capabilities gap between open and closed LMs that, with
current methods, can only be bridged using an unwieldy amount of imitation data
or by using more capable base LMs. In turn, we argue that the highest leverage
action for improving open-source models is to tackle the difficult challenge of
developing better base LMs, rather than taking the shortcut of imitating
proprietary systems." shape=box style=filled fillcolor="#98FB98"];    "Imitation Attacks and Defenses for Black-box Machine Translation Systems" [label="Imitation Attacks and Defenses for Black-box Machine Translation Systems\nAdversaries may look to steal or attack black-box NLP systems, either for
financial gain or to exploit model errors. One setting of particular interest
is machine translation (MT), where models have high commercial value and errors
can be costly. We investigate possible exploits of black-box MT systems and
explore a preliminary defense against such threats. We first show that MT
systems can be stolen by querying them with monolingual sentences and training
models to imitate their outputs. Using simulated experiments, we demonstrate
that MT model stealing is possible even when imitation models have different
input data or architectures than their target models. Applying these ideas, we
train imitation models that reach within 0.6 BLEU of three production MT
systems on both high-resource and low-resource language pairs. We then leverage
the similarity of our imitation models to transfer adversarial examples to the
production systems. We use gradient-based attacks that expose inputs which lead
to semantically-incorrect translations, dropped content, and vulgar model
outputs. To mitigate these vulnerabilities, we propose a defense that modifies
translation outputs in order to misdirect the optimization of imitation models.
This defense degrades the adversary's BLEU score and attack success rate at
some cost in the defender's BLEU and inference speed." shape=box style=filled fillcolor="#006400"];    "Towards Better Evaluation of Instruction-Following: A Case-Study in Summarization" [label="Towards Better Evaluation of Instruction-Following: A Case-Study in Summarization\nDespite recent advances, evaluating how well large language models (LLMs)
follow user instructions remains an open problem. While evaluation methods of
language models have seen a rise in prompt-based approaches, limited work on
the correctness of these methods has been conducted. In this work, we perform a
meta-evaluation of a variety of metrics to quantify how accurately they measure
the instruction-following abilities of LLMs. Our investigation is performed on
grounded query-based summarization by collecting a new short-form, real-world
dataset riSum, containing 300 document-instruction pairs with 3 answers each.
All 900 answers are rated by 3 human annotators. Using riSum, we analyze the
agreement between evaluation methods and human judgment. Finally, we propose
new LLM-based reference-free evaluation methods that improve upon established
baselines and perform on par with costly reference-based metrics that require
high-quality summaries." shape=box style=filled fillcolor="#006400"];    "Cedille: A large autoregressive French language model" [label="Cedille: A large autoregressive French language model\nScaling up the size and training of autoregressive language models has
enabled novel ways of solving Natural Language Processing tasks using zero-shot
and few-shot learning. While extreme-scale language models such as GPT-3 offer
multilingual capabilities, zero-shot learning for languages other than English
remain largely unexplored. Here, we introduce Cedille, a large open source
auto-regressive language model, specifically trained for the French language.
Our results show that Cedille outperforms existing French language models and
is competitive with GPT-3 on a range of French zero-shot benchmarks.
Furthermore, we provide an in-depth comparison of the toxicity exhibited by
these models, showing that Cedille marks an improvement in language model
safety thanks to dataset filtering." shape=box style=filled fillcolor="#006400"];    "SeDi-Instruct: Enhancing Alignment of Language Models through Self-Directed Instruction Generation" [label="SeDi-Instruct: Enhancing Alignment of Language Models through Self-Directed Instruction Generation\nThe rapid evolution of Large Language Models (LLMs) has enabled the industry
to develop various AI-based services. Instruction tuning is considered
essential in adapting foundation models for target domains to provide
high-quality services to customers. A key challenge in instruction tuning is
obtaining high-quality instruction data. Self-Instruct, which automatically
generates instruction data using ChatGPT APIs, alleviates the data scarcity
problem. To improve the quality of instruction data, Self-Instruct discards
many of the instructions generated from ChatGPT, even though it is inefficient
in terms of cost owing to many useless API calls. To generate high-quality
instruction data at a low cost, we propose a novel data generation framework,
Self-Direct Instruction generation (SeDi-Instruct), which employs
diversity-based filtering and iterative feedback task generation.
Diversity-based filtering maintains model accuracy without excessively
discarding low-quality generated instructions by enhancing the diversity of
instructions in a batch. This reduces the cost of synthesizing instruction
data. The iterative feedback task generation integrates instruction generation
and training tasks and utilizes information obtained during the training to
create high-quality instruction sets. Our results show that SeDi-Instruct
enhances the accuracy of AI models by 5.2%, compared with traditional methods,
while reducing data generation costs by 36%." shape=box style=filled fillcolor="#006400"];    "STRUDEL: Structured Dialogue Summarization for Dialogue Comprehension" [label="STRUDEL: Structured Dialogue Summarization for Dialogue Comprehension\nAbstractive dialogue summarization has long been viewed as an important
standalone task in natural language processing, but no previous work has
explored the possibility of whether abstractive dialogue summarization can also
be used as a means to boost an NLP system's performance on other important
dialogue comprehension tasks. In this paper, we propose a novel type of
dialogue summarization task - STRUctured DiaLoguE Summarization - that can help
pre-trained language models to better understand dialogues and improve their
performance on important dialogue comprehension tasks. We further collect human
annotations of STRUDEL summaries over 400 dialogues and introduce a new STRUDEL
dialogue comprehension modeling framework that integrates STRUDEL into a
graph-neural-network-based dialogue reasoning module over transformer encoder
language models to improve their dialogue comprehension abilities. In our
empirical experiments on two important downstream dialogue comprehension tasks
- dialogue question answering and dialogue response prediction - we show that
our STRUDEL dialogue comprehension model can significantly improve the dialogue
comprehension performance of transformer encoder language models." shape=box style=filled fillcolor="#006400"];    "Computational Models of Tutor Feedback in Language Acquisition" [label="Computational Models of Tutor Feedback in Language Acquisition\nThis paper investigates the role of tutor feedback in language learning using
computational models. We compare two dominant paradigms in language learning:
interactive learning and cross-situational learning - which differ primarily in
the role of social feedback such as gaze or pointing. We analyze the
relationship between these two paradigms and propose a new mixed paradigm that
combines the two paradigms and allows to test algorithms in experiments that
combine no feedback and social feedback. To deal with mixed feedback
experiments, we develop new algorithms and show how they perform with respect
to traditional knn and prototype approaches." shape=box style=filled fillcolor="#98FB98"];    "Contextual Skipgram: Training Word Representation Using Context Information" [label="Contextual Skipgram: Training Word Representation Using Context Information\nThe skip-gram (SG) model learns word representation by predicting the words
surrounding a center word from unstructured text data. However, not all words
in the context window contribute to the meaning of the center word. For
example, less relevant words could be in the context window, hindering the SG
model from learning a better quality representation. In this paper, we propose
an enhanced version of the SG that leverages context information to produce
word representation. The proposed model, Contextual Skip-gram, is designed to
predict contextual words with both the center words and the context
information. This simple idea helps to reduce the impact of irrelevant words on
the training process, thus enhancing the final performance" shape=box style=filled fillcolor="#006400"];    "Word Embedding with Neural Probabilistic Prior" [label="Word Embedding with Neural Probabilistic Prior\nTo improve word representation learning, we propose a probabilistic prior
which can be seamlessly integrated with word embedding models. Different from
previous methods, word embedding is taken as a probabilistic generative model,
and it enables us to impose a prior regularizing word representation learning.
The proposed prior not only enhances the representation of embedding vectors
but also improves the model's robustness and stability. The structure of the
proposed prior is simple and effective, and it can be easily implemented and
flexibly plugged in most existing word embedding models. Extensive experiments
show the proposed method improves word representation on various tasks." shape=box style=filled fillcolor="#006400"];    "Quantum-inspired Complex Word Embedding" [label="Quantum-inspired Complex Word Embedding\nA challenging task for word embeddings is to capture the emergent meaning or
polarity of a combination of individual words. For example, existing approaches
in word embeddings will assign high probabilities to the words "Penguin" and
"Fly" if they frequently co-occur, but it fails to capture the fact that they
occur in an opposite sense - Penguins do not fly. We hypothesize that humans do
not associate a single polarity or sentiment to each word. The word contributes
to the overall polarity of a combination of words depending upon which other
words it is combined with. This is analogous to the behavior of microscopic
particles which exist in all possible states at the same time and interfere
with each other to give rise to new states depending upon their relative
phases. We make use of the Hilbert Space representation of such particles in
Quantum Mechanics where we subscribe a relative phase to each word, which is a
complex number, and investigate two such quantum inspired models to derive the
meaning of a combination of words. The proposed models achieve better
performances than state-of-the-art non-quantum models on the binary sentence
classification task." shape=box style=filled fillcolor="#006400"];    "Text Classification based on Word Subspace with Term-Frequency" [label="Text Classification based on Word Subspace with Term-Frequency\nText classification has become indispensable due to the rapid increase of
text in digital form. Over the past three decades, efforts have been made to
approach this task using various learning algorithms and statistical models
based on bag-of-words (BOW) features. Despite its simple implementation, BOW
features lack semantic meaning representation. To solve this problem, neural
networks started to be employed to learn word vectors, such as the word2vec.
Word2vec embeds word semantic structure into vectors, where the angle between
vectors indicates the meaningful similarity between words. To measure the
similarity between texts, we propose the novel concept of word subspace, which
can represent the intrinsic variability of features in a set of word vectors.
Through this concept, it is possible to model text from word vectors while
holding semantic information. To incorporate the word frequency directly in the
subspace model, we further extend the word subspace to the term-frequency (TF)
weighted word subspace. Based on these new concepts, text classification can be
performed under the mutual subspace method (MSM) framework. The validity of our
modeling is shown through experiments on the Reuters text database, comparing
the results to various state-of-art algorithms." shape=box style=filled fillcolor="#006400"];    "Deep Learning for Prediction and Classifying the Dynamical behaviour of Piecewise Smooth Maps" [label="Deep Learning for Prediction and Classifying the Dynamical behaviour of Piecewise Smooth Maps\nThis paper explores the prediction of the dynamics of piecewise smooth maps
using various deep learning models. We have shown various novel ways of
predicting the dynamics of piecewise smooth maps using deep learning models.
Moreover, we have used machine learning models such as Decision Tree
Classifier, Logistic Regression, K-Nearest Neighbor, Random Forest, and Support
Vector Machine for predicting the border collision bifurcation in the 1D normal
form map and the 1D tent map. Further, we classified the regular and chaotic
behaviour of the 1D tent map and the 2D Lozi map using deep learning models
like Convolutional Neural Network (CNN), ResNet50, and ConvLSTM via cobweb
diagram and phase portraits. We also classified the chaotic and hyperchaotic
behaviour of the 3D piecewise smooth map using deep learning models such as the
Feed Forward Neural Network (FNN), Long Short-Term Memory (LSTM), and Recurrent
Neural Network (RNN). Finally, deep learning models such as Long Short-Term
Memory (LSTM) and Recurrent Neural Network (RNN) are used for reconstructing
the two parametric charts of 2D border collision bifurcation normal form map." shape=box style=filled fillcolor="#006400"];    "Detoxifying Language Models Risks Marginalizing Minority Voices" [label="Detoxifying Language Models Risks Marginalizing Minority Voices\nLanguage models (LMs) must be both safe and equitable to be responsibly
deployed in practice. With safety in mind, numerous detoxification techniques
(e.g., Dathathri et al. 2020; Krause et al. 2020) have been proposed to
mitigate toxic LM generations. In this work, we show that current
detoxification techniques hurt equity: they decrease the utility of LMs on
language used by marginalized groups (e.g., African-American English and
minority identity mentions). In particular, we perform automatic and human
evaluations of text generation quality when LMs are conditioned on inputs with
different dialects and group identifiers. We find that detoxification makes LMs
more brittle to distribution shift, especially on language used by marginalized
groups. We identify that these failures stem from detoxification methods
exploiting spurious correlations in toxicity datasets. Overall, our results
highlight the tension between the controllability and distributional robustness
of LMs." shape=box style=filled fillcolor="#98FB98"];    "RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models" [label="RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models\nPretrained neural language models (LMs) are prone to generating racist,
sexist, or otherwise toxic language which hinders their safe deployment. We
investigate the extent to which pretrained LMs can be prompted to generate
toxic language, and the effectiveness of controllable text generation
algorithms at preventing such toxic degeneration. We create and release
RealToxicityPrompts, a dataset of 100K naturally occurring, sentence-level
prompts derived from a large corpus of English web text, paired with toxicity
scores from a widely-used toxicity classifier. Using RealToxicityPrompts, we
find that pretrained LMs can degenerate into toxic text even from seemingly
innocuous prompts. We empirically assess several controllable generation
methods, and find that while data- or compute-intensive methods (e.g., adaptive
pretraining on non-toxic data) are more effective at steering away from
toxicity than simpler solutions (e.g., banning "bad" words), no current method
is failsafe against neural toxic degeneration. To pinpoint the potential cause
of such persistent toxic degeneration, we analyze two web text corpora used to
pretrain several LMs (including GPT-2; Radford et. al, 2019), and find a
significant amount of offensive, factually unreliable, and otherwise toxic
content. Our work provides a test bed for evaluating toxic generations by LMs
and stresses the need for better data selection processes for pretraining." shape=box style=filled fillcolor="#006400"];    "Challenges in Automated Debiasing for Toxic Language Detection" [label="Challenges in Automated Debiasing for Toxic Language Detection\nBiased associations have been a challenge in the development of classifiers
for detecting toxic language, hindering both fairness and accuracy. As
potential solutions, we investigate recently introduced debiasing methods for
text classification datasets and models, as applied to toxic language
detection. Our focus is on lexical (e.g., swear words, slurs, identity
mentions) and dialectal markers (specifically African American English). Our
comprehensive experiments establish that existing methods are limited in their
ability to prevent biased behavior in current toxicity detectors. We then
propose an automatic, dialect-aware data correction method, as a
proof-of-concept. Despite the use of synthetic labels, this method reduces
dialectal associations with toxicity. Overall, our findings show that debiasing
a model trained on biased toxic language data is not as effective as simply
relabeling the data to remove existing biases." shape=box style=filled fillcolor="#006400"];    "A Plug-and-Play Method for Controlled Text Generation" [label="A Plug-and-Play Method for Controlled Text Generation\nLarge pre-trained language models have repeatedly shown their ability to
produce fluent text. Yet even when starting from a prompt, generation can
continue in many plausible directions. Current decoding methods with the goal
of controlling generation, e.g., to ensure specific words are included, either
require additional models or fine-tuning, or work poorly when the task at hand
is semantically unconstrained, e.g., story generation. In this work, we present
a plug-and-play decoding method for controlled language generation that is so
simple and intuitive, it can be described in a single sentence: given a topic
or keyword, we add a shift to the probability distribution over our vocabulary
towards semantically similar words. We show how annealing this distribution can
be used to impose hard constraints on language generation, something no other
plug-and-play method is currently able to do with SOTA language generators.
Despite the simplicity of this approach, we see it works incredibly well in
practice: decoding from GPT-2 leads to diverse and fluent sentences while
guaranteeing the appearance of given guide words. We perform two user studies,
revealing that (1) our method outperforms competing methods in human
evaluations; and (2) forcing the guide words to appear in the generated text
has no impact on the fluency of the generated text." shape=box style=filled fillcolor="#006400"];    "Detecting Unintended Social Bias in Toxic Language Datasets" [label="Detecting Unintended Social Bias in Toxic Language Datasets\nWith the rise of online hate speech, automatic detection of Hate Speech,
Offensive texts as a natural language processing task is getting popular.
However, very little research has been done to detect unintended social bias
from these toxic language datasets. This paper introduces a new dataset
ToxicBias curated from the existing dataset of Kaggle competition named "Jigsaw
Unintended Bias in Toxicity Classification". We aim to detect social biases,
their categories, and targeted groups. The dataset contains instances annotated
for five different bias categories, viz., gender, race/ethnicity, religion,
political, and LGBTQ. We train transformer-based models using our curated
datasets and report baseline performance for bias identification, target
generation, and bias implications. Model biases and their mitigation are also
discussed in detail. Our study motivates a systematic extraction of social bias
data from toxic language datasets. All the codes and dataset used for
experiments in this work are publicly available" shape=box style=filled fillcolor="#006400"];    "Lost in Moderation: How Commercial Content Moderation APIs Over- and Under-Moderate Group-Targeted Hate Speech and Linguistic Variations" [label="Lost in Moderation: How Commercial Content Moderation APIs Over- and Under-Moderate Group-Targeted Hate Speech and Linguistic Variations\nCommercial content moderation APIs are marketed as scalable solutions to
combat online hate speech. However, the reliance on these APIs risks both
silencing legitimate speech, called over-moderation, and failing to protect
online platforms from harmful speech, known as under-moderation. To assess such
risks, this paper introduces a framework for auditing black-box NLP systems.
Using the framework, we systematically evaluate five widely used commercial
content moderation APIs. Analyzing five million queries based on four datasets,
we find that APIs frequently rely on group identity terms, such as ``black'',
to predict hate speech. While OpenAI's and Amazon's services perform slightly
better, all providers under-moderate implicit hate speech, which uses codified
messages, especially against LGBTQIA+ individuals. Simultaneously, they
over-moderate counter-speech, reclaimed slurs and content related to Black,
LGBTQIA+, Jewish, and Muslim people. We recommend that API providers offer
better guidance on API implementation and threshold setting and more
transparency on their APIs' limitations.
  Warning: This paper contains offensive and hateful terms and concepts. We
have chosen to reproduce these terms for reasons of transparency." shape=box style=filled fillcolor="#006400"];    "Multilingual Machine Translation Systems from Microsoft for WMT21 Shared Task" [label="Multilingual Machine Translation Systems from Microsoft for WMT21 Shared Task\nThis report describes Microsoft's machine translation systems for the WMT21
shared task on large-scale multilingual machine translation. We participated in
all three evaluation tracks including Large Track and two Small Tracks where
the former one is unconstrained and the latter two are fully constrained. Our
model submissions to the shared task were initialized with
DeltaLM\footnote{\url{https://aka.ms/deltalm}}, a generic pre-trained
multilingual encoder-decoder model, and fine-tuned correspondingly with the
vast collected parallel data and allowed data sources according to track
settings, together with applying progressive learning and iterative
back-translation approaches to further improve the performance. Our final
submissions ranked first on three tracks in terms of the automatic evaluation
metric." shape=box style=filled fillcolor="#98FB98"];    "A Comprehensive Survey of Multilingual Neural Machine Translation" [label="A Comprehensive Survey of Multilingual Neural Machine Translation\nWe present a survey on multilingual neural machine translation (MNMT), which
has gained a lot of traction in the recent years. MNMT has been useful in
improving translation quality as a result of translation knowledge transfer
(transfer learning). MNMT is more promising and interesting than its
statistical machine translation counterpart because end-to-end modeling and
distributed representations open new avenues for research on machine
translation. Many approaches have been proposed in order to exploit
multilingual parallel corpora for improving translation quality. However, the
lack of a comprehensive survey makes it difficult to determine which approaches
are promising and hence deserve further exploration. In this paper, we present
an in-depth survey of existing literature on MNMT. We first categorize various
approaches based on their central use-case and then further categorize them
based on resource scenarios, underlying modeling principles, core-issues and
challenges. Wherever possible we address the strengths and weaknesses of
several techniques by comparing them with each other. We also discuss the
future directions that MNMT research might take. This paper is aimed towards
both, beginners and experts in NMT. We hope this paper will serve as a starting
point as well as a source of new ideas for researchers and engineers interested
in MNMT." shape=box style=filled fillcolor="#006400"];    "The FLORES-101 Evaluation Benchmark for Low-Resource and Multilingual Machine Translation" [label="The FLORES-101 Evaluation Benchmark for Low-Resource and Multilingual Machine Translation\nOne of the biggest challenges hindering progress in low-resource and
multilingual machine translation is the lack of good evaluation benchmarks.
Current evaluation benchmarks either lack good coverage of low-resource
languages, consider only restricted domains, or are low quality because they
are constructed using semi-automatic procedures. In this work, we introduce the
FLORES-101 evaluation benchmark, consisting of 3001 sentences extracted from
English Wikipedia and covering a variety of different topics and domains. These
sentences have been translated in 101 languages by professional translators
through a carefully controlled process. The resulting dataset enables better
assessment of model quality on the long tail of low-resource languages,
including the evaluation of many-to-many multilingual translation systems, as
all translations are multilingually aligned. By publicly releasing such a
high-quality and high-coverage dataset, we hope to foster progress in the
machine translation community and beyond." shape=box style=filled fillcolor="#006400"];    "Learning Policies for Multilingual Training of Neural Machine Translation Systems" [label="Learning Policies for Multilingual Training of Neural Machine Translation Systems\nLow-resource Multilingual Neural Machine Translation (MNMT) is typically
tasked with improving the translation performance on one or more language pairs
with the aid of high-resource language pairs. In this paper, we propose two
simple search based curricula -- orderings of the multilingual training data --
which help improve translation performance in conjunction with existing
techniques such as fine-tuning. Additionally, we attempt to learn a curriculum
for MNMT from scratch jointly with the training of the translation system with
the aid of contextual multi-arm bandits. We show on the FLORES low-resource
translation dataset that these learned curricula can provide better starting
points for fine tuning and improve overall performance of the translation
system." shape=box style=filled fillcolor="#006400"];    "Massively Multilingual Neural Machine Translation" [label="Massively Multilingual Neural Machine Translation\nMultilingual neural machine translation (NMT) enables training a single model
that supports translation from multiple source languages into multiple target
languages. In this paper, we push the limits of multilingual NMT in terms of
number of languages being used. We perform extensive experiments in training
massively multilingual NMT models, translating up to 102 languages to and from
English within a single model. We explore different setups for training such
models and analyze the trade-offs between translation quality and various
modeling decisions. We report results on the publicly available TED talks
multilingual corpus where we show that massively multilingual many-to-many
models are effective in low resource settings, outperforming the previous
state-of-the-art while supporting up to 59 languages. Our experiments on a
large-scale dataset with 102 languages to and from English and up to one
million examples per direction also show promising results, surpassing strong
bilingual baselines and encouraging future work on massively multilingual NMT." shape=box style=filled fillcolor="#006400"];    "Investigating Bias in LLM-Based Bias Detection: Disparities between LLMs and Human Perception" [label="Investigating Bias in LLM-Based Bias Detection: Disparities between LLMs and Human Perception\nThe pervasive spread of misinformation and disinformation in social media
underscores the critical importance of detecting media bias. While robust Large
Language Models (LLMs) have emerged as foundational tools for bias prediction,
concerns about inherent biases within these models persist. In this work, we
investigate the presence and nature of bias within LLMs and its consequential
impact on media bias detection. Departing from conventional approaches that
focus solely on bias detection in media content, we delve into biases within
the LLM systems themselves. Through meticulous examination, we probe whether
LLMs exhibit biases, particularly in political bias prediction and text
continuation tasks. Additionally, we explore bias across diverse topics, aiming
to uncover nuanced variations in bias expression within the LLM framework.
Importantly, we propose debiasing strategies, including prompt engineering and
model fine-tuning. Extensive analysis of bias tendencies across different LLMs
sheds light on the broader landscape of bias propagation in language models.
This study advances our understanding of LLM bias, offering critical insights
into its implications for bias detection tasks and paving the way for more
robust and equitable AI systems" shape=box style=filled fillcolor="#98FB98"];    "Fairness in Large Language Models: A Taxonomic Survey" [label="Fairness in Large Language Models: A Taxonomic Survey\nLarge Language Models (LLMs) have demonstrated remarkable success across
various domains. However, despite their promising performance in numerous
real-world applications, most of these algorithms lack fairness considerations.
Consequently, they may lead to discriminatory outcomes against certain
communities, particularly marginalized populations, prompting extensive study
in fair LLMs. On the other hand, fairness in LLMs, in contrast to fairness in
traditional machine learning, entails exclusive backgrounds, taxonomies, and
fulfillment techniques. To this end, this survey presents a comprehensive
overview of recent advances in the existing literature concerning fair LLMs.
Specifically, a brief introduction to LLMs is provided, followed by an analysis
of factors contributing to bias in LLMs. Additionally, the concept of fairness
in LLMs is discussed categorically, summarizing metrics for evaluating bias in
LLMs and existing algorithms for promoting fairness. Furthermore, resources for
evaluating bias in LLMs, including toolkits and datasets, are summarized.
Finally, existing research challenges and open questions are discussed." shape=box style=filled fillcolor="#006400"];    "BiasEdit: Debiasing Stereotyped Language Models via Model Editing" [label="BiasEdit: Debiasing Stereotyped Language Models via Model Editing\nPrevious studies have established that language models manifest stereotyped
biases. Existing debiasing strategies, such as retraining a model with
counterfactual data, representation projection, and prompting often fail to
efficiently eliminate bias or directly alter the models' biased internal
representations. To address these issues, we propose BiasEdit, an efficient
model editing method to remove stereotypical bias from language models through
lightweight networks that act as editors to generate parameter updates.
BiasEdit employs a debiasing loss guiding editor networks to conduct local
edits on partial parameters of a language model for debiasing while preserving
the language modeling abilities during editing through a retention loss.
Experiments on StereoSet and Crows-Pairs demonstrate the effectiveness,
efficiency, and robustness of BiasEdit in eliminating bias compared to
tangental debiasing baselines and little to no impact on the language models'
general capabilities. In addition, we conduct bias tracing to probe bias in
various modules and explore bias editing impacts on different components of
language models." shape=box style=filled fillcolor="#006400"];    "Detecting Linguistic Indicators for Stereotype Assessment with Large Language Models" [label="Detecting Linguistic Indicators for Stereotype Assessment with Large Language Models\nSocial categories and stereotypes are embedded in language and can introduce
data bias into Large Language Models (LLMs). Despite safeguards, these biases
often persist in model behavior, potentially leading to representational harm
in outputs. While sociolinguistic research provides valuable insights into the
formation of stereotypes, NLP approaches for stereotype detection rarely draw
on this foundation and often lack objectivity, precision, and interpretability.
To fill this gap, in this work we propose a new approach that detects and
quantifies the linguistic indicators of stereotypes in a sentence. We derive
linguistic indicators from the Social Category and Stereotype Communication
(SCSC) framework which indicate strong social category formulation and
stereotyping in language, and use them to build a categorization scheme. To
automate this approach, we instruct different LLMs using in-context learning to
apply the approach to a sentence, where the LLM examines the linguistic
properties and provides a basis for a fine-grained assessment. Based on an
empirical evaluation of the importance of different linguistic indicators, we
learn a scoring function that measures the linguistic indicators of a
stereotype. Our annotations of stereotyped sentences show that these indicators
are present in these sentences and explain the strength of a stereotype. In
terms of model performance, our results show that the models generally perform
well in detecting and classifying linguistic indicators of category labels used
to denote a category, but sometimes struggle to correctly evaluate the
associated behaviors and characteristics. Using more few-shot examples within
the prompts, significantly improves performance. Model performance increases
with size, as Llama-3.3-70B-Instruct and GPT-4 achieve comparable results that
surpass those of Mixtral-8x7B-Instruct, GPT-4-mini and Llama-3.1-8B-Instruct." shape=box style=filled fillcolor="#006400"];    "How Far Can It Go?: On Intrinsic Gender Bias Mitigation for Text Classification" [label="How Far Can It Go?: On Intrinsic Gender Bias Mitigation for Text Classification\nTo mitigate gender bias in contextualized language models, different
intrinsic mitigation strategies have been proposed, alongside many bias
metrics. Considering that the end use of these language models is for
downstream tasks like text classification, it is important to understand how
these intrinsic bias mitigation strategies actually translate to fairness in
downstream tasks and the extent of this. In this work, we design a probe to
investigate the effects that some of the major intrinsic gender bias mitigation
strategies have on downstream text classification tasks. We discover that
instead of resolving gender bias, intrinsic mitigation techniques and metrics
are able to hide it in such a way that significant gender information is
retained in the embeddings. Furthermore, we show that each mitigation technique
is able to hide the bias from some of the intrinsic bias measures but not all,
and each intrinsic bias measure can be fooled by some mitigation techniques,
but not all. We confirm experimentally, that none of the intrinsic mitigation
techniques used without any other fairness intervention is able to consistently
impact extrinsic bias. We recommend that intrinsic bias mitigation techniques
should be combined with other fairness interventions for downstream tasks." shape=box style=filled fillcolor="#006400"];    "We Can Detect Your Bias: Predicting the Political Ideology of News Articles" [label="We Can Detect Your Bias: Predicting the Political Ideology of News Articles\nWe explore the task of predicting the leading political ideology or bias of
news articles. First, we collect and release a large dataset of 34,737 articles
that were manually annotated for political ideology -left, center, or right-,
which is well-balanced across both topics and media. We further use a
challenging experimental setup where the test examples come from media that
were not seen during training, which prevents the model from learning to detect
the source of the target news article instead of predicting its political
ideology. From a modeling perspective, we propose an adversarial media
adaptation, as well as a specially adapted triplet loss. We further add
background information about the source, and we show that it is quite helpful
for improving article-level prediction. Our experimental results show very
sizable improvements over using state-of-the-art pre-trained Transformers in
this challenging setup." shape=box style=filled fillcolor="#006400"];    "RadOnc-GPT: A Large Language Model for Radiation Oncology" [label="RadOnc-GPT: A Large Language Model for Radiation Oncology\nThis paper presents RadOnc-GPT, a large language model specialized for
radiation oncology through advanced tuning methods. RadOnc-GPT was finetuned on
a large dataset of radiation oncology patient records from the Mayo Clinic in
Arizona. The model employs instruction tuning on three key tasks - generating
radiotherapy treatment regimens, determining optimal radiation modalities, and
providing diagnostic descriptions/ICD codes based on patient diagnostic
details. Evaluations conducted by comparing RadOnc-GPT outputs to general large
language model outputs showed higher ROUGE scores in these three tasks. The
study demonstrated the potential of using large language models fine-tuned
using domain-specific knowledge like RadOnc-GPT to achieve transformational
capabilities in highly specialized healthcare fields such as radiation
oncology. However, our model's clinical relevance requires confirmation, and it
specializes in only the aforementioned three specific tasks and lacks broader
applicability. Furthermore, its evaluation through ROUGE scores might not
reflect the true semantic and clinical accuracy - challenges we intend to
address in future research." shape=box style=filled fillcolor="#32CD32"];    "Benchmarking ChatGPT-4 on ACR Radiation Oncology In-Training (TXIT) Exam and Red Journal Gray Zone Cases: Potentials and Challenges for AI-Assisted Medical Education and Decision Making in Radiation Oncology" [label="Benchmarking ChatGPT-4 on ACR Radiation Oncology In-Training (TXIT) Exam and Red Journal Gray Zone Cases: Potentials and Challenges for AI-Assisted Medical Education and Decision Making in Radiation Oncology\nThe potential of large language models in medicine for education and decision
making purposes has been demonstrated as they achieve decent scores on medical
exams such as the United States Medical Licensing Exam (USMLE) and the MedQA
exam. In this work, we evaluate the performance of ChatGPT-4 in the specialized
field of radiation oncology using the 38th American College of Radiology (ACR)
radiation oncology in-training (TXIT) exam and the 2022 Red Journal Gray Zone
cases. For the TXIT exam, ChatGPT-3.5 and ChatGPT-4 have achieved the scores of
63.65% and 74.57%, respectively, highlighting the advantage of the latest
ChatGPT-4 model. Based on the TXIT exam, ChatGPT-4's strong and weak areas in
radiation oncology are identified to some extent. Specifically, ChatGPT-4
demonstrates better knowledge of statistics, CNS & eye, pediatrics, biology,
and physics than knowledge of bone & soft tissue and gynecology, as per the ACR
knowledge domain. Regarding clinical care paths, ChatGPT-4 performs better in
diagnosis, prognosis, and toxicity than brachytherapy and dosimetry. It lacks
proficiency in in-depth details of clinical trials. For the Gray Zone cases,
ChatGPT-4 is able to suggest a personalized treatment approach to each case
with high correctness and comprehensiveness. Importantly, it provides novel
treatment aspects for many cases, which are not suggested by any human experts.
Both evaluations demonstrate the potential of ChatGPT-4 in medical education
for the general public and cancer patients, as well as the potential to aid
clinical decision-making, while acknowledging its limitations in certain
domains. Because of the risk of hallucination, facts provided by ChatGPT always
need to be verified." shape=box style=filled fillcolor="#98FB98"];    "GPT4Vis: What Can GPT-4 Do for Zero-shot Visual Recognition?" [label="GPT4Vis: What Can GPT-4 Do for Zero-shot Visual Recognition?\nThis paper does not present a novel method. Instead, it delves into an
essential, yet must-know baseline in light of the latest advancements in
Generative Artificial Intelligence (GenAI): the utilization of GPT-4 for visual
understanding. Our study centers on the evaluation of GPT-4's linguistic and
visual capabilities in zero-shot visual recognition tasks: Firstly, we explore
the potential of its generated rich textual descriptions across various
categories to enhance recognition performance without any training. Secondly,
we evaluate GPT-4's visual proficiency in directly recognizing diverse visual
content. We conducted extensive experiments to systematically evaluate GPT-4's
performance across images, videos, and point clouds, using 16 benchmark
datasets to measure top-1 and top-5 accuracy. Our findings show that GPT-4,
enhanced with rich linguistic descriptions, significantly improves zero-shot
recognition, offering an average top-1 accuracy increase of 7% across all
datasets. GPT-4 excels in visual recognition, outshining OpenAI-CLIP's ViT-L
and rivaling EVA-CLIP's ViT-E, particularly in video datasets HMDB-51 and
UCF-101, where it leads by 22% and 9%, respectively. We hope this research
contributes valuable data points and experience for future studies. We release
our code at https://github.com/whwu95/GPT4Vis." shape=box style=filled fillcolor="#006400"];    "Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model" [label="Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model\nPretrained general-purpose language models can achieve state-of-the-art
accuracies in various natural language processing domains by adapting to
downstream tasks via zero-shot, few-shot and fine-tuning techniques. Because of
their success, the size of these models has increased rapidly, requiring
high-performance hardware, software, and algorithmic techniques to enable
training such large models. As the result of a joint effort between Microsoft
and NVIDIA, we present details on the training of the largest monolithic
transformer based language model, Megatron-Turing NLG 530B (MT-NLG), with 530
billion parameters. In this paper, we first focus on the infrastructure as well
as the 3D parallelism methodology used to train this model using DeepSpeed and
Megatron. Next, we detail the training process, the design of our training
corpus, and our data curation techniques, which we believe is a key ingredient
to the success of the model. Finally, we discuss various evaluation results, as
well as other interesting observations and new properties exhibited by MT-NLG.
We demonstrate that MT-NLG achieves superior zero-, one-, and few-shot learning
accuracies on several NLP benchmarks and establishes new state-of-the-art
results. We believe that our contributions will help further the development of
large-scale training infrastructures, large-scale language models, and natural
language generations." shape=box style=filled fillcolor="#006400"];    "ED-SAM: An Efficient Diffusion Sampling Approach to Domain Generalization in Vision-Language Foundation Models" [label="ED-SAM: An Efficient Diffusion Sampling Approach to Domain Generalization in Vision-Language Foundation Models\nThe Vision-Language Foundation Model has recently shown outstanding
performance in various perception learning tasks. The outstanding performance
of the vision-language model mainly relies on large-scale pre-training datasets
and different data augmentation techniques. However, the domain generalization
problem of the vision-language foundation model needs to be addressed. This
problem has limited the generalizability of the vision-language foundation
model to unknown data distributions. In this paper, we introduce a new simple
but efficient Diffusion Sampling approach to Domain Generalization (ED-SAM) to
improve the generalizability of the vision-language foundation model. Our
theoretical analysis in this work reveals the critical role and relation of the
diffusion model to domain generalization in the vision-language foundation
model. Then, based on the insightful analysis, we introduce a new simple yet
effective Transport Transformation to diffusion sampling method. It can
effectively generate adversarial samples to improve the generalizability of the
foundation model against unknown data distributions. The experimental results
on different scales of vision-language pre-training datasets, including CC3M,
CC12M, and LAION400M, have consistently shown State-of-the-Art performance and
scalability of the proposed ED-SAM approach compared to the other recent
methods." shape=box style=filled fillcolor="#006400"];    "LMTurk: Few-Shot Learners as Crowdsourcing Workers in a Language-Model-as-a-Service Framework" [label="LMTurk: Few-Shot Learners as Crowdsourcing Workers in a Language-Model-as-a-Service Framework\nVast efforts have been devoted to creating high-performance few-shot
learners, i.e., large-scale pretrained language models (PLMs) that perform well
with little downstream task training data. Training PLMs has incurred
significant cost, but utilizing the few-shot learners is still challenging due
to their enormous size. This work focuses on a crucial question: How to make
effective use of these few-shot learners? We propose LMTurk, a novel approach
that treats few-shot learners as crowdsourcing workers. The rationale is that
crowdsourcing workers are in fact few-shot learners: They are shown a few
illustrative examples to learn about a task and then start annotating. LMTurk
employs few-shot learners built upon PLMs as workers. We show that the
resulting annotations can be utilized to train models that solve the task well
and are small enough to be deployable in practical scenarios. Active learning
is integrated into LMTurk to reduce the amount of queries made to PLMs,
minimizing the computational cost of running PLM inference passes. Altogether,
LMTurk is an important step towards making effective use of current PLMs." shape=box style=filled fillcolor="#006400"];    "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models" [label="Chain-of-Thought Prompting Elicits Reasoning in Large Language Models\nWe explore how generating a chain of thought -- a series of intermediate
reasoning steps -- significantly improves the ability of large language models
to perform complex reasoning. In particular, we show how such reasoning
abilities emerge naturally in sufficiently large language models via a simple
method called chain of thought prompting, where a few chain of thought
demonstrations are provided as exemplars in prompting. Experiments on three
large language models show that chain of thought prompting improves performance
on a range of arithmetic, commonsense, and symbolic reasoning tasks. The
empirical gains can be striking. For instance, prompting a 540B-parameter
language model with just eight chain of thought exemplars achieves state of the
art accuracy on the GSM8K benchmark of math word problems, surpassing even
finetuned GPT-3 with a verifier." shape=box style=filled fillcolor="#006400"];    "Exploring the Capabilities and Limitations of Large Language Models for Radiation Oncology Decision Support" [label="Exploring the Capabilities and Limitations of Large Language Models for Radiation Oncology Decision Support\nThanks to the rapidly evolving integration of LLMs into decision-support
tools, a significant transformation is happening across large-scale systems.
Like other medical fields, the use of LLMs such as GPT-4 is gaining increasing
interest in radiation oncology as well. An attempt to assess GPT-4's
performance in radiation oncology was made via a dedicated 100-question
examination on the highly specialized topic of radiation oncology physics,
revealing GPT-4's superiority over other LLMs. GPT-4's performance on a broader
field of clinical radiation oncology is further benchmarked by the ACR
Radiation Oncology In-Training (TXIT) exam where GPT-4 achieved a high accuracy
of 74.57%. Its performance on re-labelling structure names in accordance with
the AAPM TG-263 report has also been benchmarked, achieving above 96%
accuracies. Such studies shed light on the potential of LLMs in radiation
oncology. As interest in the potential and constraints of LLMs in general
healthcare applications continues to rise5, the capabilities and limitations of
LLMs in radiation oncology decision support have not yet been fully explored." shape=box style=filled fillcolor="#98FB98"];    "AI as a Medical Ally: Evaluating ChatGPT's Usage and Impact in Indian Healthcare" [label="AI as a Medical Ally: Evaluating ChatGPT's Usage and Impact in Indian Healthcare\nThis study investigates the integration and impact of Large Language Models
(LLMs), like ChatGPT, in India's healthcare sector. Our research employs a dual
approach, engaging both general users and medical professionals through surveys
and interviews respectively. Our findings reveal that healthcare professionals
value ChatGPT in medical education and preliminary clinical settings, but
exercise caution due to concerns about reliability, privacy, and the need for
cross-verification with medical references. General users show a preference for
AI interactions in healthcare, but concerns regarding accuracy and trust
persist. The study underscores the need for these technologies to complement,
not replace, human medical expertise, highlighting the importance of developing
LLMs in collaboration with healthcare providers. This paper enhances the
understanding of LLMs in healthcare, detailing current usage, user trust, and
improvement areas. Our insights inform future research and development,
underscoring the need for ethically compliant, user-focused LLM advancements
that address healthcare-specific challenges." shape=box style=filled fillcolor="#006400"];    "Enhancing Adversarial Attacks through Chain of Thought" [label="Enhancing Adversarial Attacks through Chain of Thought\nLarge language models (LLMs) have demonstrated impressive performance across
various domains but remain susceptible to safety concerns. Prior research
indicates that gradient-based adversarial attacks are particularly effective
against aligned LLMs and the chain of thought (CoT) prompting can elicit
desired answers through step-by-step reasoning. This paper proposes enhancing
the robustness of adversarial attacks on aligned LLMs by integrating CoT
prompts with the greedy coordinate gradient (GCG) technique. Using CoT triggers
instead of affirmative targets stimulates the reasoning abilities of backend
LLMs, thereby improving the transferability and universality of adversarial
attacks. We conducted an ablation study comparing our CoT-GCG approach with
Amazon Web Services auto-cot. Results revealed our approach outperformed both
the baseline GCG attack and CoT prompting. Additionally, we used Llama Guard to
evaluate potentially harmful interactions, providing a more objective risk
assessment of entire conversations compared to matching outputs to rejection
phrases. The code of this paper is available at
https://github.com/sujingbo0217/CS222W24-LLM-Attack." shape=box style=filled fillcolor="#006400"];    "Poisoning Language Models During Instruction Tuning" [label="Poisoning Language Models During Instruction Tuning\nInstruction-tuned LMs such as ChatGPT, FLAN, and InstructGPT are finetuned on
datasets that contain user-submitted examples, e.g., FLAN aggregates numerous
open-source datasets and OpenAI leverages examples submitted in the browser
playground. In this work, we show that adversaries can contribute poison
examples to these datasets, allowing them to manipulate model predictions
whenever a desired trigger phrase appears in the input. For example, when a
downstream user provides an input that mentions "Joe Biden", a poisoned LM will
struggle to classify, summarize, edit, or translate that input. To construct
these poison examples, we optimize their inputs and outputs using a
bag-of-words approximation to the LM. We evaluate our method on open-source
instruction-tuned LMs. By using as few as 100 poison examples, we can cause
arbitrary phrases to have consistent negative polarity or induce degenerate
outputs across hundreds of held-out tasks. Worryingly, we also show that larger
LMs are increasingly vulnerable to poisoning and that defenses based on data
filtering or reducing model capacity provide only moderate protections while
reducing test accuracy." shape=box style=filled fillcolor="#006400"];    "ChatGPT Needs SPADE (Sustainability, PrivAcy, Digital divide, and Ethics) Evaluation: A Review" [label="ChatGPT Needs SPADE (Sustainability, PrivAcy, Digital divide, and Ethics) Evaluation: A Review\nChatGPT is another large language model (LLM) vastly available for the
consumers on their devices but due to its performance and ability to converse
effectively, it has gained a huge popularity amongst research as well as
industrial community. Recently, many studies have been published to show the
effectiveness, efficiency, integration, and sentiments of chatGPT and other
LLMs. In contrast, this study focuses on the important aspects that are mostly
overlooked, i.e. sustainability, privacy, digital divide, and ethics and
suggests that not only chatGPT but every subsequent entry in the category of
conversational bots should undergo Sustainability, PrivAcy, Digital divide, and
Ethics (SPADE) evaluation. This paper discusses in detail the issues and
concerns raised over chatGPT in line with aforementioned characteristics. We
also discuss the recent EU AI Act briefly in accordance with the SPADE
evaluation. We support our hypothesis by some preliminary data collection and
visualizations along with hypothesized facts. We also suggest mitigations and
recommendations for each of the concerns. Furthermore, we also suggest some
policies and recommendations for EU AI policy act concerning ethics, digital
divide, and sustainability" shape=box style=filled fillcolor="#006400"];    "GPT Store Mining and Analysis" [label="GPT Store Mining and Analysis\nAs a pivotal extension of the renowned ChatGPT, the GPT Store serves as a
dynamic marketplace for various Generative Pre-trained Transformer (GPT)
models, shaping the frontier of conversational AI. This paper presents an
in-depth measurement study of the GPT Store, with a focus on the categorization
of GPTs by topic, factors influencing GPT popularity, and the potential
security risks. Our investigation starts with assessing the categorization of
GPTs in the GPT Store, analyzing how they are organized by topics, and
evaluating the effectiveness of the classification system. We then examine the
factors that affect the popularity of specific GPTs, looking into user
preferences, algorithmic influences, and market trends. Finally, the study
delves into the security risks of the GPT Store, identifying potential threats
and evaluating the robustness of existing security measures. This study offers
a detailed overview of the GPT Store's current state, shedding light on its
operational dynamics and user interaction patterns. Our findings aim to enhance
understanding of the GPT ecosystem, providing valuable insights for future
research, development, and policy-making in generative AI." shape=box style=filled fillcolor="#006400"];    "Benchmarking a foundation LLM on its ability to re-label structure names in accordance with the AAPM TG-263 report" [label="Benchmarking a foundation LLM on its ability to re-label structure names in accordance with the AAPM TG-263 report\nPurpose: To introduce the concept of using large language models (LLMs) to
re-label structure names in accordance with the American Association of
Physicists in Medicine (AAPM) Task Group (TG)-263 standard, and to establish a
benchmark for future studies to reference.
  Methods and Materials: The Generative Pre-trained Transformer (GPT)-4
application programming interface (API) was implemented as a Digital Imaging
and Communications in Medicine (DICOM) storage server, which upon receiving a
structure set DICOM file, prompts GPT-4 to re-label the structure names of both
target volumes and normal tissues according to the AAPM TG-263. Three disease
sites, prostate, head and neck, and thorax were selected for evaluation. For
each disease site category, 150 patients were randomly selected for manually
tuning the instructions prompt (in batches of 50) and 50 patients were randomly
selected for evaluation. Structure names that were considered were those that
were most likely to be relevant for studies utilizing structure contours for
many patients.
  Results: The overall re-labeling accuracy of both target volumes and normal
tissues for prostate, head and neck, and thorax cases was 96.0%, 98.5%, and
96.9% respectively. Re-labeling of target volumes was less accurate on average
except for prostate - 100%, 93.1%, and 91.1% respectively.
  Conclusions: Given the accuracy of GPT-4 in re-labeling structure names of
both target volumes and normal tissues as presented in this work, LLMs are
poised to be the preferred method for standardizing structure names in
radiation oncology, especially considering the rapid advancements in LLM
capabilities that are likely to continue." shape=box style=filled fillcolor="#98FB98"];    "Exploring Federated Deep Learning for Standardising Naming Conventions in Radiotherapy Data" [label="Exploring Federated Deep Learning for Standardising Naming Conventions in Radiotherapy Data\nStandardising structure volume names in radiotherapy (RT) data is necessary
to enable data mining and analyses, especially across multi-institutional
centres. This process is time and resource intensive, which highlights the need
for new automated and efficient approaches to handle the task. Several machine
learning-based methods have been proposed and evaluated to standardise
nomenclature. However, no studies have considered that RT patient records are
distributed across multiple data centres. This paper introduces a method that
emulates real-world environments to establish standardised nomenclature. This
is achieved by integrating decentralised real-time data and federated learning
(FL). A multimodal deep artificial neural network was proposed to standardise
RT data in federated settings. Three types of possible attributes were
extracted from the structures to train the deep learning models: tabular,
visual, and volumetric. Simulated experiments were carried out to train the
models across several scenarios including multiple data centres, input
modalities, and aggregation strategies. The models were compared against models
developed with single modalities in federated settings, in addition to models
trained in centralised settings. Categorical classification accuracy was
calculated on hold-out samples to inform the models performance. Our results
highlight the need for fusing multiple modalities when training such models,
with better performance reported with tabular-volumetric models. In addition,
we report comparable accuracy compared to models built in centralised settings.
This demonstrates the suitability of FL for handling the standardization task.
Additional ablation analyses showed that the total number of samples in the
data centres and the number of data centres highly affects the training process
and should be carefully considered when building standardisation models." shape=box style=filled fillcolor="#006400"];    "Artificial General Intelligence for Radiation Oncology" [label="Artificial General Intelligence for Radiation Oncology\nThe emergence of artificial general intelligence (AGI) is transforming
radiation oncology. As prominent vanguards of AGI, large language models (LLMs)
such as GPT-4 and PaLM 2 can process extensive texts and large vision models
(LVMs) such as the Segment Anything Model (SAM) can process extensive imaging
data to enhance the efficiency and precision of radiation therapy. This paper
explores full-spectrum applications of AGI across radiation oncology including
initial consultation, simulation, treatment planning, treatment delivery,
treatment verification, and patient follow-up. The fusion of vision data with
LLMs also creates powerful multimodal models that elucidate nuanced clinical
patterns. Together, AGI promises to catalyze a shift towards data-driven,
personalized radiation therapy. However, these models should complement human
expertise and care. This paper provides an overview of how AGI can transform
radiation oncology to elevate the standard of patient care in radiation
oncology, with the key insight being AGI's ability to exploit multimodal
clinical data at scale." shape=box style=filled fillcolor="#006400"];    "Comparative Analysis of nnUNet and MedNeXt for Head and Neck Tumor Segmentation in MRI-guided Radiotherapy" [label="Comparative Analysis of nnUNet and MedNeXt for Head and Neck Tumor Segmentation in MRI-guided Radiotherapy\nRadiation therapy (RT) is essential in treating head and neck cancer (HNC),
with magnetic resonance imaging(MRI)-guided RT offering superior soft tissue
contrast and functional imaging. However, manual tumor segmentation is
time-consuming and complex, and therfore remains a challenge. In this study, we
present our solution as team TUMOR to the HNTS-MRG24 MICCAI Challenge which is
focused on automated segmentation of primary gross tumor volumes (GTVp) and
metastatic lymph node gross tumor volume (GTVn) in pre-RT and mid-RT MRI
images. We utilized the HNTS-MRG2024 dataset, which consists of 150 MRI scans
from patients diagnosed with HNC, including original and registered pre-RT and
mid-RT T2-weighted images with corresponding segmentation masks for GTVp and
GTVn. We employed two state-of-the-art models in deep learning, nnUNet and
MedNeXt. For Task 1, we pretrained models on pre-RT registered and mid-RT
images, followed by fine-tuning on original pre-RT images. For Task 2, we
combined registered pre-RT images, registered pre-RT segmentation masks, and
mid-RT data as a multi-channel input for training. Our solution for Task 1
achieved 1st place in the final test phase with an aggregated Dice Similarity
Coefficient of 0.8254, and our solution for Task 2 ranked 8th with a score of
0.7005. The proposed solution is publicly available at Github Repository." shape=box style=filled fillcolor="#006400"];    "Performance of ChatGPT on USMLE: Unlocking the Potential of Large Language Models for AI-Assisted Medical Education" [label="Performance of ChatGPT on USMLE: Unlocking the Potential of Large Language Models for AI-Assisted Medical Education\nArtificial intelligence is gaining traction in more ways than ever before.
The popularity of language models and AI-based businesses has soared since
ChatGPT was made available to the general public via OpenAI. It is becoming
increasingly common for people to use ChatGPT both professionally and
personally. Considering the widespread use of ChatGPT and the reliance people
place on it, this study determined how reliable ChatGPT can be for answering
complex medical and clinical questions. Harvard University gross anatomy along
with the United States Medical Licensing Examination (USMLE) questionnaire were
used to accomplish the objective. The paper evaluated the obtained results
using a 2-way ANOVA and posthoc analysis. Both showed systematic covariation
between format and prompt. Furthermore, the physician adjudicators
independently rated the outcome's accuracy, concordance, and insight. As a
result of the analysis, ChatGPT-generated answers were found to be more
context-oriented and represented a better model for deductive reasoning than
regular Google search results. Furthermore, ChatGPT obtained 58.8% on logical
questions and 60% on ethical questions. This means that the ChatGPT is
approaching the passing range for logical questions and has crossed the
threshold for ethical questions. The paper believes ChatGPT and other language
learning models can be invaluable tools for e-learners; however, the study
suggests that there is still room to improve their accuracy. In order to
improve ChatGPT's performance in the future, further research is needed to
better understand how it can answer different types of questions." shape=box style=filled fillcolor="#98FB98"];    "A Categorical Archive of ChatGPT Failures" [label="A Categorical Archive of ChatGPT Failures\nLarge language models have been demonstrated to be valuable in different
fields. ChatGPT, developed by OpenAI, has been trained using massive amounts of
data and simulates human conversation by comprehending context and generating
appropriate responses. It has garnered significant attention due to its ability
to effectively answer a broad range of human inquiries, with fluent and
comprehensive answers surpassing prior public chatbots in both security and
usefulness. However, a comprehensive analysis of ChatGPT's failures is lacking,
which is the focus of this study. Eleven categories of failures, including
reasoning, factual errors, math, coding, and bias, are presented and discussed.
The risks, limitations, and societal implications of ChatGPT are also
highlighted. The goal of this study is to assist researchers and developers in
enhancing future language models and chatbots." shape=box style=filled fillcolor="#006400"];    "Complementary Advantages of ChatGPTs and Human Readers in Reasoning: Evidence from English Text Reading Comprehension" [label="Complementary Advantages of ChatGPTs and Human Readers in Reasoning: Evidence from English Text Reading Comprehension\nChatGPT has shown its great power in text processing, including its reasoning
ability from text reading. However, there has not been any direct comparison
between human readers and ChatGPT in reasoning ability related to text reading.
This study was undertaken to investigate how ChatGPTs (i.e., ChatGPT and
ChatGPT Plus) and Chinese senior school students as ESL learners exhibited
their reasoning ability from English narrative texts. Additionally, we compared
the two ChatGPTs in the reasoning performances when commands were updated
elaborately. The whole study was composed of three reasoning tests: Test 1 for
commonsense inference, Test 2 for emotional inference, and Test 3 for causal
inference. The results showed that in Test 1, the students outdid the two
ChatGPT versions in local-culture-related inferences but performed worse than
the chatbots in daily-life inferences. In Test 2, ChatGPT Plus excelled whereas
ChatGPT lagged behind in accuracy. In association with both accuracy and
frequency of correct responses, the students were inferior to the two chatbots.
Compared with ChatGPTs' better performance in positive emotions, the students
showed their superiority in inferring negative emotions. In Test 3, the
students demonstrated better logical analysis, outdoing both chatbots. In
updating command condition, ChatGPT Plus displayed good causal reasoning
ability while ChatGPT kept unchanged. Our study reveals that human readers and
ChatGPTs have their respective advantages and disadvantages in drawing
inferences from text reading comprehension, unlocking a complementary
relationship in text-based reasoning." shape=box style=filled fillcolor="#006400"];    "How Generative-AI can be Effectively used in Government Chatbots" [label="How Generative-AI can be Effectively used in Government Chatbots\nWith the rapid development of artificial intelligence and breakthroughs in
machine learning and natural language processing, intelligent
question-answering robots have become widely used in government affairs. This
paper conducts a horizontal comparison between Guangdong Province's government
chatbots, ChatGPT, and Wenxin Ernie, two large language models, to analyze the
strengths and weaknesses of existing government chatbots and AIGC technology.
The study finds significant differences between government chatbots and large
language models. China's government chatbots are still in an exploratory stage
and have a gap to close to achieve "intelligence." To explore the future
direction of government chatbots more deeply, this research proposes targeted
optimization paths to help generative AI be effectively applied in government
chatbot conversations." shape=box style=filled fillcolor="#006400"];    "ChatGPT for Teaching and Learning: An Experience from Data Science Education" [label="ChatGPT for Teaching and Learning: An Experience from Data Science Education\nChatGPT, an implementation and application of large language models, has
gained significant popularity since its initial release. Researchers have been
exploring ways to harness the practical benefits of ChatGPT in real-world
scenarios. Educational researchers have investigated its potential in various
subjects, e.g., programming, mathematics, finance, clinical decision support,
etc. However, there has been limited attention given to its application in data
science education. This paper aims to bridge that gap by utilizing ChatGPT in a
data science course, gathering perspectives from students, and presenting our
experiences and feedback on using ChatGPT for teaching and learning in data
science education. The findings not only distinguish data science education
from other disciplines but also uncover new opportunities and challenges
associated with incorporating ChatGPT into the data science curriculum." shape=box style=filled fillcolor="#006400"];    "Do Large Language Models Understand Verbal Indicators of Romantic Attraction?" [label="Do Large Language Models Understand Verbal Indicators of Romantic Attraction?\nWhat makes people 'click' on a first date and become mutually attracted to
one another? While understanding and predicting the dynamics of romantic
interactions used to be exclusive to human judgment, we show that Large
Language Models (LLMs) can detect romantic attraction during brief
getting-to-know-you interactions. Examining data from 964 speed dates, we show
that ChatGPT (and Claude 3) can predict both objective and subjective
indicators of speed dating success (r=0.12-0.23). ChatGPT's predictions of
actual matching (i.e., the exchange of contact information) were not only on
par with those of human judges who had access to the same information but
incremental to speed daters' own predictions. While some of the variance in
ChatGPT's predictions can be explained by common content dimensions (such as
the valence of the conversations) the fact that there remains a substantial
proportion of unexplained variance suggests that ChatGPT also picks up on
conversational dynamics. In addition, ChatGPT's judgments showed substantial
overlap with those made by the human observers (mean r=0.29), highlighting
similarities in their representation of romantic attraction that is, partially,
independent of accuracy." shape=box style=filled fillcolor="#006400"];    "HuaTuo: Tuning LLaMA Model with Chinese Medical Knowledge" [label="HuaTuo: Tuning LLaMA Model with Chinese Medical Knowledge\nLarge Language Models (LLMs), such as the LLaMA model, have demonstrated
their effectiveness in various general-domain natural language processing (NLP)
tasks. Nevertheless, LLMs have not yet performed optimally in biomedical domain
tasks due to the need for medical expertise in the responses. In response to
this challenge, we propose HuaTuo, a LLaMA-based model that has been
supervised-fine-tuned with generated QA (Question-Answer) instances. The
experimental results demonstrate that HuaTuo generates responses that possess
more reliable medical knowledge. Our proposed HuaTuo model is accessible at
https://github.com/SCIR-HI/Huatuo-Llama-Med-Chinese." shape=box style=filled fillcolor="#98FB98"];    "ChatDoctor: A Medical Chat Model Fine-Tuned on a Large Language Model Meta-AI (LLaMA) Using Medical Domain Knowledge" [label="ChatDoctor: A Medical Chat Model Fine-Tuned on a Large Language Model Meta-AI (LLaMA) Using Medical Domain Knowledge\nThe primary aim of this research was to address the limitations observed in
the medical knowledge of prevalent large language models (LLMs) such as
ChatGPT, by creating a specialized language model with enhanced accuracy in
medical advice. We achieved this by adapting and refining the large language
model meta-AI (LLaMA) using a large dataset of 100,000 patient-doctor dialogues
sourced from a widely used online medical consultation platform. These
conversations were cleaned and anonymized to respect privacy concerns. In
addition to the model refinement, we incorporated a self-directed information
retrieval mechanism, allowing the model to access and utilize real-time
information from online sources like Wikipedia and data from curated offline
medical databases. The fine-tuning of the model with real-world patient-doctor
interactions significantly improved the model's ability to understand patient
needs and provide informed advice. By equipping the model with self-directed
information retrieval from reliable online and offline sources, we observed
substantial improvements in the accuracy of its responses. Our proposed
ChatDoctor, represents a significant advancement in medical LLMs, demonstrating
a significant improvement in understanding patient inquiries and providing
accurate advice. Given the high stakes and low error tolerance in the medical
field, such enhancements in providing accurate and reliable information are not
only beneficial but essential." shape=box style=filled fillcolor="#006400"];    "LingYi: Medical Conversational Question Answering System based on Multi-modal Knowledge Graphs" [label="LingYi: Medical Conversational Question Answering System based on Multi-modal Knowledge Graphs\nThe medical conversational system can relieve the burden of doctors and
improve the efficiency of healthcare, especially during the pandemic. This
paper presents a medical conversational question answering (CQA) system based
on the multi-modal knowledge graph, namely "LingYi", which is designed as a
pipeline framework to maintain high flexibility. Our system utilizes automated
medical procedures including medical triage, consultation, image-text drug
recommendation and record. To conduct knowledge-grounded dialogues with
patients, we first construct a Chinese Medical Multi-Modal Knowledge Graph
(CM3KG) and collect a large-scale Chinese Medical CQA (CMCQA) dataset. Compared
with the other existing medical question-answering systems, our system adopts
several state-of-the-art technologies including medical entity disambiguation
and medical dialogue generation, which is more friendly to provide medical
services to patients. In addition, we have open-sourced our codes which contain
back-end models and front-end web pages at https://github.com/WENGSYX/LingYi.
The datasets including CM3KG at https://github.com/WENGSYX/CM3KG and CMCQA at
https://github.com/WENGSYX/CMCQA are also released to further promote future
research." shape=box style=filled fillcolor="#006400"];    "GLM: General Language Model Pretraining with Autoregressive Blank Infilling" [label="GLM: General Language Model Pretraining with Autoregressive Blank Infilling\nThere have been various types of pretraining architectures including
autoencoding models (e.g., BERT), autoregressive models (e.g., GPT), and
encoder-decoder models (e.g., T5). However, none of the pretraining frameworks
performs the best for all tasks of three main categories including natural
language understanding (NLU), unconditional generation, and conditional
generation. We propose a General Language Model (GLM) based on autoregressive
blank infilling to address this challenge. GLM improves blank filling
pretraining by adding 2D positional encodings and allowing an arbitrary order
to predict spans, which results in performance gains over BERT and T5 on NLU
tasks. Meanwhile, GLM can be pretrained for different types of tasks by varying
the number and lengths of blanks. On a wide range of tasks across NLU,
conditional and unconditional generation, GLM outperforms BERT, T5, and GPT
given the same model sizes and data, and achieves the best performance from a
single pretrained model with 1.25x parameters of BERT Large , demonstrating its
generalizability to different downstream tasks." shape=box style=filled fillcolor="#006400"];    "In ChatGPT We Trust? Measuring and Characterizing the Reliability of ChatGPT" [label="In ChatGPT We Trust? Measuring and Characterizing the Reliability of ChatGPT\nThe way users acquire information is undergoing a paradigm shift with the
advent of ChatGPT. Unlike conventional search engines, ChatGPT retrieves
knowledge from the model itself and generates answers for users. ChatGPT's
impressive question-answering (QA) capability has attracted more than 100
million users within a short period of time but has also raised concerns
regarding its reliability. In this paper, we perform the first large-scale
measurement of ChatGPT's reliability in the generic QA scenario with a
carefully curated set of 5,695 questions across ten datasets and eight domains.
We find that ChatGPT's reliability varies across different domains, especially
underperforming in law and science questions. We also demonstrate that system
roles, originally designed by OpenAI to allow users to steer ChatGPT's
behavior, can impact ChatGPT's reliability in an imperceptible way. We further
show that ChatGPT is vulnerable to adversarial examples, and even a single
character change can negatively affect its reliability in certain cases. We
believe that our study provides valuable insights into ChatGPT's reliability
and underscores the need for strengthening the reliability and security of
large language models (LLMs)." shape=box style=filled fillcolor="#006400"];    "math-PVS: A Large Language Model Framework to Map Scientific Publications to PVS Theories" [label="math-PVS: A Large Language Model Framework to Map Scientific Publications to PVS Theories\nAs artificial intelligence (AI) gains greater adoption in a wide variety of
applications, it has immense potential to contribute to mathematical discovery,
by guiding conjecture generation, constructing counterexamples, assisting in
formalizing mathematics, and discovering connections between different
mathematical areas, to name a few.
  While prior work has leveraged computers for exhaustive mathematical proof
search, recent efforts based on large language models (LLMs) aspire to position
computing platforms as co-contributors in the mathematical research process.
Despite their current limitations in logic and mathematical tasks, there is
growing interest in melding theorem proving systems with foundation models.
This work investigates the applicability of LLMs in formalizing advanced
mathematical concepts and proposes a framework that can critically review and
check mathematical reasoning in research papers. Given the noted reasoning
shortcomings of LLMs, our approach synergizes the capabilities of proof
assistants, specifically PVS, with LLMs, enabling a bridge between textual
descriptions in academic papers and formal specifications in PVS. By harnessing
the PVS environment, coupled with data ingestion and conversion mechanisms, we
envision an automated process, called \emph{math-PVS}, to extract and formalize
mathematical theorems from research papers, offering an innovative tool for
academic review and discovery." shape=box style=filled fillcolor="#32CD32"];    "Training Verifiers to Solve Math Word Problems" [label="Training Verifiers to Solve Math Word Problems\nState-of-the-art language models can match human performance on many tasks,
but they still struggle to robustly perform multi-step mathematical reasoning.
To diagnose the failures of current models and support research, we introduce
GSM8K, a dataset of 8.5K high quality linguistically diverse grade school math
word problems. We find that even the largest transformer models fail to achieve
high test performance, despite the conceptual simplicity of this problem
distribution. To increase performance, we propose training verifiers to judge
the correctness of model completions. At test time, we generate many candidate
solutions and select the one ranked highest by the verifier. We demonstrate
that verification significantly improves performance on GSM8K, and we provide
strong empirical evidence that verification scales more effectively with
increased data than a finetuning baseline." shape=box style=filled fillcolor="#98FB98"];    "Solving Math Word Problems by Combining Language Models With Symbolic Solvers" [label="Solving Math Word Problems by Combining Language Models With Symbolic Solvers\nAutomatically generating high-quality step-by-step solutions to math word
problems has many applications in education. Recently, combining large language
models (LLMs) with external tools to perform complex reasoning and calculation
has emerged as a promising direction for solving math word problems, but prior
approaches such as Program-Aided Language model (PAL) are biased towards simple
procedural problems and less effective for problems that require declarative
reasoning. We propose an approach that combines an LLM that can incrementally
formalize word problems as a set of variables and equations with an external
symbolic solver that can solve the equations. Our approach achieves comparable
accuracy to the original PAL on the GSM8K benchmark of math word problems and
outperforms PAL by an absolute 20% on ALGEBRA, a new dataset of more
challenging word problems extracted from Algebra textbooks. Our work highlights
the benefits of using declarative and incremental representations when
interfacing with an external tool for solving complex math word problems. Our
data and prompts are publicly available at
https://github.com/joyheyueya/declarative-math-word-problem." shape=box style=filled fillcolor="#006400"];    "How well do Large Language Models perform in Arithmetic tasks?" [label="How well do Large Language Models perform in Arithmetic tasks?\nLarge language models have emerged abilities including chain-of-thought to
answer math word problems step by step. Solving math word problems not only
requires abilities to disassemble problems via chain-of-thought but also needs
to calculate arithmetic expressions correctly for each step. To the best of our
knowledge, there is no work to focus on evaluating the arithmetic ability of
large language models. In this work, we propose an arithmetic dataset MATH 401
to test the latest large language models including GPT-4, ChatGPT, InstrctGPT,
Galactica, and LLaMA with various arithmetic expressions and provide a detailed
analysis of the ability of large language models. MATH 401 and evaluation codes
are released at \url{https://github.com/GanjinZero/math401-llm}." shape=box style=filled fillcolor="#006400"];    "MathQA: Towards Interpretable Math Word Problem Solving with Operation-Based Formalisms" [label="MathQA: Towards Interpretable Math Word Problem Solving with Operation-Based Formalisms\nWe introduce a large-scale dataset of math word problems and an interpretable
neural math problem solver that learns to map problems to operation programs.
Due to annotation challenges, current datasets in this domain have been either
relatively small in scale or did not offer precise operational annotations over
diverse problem types. We introduce a new representation language to model
precise operation programs corresponding to each math problem that aim to
improve both the performance and the interpretability of the learned models.
Using this representation language, our new dataset, MathQA, significantly
enhances the AQuA dataset with fully-specified operational programs. We
additionally introduce a neural sequence-to-program model enhanced with
automatic problem categorization. Our experiments show improvements over
competitive baselines in our MathQA as well as the AQuA dataset. The results
are still significantly lower than human performance indicating that the
dataset poses new challenges for future research. Our dataset is available at:
https://math-qa.github.io/math-QA/" shape=box style=filled fillcolor="#006400"];    "Learning by Fixing: Solving Math Word Problems with Weak Supervision" [label="Learning by Fixing: Solving Math Word Problems with Weak Supervision\nPrevious neural solvers of math word problems (MWPs) are learned with full
supervision and fail to generate diverse solutions. In this paper, we address
this issue by introducing a \textit{weakly-supervised} paradigm for learning
MWPs. Our method only requires the annotations of the final answers and can
generate various solutions for a single problem. To boost weakly-supervised
learning, we propose a novel \textit{learning-by-fixing} (LBF) framework, which
corrects the misperceptions of the neural network via symbolic reasoning.
Specifically, for an incorrect solution tree generated by the neural network,
the \textit{fixing} mechanism propagates the error from the root node to the
leaf nodes and infers the most probable fix that can be executed to get the
desired answer. To generate more diverse solutions, \textit{tree
regularization} is applied to guide the efficient shrinkage and exploration of
the solution space, and a \textit{memory buffer} is designed to track and save
the discovered various fixes for each problem. Experimental results on the
Math23K dataset show the proposed LBF framework significantly outperforms
reinforcement learning baselines in weakly-supervised learning. Furthermore, it
achieves comparable top-1 and much better top-3/5 answer accuracies than
fully-supervised methods, demonstrating its strength in producing diverse
solutions." shape=box style=filled fillcolor="#006400"];    "Neural Scaling Laws in Robotics" [label="Neural Scaling Laws in Robotics\nNeural scaling laws have driven significant advancements in machine learning,
particularly in domains like language modeling and computer vision. However,
the exploration of neural scaling laws within robotics has remained relatively
underexplored, despite the growing adoption of foundation models in this field.
This paper represents the first comprehensive study to quantify neural scaling
laws for Robot Foundation Models (RFMs) and Large Language Models (LLMs) in
robotics tasks. Through a meta-analysis of 327 research papers, we investigate
how data size, model size, and compute resources influence downstream
performance across a diverse set of robotic tasks. Consistent with previous
scaling law research, our results reveal that the performance of robotic models
improves with increased resources, following a power-law relationship.
Promisingly, the improvement in robotic task performance scales notably faster
than language tasks. This suggests that, while performance on downstream
robotic tasks today is often moderate-to-poor, increased data and compute are
likely to signficantly improve performance in the future. Also consistent with
previous scaling law research, we also observe the emergence of new robot
capabilities as models scale." shape=box style=filled fillcolor="#006400"];    "Notes on a Path to AI Assistance in Mathematical Reasoning" [label="Notes on a Path to AI Assistance in Mathematical Reasoning\nThese informal notes are based on the author's lecture at the National
Academies of Science, Engineering, and Mathematics workshop on "AI to Assist
Mathematical Reasoning" in June 2023. The goal is to think through a path by
which we might arrive at AI that is useful for the research mathematician." shape=box style=filled fillcolor="#98FB98"];    "Formalising perfectoid spaces" [label="Formalising perfectoid spaces\nPerfectoid spaces are sophisticated objects in arithmetic geometry introduced
by Peter Scholze in 2012. We formalised enough definitions and theorems in
topology, algebra and geometry to define perfectoid spaces in the Lean theorem
prover. This experiment confirms that a proof assistant can handle complexity
in that direction, which is rather different from formalising a long proof
about simple objects. It also confirms that mathematicians with no computer
science training can become proficient users of a proof assistant in a
relatively short period of time. Finally, we observe that formalising a piece
of mathematics that is a trending topic boosts the visibility of proof
assistants amongst pure mathematicians." shape=box style=filled fillcolor="#006400"];    "A New Operator for Egyptian Fractions" [label="A New Operator for Egyptian Fractions\nThis paper introduces a new equation for rewriting two unit fractions to
another two unit fractions. This equation is useful for optimizing the elements
of an Egyptian Fraction. Parity of the elements of the Egyptian Fractions are
also considered. And lastly, the statement that all rational numbers can be
represented as Egyptian Fraction is re-established." shape=box style=filled fillcolor="#006400"];    "Formalization of the prime number theorem and Dirichlet's theorem" [label="Formalization of the prime number theorem and Dirichlet's theorem\nWe present the formalization of Dirichlet's theorem on the infinitude of
primes in arithmetic progressions, and Selberg's elementary proof of the prime
number theorem, which asserts that the number $\pi(x)$ of primes less than $x$
is asymptotic to $x/\log x$, within the proof system Metamath." shape=box style=filled fillcolor="#006400"];    "Anisotropy as a diagnostic test for distinct tensor network wavefunctions of integer and half-integer spin Kitaev quantum spin liquids" [label="Anisotropy as a diagnostic test for distinct tensor network wavefunctions of integer and half-integer spin Kitaev quantum spin liquids\nContrasting ground states of quantum magnets with the integer and
half-integer spin moments are the manifestation of many-body quantum
interference effects. In this work, we investigate the distinct nature of the
integer and half-integer spin quantum spin liquids in the framework of the
Kitaev's model on the honeycomb lattice. The models with arbitrary spin quantum
numbers are not exactly solvable in contrast to the well-known quantum spin
liquid solution of the spin-1/2 system. We use the tensor network wavefunctions
for the integer and half-integer spin quantum spin liquid states to unveil the
important difference between these states. We find that the distinct sign
structures of the tensor network wavefunction for the integer and half-integer
spin quantum spin liquids are responsible for completely different ground
states in the spatially anisotropic limit. Hence the spatial anisotropy would
be a useful diagnostic test for distinguishing these quantum spin liquid
states, both in the numerical computations and experiments on real materials.
We support this discovery via extensive numerics including the tensor network,
DMRG, and exact diagonalization computations." shape=box style=filled fillcolor="#006400"];    "Strong non-vanishing of cohomologies and strong non-freeness of adjoint line bundles on $n$-Raynaud surfaces" [label="Strong non-vanishing of cohomologies and strong non-freeness of adjoint line bundles on $n$-Raynaud surfaces\nWe formally give the definition of $n$-Tango curve and $n$-Raynaud surface.
Then we study the pathologies on $n$-Raynaud surfaces and as a corollary we
give a simple disproof of Fujita's conjecture on surfaces in positive
characteristics." shape=box style=filled fillcolor="#006400"];    "MiniF2F: a cross-system benchmark for formal Olympiad-level mathematics" [label="MiniF2F: a cross-system benchmark for formal Olympiad-level mathematics\nWe present miniF2F, a dataset of formal Olympiad-level mathematics problems
statements intended to provide a unified cross-system benchmark for neural
theorem proving. The miniF2F benchmark currently targets Metamath, Lean,
Isabelle (partially) and HOL Light (partially) and consists of 488 problem
statements drawn from the AIME, AMC, and the International Mathematical
Olympiad (IMO), as well as material from high-school and undergraduate
mathematics courses. We report baseline results using GPT-f, a neural theorem
prover based on GPT-3 and provide an analysis of its performance. We intend for
miniF2F to be a community-driven effort and hope that our benchmark will help
spur advances in neural theorem proving." shape=box style=filled fillcolor="#98FB98"];    "Generative Language Modeling for Automated Theorem Proving" [label="Generative Language Modeling for Automated Theorem Proving\nWe explore the application of transformer-based language models to automated
theorem proving. This work is motivated by the possibility that a major
limitation of automated theorem provers compared to humans -- the generation of
original mathematical terms -- might be addressable via generation from
language models. We present an automated prover and proof assistant, GPT-f, for
the Metamath formalization language, and analyze its performance. GPT-f found
new short proofs that were accepted into the main Metamath library, which is to
our knowledge, the first time a deep-learning based system has contributed
proofs that were adopted by a formal mathematics community." shape=box style=filled fillcolor="#006400"];    "HOList: An Environment for Machine Learning of Higher-Order Theorem Proving" [label="HOList: An Environment for Machine Learning of Higher-Order Theorem Proving\nWe present an environment, benchmark, and deep learning driven automated
theorem prover for higher-order logic. Higher-order interactive theorem provers
enable the formalization of arbitrary mathematical theories and thereby present
an interesting, open-ended challenge for deep learning. We provide an
open-source framework based on the HOL Light theorem prover that can be used as
a reinforcement learning environment. HOL Light comes with a broad coverage of
basic mathematical theorems on calculus and the formal proof of the Kepler
conjecture, from which we derive a challenging benchmark for automated
reasoning. We also present a deep reinforcement learning driven automated
theorem prover, DeepHOL, with strong initial results on this benchmark." shape=box style=filled fillcolor="#006400"];    "Lean Copilot: Large Language Models as Copilots for Theorem Proving in Lean" [label="Lean Copilot: Large Language Models as Copilots for Theorem Proving in Lean\nNeural theorem proving combines large language models (LLMs) with proof
assistants such as Lean, where the correctness of formal proofs can be
rigorously verified, leaving no room for hallucination. With existing neural
theorem provers pretrained on a fixed collection of data and offering valuable
suggestions at times, it is challenging for them to continually prove novel
theorems in a fully autonomous mode, where human insights may be critical. In
this paper, we explore LLMs as copilots that assist humans in proving theorems.
We introduce Lean Copilot, an general framework for running LLM inference
natively in Lean. It enables programmers to build various LLM-based proof
automation tools that integrate seamlessly into the workflow of Lean users.
Lean users can use our pretrained models or bring their own ones that run
either locally (with or without GPUs) or on the cloud. Using Lean Copilot, we
build LLM-based tools that suggest proof steps, complete proof goals, and
select relevant premises. Experimental results on the Mathematics in Lean
textbook demonstrate the effectiveness of our method compared to existing
rule-based proof automation in Lean (aesop). When assisting humans, Lean
Copilot requires only 2.08 manually-entered proof steps on average (3.86
required by aesop); when automating the theorem proving process, Lean Copilot
automates 74.2% proof steps on average, 85% better than aesop (40.1%). We open
source all code and artifacts under a permissive MIT license to facilitate
further research." shape=box style=filled fillcolor="#006400"];    "Natural Language Comprehension with the EpiReader" [label="Natural Language Comprehension with the EpiReader\nWe present the EpiReader, a novel model for machine comprehension of text.
Machine comprehension of unstructured, real-world text is a major research goal
for natural language processing. Current tests of machine comprehension pose
questions whose answers can be inferred from some supporting text, and evaluate
a model's response to the questions. The EpiReader is an end-to-end neural
model comprising two components: the first component proposes a small set of
candidate answers after comparing a question to its supporting text, and the
second component formulates hypotheses using the proposed candidates and the
question, then reranks the hypotheses based on their estimated concordance with
the supporting text. We present experiments demonstrating that the EpiReader
sets a new state-of-the-art on the CNN and Children's Book Test machine
comprehension benchmarks, outperforming previous neural models by a significant
margin." shape=box style=filled fillcolor="#006400"];    "The LAMBADA dataset: Word prediction requiring a broad discourse context" [label="The LAMBADA dataset: Word prediction requiring a broad discourse context\nWe introduce LAMBADA, a dataset to evaluate the capabilities of computational
models for text understanding by means of a word prediction task. LAMBADA is a
collection of narrative passages sharing the characteristic that human subjects
are able to guess their last word if they are exposed to the whole passage, but
not if they only see the last sentence preceding the target word. To succeed on
LAMBADA, computational models cannot simply rely on local context, but must be
able to keep track of information in the broader discourse. We show that
LAMBADA exemplifies a wide range of linguistic phenomena, and that none of
several state-of-the-art language models reaches accuracy above 1% on this
novel benchmark. We thus propose LAMBADA as a challenging test set, meant to
encourage the development of new models capable of genuine understanding of
broad context in natural language text." shape=box style=filled fillcolor="#006400"];    "Measuring Mathematical Problem Solving With the MATH Dataset" [label="Measuring Mathematical Problem Solving With the MATH Dataset\nMany intellectual endeavors require mathematical problem solving, but this
skill remains beyond the capabilities of computers. To measure this ability in
machine learning models, we introduce MATH, a new dataset of 12,500 challenging
competition mathematics problems. Each problem in MATH has a full step-by-step
solution which can be used to teach models to generate answer derivations and
explanations. To facilitate future research and increase accuracy on MATH, we
also contribute a large auxiliary pretraining dataset which helps teach models
the fundamentals of mathematics. Even though we are able to increase accuracy
on MATH, our results show that accuracy remains relatively low, even with
enormous Transformer models. Moreover, we find that simply increasing budgets
and model parameter counts will be impractical for achieving strong
mathematical reasoning if scaling trends continue. While scaling Transformers
is automatically solving most other text-based tasks, scaling is not currently
solving MATH. To have more traction on mathematical problem solving we will
likely need new algorithmic advancements from the broader research community." shape=box style=filled fillcolor="#98FB98"];    "Measuring Massive Multitask Language Understanding" [label="Measuring Massive Multitask Language Understanding\nWe propose a new test to measure a text model's multitask accuracy. The test
covers 57 tasks including elementary mathematics, US history, computer science,
law, and more. To attain high accuracy on this test, models must possess
extensive world knowledge and problem solving ability. We find that while most
recent models have near random-chance accuracy, the very largest GPT-3 model
improves over random chance by almost 20 percentage points on average. However,
on every one of the 57 tasks, the best models still need substantial
improvements before they can reach expert-level accuracy. Models also have
lopsided performance and frequently do not know when they are wrong. Worse,
they still have near-random accuracy on some socially important subjects such
as morality and law. By comprehensively evaluating the breadth and depth of a
model's academic and professional understanding, our test can be used to
analyze models across many tasks and to identify important shortcomings." shape=box style=filled fillcolor="#006400"];    "Improving Graph Neural Network Representations of Logical Formulae with Subgraph Pooling" [label="Improving Graph Neural Network Representations of Logical Formulae with Subgraph Pooling\nRecent advances in the integration of deep learning with automated theorem
proving have centered around the representation of logical formulae as inputs
to deep learning systems. In particular, there has been a growing interest in
adapting structure-aware neural methods to work with the underlying graph
representations of logical expressions. While more effective than character and
token-level approaches, graph-based methods have often made representational
trade-offs that limited their ability to capture key structural properties of
their inputs. In this work we propose a novel approach for embedding logical
formulae that is designed to overcome the representational limitations of prior
approaches. Our architecture works for logics of different expressivity; e.g.,
first-order and higher-order logic. We evaluate our approach on two standard
datasets and show that the proposed architecture achieves state-of-the-art
performance on both premise selection and proof step classification." shape=box style=filled fillcolor="#006400"];    "BERT-JAM: Boosting BERT-Enhanced Neural Machine Translation with Joint Attention" [label="BERT-JAM: Boosting BERT-Enhanced Neural Machine Translation with Joint Attention\nBERT-enhanced neural machine translation (NMT) aims at leveraging
BERT-encoded representations for translation tasks. A recently proposed
approach uses attention mechanisms to fuse Transformer's encoder and decoder
layers with BERT's last-layer representation and shows enhanced performance.
However, their method doesn't allow for the flexible distribution of attention
between the BERT representation and the encoder/decoder representation. In this
work, we propose a novel BERT-enhanced NMT model called BERT-JAM which improves
upon existing models from two aspects: 1) BERT-JAM uses joint-attention modules
to allow the encoder/decoder layers to dynamically allocate attention between
different representations, and 2) BERT-JAM allows the encoder/decoder layers to
make use of BERT's intermediate representations by composing them using a gated
linear unit (GLU). We train BERT-JAM with a novel three-phase optimization
strategy that progressively unfreezes different components of BERT-JAM. Our
experiments show that BERT-JAM achieves SOTA BLEU scores on multiple
translation tasks." shape=box style=filled fillcolor="#006400"];    "Choose Your Programming Copilot: A Comparison of the Program Synthesis Performance of GitHub Copilot and Genetic Programming" [label="Choose Your Programming Copilot: A Comparison of the Program Synthesis Performance of GitHub Copilot and Genetic Programming\nGitHub Copilot, an extension for the Visual Studio Code development
environment powered by the large-scale language model Codex, makes automatic
program synthesis available for software developers. This model has been
extensively studied in the field of deep learning, however, a comparison to
genetic programming, which is also known for its performance in automatic
program synthesis, has not yet been carried out. In this paper, we evaluate
GitHub Copilot on standard program synthesis benchmark problems and compare the
achieved results with those from the genetic programming literature. In
addition, we discuss the performance of both approaches. We find that the
performance of the two approaches on the benchmark problems is quite similar,
however, in comparison to GitHub Copilot, the program synthesis approaches
based on genetic programming are not yet mature enough to support programmers
in practical software development. Genetic programming usually needs a huge
amount of expensive hand-labeled training cases and takes too much time to
generate solutions. Furthermore, source code generated by genetic programming
approaches is often bloated and difficult to understand. For future work on
program synthesis with genetic programming, we suggest researchers to focus on
improving the execution time, readability, and usability." shape=box style=filled fillcolor="#006400"];    "Stop Words for Processing Software Engineering Documents: Do they Matter?" [label="Stop Words for Processing Software Engineering Documents: Do they Matter?\nStop words, which are considered non-predictive, are often eliminated in
natural language processing tasks. However, the definition of uninformative
vocabulary is vague, so most algorithms use general knowledge-based stop lists
to remove stop words. There is an ongoing debate among academics about the
usefulness of stop word elimination, especially in domain-specific settings. In
this work, we investigate the usefulness of stop word removal in a software
engineering context. To do this, we replicate and experiment with three
software engineering research tools from related work. Additionally, we
construct a corpus of software engineering domain-related text from 10,000
Stack Overflow questions and identify 200 domain-specific stop words using
traditional information-theoretic methods. Our results show that the use of
domain-specific stop words significantly improved the performance of research
tools compared to the use of a general stop list and that 17 out of 19
evaluation measures showed better performance.
  Online appendix: https://zenodo.org/record/7865748" shape=box style=filled fillcolor="#006400"];    "World Models in Artificial Intelligence: Sensing, Learning, and Reasoning Like a Child" [label="World Models in Artificial Intelligence: Sensing, Learning, and Reasoning Like a Child\nWorld Models help Artificial Intelligence (AI) predict outcomes, reason about
its environment, and guide decision-making. While widely used in reinforcement
learning, they lack the structured, adaptive representations that even young
children intuitively develop. Advancing beyond pattern recognition requires
dynamic, interpretable frameworks inspired by Piaget's cognitive development
theory. We highlight six key research areas -- physics-informed learning,
neurosymbolic learning, continual learning, causal inference, human-in-the-loop
AI, and responsible AI -- as essential for enabling true reasoning in AI. By
integrating statistical learning with advances in these areas, AI can evolve
from pattern recognition to genuine understanding, adaptation and reasoning
capabilities." shape=box style=filled fillcolor="#98FB98"];    "Neurosymbolic AI: The 3rd Wave" [label="Neurosymbolic AI: The 3rd Wave\nCurrent advances in Artificial Intelligence (AI) and Machine Learning (ML)
have achieved unprecedented impact across research communities and industry.
Nevertheless, concerns about trust, safety, interpretability and accountability
of AI were raised by influential thinkers. Many have identified the need for
well-founded knowledge representation and reasoning to be integrated with deep
learning and for sound explainability. Neural-symbolic computing has been an
active area of research for many years seeking to bring together robust
learning in neural networks with reasoning and explainability via symbolic
representations for network models. In this paper, we relate recent and early
research results in neurosymbolic AI with the objective of identifying the key
ingredients of the next wave of AI systems. We focus on research that
integrates in a principled way neural network-based learning with symbolic
knowledge representation and logical reasoning. The insights provided by 20
years of neural-symbolic computing are shown to shed new light onto the
increasingly prominent role of trust, safety, interpretability and
accountability of AI. We also identify promising directions and challenges for
the next decade of AI research from the perspective of neural-symbolic systems." shape=box style=filled fillcolor="#006400"];    "Connecting the Dots in Trustworthy Artificial Intelligence: From AI Principles, Ethics, and Key Requirements to Responsible AI Systems and Regulation" [label="Connecting the Dots in Trustworthy Artificial Intelligence: From AI Principles, Ethics, and Key Requirements to Responsible AI Systems and Regulation\nTrustworthy Artificial Intelligence (AI) is based on seven technical
requirements sustained over three main pillars that should be met throughout
the system's entire life cycle: it should be (1) lawful, (2) ethical, and (3)
robust, both from a technical and a social perspective. However, attaining
truly trustworthy AI concerns a wider vision that comprises the trustworthiness
of all processes and actors that are part of the system's life cycle, and
considers previous aspects from different lenses. A more holistic vision
contemplates four essential axes: the global principles for ethical use and
development of AI-based systems, a philosophical take on AI ethics, a
risk-based approach to AI regulation, and the mentioned pillars and
requirements. The seven requirements (human agency and oversight; robustness
and safety; privacy and data governance; transparency; diversity,
non-discrimination and fairness; societal and environmental wellbeing; and
accountability) are analyzed from a triple perspective: What each requirement
for trustworthy AI is, Why it is needed, and How each requirement can be
implemented in practice. On the other hand, a practical approach to implement
trustworthy AI systems allows defining the concept of responsibility of
AI-based systems facing the law, through a given auditing process. Therefore, a
responsible AI system is the resulting notion we introduce in this work, and a
concept of utmost necessity that can be realized through auditing processes,
subject to the challenges posed by the use of regulatory sandboxes. Our
multidisciplinary vision of trustworthy AI culminates in a debate on the
diverging views published lately about the future of AI. Our reflections in
this matter conclude that regulation is a key for reaching a consensus among
these views, and that trustworthy and responsible AI systems will be crucial
for the present and future of our society." shape=box style=filled fillcolor="#006400"];    "Developing Responsible Chatbots for Financial Services: A Pattern-Oriented Responsible AI Engineering Approach" [label="Developing Responsible Chatbots for Financial Services: A Pattern-Oriented Responsible AI Engineering Approach\nThe recent release of ChatGPT has gained huge attention and discussion
worldwide, with responsible AI being a key topic of discussion. How can we
ensure that AI systems, including ChatGPT, are developed and adopted in a
responsible way? To tackle the responsible AI challenges, various ethical
principles have been released by governments, organisations, and companies.
However, those principles are very abstract and not practical enough. Further,
significant efforts have been put on algorithm-level solutions that only
address a narrow set of principles, such as fairness and privacy. To fill the
gap, we adopt a pattern-oriented responsible AI engineering approach and build
a Responsible AI Pattern Catalogue to operationalise responsible AI from a
system perspective. In this article, we first summarise the major challenges in
operationalising responsible AI at scale and introduce how we use the
Responsible AI Pattern Catalogue to address those challenges. We then examine
the risks at each stage of the chatbot development process and recommend
pattern-driven mitigations to evaluate the the usefulness of the Responsible AI
Pattern Catalogue in a real-world setting." shape=box style=filled fillcolor="#006400"];    "Socially Responsible AI Algorithms: Issues, Purposes, and Challenges" [label="Socially Responsible AI Algorithms: Issues, Purposes, and Challenges\nIn the current era, people and society have grown increasingly reliant on
artificial intelligence (AI) technologies. AI has the potential to drive us
towards a future in which all of humanity flourishes. It also comes with
substantial risks for oppression and calamity. Discussions about whether we
should (re)trust AI have repeatedly emerged in recent years and in many
quarters, including industry, academia, healthcare, services, and so on.
Technologists and AI researchers have a responsibility to develop trustworthy
AI systems. They have responded with great effort to design more responsible AI
algorithms. However, existing technical solutions are narrow in scope and have
been primarily directed towards algorithms for scoring or classification tasks,
with an emphasis on fairness and unwanted bias. To build long-lasting trust
between AI and human beings, we argue that the key is to think beyond
algorithmic fairness and connect major aspects of AI that potentially cause
AI's indifferent behavior. In this survey, we provide a systematic framework of
Socially Responsible AI Algorithms that aims to examine the subjects of AI
indifference and the need for socially responsible AI algorithms, define the
objectives, and introduce the means by which we may achieve these objectives.
We further discuss how to leverage this framework to improve societal
well-being through protection, information, and prevention/mitigation." shape=box style=filled fillcolor="#006400"];    "Strategies to architect AI Safety: Defense to guard AI from Adversaries" [label="Strategies to architect AI Safety: Defense to guard AI from Adversaries\nThe impact of designing for security of AI is critical for humanity in the AI
era. With humans increasingly becoming dependent upon AI, there is a need for
neural networks that work reliably, inspite of Adversarial attacks. The vision
for Safe and secure AI for popular use is achievable. To achieve safety of AI,
this paper explores strategies and a novel deep learning architecture. To guard
AI from adversaries, paper explores combination of 3 strategies:
  1. Introduce randomness at inference time to hide the representation learning
from adversaries.
  2. Detect presence of adversaries by analyzing the sequence of inferences.
  3. Exploit visual similarity.
  To realize these strategies, this paper designs a novel architecture, Dynamic
Neural Defense, DND. This defense has 3 deep learning architectural features:
  1. By hiding the way a neural network learns from exploratory attacks using a
random computation graph, DND evades attack.
  2. By analyzing input sequence to cloud AI inference engine with LSTM, DND
detects attack sequence.
  3. By inferring with visual similar inputs generated by VAE, any AI defended
by DND approach does not succumb to hackers.
  Thus, a roadmap to develop reliable, safe and secure AI is presented." shape=box style=filled fillcolor="#006400"];    "PoisonedRAG: Knowledge Corruption Attacks to Retrieval-Augmented Generation of Large Language Models" -> "A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions" [color="blue" style="bold"];    "PoisonedRAG: Knowledge Corruption Attacks to Retrieval-Augmented Generation of Large Language Models" -> "Making LLMs Worth Every Penny: Resource-Limited Text Classification in Banking" [color="blue" style="bold"];    "PoisonedRAG: Knowledge Corruption Attacks to Retrieval-Augmented Generation of Large Language Models" -> "PaLM 2 Technical Report" [color="blue" style="bold"];    "PoisonedRAG: Knowledge Corruption Attacks to Retrieval-Augmented Generation of Large Language Models" -> "RadOnc-GPT: A Large Language Model for Radiation Oncology" [color="blue" style="bold"];    "PoisonedRAG: Knowledge Corruption Attacks to Retrieval-Augmented Generation of Large Language Models" -> "math-PVS: A Large Language Model Framework to Map Scientific Publications to PVS Theories" [color="blue" style="bold"];    "A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions" -> "Adaptive Chameleon or Stubborn Sloth: Revealing the Behavior of Large Language Models in Knowledge Conflicts" [color="blue" style="bold"];    "A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions" -> "Textbooks Are All You Need II: phi-1.5 technical report" [color="blue" style="bold"];    "A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions" -> "Massive Editing for Large Language Models via Meta Learning" [color="blue" style="bold"];    "A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions" -> "Evaluating the Factual Consistency of Large Language Models Through News Summarization" [color="blue" style="bold"];    "A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions" -> "On Context Utilization in Summarization with Large Language Models" [color="blue" style="bold"];    "Adaptive Chameleon or Stubborn Sloth: Revealing the Behavior of Large Language Models in Knowledge Conflicts" -> "Rich Knowledge Sources Bring Complex Knowledge Conflicts: Recalibrating Models to Reflect Conflicting Evidence" [color="blue" style="bold"];    "Adaptive Chameleon or Stubborn Sloth: Revealing the Behavior of Large Language Models in Knowledge Conflicts" -> "Quantifying Memorization Across Neural Language Models" [color="blue" style="bold"];    "Adaptive Chameleon or Stubborn Sloth: Revealing the Behavior of Large Language Models in Knowledge Conflicts" -> "Multilingual Knowledge Editing with Language-Agnostic Factual Neurons" [color="blue" style="bold"];    "Adaptive Chameleon or Stubborn Sloth: Revealing the Behavior of Large Language Models in Knowledge Conflicts" -> "Knowledge Neurons in Pretrained Transformers" [color="blue" style="bold"];    "Adaptive Chameleon or Stubborn Sloth: Revealing the Behavior of Large Language Models in Knowledge Conflicts" -> "Extracting Training Data from Large Language Models" [color="blue" style="bold"];    "Textbooks Are All You Need II: phi-1.5 technical report" -> "Self-Organizing Machine Translation: Example-Driven Induction of Transfer Functions" [color="blue" style="bold"];    "Textbooks Are All You Need II: phi-1.5 technical report" -> "Sparks of Artificial General Intelligence: Early experiments with GPT-4" [color="blue" style="bold"];    "Textbooks Are All You Need II: phi-1.5 technical report" -> "Language Modeling for Code-Switching: Evaluation, Integration of Monolingual Data, and Discriminative Training" [color="blue" style="bold"];    "Textbooks Are All You Need II: phi-1.5 technical report" -> "BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions" [color="blue" style="bold"];    "Textbooks Are All You Need II: phi-1.5 technical report" -> "Learning ASR pathways: A sparse multilingual ASR model" [color="blue" style="bold"];    "Massive Editing for Large Language Models via Meta Learning" -> "Does Localization Inform Editing? Surprising Differences in Causality-Based Localization vs. Knowledge Editing in Language Models" [color="blue" style="bold"];    "Massive Editing for Large Language Models via Meta Learning" -> "Commonsense Knowledge Editing Based on Free-Text in LLMs" [color="blue" style="bold"];    "Massive Editing for Large Language Models via Meta Learning" -> "Empirical Study on Updating Key-Value Memories in Transformer Feed-forward Layers" [color="blue" style="bold"];    "Massive Editing for Large Language Models via Meta Learning" -> "Can bidirectional encoder become the ultimate winner for downstream applications of foundation models?" [color="blue" style="bold"];    "Massive Editing for Large Language Models via Meta Learning" -> "Hypernetwork Dismantling via Deep Reinforcement Learning" [color="blue" style="bold"];    "Evaluating the Factual Consistency of Large Language Models Through News Summarization" -> "FEQA: A Question Answering Evaluation Framework for Faithfulness Assessment in Abstractive Summarization" [color="blue" style="bold"];    "Evaluating the Factual Consistency of Large Language Models Through News Summarization" -> "Faithful to the Original: Fact Aware Neural Abstractive Summarization" [color="blue" style="bold"];    "Evaluating the Factual Consistency of Large Language Models Through News Summarization" -> "Scaling Instruction-Finetuned Language Models" [color="blue" style="bold"];    "Evaluating the Factual Consistency of Large Language Models Through News Summarization" -> "MultiFC: A Real-World Multi-Domain Dataset for Evidence-Based Fact Checking of Claims" [color="blue" style="bold"];    "Evaluating the Factual Consistency of Large Language Models Through News Summarization" -> "Fast Abstractive Summarization with Reinforce-Selected Sentence Rewriting" [color="blue" style="bold"];    "On Context Utilization in Summarization with Large Language Models" -> "Revisiting Zero-Shot Abstractive Summarization in the Era of Large Language Models from the Perspective of Position Bias" [color="blue" style="bold"];    "On Context Utilization in Summarization with Large Language Models" -> "Source Code Summarization in the Era of Large Language Models" [color="blue" style="bold"];    "On Context Utilization in Summarization with Large Language Models" -> "When Benchmarks are Targets: Revealing the Sensitivity of Large Language Model Leaderboards" [color="blue" style="bold"];    "On Context Utilization in Summarization with Large Language Models" -> "MovieSum: An Abstractive Summarization Dataset for Movie Screenplays" [color="blue" style="bold"];    "On Context Utilization in Summarization with Large Language Models" -> "Extending Context Window of Large Language Models via Positional Interpolation" [color="blue" style="bold"];    "Making LLMs Worth Every Penny: Resource-Limited Text Classification in Banking" -> "Efficient Intent Detection with Dual Sentence Encoders" [color="blue" style="bold"];    "Making LLMs Worth Every Penny: Resource-Limited Text Classification in Banking" -> "An Empirical Study on Large-Scale Multi-Label Text Classification Including Few and Zero-Shot Labels" [color="blue" style="bold"];    "Making LLMs Worth Every Penny: Resource-Limited Text Classification in Banking" -> "Fine-Tuning Pretrained Language Models: Weight Initializations, Data Orders, and Early Stopping" [color="blue" style="bold"];    "Making LLMs Worth Every Penny: Resource-Limited Text Classification in Banking" -> "Combining Autoregressive and Autoencoder Language Models for Text Classification" [color="blue" style="bold"];    "Making LLMs Worth Every Penny: Resource-Limited Text Classification in Banking" -> "Weak Human Preference Supervision For Deep Reinforcement Learning" [color="blue" style="bold"];    "Efficient Intent Detection with Dual Sentence Encoders" -> "The Daunting Dilemma with Sentence Encoders: Success on Standard Benchmarks, Failure in Capturing Basic Semantic Properties" [color="blue" style="bold"];    "Efficient Intent Detection with Dual Sentence Encoders" -> "Conversational Contextual Cues: The Case of Personalization and History for Response Ranking" [color="blue" style="bold"];    "Efficient Intent Detection with Dual Sentence Encoders" -> "Snips Voice Platform: an embedded Spoken Language Understanding system for private-by-design voice interfaces" [color="blue" style="bold"];    "Efficient Intent Detection with Dual Sentence Encoders" -> "Domain adaptation for sequence labeling using hidden Markov models" [color="blue" style="bold"];    "Efficient Intent Detection with Dual Sentence Encoders" -> "Transfer Fine-Tuning: A BERT Case Study" [color="blue" style="bold"];    "An Empirical Study on Large-Scale Multi-Label Text Classification Including Few and Zero-Shot Labels" -> "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer" [color="blue" style="bold"];    "An Empirical Study on Large-Scale Multi-Label Text Classification Including Few and Zero-Shot Labels" -> "A Primer in BERTology: What we know about how BERT works" [color="blue" style="bold"];    "An Empirical Study on Large-Scale Multi-Label Text Classification Including Few and Zero-Shot Labels" -> "Publicly Available Clinical BERT Embeddings" [color="blue" style="bold"];    "An Empirical Study on Large-Scale Multi-Label Text Classification Including Few and Zero-Shot Labels" -> "SciBERT: A Pretrained Language Model for Scientific Text" [color="blue" style="bold"];    "An Empirical Study on Large-Scale Multi-Label Text Classification Including Few and Zero-Shot Labels" -> "RoBERTa: A Robustly Optimized BERT Pretraining Approach" [color="blue" style="bold"];    "Fine-Tuning Pretrained Language Models: Weight Initializations, Data Orders, and Early Stopping" -> "Sentence Encoders on STILTs: Supplementary Training on Intermediate Labeled-data Tasks" [color="blue" style="bold"];    "Fine-Tuning Pretrained Language Models: Weight Initializations, Data Orders, and Early Stopping" -> "Passage Re-ranking with BERT" [color="blue" style="bold"];    "Fine-Tuning Pretrained Language Models: Weight Initializations, Data Orders, and Early Stopping" -> "MiniLM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers" [color="blue" style="bold"];    "Weak Human Preference Supervision For Deep Reinforcement Learning" -> "Emergence of Human-comparable Balancing Behaviors by Deep Reinforcement Learning" [color="blue" style="bold"];    "Weak Human Preference Supervision For Deep Reinforcement Learning" -> "Experimental Evaluation of Human Motion Prediction: Toward Safe and Efficient Human Robot Collaboration" [color="blue" style="bold"];    "Weak Human Preference Supervision For Deep Reinforcement Learning" -> "Active Hierarchical Imitation and Reinforcement Learning" [color="blue" style="bold"];    "Weak Human Preference Supervision For Deep Reinforcement Learning" -> "Reinforcement Learning from Hierarchical Critics" [color="blue" style="bold"];    "Weak Human Preference Supervision For Deep Reinforcement Learning" -> "Differential Variable Speed Limits Control for Freeway Recurrent Bottlenecks via Deep Reinforcement learning" [color="blue" style="bold"];    "PaLM 2 Technical Report" -> "The False Promise of Imitating Proprietary LLMs" [color="blue" style="bold"];    "PaLM 2 Technical Report" -> "Computational Models of Tutor Feedback in Language Acquisition" [color="blue" style="bold"];    "PaLM 2 Technical Report" -> "Detoxifying Language Models Risks Marginalizing Minority Voices" [color="blue" style="bold"];    "PaLM 2 Technical Report" -> "Multilingual Machine Translation Systems from Microsoft for WMT21 Shared Task" [color="blue" style="bold"];    "PaLM 2 Technical Report" -> "Investigating Bias in LLM-Based Bias Detection: Disparities between LLMs and Human Perception" [color="blue" style="bold"];    "The False Promise of Imitating Proprietary LLMs" -> "Imitation Attacks and Defenses for Black-box Machine Translation Systems" [color="blue" style="bold"];    "The False Promise of Imitating Proprietary LLMs" -> "Towards Better Evaluation of Instruction-Following: A Case-Study in Summarization" [color="blue" style="bold"];    "The False Promise of Imitating Proprietary LLMs" -> "Cedille: A large autoregressive French language model" [color="blue" style="bold"];    "The False Promise of Imitating Proprietary LLMs" -> "SeDi-Instruct: Enhancing Alignment of Language Models through Self-Directed Instruction Generation" [color="blue" style="bold"];    "The False Promise of Imitating Proprietary LLMs" -> "STRUDEL: Structured Dialogue Summarization for Dialogue Comprehension" [color="blue" style="bold"];    "Computational Models of Tutor Feedback in Language Acquisition" -> "Contextual Skipgram: Training Word Representation Using Context Information" [color="blue" style="bold"];    "Computational Models of Tutor Feedback in Language Acquisition" -> "Word Embedding with Neural Probabilistic Prior" [color="blue" style="bold"];    "Computational Models of Tutor Feedback in Language Acquisition" -> "Quantum-inspired Complex Word Embedding" [color="blue" style="bold"];    "Computational Models of Tutor Feedback in Language Acquisition" -> "Text Classification based on Word Subspace with Term-Frequency" [color="blue" style="bold"];    "Computational Models of Tutor Feedback in Language Acquisition" -> "Deep Learning for Prediction and Classifying the Dynamical behaviour of Piecewise Smooth Maps" [color="blue" style="bold"];    "Detoxifying Language Models Risks Marginalizing Minority Voices" -> "RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models" [color="blue" style="bold"];    "Detoxifying Language Models Risks Marginalizing Minority Voices" -> "Challenges in Automated Debiasing for Toxic Language Detection" [color="blue" style="bold"];    "Detoxifying Language Models Risks Marginalizing Minority Voices" -> "A Plug-and-Play Method for Controlled Text Generation" [color="blue" style="bold"];    "Detoxifying Language Models Risks Marginalizing Minority Voices" -> "Detecting Unintended Social Bias in Toxic Language Datasets" [color="blue" style="bold"];    "Detoxifying Language Models Risks Marginalizing Minority Voices" -> "Lost in Moderation: How Commercial Content Moderation APIs Over- and Under-Moderate Group-Targeted Hate Speech and Linguistic Variations" [color="blue" style="bold"];    "Multilingual Machine Translation Systems from Microsoft for WMT21 Shared Task" -> "A Comprehensive Survey of Multilingual Neural Machine Translation" [color="blue" style="bold"];    "Multilingual Machine Translation Systems from Microsoft for WMT21 Shared Task" -> "The FLORES-101 Evaluation Benchmark for Low-Resource and Multilingual Machine Translation" [color="blue" style="bold"];    "Multilingual Machine Translation Systems from Microsoft for WMT21 Shared Task" -> "Learning Policies for Multilingual Training of Neural Machine Translation Systems" [color="blue" style="bold"];    "Multilingual Machine Translation Systems from Microsoft for WMT21 Shared Task" -> "Massively Multilingual Neural Machine Translation" [color="blue" style="bold"];    "Investigating Bias in LLM-Based Bias Detection: Disparities between LLMs and Human Perception" -> "Fairness in Large Language Models: A Taxonomic Survey" [color="blue" style="bold"];    "Investigating Bias in LLM-Based Bias Detection: Disparities between LLMs and Human Perception" -> "BiasEdit: Debiasing Stereotyped Language Models via Model Editing" [color="blue" style="bold"];    "Investigating Bias in LLM-Based Bias Detection: Disparities between LLMs and Human Perception" -> "Detecting Linguistic Indicators for Stereotype Assessment with Large Language Models" [color="blue" style="bold"];    "Investigating Bias in LLM-Based Bias Detection: Disparities between LLMs and Human Perception" -> "How Far Can It Go?: On Intrinsic Gender Bias Mitigation for Text Classification" [color="blue" style="bold"];    "Investigating Bias in LLM-Based Bias Detection: Disparities between LLMs and Human Perception" -> "We Can Detect Your Bias: Predicting the Political Ideology of News Articles" [color="blue" style="bold"];    "RadOnc-GPT: A Large Language Model for Radiation Oncology" -> "Benchmarking ChatGPT-4 on ACR Radiation Oncology In-Training (TXIT) Exam and Red Journal Gray Zone Cases: Potentials and Challenges for AI-Assisted Medical Education and Decision Making in Radiation Oncology" [color="blue" style="bold"];    "RadOnc-GPT: A Large Language Model for Radiation Oncology" -> "Exploring the Capabilities and Limitations of Large Language Models for Radiation Oncology Decision Support" [color="blue" style="bold"];    "RadOnc-GPT: A Large Language Model for Radiation Oncology" -> "Benchmarking a foundation LLM on its ability to re-label structure names in accordance with the AAPM TG-263 report" [color="blue" style="bold"];    "RadOnc-GPT: A Large Language Model for Radiation Oncology" -> "Performance of ChatGPT on USMLE: Unlocking the Potential of Large Language Models for AI-Assisted Medical Education" [color="blue" style="bold"];    "RadOnc-GPT: A Large Language Model for Radiation Oncology" -> "HuaTuo: Tuning LLaMA Model with Chinese Medical Knowledge" [color="blue" style="bold"];    "Benchmarking ChatGPT-4 on ACR Radiation Oncology In-Training (TXIT) Exam and Red Journal Gray Zone Cases: Potentials and Challenges for AI-Assisted Medical Education and Decision Making in Radiation Oncology" -> "GPT4Vis: What Can GPT-4 Do for Zero-shot Visual Recognition?" [color="blue" style="bold"];    "Benchmarking ChatGPT-4 on ACR Radiation Oncology In-Training (TXIT) Exam and Red Journal Gray Zone Cases: Potentials and Challenges for AI-Assisted Medical Education and Decision Making in Radiation Oncology" -> "Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model" [color="blue" style="bold"];    "Benchmarking ChatGPT-4 on ACR Radiation Oncology In-Training (TXIT) Exam and Red Journal Gray Zone Cases: Potentials and Challenges for AI-Assisted Medical Education and Decision Making in Radiation Oncology" -> "ED-SAM: An Efficient Diffusion Sampling Approach to Domain Generalization in Vision-Language Foundation Models" [color="blue" style="bold"];    "Benchmarking ChatGPT-4 on ACR Radiation Oncology In-Training (TXIT) Exam and Red Journal Gray Zone Cases: Potentials and Challenges for AI-Assisted Medical Education and Decision Making in Radiation Oncology" -> "LMTurk: Few-Shot Learners as Crowdsourcing Workers in a Language-Model-as-a-Service Framework" [color="blue" style="bold"];    "Benchmarking ChatGPT-4 on ACR Radiation Oncology In-Training (TXIT) Exam and Red Journal Gray Zone Cases: Potentials and Challenges for AI-Assisted Medical Education and Decision Making in Radiation Oncology" -> "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models" [color="blue" style="bold"];    "Exploring the Capabilities and Limitations of Large Language Models for Radiation Oncology Decision Support" -> "AI as a Medical Ally: Evaluating ChatGPT's Usage and Impact in Indian Healthcare" [color="blue" style="bold"];    "Exploring the Capabilities and Limitations of Large Language Models for Radiation Oncology Decision Support" -> "Enhancing Adversarial Attacks through Chain of Thought" [color="blue" style="bold"];    "Exploring the Capabilities and Limitations of Large Language Models for Radiation Oncology Decision Support" -> "Poisoning Language Models During Instruction Tuning" [color="blue" style="bold"];    "Exploring the Capabilities and Limitations of Large Language Models for Radiation Oncology Decision Support" -> "ChatGPT Needs SPADE (Sustainability, PrivAcy, Digital divide, and Ethics) Evaluation: A Review" [color="blue" style="bold"];    "Exploring the Capabilities and Limitations of Large Language Models for Radiation Oncology Decision Support" -> "GPT Store Mining and Analysis" [color="blue" style="bold"];    "Benchmarking a foundation LLM on its ability to re-label structure names in accordance with the AAPM TG-263 report" -> "Exploring Federated Deep Learning for Standardising Naming Conventions in Radiotherapy Data" [color="blue" style="bold"];    "Benchmarking a foundation LLM on its ability to re-label structure names in accordance with the AAPM TG-263 report" -> "Artificial General Intelligence for Radiation Oncology" [color="blue" style="bold"];    "Benchmarking a foundation LLM on its ability to re-label structure names in accordance with the AAPM TG-263 report" -> "Comparative Analysis of nnUNet and MedNeXt for Head and Neck Tumor Segmentation in MRI-guided Radiotherapy" [color="blue" style="bold"];    "Performance of ChatGPT on USMLE: Unlocking the Potential of Large Language Models for AI-Assisted Medical Education" -> "A Categorical Archive of ChatGPT Failures" [color="blue" style="bold"];    "Performance of ChatGPT on USMLE: Unlocking the Potential of Large Language Models for AI-Assisted Medical Education" -> "Complementary Advantages of ChatGPTs and Human Readers in Reasoning: Evidence from English Text Reading Comprehension" [color="blue" style="bold"];    "Performance of ChatGPT on USMLE: Unlocking the Potential of Large Language Models for AI-Assisted Medical Education" -> "How Generative-AI can be Effectively used in Government Chatbots" [color="blue" style="bold"];    "Performance of ChatGPT on USMLE: Unlocking the Potential of Large Language Models for AI-Assisted Medical Education" -> "ChatGPT for Teaching and Learning: An Experience from Data Science Education" [color="blue" style="bold"];    "Performance of ChatGPT on USMLE: Unlocking the Potential of Large Language Models for AI-Assisted Medical Education" -> "Do Large Language Models Understand Verbal Indicators of Romantic Attraction?" [color="blue" style="bold"];    "HuaTuo: Tuning LLaMA Model with Chinese Medical Knowledge" -> "ChatDoctor: A Medical Chat Model Fine-Tuned on a Large Language Model Meta-AI (LLaMA) Using Medical Domain Knowledge" [color="blue" style="bold"];    "HuaTuo: Tuning LLaMA Model with Chinese Medical Knowledge" -> "LingYi: Medical Conversational Question Answering System based on Multi-modal Knowledge Graphs" [color="blue" style="bold"];    "HuaTuo: Tuning LLaMA Model with Chinese Medical Knowledge" -> "GLM: General Language Model Pretraining with Autoregressive Blank Infilling" [color="blue" style="bold"];    "HuaTuo: Tuning LLaMA Model with Chinese Medical Knowledge" -> "In ChatGPT We Trust? Measuring and Characterizing the Reliability of ChatGPT" [color="blue" style="bold"];    "math-PVS: A Large Language Model Framework to Map Scientific Publications to PVS Theories" -> "Training Verifiers to Solve Math Word Problems" [color="blue" style="bold"];    "math-PVS: A Large Language Model Framework to Map Scientific Publications to PVS Theories" -> "Notes on a Path to AI Assistance in Mathematical Reasoning" [color="blue" style="bold"];    "math-PVS: A Large Language Model Framework to Map Scientific Publications to PVS Theories" -> "MiniF2F: a cross-system benchmark for formal Olympiad-level mathematics" [color="blue" style="bold"];    "math-PVS: A Large Language Model Framework to Map Scientific Publications to PVS Theories" -> "Measuring Mathematical Problem Solving With the MATH Dataset" [color="blue" style="bold"];    "math-PVS: A Large Language Model Framework to Map Scientific Publications to PVS Theories" -> "World Models in Artificial Intelligence: Sensing, Learning, and Reasoning Like a Child" [color="blue" style="bold"];    "Training Verifiers to Solve Math Word Problems" -> "Solving Math Word Problems by Combining Language Models With Symbolic Solvers" [color="blue" style="bold"];    "Training Verifiers to Solve Math Word Problems" -> "How well do Large Language Models perform in Arithmetic tasks?" [color="blue" style="bold"];    "Training Verifiers to Solve Math Word Problems" -> "MathQA: Towards Interpretable Math Word Problem Solving with Operation-Based Formalisms" [color="blue" style="bold"];    "Training Verifiers to Solve Math Word Problems" -> "Learning by Fixing: Solving Math Word Problems with Weak Supervision" [color="blue" style="bold"];    "Training Verifiers to Solve Math Word Problems" -> "Neural Scaling Laws in Robotics" [color="blue" style="bold"];    "Notes on a Path to AI Assistance in Mathematical Reasoning" -> "Formalising perfectoid spaces" [color="blue" style="bold"];    "Notes on a Path to AI Assistance in Mathematical Reasoning" -> "A New Operator for Egyptian Fractions" [color="blue" style="bold"];    "Notes on a Path to AI Assistance in Mathematical Reasoning" -> "Formalization of the prime number theorem and Dirichlet's theorem" [color="blue" style="bold"];    "Notes on a Path to AI Assistance in Mathematical Reasoning" -> "Anisotropy as a diagnostic test for distinct tensor network wavefunctions of integer and half-integer spin Kitaev quantum spin liquids" [color="blue" style="bold"];    "Notes on a Path to AI Assistance in Mathematical Reasoning" -> "Strong non-vanishing of cohomologies and strong non-freeness of adjoint line bundles on $n$-Raynaud surfaces" [color="blue" style="bold"];    "MiniF2F: a cross-system benchmark for formal Olympiad-level mathematics" -> "Generative Language Modeling for Automated Theorem Proving" [color="blue" style="bold"];    "MiniF2F: a cross-system benchmark for formal Olympiad-level mathematics" -> "HOList: An Environment for Machine Learning of Higher-Order Theorem Proving" [color="blue" style="bold"];    "MiniF2F: a cross-system benchmark for formal Olympiad-level mathematics" -> "Lean Copilot: Large Language Models as Copilots for Theorem Proving in Lean" [color="blue" style="bold"];    "MiniF2F: a cross-system benchmark for formal Olympiad-level mathematics" -> "Natural Language Comprehension with the EpiReader" [color="blue" style="bold"];    "MiniF2F: a cross-system benchmark for formal Olympiad-level mathematics" -> "The LAMBADA dataset: Word prediction requiring a broad discourse context" [color="blue" style="bold"];    "Measuring Mathematical Problem Solving With the MATH Dataset" -> "Measuring Massive Multitask Language Understanding" [color="blue" style="bold"];    "Measuring Mathematical Problem Solving With the MATH Dataset" -> "Improving Graph Neural Network Representations of Logical Formulae with Subgraph Pooling" [color="blue" style="bold"];    "Measuring Mathematical Problem Solving With the MATH Dataset" -> "BERT-JAM: Boosting BERT-Enhanced Neural Machine Translation with Joint Attention" [color="blue" style="bold"];    "Measuring Mathematical Problem Solving With the MATH Dataset" -> "Choose Your Programming Copilot: A Comparison of the Program Synthesis Performance of GitHub Copilot and Genetic Programming" [color="blue" style="bold"];    "Measuring Mathematical Problem Solving With the MATH Dataset" -> "Stop Words for Processing Software Engineering Documents: Do they Matter?" [color="blue" style="bold"];    "World Models in Artificial Intelligence: Sensing, Learning, and Reasoning Like a Child" -> "Neurosymbolic AI: The 3rd Wave" [color="blue" style="bold"];    "World Models in Artificial Intelligence: Sensing, Learning, and Reasoning Like a Child" -> "Connecting the Dots in Trustworthy Artificial Intelligence: From AI Principles, Ethics, and Key Requirements to Responsible AI Systems and Regulation" [color="blue" style="bold"];    "World Models in Artificial Intelligence: Sensing, Learning, and Reasoning Like a Child" -> "Developing Responsible Chatbots for Financial Services: A Pattern-Oriented Responsible AI Engineering Approach" [color="blue" style="bold"];    "World Models in Artificial Intelligence: Sensing, Learning, and Reasoning Like a Child" -> "Socially Responsible AI Algorithms: Issues, Purposes, and Challenges" [color="blue" style="bold"];    "World Models in Artificial Intelligence: Sensing, Learning, and Reasoning Like a Child" -> "Strategies to architect AI Safety: Defense to guard AI from Adversaries" [color="blue" style="bold"];}
