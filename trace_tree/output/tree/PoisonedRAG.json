{
    "name": "PoisonedRAG: Knowledge Corruption Attacks to Retrieval-Augmented Generation of Large Language Models",
    "abstract": "Large language models (LLMs) have achieved remarkable success due to their\nexceptional generative capabilities. Despite their success, they also have\ninherent limitations such as a lack of up-to-date knowledge and hallucination.\nRetrieval-Augmented Generation (RAG) is a state-of-the-art technique to\nmitigate these limitations. The key idea of RAG is to ground the answer\ngeneration of an LLM on external knowledge retrieved from a knowledge database.\nExisting studies mainly focus on improving the accuracy or efficiency of RAG,\nleaving its security largely unexplored. We aim to bridge the gap in this work.\nWe find that the knowledge database in a RAG system introduces a new and\npractical attack surface. Based on this attack surface, we propose PoisonedRAG,\nthe first knowledge corruption attack to RAG, where an attacker could inject a\nfew malicious texts into the knowledge database of a RAG system to induce an\nLLM to generate an attacker-chosen target answer for an attacker-chosen target\nquestion. We formulate knowledge corruption attacks as an optimization problem,\nwhose solution is a set of malicious texts. Depending on the background\nknowledge (e.g., black-box and white-box settings) of an attacker on a RAG\nsystem, we propose two solutions to solve the optimization problem,\nrespectively. Our results show PoisonedRAG could achieve a 90% attack success\nrate when injecting five malicious texts for each target question into a\nknowledge database with millions of texts. We also evaluate several defenses\nand our results show they are insufficient to defend against PoisonedRAG,\nhighlighting the need for new defenses.",
    "children": [
        {
            "name": "From Prompt Injections to SQL Injection Attacks: How Protected is Your LLM-Integrated Web Application?",
            "abstract": "Large Language Models (LLMs) have found widespread applications in various\ndomains, including web applications, where they facilitate human interaction\nvia chatbots with natural language interfaces. Internally, aided by an\nLLM-integration middleware such as Langchain, user prompts are translated into\nSQL queries used by the LLM to provide meaningful responses to users. However,\nunsanitized user prompts can lead to SQL injection attacks, potentially\ncompromising the security of the database. Despite the growing interest in\nprompt injection vulnerabilities targeting LLMs, the specific risks of\ngenerating SQL injection attacks through prompt injections have not been\nextensively studied. In this paper, we present a comprehensive examination of\nprompt-to-SQL (P$_2$SQL) injections targeting web applications based on the\nLangchain framework. Using Langchain as our case study, we characterize\nP$_2$SQL injections, exploring their variants and impact on application\nsecurity through multiple concrete examples. Furthermore, we evaluate 7\nstate-of-the-art LLMs, demonstrating the pervasiveness of P$_2$SQL attacks\nacross language models. Our findings indicate that LLM-integrated applications\nbased on Langchain are highly susceptible to P$_2$SQL injection attacks,\nwarranting the adoption of robust defenses. To counter these attacks, we\npropose four effective defense techniques that can be integrated as extensions\nto the Langchain framework. We validate the defenses through an experimental\nevaluation with a real-world use case application.",
            "children": [
                {
                    "name": "Not what you've signed up for: Compromising Real-World LLM-Integrated Applications with Indirect Prompt Injection",
                    "abstract": "Large Language Models (LLMs) are increasingly being integrated into various\napplications. The functionalities of recent LLMs can be flexibly modulated via\nnatural language prompts. This renders them susceptible to targeted adversarial\nprompting, e.g., Prompt Injection (PI) attacks enable attackers to override\noriginal instructions and employed controls. So far, it was assumed that the\nuser is directly prompting the LLM. But, what if it is not the user prompting?\nWe argue that LLM-Integrated Applications blur the line between data and\ninstructions. We reveal new attack vectors, using Indirect Prompt Injection,\nthat enable adversaries to remotely (without a direct interface) exploit\nLLM-integrated applications by strategically injecting prompts into data likely\nto be retrieved. We derive a comprehensive taxonomy from a computer security\nperspective to systematically investigate impacts and vulnerabilities,\nincluding data theft, worming, information ecosystem contamination, and other\nnovel security risks. We demonstrate our attacks' practical viability against\nboth real-world systems, such as Bing's GPT-4 powered Chat and code-completion\nengines, and synthetic applications built on GPT-4. We show how processing\nretrieved prompts can act as arbitrary code execution, manipulate the\napplication's functionality, and control how and if other APIs are called.\nDespite the increasing integration and reliance on LLMs, effective mitigations\nof these emerging threats are currently lacking. By raising awareness of these\nvulnerabilities and providing key insights into their implications, we aim to\npromote the safe and responsible deployment of these powerful models and the\ndevelopment of robust defenses that protect users and systems from potential\nattacks.",
                    "children": [
                        {
                            "name": "Assessing Prompt Injection Risks in 200+ Custom GPTs",
                            "abstract": "In the rapidly evolving landscape of artificial intelligence, ChatGPT has\nbeen widely used in various applications. The new feature - customization of\nChatGPT models by users to cater to specific needs has opened new frontiers in\nAI utility. However, this study reveals a significant security vulnerability\ninherent in these user-customized GPTs: prompt injection attacks. Through\ncomprehensive testing of over 200 user-designed GPT models via adversarial\nprompts, we demonstrate that these systems are susceptible to prompt\ninjections. Through prompt injection, an adversary can not only extract the\ncustomized system prompts but also access the uploaded files. This paper\nprovides a first-hand analysis of the prompt injection, alongside the\nevaluation of the possible mitigation of such attacks. Our findings underscore\nthe urgent need for robust security frameworks in the design and deployment of\ncustomizable GPT models. The intent of this paper is to raise awareness and\nprompt action in the AI community, ensuring that the benefits of GPT\ncustomization do not come at the cost of compromised security and privacy.",
                            "children": []
                        },
                        {
                            "name": "All in How You Ask for It: Simple Black-Box Method for Jailbreak Attacks",
                            "abstract": "Large Language Models (LLMs), such as ChatGPT, encounter `jailbreak'\nchallenges, wherein safeguards are circumvented to generate ethically harmful\nprompts. This study introduces a straightforward black-box method for\nefficiently crafting jailbreak prompts, addressing the significant complexity\nand computational costs associated with conventional methods. Our technique\niteratively transforms harmful prompts into benign expressions directly\nutilizing the target LLM, predicated on the hypothesis that LLMs can\nautonomously generate expressions that evade safeguards. Through experiments\nconducted with ChatGPT (GPT-3.5 and GPT-4) and Gemini-Pro, our method\nconsistently achieved an attack success rate exceeding 80% within an average of\nfive iterations for forbidden questions and proved robust against model\nupdates. The jailbreak prompts generated were not only naturally-worded and\nsuccinct but also challenging to defend against. These findings suggest that\nthe creation of effective jailbreak prompts is less complex than previously\nbelieved, underscoring the heightened risk posed by black-box jailbreak\nattacks.",
                            "children": []
                        },
                        {
                            "name": "Great, Now Write an Article About That: The Crescendo Multi-Turn LLM Jailbreak Attack",
                            "abstract": "Large Language Models (LLMs) have risen significantly in popularity and are\nincreasingly being adopted across multiple applications. These LLMs are heavily\naligned to resist engaging in illegal or unethical topics as a means to avoid\ncontributing to responsible AI harms. However, a recent line of attacks, known\nas jailbreaks, seek to overcome this alignment. Intuitively, jailbreak attacks\naim to narrow the gap between what the model can do and what it is willing to\ndo. In this paper, we introduce a novel jailbreak attack called Crescendo.\nUnlike existing jailbreak methods, Crescendo is a simple multi-turn jailbreak\nthat interacts with the model in a seemingly benign manner. It begins with a\ngeneral prompt or question about the task at hand and then gradually escalates\nthe dialogue by referencing the model's replies progressively leading to a\nsuccessful jailbreak. We evaluate Crescendo on various public systems,\nincluding ChatGPT, Gemini Pro, Gemini-Ultra, LlaMA-2 70b and LlaMA-3 70b Chat,\nand Anthropic Chat. Our results demonstrate the strong efficacy of Crescendo,\nwith it achieving high attack success rates across all evaluated models and\ntasks. Furthermore, we present Crescendomation, a tool that automates the\nCrescendo attack and demonstrate its efficacy against state-of-the-art models\nthrough our evaluations. Crescendomation surpasses other state-of-the-art\njailbreaking techniques on the AdvBench subset dataset, achieving 29-61% higher\nperformance on GPT-4 and 49-71% on Gemini-Pro. Finally, we also demonstrate\nCrescendo's ability to jailbreak multimodal models.",
                            "children": []
                        }
                    ]
                },
                {
                    "name": "Automatic and Universal Prompt Injection Attacks against Large Language Models",
                    "abstract": "Large Language Models (LLMs) excel in processing and generating human\nlanguage, powered by their ability to interpret and follow instructions.\nHowever, their capabilities can be exploited through prompt injection attacks.\nThese attacks manipulate LLM-integrated applications into producing responses\naligned with the attacker's injected content, deviating from the user's actual\nrequests. The substantial risks posed by these attacks underscore the need for\na thorough understanding of the threats. Yet, research in this area faces\nchallenges due to the lack of a unified goal for such attacks and their\nreliance on manually crafted prompts, complicating comprehensive assessments of\nprompt injection robustness. We introduce a unified framework for understanding\nthe objectives of prompt injection attacks and present an automated\ngradient-based method for generating highly effective and universal prompt\ninjection data, even in the face of defensive measures. With only five training\nsamples (0.3% relative to the test data), our attack can achieve superior\nperformance compared with baselines. Our findings emphasize the importance of\ngradient-based testing, which can avoid overestimation of robustness,\nespecially for defense mechanisms.",
                    "children": [
                        {
                            "name": "Tensor Trust: Interpretable Prompt Injection Attacks from an Online Game",
                            "abstract": "While Large Language Models (LLMs) are increasingly being used in real-world\napplications, they remain vulnerable to prompt injection attacks: malicious\nthird party prompts that subvert the intent of the system designer. To help\nresearchers study this problem, we present a dataset of over 126,000 prompt\ninjection attacks and 46,000 prompt-based \"defenses\" against prompt injection,\nall created by players of an online game called Tensor Trust. To the best of\nour knowledge, this is currently the largest dataset of human-generated\nadversarial examples for instruction-following LLMs. The attacks in our dataset\nhave a lot of easily interpretable stucture, and shed light on the weaknesses\nof LLMs. We also use the dataset to create a benchmark for resistance to two\ntypes of prompt injection, which we refer to as prompt extraction and prompt\nhijacking. Our benchmark results show that many models are vulnerable to the\nattack strategies in the Tensor Trust dataset. Furthermore, we show that some\nattack strategies from the dataset generalize to deployed LLM-based\napplications, even though they have a very different set of constraints to the\ngame. We release all data and source code at https://tensortrust.ai/paper",
                            "children": []
                        },
                        {
                            "name": "StruQ: Defending Against Prompt Injection with Structured Queries",
                            "abstract": "Recent advances in Large Language Models (LLMs) enable exciting\nLLM-integrated applications, which perform text-based tasks by utilizing their\nadvanced language understanding capabilities. However, as LLMs have improved,\nso have the attacks against them. Prompt injection attacks are an important\nthreat: they trick the model into deviating from the original application's\ninstructions and instead follow user directives. These attacks rely on the\nLLM's ability to follow instructions and inability to separate prompts and user\ndata. We introduce structured queries, a general approach to tackle this\nproblem. Structured queries separate prompts and data into two channels. We\nimplement a system that supports structured queries. This system is made of (1)\na secure front-end that formats a prompt and user data into a special format,\nand (2) a specially trained LLM that can produce high-quality outputs from\nthese inputs. The LLM is trained using a novel fine-tuning strategy: we convert\na base (non-instruction-tuned) LLM to a structured instruction-tuned model that\nwill only follow instructions in the prompt portion of a query. To do so, we\naugment standard instruction tuning datasets with examples that also include\ninstructions in the data portion of the query, and fine-tune the model to\nignore these. Our system significantly improves resistance to prompt injection\nattacks, with little or no impact on utility. Our code is released at\nhttps://github.com/Sizhe-Chen/StruQ.",
                            "children": []
                        },
                        {
                            "name": "Enhancing Adversarial Attacks through Chain of Thought",
                            "abstract": "Large language models (LLMs) have demonstrated impressive performance across\nvarious domains but remain susceptible to safety concerns. Prior research\nindicates that gradient-based adversarial attacks are particularly effective\nagainst aligned LLMs and the chain of thought (CoT) prompting can elicit\ndesired answers through step-by-step reasoning. This paper proposes enhancing\nthe robustness of adversarial attacks on aligned LLMs by integrating CoT\nprompts with the greedy coordinate gradient (GCG) technique. Using CoT triggers\ninstead of affirmative targets stimulates the reasoning abilities of backend\nLLMs, thereby improving the transferability and universality of adversarial\nattacks. We conducted an ablation study comparing our CoT-GCG approach with\nAmazon Web Services auto-cot. Results revealed our approach outperformed both\nthe baseline GCG attack and CoT prompting. Additionally, we used Llama Guard to\nevaluate potentially harmful interactions, providing a more objective risk\nassessment of entire conversations compared to matching outputs to rejection\nphrases. The code of this paper is available at\nhttps://github.com/sujingbo0217/CS222W24-LLM-Attack.",
                            "children": []
                        }
                    ]
                },
                {
                    "name": "Spear Phishing With Large Language Models",
                    "abstract": "Recent progress in artificial intelligence (AI), particularly in the domain\nof large language models (LLMs), has resulted in powerful and versatile\ndual-use systems. This intelligence can be put towards a wide variety of\nbeneficial tasks, yet it can also be used to cause harm. This study explores\none such harm by examining how LLMs can be used for spear phishing, a form of\ncybercrime that involves manipulating targets into divulging sensitive\ninformation. I first explore LLMs' ability to assist with the reconnaissance\nand message generation stages of a spear phishing attack, where I find that\nLLMs are capable of assisting with the email generation phase of a spear\nphishing attack. To explore how LLMs could potentially be harnessed to scale\nspear phishing campaigns, I then create unique spear phishing messages for over\n600 British Members of Parliament using OpenAI's GPT-3.5 and GPT-4 models. My\nfindings provide some evidence that these messages are not only realistic but\nalso cost-effective, with each email costing only a fraction of a cent to\ngenerate. Next, I demonstrate how basic prompt engineering can circumvent\nsafeguards installed in LLMs, highlighting the need for further research into\nrobust interventions that can help prevent models from being misused. To\nfurther address these evolving risks, I explore two potential solutions:\nstructured access schemes, such as application programming interfaces, and\nLLM-based defensive systems.",
                    "children": [
                        {
                            "name": "Prompted Contextual Vectors for Spear-Phishing Detection",
                            "abstract": "Spear-phishing attacks present a significant security challenge, with large\nlanguage models (LLMs) escalating the threat by generating convincing emails\nand facilitating target reconnaissance. To address this, we propose a detection\napproach based on a novel document vectorization method that utilizes an\nensemble of LLMs to create representation vectors. By prompting LLMs to reason\nand respond to human-crafted questions, we quantify the presence of common\npersuasion principles in the email's content, producing prompted contextual\ndocument vectors for a downstream supervised machine learning model. We\nevaluate our method using a unique dataset generated by a proprietary system\nthat automates target reconnaissance and spear-phishing email creation. Our\nmethod achieves a 91\\% F1 score in identifying LLM-generated spear-phishing\nemails, with the training set comprising only traditional phishing and benign\nemails. Key contributions include a novel document vectorization method\nutilizing LLM reasoning, a publicly available dataset of high-quality\nspear-phishing emails, and the demonstrated effectiveness of our method in\ndetecting such emails. This methodology can be utilized for various document\nclassification tasks, particularly in adversarial problem domains.",
                            "children": []
                        },
                        {
                            "name": "A Survey on Large Language Model (LLM) Security and Privacy: The Good, the Bad, and the Ugly",
                            "abstract": "Large Language Models (LLMs), such as ChatGPT and Bard, have revolutionized\nnatural language understanding and generation. They possess deep language\ncomprehension, human-like text generation capabilities, contextual awareness,\nand robust problem-solving skills, making them invaluable in various domains\n(e.g., search engines, customer support, translation). In the meantime, LLMs\nhave also gained traction in the security community, revealing security\nvulnerabilities and showcasing their potential in security-related tasks. This\npaper explores the intersection of LLMs with security and privacy.\nSpecifically, we investigate how LLMs positively impact security and privacy,\npotential risks and threats associated with their use, and inherent\nvulnerabilities within LLMs. Through a comprehensive literature review, the\npaper categorizes the papers into \"The Good\" (beneficial LLM applications),\n\"The Bad\" (offensive applications), and \"The Ugly\" (vulnerabilities of LLMs and\ntheir defenses). We have some interesting findings. For example, LLMs have\nproven to enhance code security (code vulnerability detection) and data privacy\n(data confidentiality protection), outperforming traditional methods. However,\nthey can also be harnessed for various attacks (particularly user-level\nattacks) due to their human-like reasoning abilities. We have identified areas\nthat require further research efforts. For example, Research on model and\nparameter extraction attacks is limited and often theoretical, hindered by LLM\nparameter scale and confidentiality. Safe instruction tuning, a recent\ndevelopment, requires more exploration. We hope that our work can shed light on\nthe LLMs' potential to both bolster and jeopardize cybersecurity.",
                            "children": []
                        },
                        {
                            "name": "Devising and Detecting Phishing: Large Language Models vs. Smaller Human Models",
                            "abstract": "AI programs, built using large language models, make it possible to\nautomatically create phishing emails based on a few data points about a user.\nThey stand in contrast to traditional phishing emails that hackers manually\ndesign using general rules gleaned from experience. The V-Triad is an advanced\nset of rules for manually designing phishing emails to exploit our cognitive\nheuristics and biases. In this study, we compare the performance of phishing\nemails created automatically by GPT-4 and manually using the V-Triad. We also\ncombine GPT-4 with the V-Triad to assess their combined potential. A fourth\ngroup, exposed to generic phishing emails, was our control group. We utilized a\nfactorial approach, sending emails to 112 randomly selected participants\nrecruited for the study. The control group emails received a click-through rate\nbetween 19-28%, the GPT-generated emails 30-44%, emails generated by the\nV-Triad 69-79%, and emails generated by GPT and the V-Triad 43-81%. Each\nparticipant was asked to explain why they pressed or did not press a link in\nthe email. These answers often contradict each other, highlighting the need for\npersonalized content. The cues that make one person avoid phishing emails make\nanother person fall for them. Next, we used four popular large language models\n(GPT, Claude, PaLM, and LLaMA) to detect the intention of phishing emails and\ncompare the results to human detection. The language models demonstrated a\nstrong ability to detect malicious intent, even in non-obvious phishing emails.\nThey sometimes surpassed human detection, although often being slightly less\naccurate than humans. Finally, we make an analysis of the economic aspects of\nAI-enabled phishing attacks, showing how large language models can increase the\nincentives of phishing and spear phishing by reducing their costs.",
                            "children": []
                        }
                    ]
                }
            ]
        },
        {
            "name": "A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions",
            "abstract": "The emergence of large language models (LLMs) has marked a significant\nbreakthrough in natural language processing (NLP), fueling a paradigm shift in\ninformation acquisition. Nevertheless, LLMs are prone to hallucination,\ngenerating plausible yet nonfactual content. This phenomenon raises significant\nconcerns over the reliability of LLMs in real-world information retrieval (IR)\nsystems and has attracted intensive research to detect and mitigate such\nhallucinations. Given the open-ended general-purpose attributes inherent to\nLLMs, LLM hallucinations present distinct challenges that diverge from prior\ntask-specific models. This divergence highlights the urgency for a nuanced\nunderstanding and comprehensive overview of recent advances in LLM\nhallucinations. In this survey, we begin with an innovative taxonomy of\nhallucination in the era of LLM and then delve into the factors contributing to\nhallucinations. Subsequently, we present a thorough overview of hallucination\ndetection methods and benchmarks. Our discussion then transfers to\nrepresentative methodologies for mitigating LLM hallucinations. Additionally,\nwe delve into the current limitations faced by retrieval-augmented LLMs in\ncombating hallucinations, offering insights for developing more robust IR\nsystems. Finally, we highlight the promising research directions on LLM\nhallucinations, including hallucination in large vision-language models and\nunderstanding of knowledge boundaries in LLM hallucinations.",
            "children": [
                {
                    "name": "The Dawn After the Dark: An Empirical Study on Factuality Hallucination in Large Language Models",
                    "abstract": "In the era of large language models (LLMs), hallucination (i.e., the tendency\nto generate factually incorrect content) poses great challenge to trustworthy\nand reliable deployment of LLMs in real-world applications. To tackle the LLM\nhallucination, three key questions should be well studied: how to detect\nhallucinations (detection), why do LLMs hallucinate (source), and what can be\ndone to mitigate them (mitigation). To address these challenges, this work\npresents a systematic empirical study on LLM hallucination, focused on the the\nthree aspects of hallucination detection, source and mitigation. Specially, we\nconstruct a new hallucination benchmark HaluEval 2.0, and designs a simple yet\neffective detection method for LLM hallucination. Furthermore, we zoom into the\ndifferent training or utilization stages of LLMs and extensively analyze the\npotential factors that lead to the LLM hallucination. Finally, we implement and\nexamine a series of widely used techniques to mitigate the hallucinations in\nLLMs. Our work has led to several important findings to understand the\nhallucination origin and mitigate the hallucinations in LLMs. Our code and data\ncan be accessed at https://github.com/RUCAIBox/HaluEval-2.0.",
                    "children": [
                        {
                            "name": "Hallucination Diversity-Aware Active Learning for Text Summarization",
                            "abstract": "Large Language Models (LLMs) have shown propensity to generate hallucinated\noutputs, i.e., texts that are factually incorrect or unsupported. Existing\nmethods for alleviating hallucinations typically require costly human\nannotations to identify and correct hallucinations in LLM outputs. Moreover,\nmost of these methods focus on a specific type of hallucination, e.g., entity\nor token errors, which limits their effectiveness in addressing various types\nof hallucinations exhibited in LLM outputs. To our best knowledge, in this\npaper we propose the first active learning framework to alleviate LLM\nhallucinations, reducing costly human annotations of hallucination needed. By\nmeasuring fine-grained hallucinations from errors in semantic frame, discourse\nand content verifiability in text summarization, we propose HAllucination\nDiversity-Aware Sampling (HADAS) to select diverse hallucinations for\nannotations in active learning for LLM finetuning. Extensive experiments on\nthree datasets and different backbone models demonstrate advantages of our\nmethod in effectively and efficiently mitigating LLM hallucinations.",
                            "children": []
                        },
                        {
                            "name": "Do Language Models Know When They're Hallucinating References?",
                            "abstract": "State-of-the-art language models (LMs) are notoriously susceptible to\ngenerating hallucinated information. Such inaccurate outputs not only undermine\nthe reliability of these models but also limit their use and raise serious\nconcerns about misinformation and propaganda. In this work, we focus on\nhallucinated book and article references and present them as the \"model\norganism\" of language model hallucination research, due to their frequent and\neasy-to-discern nature. We posit that if a language model cites a particular\nreference in its output, then it should ideally possess sufficient information\nabout its authors and content, among other relevant details. Using this basic\ninsight, we illustrate that one can identify hallucinated references without\never consulting any external resources, by asking a set of direct or indirect\nqueries to the language model about the references. These queries can be\nconsidered as \"consistency checks.\" Our findings highlight that while LMs,\nincluding GPT-4, often produce inconsistent author lists for hallucinated\nreferences, they also often accurately recall the authors of real references.\nIn this sense, the LM can be said to \"know\" when it is hallucinating\nreferences. Furthermore, these findings show how hallucinated references can be\ndissected to shed light on their nature. Replication code and results can be\nfound at https://github.com/microsoft/hallucinated-references.",
                            "children": []
                        }
                    ]
                },
                {
                    "name": "A Survey of Hallucination in Large Foundation Models",
                    "abstract": "Hallucination in a foundation model (FM) refers to the generation of content\nthat strays from factual reality or includes fabricated information. This\nsurvey paper provides an extensive overview of recent efforts that aim to\nidentify, elucidate, and tackle the problem of hallucination, with a particular\nfocus on ``Large'' Foundation Models (LFMs). The paper classifies various types\nof hallucination phenomena that are specific to LFMs and establishes evaluation\ncriteria for assessing the extent of hallucination. It also examines existing\nstrategies for mitigating hallucination in LFMs and discusses potential\ndirections for future research in this area. Essentially, the paper offers a\ncomprehensive examination of the challenges and solutions related to\nhallucination in LFMs.",
                    "children": [
                        {
                            "name": "Halo: Estimation and Reduction of Hallucinations in Open-Source Weak Large Language Models",
                            "abstract": "Large Language Models (LLMs) have revolutionized Natural Language Processing\n(NLP). Although convenient for research and practical applications, open-source\nLLMs with fewer parameters often suffer from severe hallucinations compared to\ntheir larger counterparts. This paper focuses on measuring and reducing\nhallucinations in BLOOM 7B, a representative of such weaker open-source LLMs\nthat are publicly available for research and commercial applications. We\nintroduce HaloCheck, a lightweight BlackBox knowledge-free framework designed\nto quantify the severity of hallucinations in LLMs. Additionally, we explore\ntechniques like knowledge injection and teacher-student approaches to alleviate\nhallucinations in low-parameter LLMs. Our experiments effectively demonstrate\nthe reduction of hallucinations in challenging domains for these LLMs.",
                            "children": []
                        },
                        {
                            "name": "Plausible May Not Be Faithful: Probing Object Hallucination in Vision-Language Pre-training",
                            "abstract": "Large-scale vision-language pre-trained (VLP) models are prone to hallucinate\nnon-existent visual objects when generating text based on visual information.\nIn this paper, we systematically study the object hallucination problem from\nthree aspects. First, we examine recent state-of-the-art VLP models, showing\nthat they still hallucinate frequently, and models achieving better scores on\nstandard metrics (e.g., CIDEr) could be more unfaithful. Second, we investigate\nhow different types of image encoding in VLP influence hallucination, including\nregion-based, grid-based, and patch-based. Surprisingly, we find that\npatch-based features perform the best and smaller patch resolution yields a\nnon-trivial reduction in object hallucination. Third, we decouple various VLP\nobjectives and demonstrate that token-level image-text alignment and controlled\ngeneration are crucial to reducing hallucination. Based on that, we propose a\nsimple yet effective VLP loss named ObjMLM to further mitigate object\nhallucination. Results show that it reduces object hallucination by up to 17.4%\nwhen tested on two benchmarks (COCO Caption for in-domain and NoCaps for\nout-of-domain evaluation).",
                            "children": []
                        }
                    ]
                },
                {
                    "name": "Detecting Hallucinated Content in Conditional Neural Sequence Generation",
                    "abstract": "Neural sequence models can generate highly fluent sentences, but recent\nstudies have also shown that they are also prone to hallucinate additional\ncontent not supported by the input. These variety of fluent but wrong outputs\nare particularly problematic, as it will not be possible for users to tell they\nare being presented incorrect content. To detect these errors, we propose a\ntask to predict whether each token in the output sequence is hallucinated (not\ncontained in the input) and collect new manually annotated evaluation sets for\nthis task. We also introduce a method for learning to detect hallucinations\nusing pretrained language models fine tuned on synthetic data that includes\nautomatically inserted hallucinations Experiments on machine translation (MT)\nand abstractive summarization demonstrate that our proposed approach\nconsistently outperforms strong baselines on all benchmark datasets. We further\ndemonstrate how to use the token-level hallucination labels to define a\nfine-grained loss over the target sequence in low-resource MT and achieve\nsignificant improvements over strong baseline methods. We also apply our method\nto word-level quality estimation for MT and show its effectiveness in both\nsupervised and unsupervised settings. Codes and data available at\nhttps://github.com/violet-zct/fairseq-detect-hallucination.",
                    "children": [
                        {
                            "name": "Improved Natural Language Generation via Loss Truncation",
                            "abstract": "Neural language models are usually trained to match the distributional\nproperties of a large-scale corpus by minimizing the log loss. While\nstraightforward to optimize, this approach forces the model to reproduce all\nvariations in the dataset, including noisy and invalid references (e.g.,\nmisannotation and hallucinated facts). Worse, the commonly used log loss is\noverly sensitive to such phenomena and even a small fraction of noisy data can\ndegrade performance. In this work, we show that the distinguishability of the\nmodels and reference serves as a principled and robust alternative for handling\ninvalid references. To optimize distinguishability, we propose loss truncation,\nwhich adaptively removes high loss examples during training. We show this is as\neasy to optimize as log loss and tightly bounds distinguishability under noise.\nEmpirically, we demonstrate that loss truncation outperforms existing baselines\non distinguishability on a summarization task, and show that samples generated\nby the loss truncation model have factual accuracy ratings that exceed those of\nbaselines and match human references.",
                            "children": []
                        },
                        {
                            "name": "Domain Robustness in Neural Machine Translation",
                            "abstract": "Translating text that diverges from the training domain is a key challenge\nfor machine translation. Domain robustness---the generalization of models to\nunseen test domains---is low for both statistical (SMT) and neural machine\ntranslation (NMT). In this paper, we study the performance of SMT and NMT\nmodels on out-of-domain test sets. We find that in unknown domains, SMT and NMT\nsuffer from very different problems: SMT systems are mostly adequate but not\nfluent, while NMT systems are mostly fluent, but not adequate. For NMT, we\nidentify such hallucinations (translations that are fluent but unrelated to the\nsource) as a key reason for low domain robustness. To mitigate this problem, we\nempirically compare methods that are reported to improve adequacy or in-domain\nrobustness in terms of their effectiveness at improving domain robustness. In\nexperiments on German to English OPUS data, and German to Romansh (a\nlow-resource setting) we find that several methods improve domain robustness.\nWhile those methods do lead to higher BLEU scores overall, they only slightly\nincrease the adequacy of translations compared to SMT.",
                            "children": []
                        },
                        {
                            "name": "Six Challenges for Neural Machine Translation",
                            "abstract": "We explore six challenges for neural machine translation: domain mismatch,\namount of training data, rare words, long sentences, word alignment, and beam\nsearch. We show both deficiencies and improvements over the quality of\nphrase-based statistical machine translation.",
                            "children": []
                        }
                    ]
                }
            ]
        },
        {
            "name": "Evaluating Large Language Models for Document-grounded Response Generation in Information-Seeking Dialogues",
            "abstract": "In this paper, we investigate the use of large language models (LLMs) like\nChatGPT for document-grounded response generation in the context of\ninformation-seeking dialogues. For evaluation, we use the MultiDoc2Dial corpus\nof task-oriented dialogues in four social service domains previously used in\nthe DialDoc 2022 Shared Task. Information-seeking dialogue turns are grounded\nin multiple documents providing relevant information. We generate dialogue\ncompletion responses by prompting a ChatGPT model, using two methods:\nChat-Completion and LlamaIndex. ChatCompletion uses knowledge from ChatGPT\nmodel pretraining while LlamaIndex also extracts relevant information from\ndocuments. Observing that document-grounded response generation via LLMs cannot\nbe adequately assessed by automatic evaluation metrics as they are\nsignificantly more verbose, we perform a human evaluation where annotators rate\nthe output of the shared task winning system, the two Chat-GPT variants\noutputs, and human responses. While both ChatGPT variants are more likely to\ninclude information not present in the relevant segments, possibly including a\npresence of hallucinations, they are rated higher than both the shared task\nwinning system and human responses.",
            "children": [
                {
                    "name": "A Statistical Framework for Ranking LLM-Based Chatbots",
                    "abstract": "Large language models (LLMs) have transformed natural language processing,\nwith frameworks like Chatbot Arena providing pioneering platforms for\nevaluating these models. By facilitating millions of pairwise comparisons based\non human judgments, Chatbot Arena has become a cornerstone in LLM evaluation,\noffering rich datasets for ranking models in open-ended conversational tasks.\nBuilding upon this foundation, we propose a statistical framework that\nincorporates key advancements to address specific challenges in pairwise\ncomparison analysis. First, we introduce a factored tie model that enhances the\nability to handle ties -- an integral aspect of human-judged comparisons --\nsignificantly improving the model's fit to observed data. Second, we extend the\nframework to model covariance between competitors, enabling deeper insights\ninto performance relationships and facilitating intuitive groupings into\nperformance tiers. Third, we resolve optimization challenges arising from\nparameter non-uniqueness by introducing novel constraints, ensuring stable and\ninterpretable parameter estimation. Through rigorous evaluation and extensive\nexperimentation, our framework demonstrates substantial improvements over\nexisting methods in modeling pairwise comparison data. To support\nreproducibility and practical adoption, we release leaderbot, an open-source\nPython package implementing our models and analyses.",
                    "children": [
                        {
                            "name": "MORTAR: Metamorphic Multi-turn Testing for LLM-based Dialogue Systems",
                            "abstract": "With the widespread application of LLM-based dialogue systems in daily life,\nquality assurance has become more important than ever. Recent research has\nsuccessfully introduced methods to identify unexpected behaviour in single-turn\nscenarios. However, multi-turn dialogue testing remains underexplored, with the\nOracle problem in multi-turn testing posing a persistent challenge for dialogue\nsystem developers and researchers. In this paper, we propose MORTAR, a\nMetamORphic multi-TuRn diAlogue testing appRoach, which mitigates the test\noracle problem in the assessment of LLM-based dialogue systems. MORTAR\nautomates the generation of follow-up question-answer (QA) dialogue test cases\nwith multiple dialogue-level perturbations and metamorphic relations. MORTAR\nemploys a novel knowledge graph-based dialogue information model which\neffectively generates perturbed dialogue test datasets and detects bugs of\nmulti-turn dialogue systems in a low-cost manner. The proposed approach does\nnot require an LLM as a judge, eliminating potential of any biases in the\nevaluation step. According to the experiment results on multiple LLM-based\ndialogue systems and comparisons with single-turn metamorphic testing\napproaches, MORTAR explores more unique bugs in LLM-based dialogue systems,\nespecially for severe bugs that MORTAR detects up to four times more unique\nbugs than the most effective existing metamorphic testing approach.",
                            "children": []
                        },
                        {
                            "name": "I like fish, especially dolphins: Addressing Contradictions in Dialogue Modeling",
                            "abstract": "To quantify how well natural language understanding models can capture\nconsistency in a general conversation, we introduce the DialoguE COntradiction\nDEtection task (DECODE) and a new conversational dataset containing both\nhuman-human and human-bot contradictory dialogues. We then compare a structured\nutterance-based approach of using pre-trained Transformer models for\ncontradiction detection with the typical unstructured approach. Results reveal\nthat: (i) our newly collected dataset is notably more effective at providing\nsupervision for the dialogue contradiction detection task than existing NLI\ndata including those aimed to cover the dialogue domain; (ii) the structured\nutterance-based approach is more robust and transferable on both analysis and\nout-of-distribution dialogues than its unstructured counterpart. We also show\nthat our best contradiction detection model correlates well with human\njudgments and further provide evidence for its usage in both automatically\nevaluating and improving the consistency of state-of-the-art generative\nchatbots.",
                            "children": []
                        },
                        {
                            "name": "RoBERTa: A Robustly Optimized BERT Pretraining Approach",
                            "abstract": "Language model pretraining has led to significant performance gains but\ncareful comparison between different approaches is challenging. Training is\ncomputationally expensive, often done on private datasets of different sizes,\nand, as we will show, hyperparameter choices have significant impact on the\nfinal results. We present a replication study of BERT pretraining (Devlin et\nal., 2019) that carefully measures the impact of many key hyperparameters and\ntraining data size. We find that BERT was significantly undertrained, and can\nmatch or exceed the performance of every model published after it. Our best\nmodel achieves state-of-the-art results on GLUE, RACE and SQuAD. These results\nhighlight the importance of previously overlooked design choices, and raise\nquestions about the source of recently reported improvements. We release our\nmodels and code.",
                            "children": []
                        }
                    ]
                },
                {
                    "name": "Prompted LLMs as Chatbot Modules for Long Open-domain Conversation",
                    "abstract": "In this paper, we propose MPC (Modular Prompted Chatbot), a new approach for\ncreating high-quality conversational agents without the need for fine-tuning.\nOur method utilizes pre-trained large language models (LLMs) as individual\nmodules for long-term consistency and flexibility, by using techniques such as\nfew-shot prompting, chain-of-thought (CoT), and external memory. Our human\nevaluation results show that MPC is on par with fine-tuned chatbot models in\nopen-domain conversations, making it an effective solution for creating\nconsistent and engaging chatbots.",
                    "children": [
                        {
                            "name": "BlenderBot 3: a deployed conversational agent that continually learns to responsibly engage",
                            "abstract": "We present BlenderBot 3, a 175B parameter dialogue model capable of\nopen-domain conversation with access to the internet and a long-term memory,\nand having been trained on a large number of user defined tasks. We release\nboth the model weights and code, and have also deployed the model on a public\nweb page to interact with organic users. This technical report describes how\nthe model was built (architecture, model and training scheme), and details of\nits deployment, including safety mechanisms. Human evaluations show its\nsuperiority to existing open-domain dialogue agents, including its predecessors\n(Roller et al., 2021; Komeili et al., 2022). Finally, we detail our plan for\ncontinual learning using the data collected from deployment, which will also be\npublicly released. The goal of this research program is thus to enable the\ncommunity to study ever-improving responsible agents that learn through\ninteraction.",
                            "children": []
                        },
                        {
                            "name": "A Framework for Evaluating Appropriateness, Trustworthiness, and Safety in Mental Wellness AI Chatbots",
                            "abstract": "Large language model (LLM) chatbots are susceptible to biases and\nhallucinations, but current evaluations of mental wellness technologies lack\ncomprehensive case studies to evaluate their practical applications. Here, we\naddress this gap by introducing the MHealth-EVAL framework, a new role-play\nbased interactive evaluation method designed specifically for evaluating the\nappropriateness, trustworthiness, and safety of mental wellness chatbots. We\nalso introduce Psyfy, a new chatbot leveraging LLMs to facilitate\ntransdiagnostic Cognitive Behavioral Therapy (CBT). We demonstrate the\nMHealth-EVAL framework's utility through a comparative study of two versions of\nPsyfy against standard baseline chatbots. Our results showed that Psyfy\nchatbots outperformed the baseline chatbots in delivering appropriate\nresponses, engaging users, and avoiding untrustworthy responses. However, both\nPsyfy and the baseline chatbots exhibited some limitations, such as providing\npredominantly US-centric resources. While Psyfy chatbots were able to identify\nmost unsafe situations and avoid giving unsafe responses, they sometimes\nstruggled to recognize subtle harmful intentions when prompted in role play\nscenarios. Our study demonstrates a practical application of the MHealth-EVAL\nframework and showcases Psyfy's utility in harnessing LLMs to enhance user\nengagement and provide flexible and appropriate responses aligned with an\nevidence-based CBT approach.",
                            "children": []
                        },
                        {
                            "name": "OmniDialog: An Omnipotent Pre-training Model for Task-Oriented Dialogue System",
                            "abstract": "Pre-trained conversation models (PCMs) have demonstrated remarkable results\nin task-oriented dialogue (TOD) systems. Many PCMs focus predominantly on\ndialogue management tasks like dialogue state tracking, dialogue generation\ntasks like response generation, or both. However, the existing PCMs seldom\nconsider dialogue comprehension tasks, such as dialogue question answering and\nsummarization tasks. These tasks allow PCMs to glean dialogue context from\nvarious angles. This observation naturally raises the question: Can the\nperformance of downstream dialogue tasks be enhanced if a PCM is pre-trained on\ndialogue management, generation, and comprehension tasks?\n  To investigate this, we proposed an Omnipotent Dialogue pre-training model\n(OmniDialog). It unifies these three dialogue tasks into a monolithic framework\nby multi-task learning, fostering inter-task communication. The pre-training\ncorpus of OmniDialog spans $\\mathbf{7}$ dialogue-focused tasks, drawing from\n$\\mathbf{15}$ datasets and encompassing over $\\mathbf{3.2}$ million dialogue\nutterances. To our knowledge, OmniDialog is a pioneering PCM pre-trained across\ndialogue management, generation, and comprehension domains. We evaluated its\nperformance across four tasks: dialogue summarization, end-to-end dialogue\nmodeling, dialogue state tracking, and intent classification. The results\nunderscore its efficacy in domain transfer learning, low-resource, and\nfull-dataset scenarios. Furthermore, to glean a nuanced understanding of\nOmniDialog's strengths and potential pitfalls, we designed a fine-grained\nanalysis framework for dialogue-centric tasks. Experimental results show that\nthe OmniDialog is good at hard samples, such as long dialogues and lengthy\nresponses.",
                            "children": []
                        }
                    ]
                },
                {
                    "name": "Decomposed Prompting: Unveiling Multilingual Linguistic Structure Knowledge in English-Centric Large Language Models",
                    "abstract": "Despite the predominance of English in their training data, English-centric\nLarge Language Models (LLMs) like GPT-3 and LLaMA display a remarkable ability\nto perform multilingual tasks, raising questions about the depth and nature of\ntheir cross-lingual capabilities. This paper introduces the decomposed\nprompting approach to probe the linguistic structure understanding of these\nLLMs in sequence labeling tasks. Diverging from the single text-to-text prompt,\nour method generates for each token of the input sentence an individual prompt\nwhich asks for its linguistic label. We assess our method on the Universal\nDependencies part-of-speech tagging dataset for 38 languages, utilizing both\nEnglish-centric and multilingual LLMs. Our findings show that decomposed\nprompting surpasses the iterative prompting baseline in efficacy and efficiency\nunder zero- and few-shot settings. Further analysis reveals the influence of\nevaluation methods and the use of instructions in prompts. Our multilingual\ninvestigation shows that English-centric language models perform better on\naverage than multilingual models. Our study offers insights into the\nmultilingual transferability of English-centric LLMs, contributing to the\nunderstanding of their multilingual linguistic knowledge.",
                    "children": [
                        {
                            "name": "The language of prompting: What linguistic properties make a prompt successful?",
                            "abstract": "The latest generation of LLMs can be prompted to achieve impressive zero-shot\nor few-shot performance in many NLP tasks. However, since performance is highly\nsensitive to the choice of prompts, considerable effort has been devoted to\ncrowd-sourcing prompts or designing methods for prompt optimisation. Yet, we\nstill lack a systematic understanding of how linguistic properties of prompts\ncorrelate with task performance. In this work, we investigate how LLMs of\ndifferent sizes, pre-trained and instruction-tuned, perform on prompts that are\nsemantically equivalent, but vary in linguistic structure. We investigate both\ngrammatical properties such as mood, tense, aspect and modality, as well as\nlexico-semantic variation through the use of synonyms. Our findings contradict\nthe common assumption that LLMs achieve optimal performance on lower perplexity\nprompts that reflect language use in pretraining or instruction-tuning data.\nPrompts transfer poorly between datasets or models, and performance cannot\ngenerally be explained by perplexity, word frequency, ambiguity or prompt\nlength. Based on our results, we put forward a proposal for a more robust and\ncomprehensive evaluation standard for prompting research.",
                            "children": []
                        },
                        {
                            "name": "Turning English-centric LLMs Into Polyglots: How Much Multilinguality Is Needed?",
                            "abstract": "The vast majority of today's large language models (LLMs) are\nEnglish-centric, having been pretrained predominantly on English text. Yet, in\norder to meet user expectations, models need to be able to respond\nappropriately in multiple languages once deployed in downstream applications.\nThis requires strong cross-lingual transfer abilities. In this work, we\ninvestigate the minimal amount of multilinguality required during finetuning to\nelicit cross-lingual generalisation in English-centric LLMs. In experiments\nacross four LLMs, we find that multilingual instruction tuning with as few as\ntwo to three languages is both necessary and sufficient to elicit effective\ncross-lingual generalisation, with the limiting factor being the degree to\nwhich a target language is seen during pretraining. Evaluations on five\ndifferent tasks further reveal that multilingual instruction tuning is most\nbeneficial for generative tasks that assume input/output language agreement,\nsuch as in chat settings, while being of less importance for highly structured\nclassification-style tasks. Our code and data is available at\nhttps://github.com/ZurichNLP/multilingual-instruction-tuning.",
                            "children": []
                        },
                        {
                            "name": "ToPro: Token-Level Prompt Decomposition for Cross-Lingual Sequence Labeling Tasks",
                            "abstract": "Prompt-based methods have been successfully applied to multilingual\npretrained language models for zero-shot cross-lingual understanding. However,\nmost previous studies primarily focused on sentence-level classification tasks,\nand only a few considered token-level labeling tasks such as Named Entity\nRecognition (NER) and Part-of-Speech (POS) tagging. In this paper, we propose\nToken-Level Prompt Decomposition (ToPro), which facilitates the prompt-based\nmethod for token-level sequence labeling tasks. The ToPro method decomposes an\ninput sentence into single tokens and applies one prompt template to each\ntoken. Our experiments on multilingual NER and POS tagging datasets demonstrate\nthat ToPro-based fine-tuning outperforms Vanilla fine-tuning and Prompt-Tuning\nin zero-shot cross-lingual transfer, especially for languages that are\ntypologically different from the source language English. Our method also\nattains state-of-the-art performance when employed with the mT5 model. Besides,\nour exploratory study in multilingual large language models shows that ToPro\nperforms much better than the current in-context learning method. Overall, the\nperformance improvements show that ToPro could potentially serve as a novel and\nsimple benchmarking method for sequence labeling tasks.",
                            "children": []
                        }
                    ]
                }
            ]
        }
    ]
}