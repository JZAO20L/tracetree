{
    "name": "LightRAG: Simple and Fast Retrieval-Augmented Generation",
    "abstract": "Retrieval-Augmented Generation (RAG) systems enhance large language models\n(LLMs) by integrating external knowledge sources, enabling more accurate and\ncontextually relevant responses tailored to user needs. However, existing RAG\nsystems have significant limitations, including reliance on flat data\nrepresentations and inadequate contextual awareness, which can lead to\nfragmented answers that fail to capture complex inter-dependencies. To address\nthese challenges, we propose LightRAG, which incorporates graph structures into\ntext indexing and retrieval processes. This innovative framework employs a\ndual-level retrieval system that enhances comprehensive information retrieval\nfrom both low-level and high-level knowledge discovery. Additionally, the\nintegration of graph structures with vector representations facilitates\nefficient retrieval of related entities and their relationships, significantly\nimproving response times while maintaining contextual relevance. This\ncapability is further enhanced by an incremental update algorithm that ensures\nthe timely integration of new data, allowing the system to remain effective and\nresponsive in rapidly changing data environments. Extensive experimental\nvalidation demonstrates considerable improvements in retrieval accuracy and\nefficiency compared to existing approaches. We have made our LightRAG\nopen-source and available at the link: https://github.com/HKUDS/LightRAG.",
    "children": [
        {
            "name": "TS-RIR: Translated synthetic room impulse responses for speech augmentation",
            "abstract": "We present a method for improving the quality of synthetic room impulse\nresponses for far-field speech recognition. We bridge the gap between the\nfidelity of synthetic room impulse responses (RIRs) and the real room impulse\nresponses using our novel, TS-RIRGAN architecture. Given a synthetic RIR in the\nform of raw audio, we use TS-RIRGAN to translate it into a real RIR. We also\nperform real-world sub-band room equalization on the translated synthetic RIR.\nOur overall approach improves the quality of synthetic RIRs by compensating\nlow-frequency wave effects, similar to those in real RIRs. We evaluate the\nperformance of improved synthetic RIRs on a far-field speech dataset augmented\nby convolving the LibriSpeech clean speech dataset [1] with RIRs and adding\nbackground noise. We show that far-field speech augmented using our improved\nsynthetic RIRs reduces the word error rate by up to 19.9% in Kaldi far-field\nautomatic speech recognition benchmark [2].",
            "children": [
                {
                    "name": "Low-frequency Compensated Synthetic Impulse Responses for Improved Far-field Speech Recognition",
                    "abstract": "We propose a method for generating low-frequency compensated synthetic\nimpulse responses that improve the performance of far-field speech recognition\nsystems trained on artificially augmented datasets. We design linear-phase\nfilters that adapt the simulated impulse responses to equalization\ndistributions corresponding to real-world captured impulse responses. Our\nfiltered synthetic impulse responses are then used to augment clean speech data\nfrom LibriSpeech dataset [1]. We evaluate the performance of our method on the\nreal-world LibriSpeech test set. In practice, our low-frequency compensated\nsynthetic dataset can reduce the word-error-rate by up to 8.8% for far-field\nspeech recognition.",
                    "children": []
                },
                {
                    "name": "Blind Identification of Binaural Room Impulse Responses from Smart Glasses",
                    "abstract": "Smart glasses are increasingly recognized as a key medium for augmented\nreality, offering a hands-free platform with integrated microphones and\nnon-ear-occluding loudspeakers to seamlessly mix virtual sound sources into the\nreal-world acoustic scene. To convincingly integrate virtual sound sources, the\nroom acoustic rendering of the virtual sources must match the real-world\nacoustics. Information about a user's acoustic environment however is typically\nnot available. This work uses a microphone array in a pair of smart glasses to\nblindly identify binaural room impulse responses (BRIRs) from a few seconds of\nspeech in the real-world environment. The proposed method uses dereverberation\nand beamforming to generate a pseudo reference signal that is used by a\nmultichannel Wiener filter to estimate room impulse responses which are then\nconverted to BRIRs. The multichannel room impulse responses can be used to\nestimate room acoustic parameters which is shown to outperform baseline\nalgorithms in the estimation of reverberation time and direct-to-reverberant\nenergy ratio. Results from a listening experiment further indicate that the\nestimated BRIRs often reproduce the real-world room acoustics perceptually more\nconvincingly than measured BRIRs from other rooms of similar size.",
                    "children": []
                }
            ]
        },
        {
            "name": "Significance of Data Augmentation for Improving Cleft Lip and Palate Speech Recognition",
            "abstract": "The automatic recognition of pathological speech, particularly from children\nwith any articulatory impairment, is a challenging task due to various reasons.\nThe lack of available domain specific data is one such obstacle that hinders\nits usage for different speech-based applications targeting pathological\nspeakers. In line with the challenge, in this work, we investigate a few data\naugmentation techniques to simulate training data for improving the children\nspeech recognition considering the case of cleft lip and palate (CLP) speech.\nThe augmentation techniques explored in this study, include vocal tract length\nperturbation (VTLP), reverberation, speaking rate, pitch modification, and\nspeech feature modification using cycle consistent adversarial networks\n(CycleGAN). Our study finds that the data augmentation methods significantly\nimprove the CLP speech recognition performance, which is more evident when we\nused feature modification using CycleGAN, VTLP and reverberation based methods.\nMore specifically, the results from this study show that our systems produce an\nimproved phone error rate compared to the systems without data augmentation.",
            "children": [
                {
                    "name": "Improving Hypernasality Estimation with Automatic Speech Recognition in Cleft Palate Speech",
                    "abstract": "Hypernasality is an abnormal resonance in human speech production, especially\nin patients with craniofacial anomalies such as cleft palate. In clinical\napplication, hypernasality estimation is crucial in cleft palate diagnosis, as\nits results determine the subsequent surgery and additional speech therapy.\nTherefore, designing an automatic hypernasality assessment method will\nfacilitate speech-language pathologists to make precise diagnoses. Existing\nmethods for hypernasality estimation only conduct acoustic analysis based on\nlow-resource cleft palate dataset, by using statistical or neural network-based\nfeatures. In this paper, we propose a novel approach that uses automatic speech\nrecognition model to improve hypernasality estimation. Specifically, we first\npre-train an encoder-decoder framework in an automatic speech recognition (ASR)\nobjective by using speech-to-text dataset, and then fine-tune ASR encoder on\nthe cleft palate dataset for hypernasality estimation. Benefiting from such\ndesign, our model for hypernasality estimation can enjoy the advantages of ASR\nmodel: 1) compared with low-resource cleft palate dataset, the ASR task usually\nincludes large-scale speech data in the general domain, which enables better\nmodel generalization; 2) the text annotations in ASR dataset guide model to\nextract better acoustic features. Experimental results on two cleft palate\ndatasets demonstrate that our method achieves superior performance compared\nwith previous approaches.",
                    "children": []
                }
            ]
        }
    ]
}