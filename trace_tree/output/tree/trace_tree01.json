{
    "name": "PoisonedRAG: Knowledge Corruption Attacks to Retrieval-Augmented Generation of Large Language Models",
    "abstract": "Large language models (LLMs) have achieved remarkable success due to their\nexceptional generative capabilities. Despite their success, they also have\ninherent limitations such as a lack of up-to-date knowledge and hallucination.\nRetrieval-Augmented Generation (RAG) is a state-of-the-art technique to\nmitigate these limitations. The key idea of RAG is to ground the answer\ngeneration of an LLM on external knowledge retrieved from a knowledge database.\nExisting studies mainly focus on improving the accuracy or efficiency of RAG,\nleaving its security largely unexplored. We aim to bridge the gap in this work.\nWe find that the knowledge database in a RAG system introduces a new and\npractical attack surface. Based on this attack surface, we propose PoisonedRAG,\nthe first knowledge corruption attack to RAG, where an attacker could inject a\nfew malicious texts into the knowledge database of a RAG system to induce an\nLLM to generate an attacker-chosen target answer for an attacker-chosen target\nquestion. We formulate knowledge corruption attacks as an optimization problem,\nwhose solution is a set of malicious texts. Depending on the background\nknowledge (e.g., black-box and white-box settings) of an attacker on a RAG\nsystem, we propose two solutions to solve the optimization problem,\nrespectively. Our results show PoisonedRAG could achieve a 90% attack success\nrate when injecting five malicious texts for each target question into a\nknowledge database with millions of texts. We also evaluate several defenses\nand our results show they are insufficient to defend against PoisonedRAG,\nhighlighting the need for new defenses.",
    "children": [
        {
            "name": "A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions",
            "abstract": "The emergence of large language models (LLMs) has marked a significant\nbreakthrough in natural language processing (NLP), fueling a paradigm shift in\ninformation acquisition. Nevertheless, LLMs are prone to hallucination,\ngenerating plausible yet nonfactual content. This phenomenon raises significant\nconcerns over the reliability of LLMs in real-world information retrieval (IR)\nsystems and has attracted intensive research to detect and mitigate such\nhallucinations. Given the open-ended general-purpose attributes inherent to\nLLMs, LLM hallucinations present distinct challenges that diverge from prior\ntask-specific models. This divergence highlights the urgency for a nuanced\nunderstanding and comprehensive overview of recent advances in LLM\nhallucinations. In this survey, we begin with an innovative taxonomy of\nhallucination in the era of LLM and then delve into the factors contributing to\nhallucinations. Subsequently, we present a thorough overview of hallucination\ndetection methods and benchmarks. Our discussion then transfers to\nrepresentative methodologies for mitigating LLM hallucinations. Additionally,\nwe delve into the current limitations faced by retrieval-augmented LLMs in\ncombating hallucinations, offering insights for developing more robust IR\nsystems. Finally, we highlight the promising research directions on LLM\nhallucinations, including hallucination in large vision-language models and\nunderstanding of knowledge boundaries in LLM hallucinations.",
            "children": [
                {
                    "name": "Adaptive Chameleon or Stubborn Sloth: Revealing the Behavior of Large Language Models in Knowledge Conflicts",
                    "abstract": "By providing external information to large language models (LLMs), tool\naugmentation (including retrieval augmentation) has emerged as a promising\nsolution for addressing the limitations of LLMs' static parametric memory.\nHowever, how receptive are LLMs to such external evidence, especially when the\nevidence conflicts with their parametric memory? We present the first\ncomprehensive and controlled investigation into the behavior of LLMs when\nencountering knowledge conflicts. We propose a systematic framework to elicit\nhigh-quality parametric memory from LLMs and construct the corresponding\ncounter-memory, which enables us to conduct a series of controlled experiments.\nOur investigation reveals seemingly contradicting behaviors of LLMs. On the one\nhand, different from prior wisdom, we find that LLMs can be highly receptive to\nexternal evidence even when that conflicts with their parametric memory, given\nthat the external evidence is coherent and convincing. On the other hand, LLMs\nalso demonstrate a strong confirmation bias when the external evidence contains\nsome information that is consistent with their parametric memory, despite being\npresented with conflicting evidence at the same time. These results pose\nimportant implications that are worth careful consideration for the further\ndevelopment and deployment of tool- and retrieval-augmented LLMs. Resources are\navailable at https://github.com/OSU-NLP-Group/LLM-Knowledge-Conflict.",
                    "children": [
                        {
                            "name": "Rich Knowledge Sources Bring Complex Knowledge Conflicts: Recalibrating Models to Reflect Conflicting Evidence",
                            "abstract": "Question answering models can use rich knowledge sources -- up to one hundred\nretrieved passages and parametric knowledge in the large-scale language model\n(LM). Prior work assumes information in such knowledge sources is consistent\nwith each other, paying little attention to how models blend information stored\nin their LM parameters with that from retrieved evidence documents. In this\npaper, we simulate knowledge conflicts (i.e., where parametric knowledge\nsuggests one answer and different passages suggest different answers) and\nexamine model behaviors. We find retrieval performance heavily impacts which\nsources models rely on, and current models mostly rely on non-parametric\nknowledge in their best-performing settings. We discover a troubling trend that\ncontradictions among knowledge sources affect model confidence only marginally.\nTo address this issue, we present a new calibration study, where models are\ndiscouraged from presenting any single answer when presented with multiple\nconflicting answer candidates in retrieved evidences.",
                            "children": []
                        },
                        {
                            "name": "Quantifying Memorization Across Neural Language Models",
                            "abstract": "Large language models (LMs) have been shown to memorize parts of their\ntraining data, and when prompted appropriately, they will emit the memorized\ntraining data verbatim. This is undesirable because memorization violates\nprivacy (exposing user data), degrades utility (repeated easy-to-memorize text\nis often low quality), and hurts fairness (some texts are memorized over\nothers).\n  We describe three log-linear relationships that quantify the degree to which\nLMs emit memorized training data. Memorization significantly grows as we\nincrease (1) the capacity of a model, (2) the number of times an example has\nbeen duplicated, and (3) the number of tokens of context used to prompt the\nmodel. Surprisingly, we find the situation becomes more complicated when\ngeneralizing these results across model families. On the whole, we find that\nmemorization in LMs is more prevalent than previously believed and will likely\nget worse as models continues to scale, at least without active mitigations.",
                            "children": []
                        },
                        {
                            "name": "Multilingual Knowledge Editing with Language-Agnostic Factual Neurons",
                            "abstract": "Multilingual knowledge editing (MKE) aims to simultaneously update factual\nknowledge across multiple languages within large language models (LLMs).\nPrevious research indicates that the same knowledge across different languages\nwithin LLMs exhibits a degree of shareability. However, most existing MKE\nmethods overlook the connections of the same knowledge between different\nlanguages, resulting in knowledge conflicts and limited edit performance. To\naddress this issue, we first investigate how LLMs process multilingual factual\nknowledge and discover that the same factual knowledge in different languages\ngenerally activates a shared set of neurons, which we call language-agnostic\nfactual neurons (LAFNs). These neurons represent the same factual knowledge\nshared across languages and imply the semantic connections among multilingual\nknowledge. Inspired by this finding, we propose a new MKE method by Locating\nand Updating Language-Agnostic Factual Neurons (LU-LAFNs) to edit multilingual\nknowledge simultaneously, which avoids knowledge conflicts and thus improves\nedit performance. Experimental results on Bi-ZsRE and MzsRE benchmarks\ndemonstrate that our method achieves the best edit performance, indicating the\neffectiveness and importance of modeling the semantic connections among\nmultilingual knowledge.",
                            "children": []
                        },
                        {
                            "name": "Knowledge Neurons in Pretrained Transformers",
                            "abstract": "Large-scale pretrained language models are surprisingly good at recalling\nfactual knowledge presented in the training corpus. In this paper, we present\npreliminary studies on how factual knowledge is stored in pretrained\nTransformers by introducing the concept of knowledge neurons. Specifically, we\nexamine the fill-in-the-blank cloze task for BERT. Given a relational fact, we\npropose a knowledge attribution method to identify the neurons that express the\nfact. We find that the activation of such knowledge neurons is positively\ncorrelated to the expression of their corresponding facts. In our case studies,\nwe attempt to leverage knowledge neurons to edit (such as update, and erase)\nspecific factual knowledge without fine-tuning. Our results shed light on\nunderstanding the storage of knowledge within pretrained Transformers. The code\nis available at https://github.com/Hunter-DDM/knowledge-neurons.",
                            "children": []
                        },
                        {
                            "name": "Extracting Training Data from Large Language Models",
                            "abstract": "It has become common to publish large (billion parameter) language models\nthat have been trained on private datasets. This paper demonstrates that in\nsuch settings, an adversary can perform a training data extraction attack to\nrecover individual training examples by querying the language model.\n  We demonstrate our attack on GPT-2, a language model trained on scrapes of\nthe public Internet, and are able to extract hundreds of verbatim text\nsequences from the model's training data. These extracted examples include\n(public) personally identifiable information (names, phone numbers, and email\naddresses), IRC conversations, code, and 128-bit UUIDs. Our attack is possible\neven though each of the above sequences are included in just one document in\nthe training data.\n  We comprehensively evaluate our extraction attack to understand the factors\nthat contribute to its success. Worryingly, we find that larger models are more\nvulnerable than smaller models. We conclude by drawing lessons and discussing\npossible safeguards for training large language models.",
                            "children": []
                        }
                    ]
                },
                {
                    "name": "Textbooks Are All You Need II: phi-1.5 technical report",
                    "abstract": "We continue the investigation into the power of smaller Transformer-based\nlanguage models as initiated by \\textbf{TinyStories} -- a 10 million parameter\nmodel that can produce coherent English -- and the follow-up work on\n\\textbf{phi-1}, a 1.3 billion parameter model with Python coding performance\nclose to the state-of-the-art. The latter work proposed to use existing Large\nLanguage Models (LLMs) to generate ``textbook quality\" data as a way to enhance\nthe learning process compared to traditional web data. We follow the\n``Textbooks Are All You Need\" approach, focusing this time on common sense\nreasoning in natural language, and create a new 1.3 billion parameter model\nnamed \\textbf{phi-1.5}, with performance on natural language tasks comparable\nto models 5x larger, and surpassing most non-frontier LLMs on more complex\nreasoning tasks such as grade-school mathematics and basic coding. More\ngenerally, \\textbf{phi-1.5} exhibits many of the traits of much larger LLMs,\nboth good -- such as the ability to ``think step by step\" or perform some\nrudimentary in-context learning -- and bad, including hallucinations and the\npotential for toxic and biased generations -- encouragingly though, we are\nseeing improvement on that front thanks to the absence of web data. We\nopen-source \\textbf{phi-1.5} to promote further research on these urgent\ntopics.",
                    "children": [
                        {
                            "name": "Self-Organizing Machine Translation: Example-Driven Induction of Transfer Functions",
                            "abstract": "With the advent of faster computers, the notion of doing machine translation\nfrom a huge stored database of translation examples is no longer unreasonable.\nThis paper describes an attempt to merge the Example-Based Machine Translation\n(EBMT) approach with psycholinguistic principles. A new formalism for context-\nfree grammars, called *marker-normal form*, is demonstrated and used to\ndescribe language data in a way compatible with psycholinguistic theories. By\nembedding this formalism in a standard multivariate optimization framework, a\nsystem can be built that infers correct transfer functions for a set of\nbilingual sentence pairs and then uses those functions to translate novel\nsentences. The validity of this line of reasoning has been tested in the\ndevelopment of a system called METLA-1. This system has been used to infer\nEnglish->French and English->Urdu transfer functions from small corpora. The\nresults of those experiments are examined, both in engineering terms as well as\nin more linguistic terms. In general, the results of these experiments were\npsycho- logically and linguistically well-grounded while still achieving a\nrespectable level of success when compared against a similar prototype using\nHidden Markov Models.",
                            "children": []
                        },
                        {
                            "name": "Sparks of Artificial General Intelligence: Early experiments with GPT-4",
                            "abstract": "Artificial intelligence (AI) researchers have been developing and refining\nlarge language models (LLMs) that exhibit remarkable capabilities across a\nvariety of domains and tasks, challenging our understanding of learning and\ncognition. The latest model developed by OpenAI, GPT-4, was trained using an\nunprecedented scale of compute and data. In this paper, we report on our\ninvestigation of an early version of GPT-4, when it was still in active\ndevelopment by OpenAI. We contend that (this early version of) GPT-4 is part of\na new cohort of LLMs (along with ChatGPT and Google's PaLM for example) that\nexhibit more general intelligence than previous AI models. We discuss the\nrising capabilities and implications of these models. We demonstrate that,\nbeyond its mastery of language, GPT-4 can solve novel and difficult tasks that\nspan mathematics, coding, vision, medicine, law, psychology and more, without\nneeding any special prompting. Moreover, in all of these tasks, GPT-4's\nperformance is strikingly close to human-level performance, and often vastly\nsurpasses prior models such as ChatGPT. Given the breadth and depth of GPT-4's\ncapabilities, we believe that it could reasonably be viewed as an early (yet\nstill incomplete) version of an artificial general intelligence (AGI) system.\nIn our exploration of GPT-4, we put special emphasis on discovering its\nlimitations, and we discuss the challenges ahead for advancing towards deeper\nand more comprehensive versions of AGI, including the possible need for\npursuing a new paradigm that moves beyond next-word prediction. We conclude\nwith reflections on societal influences of the recent technological leap and\nfuture research directions.",
                            "children": []
                        },
                        {
                            "name": "Language Modeling for Code-Switching: Evaluation, Integration of Monolingual Data, and Discriminative Training",
                            "abstract": "We focus on the problem of language modeling for code-switched language, in\nthe context of automatic speech recognition (ASR). Language modeling for\ncode-switched language is challenging for (at least) three reasons: (1) lack of\navailable large-scale code-switched data for training; (2) lack of a replicable\nevaluation setup that is ASR directed yet isolates language modeling\nperformance from the other intricacies of the ASR system; and (3) the reliance\non generative modeling. We tackle these three issues: we propose an\nASR-motivated evaluation setup which is decoupled from an ASR system and the\nchoice of vocabulary, and provide an evaluation dataset for English-Spanish\ncode-switching. This setup lends itself to a discriminative training approach,\nwhich we demonstrate to work better than generative language modeling. Finally,\nwe explore a variety of training protocols and verify the effectiveness of\ntraining with large amounts of monolingual data followed by fine-tuning with\nsmall amounts of code-switched data, for both the generative and discriminative\ncases.",
                            "children": []
                        },
                        {
                            "name": "BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions",
                            "abstract": "In this paper we study yes/no questions that are naturally occurring ---\nmeaning that they are generated in unprompted and unconstrained settings. We\nbuild a reading comprehension dataset, BoolQ, of such questions, and show that\nthey are unexpectedly challenging. They often query for complex, non-factoid\ninformation, and require difficult entailment-like inference to solve. We also\nexplore the effectiveness of a range of transfer learning baselines. We find\nthat transferring from entailment data is more effective than transferring from\nparaphrase or extractive QA data, and that it, surprisingly, continues to be\nvery beneficial even when starting from massive pre-trained language models\nsuch as BERT. Our best method trains BERT on MultiNLI and then re-trains it on\nour train set. It achieves 80.4% accuracy compared to 90% accuracy of human\nannotators (and 62% majority-baseline), leaving a significant gap for future\nwork.",
                            "children": []
                        },
                        {
                            "name": "Learning ASR pathways: A sparse multilingual ASR model",
                            "abstract": "Neural network pruning compresses automatic speech recognition (ASR) models\neffectively. However, in multilingual ASR, language-agnostic pruning may lead\nto severe performance drops on some languages because language-agnostic pruning\nmasks may not fit all languages and discard important language-specific\nparameters. In this work, we present ASR pathways, a sparse multilingual ASR\nmodel that activates language-specific sub-networks (\"pathways\"), such that the\nparameters for each language are learned explicitly. With the overlapping\nsub-networks, the shared parameters can also enable knowledge transfer for\nlower-resource languages via joint multilingual training. We propose a novel\nalgorithm to learn ASR pathways, and evaluate the proposed method on 4\nlanguages with a streaming RNN-T model. Our proposed ASR pathways outperform\nboth dense models and a language-agnostically pruned model, and provide better\nperformance on low-resource languages compared to the monolingual sparse\nmodels.",
                            "children": []
                        }
                    ]
                },
                {
                    "name": "Massive Editing for Large Language Models via Meta Learning",
                    "abstract": "While large language models (LLMs) have enabled learning knowledge from the\npre-training corpora, the acquired knowledge may be fundamentally incorrect or\noutdated over time, which necessitates rectifying the knowledge of the language\nmodel (LM) after the training. A promising approach involves employing a\nhyper-network to generate parameter shift, whereas existing hyper-networks\nsuffer from inferior scalability in synchronous editing operation amount. To\nmitigate the problem, we propose the MAssive Language Model Editing Network\n(MALMEN), which formulates the parameter shift aggregation as the least square\nproblem, subsequently updating the LM parameters using the normal equation. To\naccommodate editing multiple facts simultaneously with limited memory budgets,\nwe separate the computation on the hyper-network and LM, enabling arbitrary\nbatch size on both neural networks. Our method is evaluated by editing up to\nthousands of facts on LMs with different architectures, i.e., BERT-base, GPT-2,\nT5-XL (2.8B), and GPT-J (6B), across various knowledge-intensive NLP tasks,\ni.e., closed book fact-checking and question answering. Remarkably, MALMEN is\ncapable of editing hundreds of times more facts than strong baselines with the\nidentical hyper-network architecture and outperforms editor specifically\ndesigned for GPT. Our code is available at\nhttps://github.com/ChenmienTan/malmen.",
                    "children": [
                        {
                            "name": "Does Localization Inform Editing? Surprising Differences in Causality-Based Localization vs. Knowledge Editing in Language Models",
                            "abstract": "Language models learn a great quantity of factual information during\npretraining, and recent work localizes this information to specific model\nweights like mid-layer MLP weights. In this paper, we find that we can change\nhow a fact is stored in a model by editing weights that are in a different\nlocation than where existing methods suggest that the fact is stored. This is\nsurprising because we would expect that localizing facts to specific model\nparameters would tell us where to manipulate knowledge in models, and this\nassumption has motivated past work on model editing methods. Specifically, we\nshow that localization conclusions from representation denoising (also known as\nCausal Tracing) do not provide any insight into which model MLP layer would be\nbest to edit in order to override an existing stored fact with a new one. This\nfinding raises questions about how past work relies on Causal Tracing to select\nwhich model layers to edit. Next, we consider several variants of the editing\nproblem, including erasing and amplifying facts. For one of our editing\nproblems, editing performance does relate to localization results from\nrepresentation denoising, but we find that which layer we edit is a far better\npredictor of performance. Our results suggest, counterintuitively, that better\nmechanistic understanding of how pretrained language models work may not always\ntranslate to insights about how to best change their behavior. Our code is\navailable at https://github.com/google/belief-localization",
                            "children": []
                        },
                        {
                            "name": "Commonsense Knowledge Editing Based on Free-Text in LLMs",
                            "abstract": "Knowledge editing technology is crucial for maintaining the accuracy and\ntimeliness of large language models (LLMs) . However, the setting of this task\noverlooks a significant portion of commonsense knowledge based on free-text in\nthe real world, characterized by broad knowledge scope, long content and non\ninstantiation. The editing objects of previous methods (e.g., MEMIT) were\nsingle token or entity, which were not suitable for commonsense knowledge in\nfree-text form. To address the aforementioned challenges, we conducted\nexperiments from two perspectives: knowledge localization and knowledge\nediting. Firstly, we introduced Knowledge Localization for Free-Text(KLFT)\nmethod, revealing the challenges associated with the distribution of\ncommonsense knowledge in MLP and Attention layers, as well as in decentralized\ndistribution. Next, we propose a Dynamics-aware Editing Method(DEM), which\nutilizes a Dynamics-aware Module to locate the parameter positions\ncorresponding to commonsense knowledge, and uses Knowledge Editing Module to\nupdate knowledge. The DEM method fully explores the potential of the MLP and\nAttention layers, and successfully edits commonsense knowledge based on\nfree-text. The experimental results indicate that the DEM can achieve excellent\nediting performance.",
                            "children": []
                        },
                        {
                            "name": "Empirical Study on Updating Key-Value Memories in Transformer Feed-forward Layers",
                            "abstract": "The feed-forward networks (FFNs) in transformers are recognized as a group of\nkey-value neural memories to restore abstract high-level knowledge. In this\nwork, we conduct an empirical ablation study on updating keys (the 1st layer in\nthe FFNs layer) or values (the 2nd layer in the FFNs layer). We compare those\ntwo methods in various knowledge editing and fine-tuning tasks of large\nlanguage models to draw insights to understand FFNs further. Code is available\nat $\\href{https://github.com/qiuzh20/Tuning-keys-v.s.-values}{this\\,repo}$.",
                            "children": []
                        },
                        {
                            "name": "Can bidirectional encoder become the ultimate winner for downstream applications of foundation models?",
                            "abstract": "Over the past few decades, Artificial Intelligence(AI) has progressed from\nthe initial machine learning stage to the deep learning stage, and now to the\nstage of foundational models. Foundational models have the characteristics of\npre-training, transfer learning, and self-supervised learning, and pre-trained\nmodels can be fine-tuned and applied to various downstream tasks. Under the\nframework of foundational models, models such as Bidirectional Encoder\nRepresentations from Transformers(BERT) and Generative Pre-trained\nTransformer(GPT) have greatly advanced the development of natural language\nprocessing(NLP), especially the emergence of many models based on BERT. BERT\nbroke through the limitation of only using one-way methods for language\nmodeling in pre-training by using a masked language model. It can capture\nbidirectional context information to predict the masked words in the sequence,\nthis can improve the feature extraction ability of the model. This makes the\nmodel very useful for downstream tasks, especially for specialized\napplications. The model using the bidirectional encoder can better understand\nthe domain knowledge and be better applied to these downstream tasks. So we\nhope to help understand how this technology has evolved and improved model\nperformance in various natural language processing tasks under the background\nof foundational models and reveal its importance in capturing context\ninformation and improving the model's performance on downstream tasks. This\narticle analyzes one-way and bidirectional models based on GPT and BERT and\ncompares their differences based on the purpose of the model. It also briefly\nanalyzes BERT and the improvements of some models based on BERT. The model's\nperformance on the Stanford Question Answering Dataset(SQuAD) and General\nLanguage Understanding Evaluation(GLUE) was compared.",
                            "children": []
                        },
                        {
                            "name": "Hypernetwork Dismantling via Deep Reinforcement Learning",
                            "abstract": "Network dismantling aims to degrade the connectivity of a network by removing\nan optimal set of nodes. It has been widely adopted in many real-world\napplications such as epidemic control and rumor containment. However,\nconventional methods usually focus on simple network modeling with only\npairwise interactions, while group-wise interactions modeled by hypernetwork\nare ubiquitous and critical. In this work, we formulate the hypernetwork\ndismantling problem as a node sequence decision problem and propose a deep\nreinforcement learning (DRL)-based hypernetwork dismantling framework. Besides,\nwe design a novel inductive hypernetwork embedding method to ensure the\ntransferability to various real-world hypernetworks. Our framework first\ngenerates small-scale synthetic hypernetworks and embeds the nodes and\nhypernetworks into a low dimensional vector space to represent the action and\nstate space in DRL, respectively. Then trial-and-error dismantling tasks are\nconducted by an agent on these synthetic hypernetworks, and the dismantling\nstrategy is continuously optimized. Finally, the well-optimized strategy is\napplied to real-world hypernetwork dismantling tasks. Experimental results on\nfive real-world hypernetworks demonstrate the effectiveness of our proposed\nframework.",
                            "children": []
                        }
                    ]
                },
                {
                    "name": "Evaluating the Factual Consistency of Large Language Models Through News Summarization",
                    "abstract": "While large language models (LLMs) have proven to be effective on a large\nvariety of tasks, they are also known to hallucinate information. To measure\nwhether an LLM prefers factually consistent continuations of its input, we\npropose a new benchmark called FIB(Factual Inconsistency Benchmark) that\nfocuses on the task of summarization. Specifically, our benchmark involves\ncomparing the scores an LLM assigns to a factually consistent versus a\nfactually inconsistent summary for an input news article. For factually\nconsistent summaries, we use human-written reference summaries that we manually\nverify as factually consistent. To generate summaries that are factually\ninconsistent, we generate summaries from a suite of summarization models that\nwe have manually annotated as factually inconsistent. A model's factual\nconsistency is then measured according to its accuracy, i.e.\\ the proportion of\ndocuments where it assigns a higher score to the factually consistent summary.\nTo validate the usefulness of FIB, we evaluate 23 large language models ranging\nfrom 1B to 176B parameters from six different model families including BLOOM\nand OPT. We find that existing LLMs generally assign a higher score to\nfactually consistent summaries than to factually inconsistent summaries.\nHowever, if the factually inconsistent summaries occur verbatim in the\ndocument, then LLMs assign a higher score to these factually inconsistent\nsummaries than factually consistent summaries. We validate design choices in\nour benchmark including the scoring method and source of distractor summaries.\nOur code and benchmark data can be found at https://github.com/r-three/fib.",
                    "children": [
                        {
                            "name": "FEQA: A Question Answering Evaluation Framework for Faithfulness Assessment in Abstractive Summarization",
                            "abstract": "Neural abstractive summarization models are prone to generate content\ninconsistent with the source document, i.e. unfaithful. Existing automatic\nmetrics do not capture such mistakes effectively. We tackle the problem of\nevaluating faithfulness of a generated summary given its source document. We\nfirst collected human annotations of faithfulness for outputs from numerous\nmodels on two datasets. We find that current models exhibit a trade-off between\nabstractiveness and faithfulness: outputs with less word overlap with the\nsource document are more likely to be unfaithful. Next, we propose an automatic\nquestion answering (QA) based metric for faithfulness, FEQA, which leverages\nrecent advances in reading comprehension. Given question-answer pairs generated\nfrom the summary, a QA model extracts answers from the document; non-matched\nanswers indicate unfaithful information in the summary. Among metrics based on\nword overlap, embedding similarity, and learned language understanding models,\nour QA-based metric has significantly higher correlation with human\nfaithfulness scores, especially on highly abstractive summaries.",
                            "children": []
                        },
                        {
                            "name": "Faithful to the Original: Fact Aware Neural Abstractive Summarization",
                            "abstract": "Unlike extractive summarization, abstractive summarization has to fuse\ndifferent parts of the source text, which inclines to create fake facts. Our\npreliminary study reveals nearly 30% of the outputs from a state-of-the-art\nneural summarization system suffer from this problem. While previous\nabstractive summarization approaches usually focus on the improvement of\ninformativeness, we argue that faithfulness is also a vital prerequisite for a\npractical abstractive summarization system. To avoid generating fake facts in a\nsummary, we leverage open information extraction and dependency parse\ntechnologies to extract actual fact descriptions from the source text. The\ndual-attention sequence-to-sequence framework is then proposed to force the\ngeneration conditioned on both the source text and the extracted fact\ndescriptions. Experiments on the Gigaword benchmark dataset demonstrate that\nour model can greatly reduce fake summaries by 80%. Notably, the fact\ndescriptions also bring significant improvement on informativeness since they\noften condense the meaning of the source text.",
                            "children": []
                        },
                        {
                            "name": "Scaling Instruction-Finetuned Language Models",
                            "abstract": "Finetuning language models on a collection of datasets phrased as\ninstructions has been shown to improve model performance and generalization to\nunseen tasks. In this paper we explore instruction finetuning with a particular\nfocus on (1) scaling the number of tasks, (2) scaling the model size, and (3)\nfinetuning on chain-of-thought data. We find that instruction finetuning with\nthe above aspects dramatically improves performance on a variety of model\nclasses (PaLM, T5, U-PaLM), prompting setups (zero-shot, few-shot, CoT), and\nevaluation benchmarks (MMLU, BBH, TyDiQA, MGSM, open-ended generation). For\ninstance, Flan-PaLM 540B instruction-finetuned on 1.8K tasks outperforms PALM\n540B by a large margin (+9.4% on average). Flan-PaLM 540B achieves\nstate-of-the-art performance on several benchmarks, such as 75.2% on five-shot\nMMLU. We also publicly release Flan-T5 checkpoints, which achieve strong\nfew-shot performance even compared to much larger models, such as PaLM 62B.\nOverall, instruction finetuning is a general method for improving the\nperformance and usability of pretrained language models.",
                            "children": []
                        },
                        {
                            "name": "MultiFC: A Real-World Multi-Domain Dataset for Evidence-Based Fact Checking of Claims",
                            "abstract": "We contribute the largest publicly available dataset of naturally occurring\nfactual claims for the purpose of automatic claim verification. It is collected\nfrom 26 fact checking websites in English, paired with textual sources and rich\nmetadata, and labelled for veracity by human expert journalists. We present an\nin-depth analysis of the dataset, highlighting characteristics and challenges.\nFurther, we present results for automatic veracity prediction, both with\nestablished baselines and with a novel method for joint ranking of evidence\npages and predicting veracity that outperforms all baselines. Significant\nperformance increases are achieved by encoding evidence, and by modelling\nmetadata. Our best-performing model achieves a Macro F1 of 49.2%, showing that\nthis is a challenging testbed for claim veracity prediction.",
                            "children": []
                        },
                        {
                            "name": "Fast Abstractive Summarization with Reinforce-Selected Sentence Rewriting",
                            "abstract": "Inspired by how humans summarize long documents, we propose an accurate and\nfast summarization model that first selects salient sentences and then rewrites\nthem abstractively (i.e., compresses and paraphrases) to generate a concise\noverall summary. We use a novel sentence-level policy gradient method to bridge\nthe non-differentiable computation between these two neural networks in a\nhierarchical way, while maintaining language fluency. Empirically, we achieve\nthe new state-of-the-art on all metrics (including human evaluation) on the\nCNN/Daily Mail dataset, as well as significantly higher abstractiveness scores.\nMoreover, by first operating at the sentence-level and then the word-level, we\nenable parallel decoding of our neural generative model that results in\nsubstantially faster (10-20x) inference speed as well as 4x faster training\nconvergence than previous long-paragraph encoder-decoder models. We also\ndemonstrate the generalization of our model on the test-only DUC-2002 dataset,\nwhere we achieve higher scores than a state-of-the-art model.",
                            "children": []
                        }
                    ]
                },
                {
                    "name": "On Context Utilization in Summarization with Large Language Models",
                    "abstract": "Large language models (LLMs) excel in abstractive summarization tasks,\ndelivering fluent and pertinent summaries. Recent advancements have extended\ntheir capabilities to handle long-input contexts, exceeding 100k tokens.\nHowever, in question answering, language models exhibit uneven utilization of\ntheir input context. They tend to favor the initial and final segments,\nresulting in a U-shaped performance pattern concerning where the answer is\nlocated within the input. This bias raises concerns, particularly in\nsummarization where crucial content may be dispersed throughout the source\ndocument(s). Besides, in summarization, mapping facts from the source to the\nsummary is not trivial as salient content is usually re-phrased. In this paper,\nwe conduct the first comprehensive study on context utilization and position\nbias in summarization. Our analysis encompasses 6 LLMs, 10 datasets, and 5\nevaluation metrics. We introduce a new evaluation benchmark called MiddleSum on\nthe which we benchmark two alternative inference methods to alleviate position\nbias: hierarchical summarization and incremental summarization. Our code and\ndata can be found here: https://github.com/ntunlp/MiddleSum.",
                    "children": [
                        {
                            "name": "Revisiting Zero-Shot Abstractive Summarization in the Era of Large Language Models from the Perspective of Position Bias",
                            "abstract": "We characterize and study zero-shot abstractive summarization in Large\nLanguage Models (LLMs) by measuring position bias, which we propose as a\ngeneral formulation of the more restrictive lead bias phenomenon studied\npreviously in the literature. Position bias captures the tendency of a model\nunfairly prioritizing information from certain parts of the input text over\nothers, leading to undesirable behavior. Through numerous experiments on four\ndiverse real-world datasets, we study position bias in multiple LLM models such\nas GPT 3.5-Turbo, Llama-2, and Dolly-v2, as well as state-of-the-art pretrained\nencoder-decoder abstractive summarization models such as Pegasus and BART. Our\nfindings lead to novel insights and discussion on performance and position bias\nof models for zero-shot summarization tasks.",
                            "children": []
                        },
                        {
                            "name": "Source Code Summarization in the Era of Large Language Models",
                            "abstract": "To support software developers in understanding and maintaining programs,\nvarious automatic (source) code summarization techniques have been proposed to\ngenerate a concise natural language summary (i.e., comment) for a given code\nsnippet. Recently, the emergence of large language models (LLMs) has led to a\ngreat boost in the performance of code-related tasks. In this paper, we\nundertake a systematic and comprehensive study on code summarization in the era\nof LLMs, which covers multiple aspects involved in the workflow of LLM-based\ncode summarization. Specifically, we begin by examining prevalent automated\nevaluation methods for assessing the quality of summaries generated by LLMs and\nfind that the results of the GPT-4 evaluation method are most closely aligned\nwith human evaluation. Then, we explore the effectiveness of five prompting\ntechniques (zero-shot, few-shot, chain-of-thought, critique, and expert) in\nadapting LLMs to code summarization tasks. Contrary to expectations, advanced\nprompting techniques may not outperform simple zero-shot prompting. Next, we\ninvestigate the impact of LLMs' model settings (including top\\_p and\ntemperature parameters) on the quality of generated summaries. We find the\nimpact of the two parameters on summary quality varies by the base LLM and\nprogramming language, but their impacts are similar. Moreover, we canvass LLMs'\nabilities to summarize code snippets in distinct types of programming\nlanguages. The results reveal that LLMs perform suboptimally when summarizing\ncode written in logic programming languages compared to other language types.\nFinally, we unexpectedly find that CodeLlama-Instruct with 7B parameters can\noutperform advanced GPT-4 in generating summaries describing code\nimplementation details and asserting code properties. We hope that our findings\ncan provide a comprehensive understanding of code summarization in the era of\nLLMs.",
                            "children": []
                        },
                        {
                            "name": "When Benchmarks are Targets: Revealing the Sensitivity of Large Language Model Leaderboards",
                            "abstract": "Large Language Model (LLM) leaderboards based on benchmark rankings are\nregularly used to guide practitioners in model selection. Often, the published\nleaderboard rankings are taken at face value - we show this is a (potentially\ncostly) mistake. Under existing leaderboards, the relative performance of LLMs\nis highly sensitive to (often minute) details. We show that for popular\nmultiple-choice question benchmarks (e.g., MMLU), minor perturbations to the\nbenchmark, such as changing the order of choices or the method of answer\nselection, result in changes in rankings up to 8 positions. We explain this\nphenomenon by conducting systematic experiments over three broad categories of\nbenchmark perturbations and identifying the sources of this behavior. Our\nanalysis results in several best-practice recommendations, including the\nadvantage of a hybrid scoring method for answer selection. Our study highlights\nthe dangers of relying on simple benchmark evaluations and charts the path for\nmore robust evaluation schemes on the existing benchmarks. The code for this\npaper is available at\nhttps://github.com/National-Center-for-AI-Saudi-Arabia/lm-evaluation-harness.",
                            "children": []
                        },
                        {
                            "name": "MovieSum: An Abstractive Summarization Dataset for Movie Screenplays",
                            "abstract": "Movie screenplay summarization is challenging, as it requires an\nunderstanding of long input contexts and various elements unique to movies.\nLarge language models have shown significant advancements in document\nsummarization, but they often struggle with processing long input contexts.\nFurthermore, while television transcripts have received attention in recent\nstudies, movie screenplay summarization remains underexplored. To stimulate\nresearch in this area, we present a new dataset, MovieSum, for abstractive\nsummarization of movie screenplays. This dataset comprises 2200 movie\nscreenplays accompanied by their Wikipedia plot summaries. We manually\nformatted the movie screenplays to represent their structural elements.\nCompared to existing datasets, MovieSum possesses several distinctive features:\n(1) It includes movie screenplays, which are longer than scripts of TV\nepisodes. (2) It is twice the size of previous movie screenplay datasets. (3)\nIt provides metadata with IMDb IDs to facilitate access to additional external\nknowledge. We also show the results of recently released large language models\napplied to summarization on our dataset to provide a detailed baseline.",
                            "children": []
                        },
                        {
                            "name": "Extending Context Window of Large Language Models via Positional Interpolation",
                            "abstract": "We present Position Interpolation (PI) that extends the context window sizes\nof RoPE-based pretrained LLMs such as LLaMA models to up to 32768 with minimal\nfine-tuning (within 1000 steps), while demonstrating strong empirical results\non various tasks that require long context, including passkey retrieval,\nlanguage modeling, and long document summarization from LLaMA 7B to 65B.\nMeanwhile, the extended model by Position Interpolation preserve quality\nrelatively well on tasks within its original context window. To achieve this\ngoal, Position Interpolation linearly down-scales the input position indices to\nmatch the original context window size, rather than extrapolating beyond the\ntrained context length which may lead to catastrophically high attention scores\nthat completely ruin the self-attention mechanism. Our theoretical study shows\nthat the upper bound of interpolation is at least $\\sim 600 \\times$ smaller\nthan that of extrapolation, further demonstrating its stability. Models\nextended via Position Interpolation retain its original architecture and can\nreuse most pre-existing optimization and infrastructure.",
                            "children": []
                        }
                    ]
                }
            ]
        },
        {
            "name": "Making LLMs Worth Every Penny: Resource-Limited Text Classification in Banking",
            "abstract": "Standard Full-Data classifiers in NLP demand thousands of labeled examples,\nwhich is impractical in data-limited domains. Few-shot methods offer an\nalternative, utilizing contrastive learning techniques that can be effective\nwith as little as 20 examples per class. Similarly, Large Language Models\n(LLMs) like GPT-4 can perform effectively with just 1-5 examples per class.\nHowever, the performance-cost trade-offs of these methods remain underexplored,\na critical concern for budget-limited organizations. Our work addresses this\ngap by studying the aforementioned approaches over the Banking77 financial\nintent detection dataset, including the evaluation of cutting-edge LLMs by\nOpenAI, Cohere, and Anthropic in a comprehensive set of few-shot scenarios. We\ncomplete the picture with two additional methods: first, a cost-effective\nquerying method for LLMs based on retrieval-augmented generation (RAG), able to\nreduce operational costs multiple times compared to classic few-shot\napproaches, and second, a data augmentation method using GPT-4, able to improve\nperformance in data-limited scenarios. Finally, to inspire future research, we\nprovide a human expert's curated subset of Banking77, along with extensive\nerror analysis.",
            "children": [
                {
                    "name": "Efficient Intent Detection with Dual Sentence Encoders",
                    "abstract": "Building conversational systems in new domains and with added functionality\nrequires resource-efficient models that work under low-data regimes (i.e., in\nfew-shot setups). Motivated by these requirements, we introduce intent\ndetection methods backed by pretrained dual sentence encoders such as USE and\nConveRT. We demonstrate the usefulness and wide applicability of the proposed\nintent detectors, showing that: 1) they outperform intent detectors based on\nfine-tuning the full BERT-Large model or using BERT as a fixed black-box\nencoder on three diverse intent detection data sets; 2) the gains are\nespecially pronounced in few-shot setups (i.e., with only 10 or 30 annotated\nexamples per intent); 3) our intent detectors can be trained in a matter of\nminutes on a single CPU; and 4) they are stable across different hyperparameter\nsettings. In hope of facilitating and democratizing research focused on\nintention detection, we release our code, as well as a new challenging\nsingle-domain intent detection dataset comprising 13,083 annotated examples\nover 77 intents.",
                    "children": [
                        {
                            "name": "The Daunting Dilemma with Sentence Encoders: Success on Standard Benchmarks, Failure in Capturing Basic Semantic Properties",
                            "abstract": "In this paper, we adopted a retrospective approach to examine and compare\nfive existing popular sentence encoders, i.e., Sentence-BERT, Universal\nSentence Encoder (USE), LASER, InferSent, and Doc2vec, in terms of their\nperformance on downstream tasks versus their capability to capture basic\nsemantic properties. Initially, we evaluated all five sentence encoders on the\npopular SentEval benchmark and found that multiple sentence encoders perform\nquite well on a variety of popular downstream tasks. However, being unable to\nfind a single winner in all cases, we designed further experiments to gain a\ndeeper understanding of their behavior. Specifically, we proposed four semantic\nevaluation criteria, i.e., Paraphrasing, Synonym Replacement, Antonym\nReplacement, and Sentence Jumbling, and evaluated the same five sentence\nencoders using these criteria. We found that the Sentence-Bert and USE models\npass the paraphrasing criterion, with SBERT being the superior between the two.\nLASER dominates in the case of the synonym replacement criterion.\nInterestingly, all the sentence encoders failed the antonym replacement and\njumbling criteria. These results suggest that although these popular sentence\nencoders perform quite well on the SentEval benchmark, they still struggle to\ncapture some basic semantic properties, thus, posing a daunting dilemma in NLP\nresearch.",
                            "children": []
                        },
                        {
                            "name": "Conversational Contextual Cues: The Case of Personalization and History for Response Ranking",
                            "abstract": "We investigate the task of modeling open-domain, multi-turn, unstructured,\nmulti-participant, conversational dialogue. We specifically study the effect of\nincorporating different elements of the conversation. Unlike previous efforts,\nwhich focused on modeling messages and responses, we extend the modeling to\nlong context and participant's history. Our system does not rely on handwritten\nrules or engineered features; instead, we train deep neural networks on a large\nconversational dataset. In particular, we exploit the structure of Reddit\ncomments and posts to extract 2.1 billion messages and 133 million\nconversations. We evaluate our models on the task of predicting the next\nresponse in a conversation, and we find that modeling both context and\nparticipants improves prediction accuracy.",
                            "children": []
                        },
                        {
                            "name": "Snips Voice Platform: an embedded Spoken Language Understanding system for private-by-design voice interfaces",
                            "abstract": "This paper presents the machine learning architecture of the Snips Voice\nPlatform, a software solution to perform Spoken Language Understanding on\nmicroprocessors typical of IoT devices. The embedded inference is fast and\naccurate while enforcing privacy by design, as no personal user data is ever\ncollected. Focusing on Automatic Speech Recognition and Natural Language\nUnderstanding, we detail our approach to training high-performance Machine\nLearning models that are small enough to run in real-time on small devices.\nAdditionally, we describe a data generation procedure that provides sufficient,\nhigh-quality training data without compromising user privacy.",
                            "children": []
                        },
                        {
                            "name": "Domain adaptation for sequence labeling using hidden Markov models",
                            "abstract": "Most natural language processing systems based on machine learning are not\nrobust to domain shift. For example, a state-of-the-art syntactic dependency\nparser trained on Wall Street Journal sentences has an absolute drop in\nperformance of more than ten points when tested on textual data from the Web.\nAn efficient solution to make these methods more robust to domain shift is to\nfirst learn a word representation using large amounts of unlabeled data from\nboth domains, and then use this representation as features in a supervised\nlearning algorithm. In this paper, we propose to use hidden Markov models to\nlearn word representations for part-of-speech tagging. In particular, we study\nthe influence of using data from the source, the target or both domains to\nlearn the representation and the different ways to represent words using an\nHMM.",
                            "children": []
                        },
                        {
                            "name": "Transfer Fine-Tuning: A BERT Case Study",
                            "abstract": "A semantic equivalence assessment is defined as a task that assesses semantic\nequivalence in a sentence pair by binary judgment (i.e., paraphrase\nidentification) or grading (i.e., semantic textual similarity measurement). It\nconstitutes a set of tasks crucial for research on natural language\nunderstanding. Recently, BERT realized a breakthrough in sentence\nrepresentation learning (Devlin et al., 2019), which is broadly transferable to\nvarious NLP tasks. While BERT's performance improves by increasing its model\nsize, the required computational power is an obstacle preventing practical\napplications from adopting the technology. Herein, we propose to inject phrasal\nparaphrase relations into BERT in order to generate suitable representations\nfor semantic equivalence assessment instead of increasing the model size.\nExperiments on standard natural language understanding tasks confirm that our\nmethod effectively improves a smaller BERT model while maintaining the model\nsize. The generated model exhibits superior performance compared to a larger\nBERT model on semantic equivalence assessment tasks. Furthermore, it achieves\nlarger performance gains on tasks with limited training datasets for\nfine-tuning, which is a property desirable for transfer learning.",
                            "children": []
                        }
                    ]
                },
                {
                    "name": "An Empirical Study on Large-Scale Multi-Label Text Classification Including Few and Zero-Shot Labels",
                    "abstract": "Large-scale Multi-label Text Classification (LMTC) has a wide range of\nNatural Language Processing (NLP) applications and presents interesting\nchallenges. First, not all labels are well represented in the training set, due\nto the very large label set and the skewed label distributions of LMTC\ndatasets. Also, label hierarchies and differences in human labelling guidelines\nmay affect graph-aware annotation proximity. Finally, the label hierarchies are\nperiodically updated, requiring LMTC models capable of zero-shot\ngeneralization. Current state-of-the-art LMTC models employ Label-Wise\nAttention Networks (LWANs), which (1) typically treat LMTC as flat multi-label\nclassification; (2) may use the label hierarchy to improve zero-shot learning,\nalthough this practice is vastly understudied; and (3) have not been combined\nwith pre-trained Transformers (e.g. BERT), which have led to state-of-the-art\nresults in several NLP benchmarks. Here, for the first time, we empirically\nevaluate a battery of LMTC methods from vanilla LWANs to hierarchical\nclassification approaches and transfer learning, on frequent, few, and\nzero-shot learning on three datasets from different domains. We show that\nhierarchical methods based on Probabilistic Label Trees (PLTs) outperform\nLWANs. Furthermore, we show that Transformer-based approaches outperform the\nstate-of-the-art in two of the datasets, and we propose a new state-of-the-art\nmethod which combines BERT with LWANs. Finally, we propose new models that\nleverage the label hierarchy to improve few and zero-shot learning, considering\non each dataset a graph-aware annotation proximity measure that we introduce.",
                    "children": [
                        {
                            "name": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
                            "abstract": "Transfer learning, where a model is first pre-trained on a data-rich task\nbefore being fine-tuned on a downstream task, has emerged as a powerful\ntechnique in natural language processing (NLP). The effectiveness of transfer\nlearning has given rise to a diversity of approaches, methodology, and\npractice. In this paper, we explore the landscape of transfer learning\ntechniques for NLP by introducing a unified framework that converts all\ntext-based language problems into a text-to-text format. Our systematic study\ncompares pre-training objectives, architectures, unlabeled data sets, transfer\napproaches, and other factors on dozens of language understanding tasks. By\ncombining the insights from our exploration with scale and our new ``Colossal\nClean Crawled Corpus'', we achieve state-of-the-art results on many benchmarks\ncovering summarization, question answering, text classification, and more. To\nfacilitate future work on transfer learning for NLP, we release our data set,\npre-trained models, and code.",
                            "children": []
                        },
                        {
                            "name": "A Primer in BERTology: What we know about how BERT works",
                            "abstract": "Transformer-based models have pushed state of the art in many areas of NLP,\nbut our understanding of what is behind their success is still limited. This\npaper is the first survey of over 150 studies of the popular BERT model. We\nreview the current state of knowledge about how BERT works, what kind of\ninformation it learns and how it is represented, common modifications to its\ntraining objectives and architecture, the overparameterization issue and\napproaches to compression. We then outline directions for future research.",
                            "children": []
                        },
                        {
                            "name": "Publicly Available Clinical BERT Embeddings",
                            "abstract": "Contextual word embedding models such as ELMo (Peters et al., 2018) and BERT\n(Devlin et al., 2018) have dramatically improved performance for many natural\nlanguage processing (NLP) tasks in recent months. However, these models have\nbeen minimally explored on specialty corpora, such as clinical text; moreover,\nin the clinical domain, no publicly-available pre-trained BERT models yet\nexist. In this work, we address this need by exploring and releasing BERT\nmodels for clinical text: one for generic clinical text and another for\ndischarge summaries specifically. We demonstrate that using a domain-specific\nmodel yields performance improvements on three common clinical NLP tasks as\ncompared to nonspecific embeddings. These domain-specific models are not as\nperformant on two clinical de-identification tasks, and argue that this is a\nnatural consequence of the differences between de-identified source text and\nsynthetically non de-identified task text.",
                            "children": []
                        },
                        {
                            "name": "SciBERT: A Pretrained Language Model for Scientific Text",
                            "abstract": "Obtaining large-scale annotated data for NLP tasks in the scientific domain\nis challenging and expensive. We release SciBERT, a pretrained language model\nbased on BERT (Devlin et al., 2018) to address the lack of high-quality,\nlarge-scale labeled scientific data. SciBERT leverages unsupervised pretraining\non a large multi-domain corpus of scientific publications to improve\nperformance on downstream scientific NLP tasks. We evaluate on a suite of tasks\nincluding sequence tagging, sentence classification and dependency parsing,\nwith datasets from a variety of scientific domains. We demonstrate\nstatistically significant improvements over BERT and achieve new\nstate-of-the-art results on several of these tasks. The code and pretrained\nmodels are available at https://github.com/allenai/scibert/.",
                            "children": []
                        },
                        {
                            "name": "RoBERTa: A Robustly Optimized BERT Pretraining Approach",
                            "abstract": "Language model pretraining has led to significant performance gains but\ncareful comparison between different approaches is challenging. Training is\ncomputationally expensive, often done on private datasets of different sizes,\nand, as we will show, hyperparameter choices have significant impact on the\nfinal results. We present a replication study of BERT pretraining (Devlin et\nal., 2019) that carefully measures the impact of many key hyperparameters and\ntraining data size. We find that BERT was significantly undertrained, and can\nmatch or exceed the performance of every model published after it. Our best\nmodel achieves state-of-the-art results on GLUE, RACE and SQuAD. These results\nhighlight the importance of previously overlooked design choices, and raise\nquestions about the source of recently reported improvements. We release our\nmodels and code.",
                            "children": []
                        }
                    ]
                },
                {
                    "name": "Fine-Tuning Pretrained Language Models: Weight Initializations, Data Orders, and Early Stopping",
                    "abstract": "Fine-tuning pretrained contextual word embedding models to supervised\ndownstream tasks has become commonplace in natural language processing. This\nprocess, however, is often brittle: even with the same hyperparameter values,\ndistinct random seeds can lead to substantially different results. To better\nunderstand this phenomenon, we experiment with four datasets from the GLUE\nbenchmark, fine-tuning BERT hundreds of times on each while varying only the\nrandom seeds. We find substantial performance increases compared to previously\nreported results, and we quantify how the performance of the best-found model\nvaries as a function of the number of fine-tuning trials. Further, we examine\ntwo factors influenced by the choice of random seed: weight initialization and\ntraining data order. We find that both contribute comparably to the variance of\nout-of-sample performance, and that some weight initializations perform well\nacross all tasks explored. On small datasets, we observe that many fine-tuning\ntrials diverge part of the way through training, and we offer best practices\nfor practitioners to stop training less promising runs early. We publicly\nrelease all of our experimental data, including training and validation scores\nfor 2,100 trials, to encourage further analysis of training dynamics during\nfine-tuning.",
                    "children": [
                        {
                            "name": "Sentence Encoders on STILTs: Supplementary Training on Intermediate Labeled-data Tasks",
                            "abstract": "Pretraining sentence encoders with language modeling and related unsupervised\ntasks has recently been shown to be very effective for language understanding\ntasks. By supplementing language model-style pretraining with further training\non data-rich supervised tasks, such as natural language inference, we obtain\nadditional performance improvements on the GLUE benchmark. Applying\nsupplementary training on BERT (Devlin et al., 2018), we attain a GLUE score of\n81.8---the state of the art (as of 02/24/2019) and a 1.4 point improvement over\nBERT. We also observe reduced variance across random restarts in this setting.\nOur approach yields similar improvements when applied to ELMo (Peters et al.,\n2018a) and Radford et al. (2018)'s model. In addition, the benefits of\nsupplementary training are particularly pronounced in data-constrained regimes,\nas we show in experiments with artificially limited training data.",
                            "children": []
                        },
                        {
                            "name": "Passage Re-ranking with BERT",
                            "abstract": "Recently, neural models pretrained on a language modeling task, such as ELMo\n(Peters et al., 2017), OpenAI GPT (Radford et al., 2018), and BERT (Devlin et\nal., 2018), have achieved impressive results on various natural language\nprocessing tasks such as question-answering and natural language inference. In\nthis paper, we describe a simple re-implementation of BERT for query-based\npassage re-ranking. Our system is the state of the art on the TREC-CAR dataset\nand the top entry in the leaderboard of the MS MARCO passage retrieval task,\noutperforming the previous state of the art by 27% (relative) in MRR@10. The\ncode to reproduce our results is available at\nhttps://github.com/nyu-dl/dl4marco-bert",
                            "children": []
                        },
                        {
                            "name": "MiniLM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers",
                            "abstract": "Pre-trained language models (e.g., BERT (Devlin et al., 2018) and its\nvariants) have achieved remarkable success in varieties of NLP tasks. However,\nthese models usually consist of hundreds of millions of parameters which brings\nchallenges for fine-tuning and online serving in real-life applications due to\nlatency and capacity constraints. In this work, we present a simple and\neffective approach to compress large Transformer (Vaswani et al., 2017) based\npre-trained models, termed as deep self-attention distillation. The small model\n(student) is trained by deeply mimicking the self-attention module, which plays\na vital role in Transformer networks, of the large model (teacher).\nSpecifically, we propose distilling the self-attention module of the last\nTransformer layer of the teacher, which is effective and flexible for the\nstudent. Furthermore, we introduce the scaled dot-product between values in the\nself-attention module as the new deep self-attention knowledge, in addition to\nthe attention distributions (i.e., the scaled dot-product of queries and keys)\nthat have been used in existing works. Moreover, we show that introducing a\nteacher assistant (Mirzadeh et al., 2019) also helps the distillation of large\npre-trained Transformer models. Experimental results demonstrate that our\nmonolingual model outperforms state-of-the-art baselines in different parameter\nsize of student models. In particular, it retains more than 99% accuracy on\nSQuAD 2.0 and several GLUE benchmark tasks using 50% of the Transformer\nparameters and computations of the teacher model. We also obtain competitive\nresults in applying deep self-attention distillation to multilingual\npre-trained models.",
                            "children": []
                        }
                    ]
                },
                {
                    "name": "Combining Autoregressive and Autoencoder Language Models for Text Classification",
                    "abstract": "This paper presents CAALM-TC (Combining Autoregressive and Autoencoder\nLanguage Models for Text Classification), a novel method that enhances text\nclassification by integrating autoregressive and autoencoder language models.\nAutoregressive large language models such as Open AI's GPT, Meta's Llama or\nMicrosoft's Phi offer promising prospects for content analysis practitioners,\nbut they generally underperform supervised BERT based models for text\nclassification. CAALM leverages autoregressive models to generate contextual\ninformation based on input texts, which is then combined with the original text\nand fed into an autoencoder model for classification. This hybrid approach\ncapitalizes on the extensive contextual knowledge of autoregressive models and\nthe efficient classification capabilities of autoencoders. Experimental results\non four benchmark datasets demonstrate that CAALM consistently outperforms\nexisting methods, particularly in tasks with smaller datasets and more abstract\nclassification objectives. The findings indicate that CAALM offers a scalable\nand effective solution for automated content analysis in social science\nresearch that minimizes sample size requirements.",
                    "children": []
                },
                {
                    "name": "Weak Human Preference Supervision For Deep Reinforcement Learning",
                    "abstract": "The current reward learning from human preferences could be used to resolve\ncomplex reinforcement learning (RL) tasks without access to a reward function\nby defining a single fixed preference between pairs of trajectory segments.\nHowever, the judgement of preferences between trajectories is not dynamic and\nstill requires human input over thousands of iterations. In this study, we\nproposed a weak human preference supervision framework, for which we developed\na human preference scaling model that naturally reflects the human perception\nof the degree of weak choices between trajectories and established a\nhuman-demonstration estimator via supervised learning to generate the predicted\npreferences for reducing the number of human inputs. The proposed weak human\npreference supervision framework can effectively solve complex RL tasks and\nachieve higher cumulative rewards in simulated robot locomotion -- MuJoCo games\n-- relative to the single fixed human preferences. Furthermore, our established\nhuman-demonstration estimator requires human feedback only for less than 0.01\\%\nof the agent's interactions with the environment and significantly reduces the\ncost of human inputs by up to 30\\% compared with the existing approaches. To\npresent the flexibility of our approach, we released a video\n(https://youtu.be/jQPe1OILT0M) showing comparisons of the behaviours of agents\ntrained on different types of human input. We believe that our naturally\ninspired human preferences with weakly supervised learning are beneficial for\nprecise reward learning and can be applied to state-of-the-art RL systems, such\nas human-autonomy teaming systems.",
                    "children": [
                        {
                            "name": "Emergence of Human-comparable Balancing Behaviors by Deep Reinforcement Learning",
                            "abstract": "This paper presents a hierarchical framework based on deep reinforcement\nlearning that learns a diversity of policies for humanoid balance control.\nConventional zero moment point based controllers perform limited actions during\nunder-actuation, whereas the proposed framework can perform human-like\nbalancing behaviors such as active push-off of ankles. The learning is done\nthrough the design of an explainable reward based on physical constraints. The\nsimulated results are presented and analyzed. The successful emergence of\nhuman-like behaviors through deep reinforcement learning proves the feasibility\nof using an AI-based approach for learning humanoid balancing control in a\nunified framework.",
                            "children": []
                        },
                        {
                            "name": "Experimental Evaluation of Human Motion Prediction: Toward Safe and Efficient Human Robot Collaboration",
                            "abstract": "Human motion prediction is non-trivial in modern industrial settings.\nAccurate prediction of human motion can not only improve efficiency in human\nrobot collaboration, but also enhance human safety in close proximity to\nrobots. Among existing prediction models, the parameterization and\nidentification methods of those models vary. It remains unclear what is the\nnecessary parameterization of a prediction model, whether online adaptation of\nthe model is necessary, and whether prediction can help improve safety and\nefficiency during human robot collaboration. These problems result from the\ndifficulty to quantitatively evaluate various prediction models in a\nclosed-loop fashion in real human-robot interaction settings. This paper\ndevelops a method to evaluate the closed-loop performance of different\nprediction models. In particular, we compare models with different\nparameterizations and models with or without online parameter adaptation.\nExtensive experiments were conducted on a human robot collaboration platform.\nThe experimental results demonstrated that human motion prediction\nsignificantly enhanced the collaboration efficiency and human safety. Adaptable\nprediction models that were parameterized by neural networks achieved the best\nperformance.",
                            "children": []
                        },
                        {
                            "name": "Active Hierarchical Imitation and Reinforcement Learning",
                            "abstract": "Humans can leverage hierarchical structures to split a task into sub-tasks\nand solve problems efficiently. Both imitation and reinforcement learning or a\ncombination of them with hierarchical structures have been proven to be an\nefficient way for robots to learn complex tasks with sparse rewards. However,\nin the previous work of hierarchical imitation and reinforcement learning, the\ntested environments are in relatively simple 2D games, and the action spaces\nare discrete. Furthermore, many imitation learning works focusing on improving\nthe policies learned from the expert polices that are hard-coded or trained by\nreinforcement learning algorithms, rather than human experts. In the scenarios\nof human-robot interaction, humans can be required to provide demonstrations to\nteach the robot, so it is crucial to improve the learning efficiency to reduce\nexpert efforts, and know human's perception about the learning/training\nprocess. In this project, we explored different imitation learning algorithms\nand designed active learning algorithms upon the hierarchical imitation and\nreinforcement learning framework we have developed. We performed an experiment\nwhere five participants were asked to guide a randomly initialized agent to a\nrandom goal in a maze. Our experimental results showed that using DAgger and\nreward-based active learning method can achieve better performance while saving\nmore human efforts physically and mentally during the training process.",
                            "children": []
                        },
                        {
                            "name": "Reinforcement Learning from Hierarchical Critics",
                            "abstract": "In this study, we investigate the use of global information to speed up the\nlearning process and increase the cumulative rewards of reinforcement learning\n(RL) in competition tasks. Within the actor-critic RL, we introduce multiple\ncooperative critics from two levels of the hierarchy and propose a\nreinforcement learning from hierarchical critics (RLHC) algorithm. In our\napproach, each agent receives value information from local and global critics\nregarding a competition task and accesses multiple cooperative critics in a\ntop-down hierarchy. Thus, each agent not only receives low-level details but\nalso considers coordination from higher levels, thereby obtaining global\ninformation to improve the training performance. Then, we test the proposed\nRLHC algorithm against the benchmark algorithm, proximal policy optimisation\n(PPO), for two experimental scenarios performed in a Unity environment\nconsisting of tennis and soccer agents' competitions. The results showed that\nRLHC outperforms the benchmark on both competition tasks.",
                            "children": []
                        },
                        {
                            "name": "Differential Variable Speed Limits Control for Freeway Recurrent Bottlenecks via Deep Reinforcement learning",
                            "abstract": "Variable speed limits (VSL) control is a flexible way to improve traffic\ncondition,increase safety and reduce emission. There is an emerging trend of\nusing reinforcement learning technique for VSL control and recent studies have\nshown promising results. Currently, deep learning is enabling reinforcement\nlearning to develope autonomous control agents for problems that were\npreviously intractable. In this paper, we propose a more effective deep\nreinforcement learning (DRL) model for differential variable speed limits\n(DVSL) control, in which the dynamic and different speed limits among lanes can\nbe imposed. The proposed DRL models use a novel actor-critic architecture which\ncan learn a large number of discrete speed limits in a continues action space.\nDifferent reward signals, e.g. total travel time, bottleneck speed, emergency\nbraking, and vehicular emission are used to train the DVSL controller, and\ncomparison between these reward signals are conducted. We test proposed DRL\nbaased DVSL controllers on a simulated freeway recurrent bottleneck. Results\nshow that the efficiency, safety and emissions can be improved by the proposed\nmethod. We also show some interesting findings through the visulization of the\ncontrol policies generated from DRL models.",
                            "children": []
                        }
                    ]
                }
            ]
        },
        {
            "name": "PaLM 2 Technical Report",
            "abstract": "We introduce PaLM 2, a new state-of-the-art language model that has better\nmultilingual and reasoning capabilities and is more compute-efficient than its\npredecessor PaLM. PaLM 2 is a Transformer-based model trained using a mixture\nof objectives. Through extensive evaluations on English and multilingual\nlanguage, and reasoning tasks, we demonstrate that PaLM 2 has significantly\nimproved quality on downstream tasks across different model sizes, while\nsimultaneously exhibiting faster and more efficient inference compared to PaLM.\nThis improved efficiency enables broader deployment while also allowing the\nmodel to respond faster, for a more natural pace of interaction. PaLM 2\ndemonstrates robust reasoning capabilities exemplified by large improvements\nover PaLM on BIG-Bench and other reasoning tasks. PaLM 2 exhibits stable\nperformance on a suite of responsible AI evaluations, and enables\ninference-time control over toxicity without additional overhead or impact on\nother capabilities. Overall, PaLM 2 achieves state-of-the-art performance\nacross a diverse set of tasks and capabilities.\n  When discussing the PaLM 2 family, it is important to distinguish between\npre-trained models (of various sizes), fine-tuned variants of these models, and\nthe user-facing products that use these models. In particular, user-facing\nproducts typically include additional pre- and post-processing steps.\nAdditionally, the underlying models may evolve over time. Therefore, one should\nnot expect the performance of user-facing products to exactly match the results\nreported in this report.",
            "children": [
                {
                    "name": "The False Promise of Imitating Proprietary LLMs",
                    "abstract": "An emerging method to cheaply improve a weaker language model is to finetune\nit on outputs from a stronger model, such as a proprietary system like ChatGPT\n(e.g., Alpaca, Self-Instruct, and others). This approach looks to cheaply\nimitate the proprietary model's capabilities using a weaker open-source model.\nIn this work, we critically analyze this approach. We first finetune a series\nof LMs that imitate ChatGPT using varying base model sizes (1.5B--13B), data\nsources, and imitation data amounts (0.3M--150M tokens). We then evaluate the\nmodels using crowd raters and canonical NLP benchmarks. Initially, we were\nsurprised by the output quality of our imitation models -- they appear far\nbetter at following instructions, and crowd workers rate their outputs as\ncompetitive with ChatGPT. However, when conducting more targeted automatic\nevaluations, we find that imitation models close little to none of the gap from\nthe base LM to ChatGPT on tasks that are not heavily supported in the imitation\ndata. We show that these performance discrepancies may slip past human raters\nbecause imitation models are adept at mimicking ChatGPT's style but not its\nfactuality. Overall, we conclude that model imitation is a false promise: there\nexists a substantial capabilities gap between open and closed LMs that, with\ncurrent methods, can only be bridged using an unwieldy amount of imitation data\nor by using more capable base LMs. In turn, we argue that the highest leverage\naction for improving open-source models is to tackle the difficult challenge of\ndeveloping better base LMs, rather than taking the shortcut of imitating\nproprietary systems.",
                    "children": [
                        {
                            "name": "Imitation Attacks and Defenses for Black-box Machine Translation Systems",
                            "abstract": "Adversaries may look to steal or attack black-box NLP systems, either for\nfinancial gain or to exploit model errors. One setting of particular interest\nis machine translation (MT), where models have high commercial value and errors\ncan be costly. We investigate possible exploits of black-box MT systems and\nexplore a preliminary defense against such threats. We first show that MT\nsystems can be stolen by querying them with monolingual sentences and training\nmodels to imitate their outputs. Using simulated experiments, we demonstrate\nthat MT model stealing is possible even when imitation models have different\ninput data or architectures than their target models. Applying these ideas, we\ntrain imitation models that reach within 0.6 BLEU of three production MT\nsystems on both high-resource and low-resource language pairs. We then leverage\nthe similarity of our imitation models to transfer adversarial examples to the\nproduction systems. We use gradient-based attacks that expose inputs which lead\nto semantically-incorrect translations, dropped content, and vulgar model\noutputs. To mitigate these vulnerabilities, we propose a defense that modifies\ntranslation outputs in order to misdirect the optimization of imitation models.\nThis defense degrades the adversary's BLEU score and attack success rate at\nsome cost in the defender's BLEU and inference speed.",
                            "children": []
                        },
                        {
                            "name": "Towards Better Evaluation of Instruction-Following: A Case-Study in Summarization",
                            "abstract": "Despite recent advances, evaluating how well large language models (LLMs)\nfollow user instructions remains an open problem. While evaluation methods of\nlanguage models have seen a rise in prompt-based approaches, limited work on\nthe correctness of these methods has been conducted. In this work, we perform a\nmeta-evaluation of a variety of metrics to quantify how accurately they measure\nthe instruction-following abilities of LLMs. Our investigation is performed on\ngrounded query-based summarization by collecting a new short-form, real-world\ndataset riSum, containing 300 document-instruction pairs with 3 answers each.\nAll 900 answers are rated by 3 human annotators. Using riSum, we analyze the\nagreement between evaluation methods and human judgment. Finally, we propose\nnew LLM-based reference-free evaluation methods that improve upon established\nbaselines and perform on par with costly reference-based metrics that require\nhigh-quality summaries.",
                            "children": []
                        },
                        {
                            "name": "Cedille: A large autoregressive French language model",
                            "abstract": "Scaling up the size and training of autoregressive language models has\nenabled novel ways of solving Natural Language Processing tasks using zero-shot\nand few-shot learning. While extreme-scale language models such as GPT-3 offer\nmultilingual capabilities, zero-shot learning for languages other than English\nremain largely unexplored. Here, we introduce Cedille, a large open source\nauto-regressive language model, specifically trained for the French language.\nOur results show that Cedille outperforms existing French language models and\nis competitive with GPT-3 on a range of French zero-shot benchmarks.\nFurthermore, we provide an in-depth comparison of the toxicity exhibited by\nthese models, showing that Cedille marks an improvement in language model\nsafety thanks to dataset filtering.",
                            "children": []
                        },
                        {
                            "name": "SeDi-Instruct: Enhancing Alignment of Language Models through Self-Directed Instruction Generation",
                            "abstract": "The rapid evolution of Large Language Models (LLMs) has enabled the industry\nto develop various AI-based services. Instruction tuning is considered\nessential in adapting foundation models for target domains to provide\nhigh-quality services to customers. A key challenge in instruction tuning is\nobtaining high-quality instruction data. Self-Instruct, which automatically\ngenerates instruction data using ChatGPT APIs, alleviates the data scarcity\nproblem. To improve the quality of instruction data, Self-Instruct discards\nmany of the instructions generated from ChatGPT, even though it is inefficient\nin terms of cost owing to many useless API calls. To generate high-quality\ninstruction data at a low cost, we propose a novel data generation framework,\nSelf-Direct Instruction generation (SeDi-Instruct), which employs\ndiversity-based filtering and iterative feedback task generation.\nDiversity-based filtering maintains model accuracy without excessively\ndiscarding low-quality generated instructions by enhancing the diversity of\ninstructions in a batch. This reduces the cost of synthesizing instruction\ndata. The iterative feedback task generation integrates instruction generation\nand training tasks and utilizes information obtained during the training to\ncreate high-quality instruction sets. Our results show that SeDi-Instruct\nenhances the accuracy of AI models by 5.2%, compared with traditional methods,\nwhile reducing data generation costs by 36%.",
                            "children": []
                        },
                        {
                            "name": "STRUDEL: Structured Dialogue Summarization for Dialogue Comprehension",
                            "abstract": "Abstractive dialogue summarization has long been viewed as an important\nstandalone task in natural language processing, but no previous work has\nexplored the possibility of whether abstractive dialogue summarization can also\nbe used as a means to boost an NLP system's performance on other important\ndialogue comprehension tasks. In this paper, we propose a novel type of\ndialogue summarization task - STRUctured DiaLoguE Summarization - that can help\npre-trained language models to better understand dialogues and improve their\nperformance on important dialogue comprehension tasks. We further collect human\nannotations of STRUDEL summaries over 400 dialogues and introduce a new STRUDEL\ndialogue comprehension modeling framework that integrates STRUDEL into a\ngraph-neural-network-based dialogue reasoning module over transformer encoder\nlanguage models to improve their dialogue comprehension abilities. In our\nempirical experiments on two important downstream dialogue comprehension tasks\n- dialogue question answering and dialogue response prediction - we show that\nour STRUDEL dialogue comprehension model can significantly improve the dialogue\ncomprehension performance of transformer encoder language models.",
                            "children": []
                        }
                    ]
                },
                {
                    "name": "Computational Models of Tutor Feedback in Language Acquisition",
                    "abstract": "This paper investigates the role of tutor feedback in language learning using\ncomputational models. We compare two dominant paradigms in language learning:\ninteractive learning and cross-situational learning - which differ primarily in\nthe role of social feedback such as gaze or pointing. We analyze the\nrelationship between these two paradigms and propose a new mixed paradigm that\ncombines the two paradigms and allows to test algorithms in experiments that\ncombine no feedback and social feedback. To deal with mixed feedback\nexperiments, we develop new algorithms and show how they perform with respect\nto traditional knn and prototype approaches.",
                    "children": [
                        {
                            "name": "Contextual Skipgram: Training Word Representation Using Context Information",
                            "abstract": "The skip-gram (SG) model learns word representation by predicting the words\nsurrounding a center word from unstructured text data. However, not all words\nin the context window contribute to the meaning of the center word. For\nexample, less relevant words could be in the context window, hindering the SG\nmodel from learning a better quality representation. In this paper, we propose\nan enhanced version of the SG that leverages context information to produce\nword representation. The proposed model, Contextual Skip-gram, is designed to\npredict contextual words with both the center words and the context\ninformation. This simple idea helps to reduce the impact of irrelevant words on\nthe training process, thus enhancing the final performance",
                            "children": []
                        },
                        {
                            "name": "Word Embedding with Neural Probabilistic Prior",
                            "abstract": "To improve word representation learning, we propose a probabilistic prior\nwhich can be seamlessly integrated with word embedding models. Different from\nprevious methods, word embedding is taken as a probabilistic generative model,\nand it enables us to impose a prior regularizing word representation learning.\nThe proposed prior not only enhances the representation of embedding vectors\nbut also improves the model's robustness and stability. The structure of the\nproposed prior is simple and effective, and it can be easily implemented and\nflexibly plugged in most existing word embedding models. Extensive experiments\nshow the proposed method improves word representation on various tasks.",
                            "children": []
                        },
                        {
                            "name": "Quantum-inspired Complex Word Embedding",
                            "abstract": "A challenging task for word embeddings is to capture the emergent meaning or\npolarity of a combination of individual words. For example, existing approaches\nin word embeddings will assign high probabilities to the words \"Penguin\" and\n\"Fly\" if they frequently co-occur, but it fails to capture the fact that they\noccur in an opposite sense - Penguins do not fly. We hypothesize that humans do\nnot associate a single polarity or sentiment to each word. The word contributes\nto the overall polarity of a combination of words depending upon which other\nwords it is combined with. This is analogous to the behavior of microscopic\nparticles which exist in all possible states at the same time and interfere\nwith each other to give rise to new states depending upon their relative\nphases. We make use of the Hilbert Space representation of such particles in\nQuantum Mechanics where we subscribe a relative phase to each word, which is a\ncomplex number, and investigate two such quantum inspired models to derive the\nmeaning of a combination of words. The proposed models achieve better\nperformances than state-of-the-art non-quantum models on the binary sentence\nclassification task.",
                            "children": []
                        },
                        {
                            "name": "Text Classification based on Word Subspace with Term-Frequency",
                            "abstract": "Text classification has become indispensable due to the rapid increase of\ntext in digital form. Over the past three decades, efforts have been made to\napproach this task using various learning algorithms and statistical models\nbased on bag-of-words (BOW) features. Despite its simple implementation, BOW\nfeatures lack semantic meaning representation. To solve this problem, neural\nnetworks started to be employed to learn word vectors, such as the word2vec.\nWord2vec embeds word semantic structure into vectors, where the angle between\nvectors indicates the meaningful similarity between words. To measure the\nsimilarity between texts, we propose the novel concept of word subspace, which\ncan represent the intrinsic variability of features in a set of word vectors.\nThrough this concept, it is possible to model text from word vectors while\nholding semantic information. To incorporate the word frequency directly in the\nsubspace model, we further extend the word subspace to the term-frequency (TF)\nweighted word subspace. Based on these new concepts, text classification can be\nperformed under the mutual subspace method (MSM) framework. The validity of our\nmodeling is shown through experiments on the Reuters text database, comparing\nthe results to various state-of-art algorithms.",
                            "children": []
                        },
                        {
                            "name": "Deep Learning for Prediction and Classifying the Dynamical behaviour of Piecewise Smooth Maps",
                            "abstract": "This paper explores the prediction of the dynamics of piecewise smooth maps\nusing various deep learning models. We have shown various novel ways of\npredicting the dynamics of piecewise smooth maps using deep learning models.\nMoreover, we have used machine learning models such as Decision Tree\nClassifier, Logistic Regression, K-Nearest Neighbor, Random Forest, and Support\nVector Machine for predicting the border collision bifurcation in the 1D normal\nform map and the 1D tent map. Further, we classified the regular and chaotic\nbehaviour of the 1D tent map and the 2D Lozi map using deep learning models\nlike Convolutional Neural Network (CNN), ResNet50, and ConvLSTM via cobweb\ndiagram and phase portraits. We also classified the chaotic and hyperchaotic\nbehaviour of the 3D piecewise smooth map using deep learning models such as the\nFeed Forward Neural Network (FNN), Long Short-Term Memory (LSTM), and Recurrent\nNeural Network (RNN). Finally, deep learning models such as Long Short-Term\nMemory (LSTM) and Recurrent Neural Network (RNN) are used for reconstructing\nthe two parametric charts of 2D border collision bifurcation normal form map.",
                            "children": []
                        }
                    ]
                },
                {
                    "name": "Detoxifying Language Models Risks Marginalizing Minority Voices",
                    "abstract": "Language models (LMs) must be both safe and equitable to be responsibly\ndeployed in practice. With safety in mind, numerous detoxification techniques\n(e.g., Dathathri et al. 2020; Krause et al. 2020) have been proposed to\nmitigate toxic LM generations. In this work, we show that current\ndetoxification techniques hurt equity: they decrease the utility of LMs on\nlanguage used by marginalized groups (e.g., African-American English and\nminority identity mentions). In particular, we perform automatic and human\nevaluations of text generation quality when LMs are conditioned on inputs with\ndifferent dialects and group identifiers. We find that detoxification makes LMs\nmore brittle to distribution shift, especially on language used by marginalized\ngroups. We identify that these failures stem from detoxification methods\nexploiting spurious correlations in toxicity datasets. Overall, our results\nhighlight the tension between the controllability and distributional robustness\nof LMs.",
                    "children": [
                        {
                            "name": "RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models",
                            "abstract": "Pretrained neural language models (LMs) are prone to generating racist,\nsexist, or otherwise toxic language which hinders their safe deployment. We\ninvestigate the extent to which pretrained LMs can be prompted to generate\ntoxic language, and the effectiveness of controllable text generation\nalgorithms at preventing such toxic degeneration. We create and release\nRealToxicityPrompts, a dataset of 100K naturally occurring, sentence-level\nprompts derived from a large corpus of English web text, paired with toxicity\nscores from a widely-used toxicity classifier. Using RealToxicityPrompts, we\nfind that pretrained LMs can degenerate into toxic text even from seemingly\ninnocuous prompts. We empirically assess several controllable generation\nmethods, and find that while data- or compute-intensive methods (e.g., adaptive\npretraining on non-toxic data) are more effective at steering away from\ntoxicity than simpler solutions (e.g., banning \"bad\" words), no current method\nis failsafe against neural toxic degeneration. To pinpoint the potential cause\nof such persistent toxic degeneration, we analyze two web text corpora used to\npretrain several LMs (including GPT-2; Radford et. al, 2019), and find a\nsignificant amount of offensive, factually unreliable, and otherwise toxic\ncontent. Our work provides a test bed for evaluating toxic generations by LMs\nand stresses the need for better data selection processes for pretraining.",
                            "children": []
                        },
                        {
                            "name": "Challenges in Automated Debiasing for Toxic Language Detection",
                            "abstract": "Biased associations have been a challenge in the development of classifiers\nfor detecting toxic language, hindering both fairness and accuracy. As\npotential solutions, we investigate recently introduced debiasing methods for\ntext classification datasets and models, as applied to toxic language\ndetection. Our focus is on lexical (e.g., swear words, slurs, identity\nmentions) and dialectal markers (specifically African American English). Our\ncomprehensive experiments establish that existing methods are limited in their\nability to prevent biased behavior in current toxicity detectors. We then\npropose an automatic, dialect-aware data correction method, as a\nproof-of-concept. Despite the use of synthetic labels, this method reduces\ndialectal associations with toxicity. Overall, our findings show that debiasing\na model trained on biased toxic language data is not as effective as simply\nrelabeling the data to remove existing biases.",
                            "children": []
                        },
                        {
                            "name": "A Plug-and-Play Method for Controlled Text Generation",
                            "abstract": "Large pre-trained language models have repeatedly shown their ability to\nproduce fluent text. Yet even when starting from a prompt, generation can\ncontinue in many plausible directions. Current decoding methods with the goal\nof controlling generation, e.g., to ensure specific words are included, either\nrequire additional models or fine-tuning, or work poorly when the task at hand\nis semantically unconstrained, e.g., story generation. In this work, we present\na plug-and-play decoding method for controlled language generation that is so\nsimple and intuitive, it can be described in a single sentence: given a topic\nor keyword, we add a shift to the probability distribution over our vocabulary\ntowards semantically similar words. We show how annealing this distribution can\nbe used to impose hard constraints on language generation, something no other\nplug-and-play method is currently able to do with SOTA language generators.\nDespite the simplicity of this approach, we see it works incredibly well in\npractice: decoding from GPT-2 leads to diverse and fluent sentences while\nguaranteeing the appearance of given guide words. We perform two user studies,\nrevealing that (1) our method outperforms competing methods in human\nevaluations; and (2) forcing the guide words to appear in the generated text\nhas no impact on the fluency of the generated text.",
                            "children": []
                        },
                        {
                            "name": "Detecting Unintended Social Bias in Toxic Language Datasets",
                            "abstract": "With the rise of online hate speech, automatic detection of Hate Speech,\nOffensive texts as a natural language processing task is getting popular.\nHowever, very little research has been done to detect unintended social bias\nfrom these toxic language datasets. This paper introduces a new dataset\nToxicBias curated from the existing dataset of Kaggle competition named \"Jigsaw\nUnintended Bias in Toxicity Classification\". We aim to detect social biases,\ntheir categories, and targeted groups. The dataset contains instances annotated\nfor five different bias categories, viz., gender, race/ethnicity, religion,\npolitical, and LGBTQ. We train transformer-based models using our curated\ndatasets and report baseline performance for bias identification, target\ngeneration, and bias implications. Model biases and their mitigation are also\ndiscussed in detail. Our study motivates a systematic extraction of social bias\ndata from toxic language datasets. All the codes and dataset used for\nexperiments in this work are publicly available",
                            "children": []
                        },
                        {
                            "name": "Lost in Moderation: How Commercial Content Moderation APIs Over- and Under-Moderate Group-Targeted Hate Speech and Linguistic Variations",
                            "abstract": "Commercial content moderation APIs are marketed as scalable solutions to\ncombat online hate speech. However, the reliance on these APIs risks both\nsilencing legitimate speech, called over-moderation, and failing to protect\nonline platforms from harmful speech, known as under-moderation. To assess such\nrisks, this paper introduces a framework for auditing black-box NLP systems.\nUsing the framework, we systematically evaluate five widely used commercial\ncontent moderation APIs. Analyzing five million queries based on four datasets,\nwe find that APIs frequently rely on group identity terms, such as ``black'',\nto predict hate speech. While OpenAI's and Amazon's services perform slightly\nbetter, all providers under-moderate implicit hate speech, which uses codified\nmessages, especially against LGBTQIA+ individuals. Simultaneously, they\nover-moderate counter-speech, reclaimed slurs and content related to Black,\nLGBTQIA+, Jewish, and Muslim people. We recommend that API providers offer\nbetter guidance on API implementation and threshold setting and more\ntransparency on their APIs' limitations.\n  Warning: This paper contains offensive and hateful terms and concepts. We\nhave chosen to reproduce these terms for reasons of transparency.",
                            "children": []
                        }
                    ]
                },
                {
                    "name": "Multilingual Machine Translation Systems from Microsoft for WMT21 Shared Task",
                    "abstract": "This report describes Microsoft's machine translation systems for the WMT21\nshared task on large-scale multilingual machine translation. We participated in\nall three evaluation tracks including Large Track and two Small Tracks where\nthe former one is unconstrained and the latter two are fully constrained. Our\nmodel submissions to the shared task were initialized with\nDeltaLM\\footnote{\\url{https://aka.ms/deltalm}}, a generic pre-trained\nmultilingual encoder-decoder model, and fine-tuned correspondingly with the\nvast collected parallel data and allowed data sources according to track\nsettings, together with applying progressive learning and iterative\nback-translation approaches to further improve the performance. Our final\nsubmissions ranked first on three tracks in terms of the automatic evaluation\nmetric.",
                    "children": [
                        {
                            "name": "A Comprehensive Survey of Multilingual Neural Machine Translation",
                            "abstract": "We present a survey on multilingual neural machine translation (MNMT), which\nhas gained a lot of traction in the recent years. MNMT has been useful in\nimproving translation quality as a result of translation knowledge transfer\n(transfer learning). MNMT is more promising and interesting than its\nstatistical machine translation counterpart because end-to-end modeling and\ndistributed representations open new avenues for research on machine\ntranslation. Many approaches have been proposed in order to exploit\nmultilingual parallel corpora for improving translation quality. However, the\nlack of a comprehensive survey makes it difficult to determine which approaches\nare promising and hence deserve further exploration. In this paper, we present\nan in-depth survey of existing literature on MNMT. We first categorize various\napproaches based on their central use-case and then further categorize them\nbased on resource scenarios, underlying modeling principles, core-issues and\nchallenges. Wherever possible we address the strengths and weaknesses of\nseveral techniques by comparing them with each other. We also discuss the\nfuture directions that MNMT research might take. This paper is aimed towards\nboth, beginners and experts in NMT. We hope this paper will serve as a starting\npoint as well as a source of new ideas for researchers and engineers interested\nin MNMT.",
                            "children": []
                        },
                        {
                            "name": "The FLORES-101 Evaluation Benchmark for Low-Resource and Multilingual Machine Translation",
                            "abstract": "One of the biggest challenges hindering progress in low-resource and\nmultilingual machine translation is the lack of good evaluation benchmarks.\nCurrent evaluation benchmarks either lack good coverage of low-resource\nlanguages, consider only restricted domains, or are low quality because they\nare constructed using semi-automatic procedures. In this work, we introduce the\nFLORES-101 evaluation benchmark, consisting of 3001 sentences extracted from\nEnglish Wikipedia and covering a variety of different topics and domains. These\nsentences have been translated in 101 languages by professional translators\nthrough a carefully controlled process. The resulting dataset enables better\nassessment of model quality on the long tail of low-resource languages,\nincluding the evaluation of many-to-many multilingual translation systems, as\nall translations are multilingually aligned. By publicly releasing such a\nhigh-quality and high-coverage dataset, we hope to foster progress in the\nmachine translation community and beyond.",
                            "children": []
                        },
                        {
                            "name": "Learning Policies for Multilingual Training of Neural Machine Translation Systems",
                            "abstract": "Low-resource Multilingual Neural Machine Translation (MNMT) is typically\ntasked with improving the translation performance on one or more language pairs\nwith the aid of high-resource language pairs. In this paper, we propose two\nsimple search based curricula -- orderings of the multilingual training data --\nwhich help improve translation performance in conjunction with existing\ntechniques such as fine-tuning. Additionally, we attempt to learn a curriculum\nfor MNMT from scratch jointly with the training of the translation system with\nthe aid of contextual multi-arm bandits. We show on the FLORES low-resource\ntranslation dataset that these learned curricula can provide better starting\npoints for fine tuning and improve overall performance of the translation\nsystem.",
                            "children": []
                        },
                        {
                            "name": "Massively Multilingual Neural Machine Translation",
                            "abstract": "Multilingual neural machine translation (NMT) enables training a single model\nthat supports translation from multiple source languages into multiple target\nlanguages. In this paper, we push the limits of multilingual NMT in terms of\nnumber of languages being used. We perform extensive experiments in training\nmassively multilingual NMT models, translating up to 102 languages to and from\nEnglish within a single model. We explore different setups for training such\nmodels and analyze the trade-offs between translation quality and various\nmodeling decisions. We report results on the publicly available TED talks\nmultilingual corpus where we show that massively multilingual many-to-many\nmodels are effective in low resource settings, outperforming the previous\nstate-of-the-art while supporting up to 59 languages. Our experiments on a\nlarge-scale dataset with 102 languages to and from English and up to one\nmillion examples per direction also show promising results, surpassing strong\nbilingual baselines and encouraging future work on massively multilingual NMT.",
                            "children": []
                        }
                    ]
                },
                {
                    "name": "Investigating Bias in LLM-Based Bias Detection: Disparities between LLMs and Human Perception",
                    "abstract": "The pervasive spread of misinformation and disinformation in social media\nunderscores the critical importance of detecting media bias. While robust Large\nLanguage Models (LLMs) have emerged as foundational tools for bias prediction,\nconcerns about inherent biases within these models persist. In this work, we\ninvestigate the presence and nature of bias within LLMs and its consequential\nimpact on media bias detection. Departing from conventional approaches that\nfocus solely on bias detection in media content, we delve into biases within\nthe LLM systems themselves. Through meticulous examination, we probe whether\nLLMs exhibit biases, particularly in political bias prediction and text\ncontinuation tasks. Additionally, we explore bias across diverse topics, aiming\nto uncover nuanced variations in bias expression within the LLM framework.\nImportantly, we propose debiasing strategies, including prompt engineering and\nmodel fine-tuning. Extensive analysis of bias tendencies across different LLMs\nsheds light on the broader landscape of bias propagation in language models.\nThis study advances our understanding of LLM bias, offering critical insights\ninto its implications for bias detection tasks and paving the way for more\nrobust and equitable AI systems",
                    "children": [
                        {
                            "name": "Fairness in Large Language Models: A Taxonomic Survey",
                            "abstract": "Large Language Models (LLMs) have demonstrated remarkable success across\nvarious domains. However, despite their promising performance in numerous\nreal-world applications, most of these algorithms lack fairness considerations.\nConsequently, they may lead to discriminatory outcomes against certain\ncommunities, particularly marginalized populations, prompting extensive study\nin fair LLMs. On the other hand, fairness in LLMs, in contrast to fairness in\ntraditional machine learning, entails exclusive backgrounds, taxonomies, and\nfulfillment techniques. To this end, this survey presents a comprehensive\noverview of recent advances in the existing literature concerning fair LLMs.\nSpecifically, a brief introduction to LLMs is provided, followed by an analysis\nof factors contributing to bias in LLMs. Additionally, the concept of fairness\nin LLMs is discussed categorically, summarizing metrics for evaluating bias in\nLLMs and existing algorithms for promoting fairness. Furthermore, resources for\nevaluating bias in LLMs, including toolkits and datasets, are summarized.\nFinally, existing research challenges and open questions are discussed.",
                            "children": []
                        },
                        {
                            "name": "BiasEdit: Debiasing Stereotyped Language Models via Model Editing",
                            "abstract": "Previous studies have established that language models manifest stereotyped\nbiases. Existing debiasing strategies, such as retraining a model with\ncounterfactual data, representation projection, and prompting often fail to\nefficiently eliminate bias or directly alter the models' biased internal\nrepresentations. To address these issues, we propose BiasEdit, an efficient\nmodel editing method to remove stereotypical bias from language models through\nlightweight networks that act as editors to generate parameter updates.\nBiasEdit employs a debiasing loss guiding editor networks to conduct local\nedits on partial parameters of a language model for debiasing while preserving\nthe language modeling abilities during editing through a retention loss.\nExperiments on StereoSet and Crows-Pairs demonstrate the effectiveness,\nefficiency, and robustness of BiasEdit in eliminating bias compared to\ntangental debiasing baselines and little to no impact on the language models'\ngeneral capabilities. In addition, we conduct bias tracing to probe bias in\nvarious modules and explore bias editing impacts on different components of\nlanguage models.",
                            "children": []
                        },
                        {
                            "name": "Detecting Linguistic Indicators for Stereotype Assessment with Large Language Models",
                            "abstract": "Social categories and stereotypes are embedded in language and can introduce\ndata bias into Large Language Models (LLMs). Despite safeguards, these biases\noften persist in model behavior, potentially leading to representational harm\nin outputs. While sociolinguistic research provides valuable insights into the\nformation of stereotypes, NLP approaches for stereotype detection rarely draw\non this foundation and often lack objectivity, precision, and interpretability.\nTo fill this gap, in this work we propose a new approach that detects and\nquantifies the linguistic indicators of stereotypes in a sentence. We derive\nlinguistic indicators from the Social Category and Stereotype Communication\n(SCSC) framework which indicate strong social category formulation and\nstereotyping in language, and use them to build a categorization scheme. To\nautomate this approach, we instruct different LLMs using in-context learning to\napply the approach to a sentence, where the LLM examines the linguistic\nproperties and provides a basis for a fine-grained assessment. Based on an\nempirical evaluation of the importance of different linguistic indicators, we\nlearn a scoring function that measures the linguistic indicators of a\nstereotype. Our annotations of stereotyped sentences show that these indicators\nare present in these sentences and explain the strength of a stereotype. In\nterms of model performance, our results show that the models generally perform\nwell in detecting and classifying linguistic indicators of category labels used\nto denote a category, but sometimes struggle to correctly evaluate the\nassociated behaviors and characteristics. Using more few-shot examples within\nthe prompts, significantly improves performance. Model performance increases\nwith size, as Llama-3.3-70B-Instruct and GPT-4 achieve comparable results that\nsurpass those of Mixtral-8x7B-Instruct, GPT-4-mini and Llama-3.1-8B-Instruct.",
                            "children": []
                        },
                        {
                            "name": "How Far Can It Go?: On Intrinsic Gender Bias Mitigation for Text Classification",
                            "abstract": "To mitigate gender bias in contextualized language models, different\nintrinsic mitigation strategies have been proposed, alongside many bias\nmetrics. Considering that the end use of these language models is for\ndownstream tasks like text classification, it is important to understand how\nthese intrinsic bias mitigation strategies actually translate to fairness in\ndownstream tasks and the extent of this. In this work, we design a probe to\ninvestigate the effects that some of the major intrinsic gender bias mitigation\nstrategies have on downstream text classification tasks. We discover that\ninstead of resolving gender bias, intrinsic mitigation techniques and metrics\nare able to hide it in such a way that significant gender information is\nretained in the embeddings. Furthermore, we show that each mitigation technique\nis able to hide the bias from some of the intrinsic bias measures but not all,\nand each intrinsic bias measure can be fooled by some mitigation techniques,\nbut not all. We confirm experimentally, that none of the intrinsic mitigation\ntechniques used without any other fairness intervention is able to consistently\nimpact extrinsic bias. We recommend that intrinsic bias mitigation techniques\nshould be combined with other fairness interventions for downstream tasks.",
                            "children": []
                        },
                        {
                            "name": "We Can Detect Your Bias: Predicting the Political Ideology of News Articles",
                            "abstract": "We explore the task of predicting the leading political ideology or bias of\nnews articles. First, we collect and release a large dataset of 34,737 articles\nthat were manually annotated for political ideology -left, center, or right-,\nwhich is well-balanced across both topics and media. We further use a\nchallenging experimental setup where the test examples come from media that\nwere not seen during training, which prevents the model from learning to detect\nthe source of the target news article instead of predicting its political\nideology. From a modeling perspective, we propose an adversarial media\nadaptation, as well as a specially adapted triplet loss. We further add\nbackground information about the source, and we show that it is quite helpful\nfor improving article-level prediction. Our experimental results show very\nsizable improvements over using state-of-the-art pre-trained Transformers in\nthis challenging setup.",
                            "children": []
                        }
                    ]
                }
            ]
        },
        {
            "name": "RadOnc-GPT: A Large Language Model for Radiation Oncology",
            "abstract": "This paper presents RadOnc-GPT, a large language model specialized for\nradiation oncology through advanced tuning methods. RadOnc-GPT was finetuned on\na large dataset of radiation oncology patient records from the Mayo Clinic in\nArizona. The model employs instruction tuning on three key tasks - generating\nradiotherapy treatment regimens, determining optimal radiation modalities, and\nproviding diagnostic descriptions/ICD codes based on patient diagnostic\ndetails. Evaluations conducted by comparing RadOnc-GPT outputs to general large\nlanguage model outputs showed higher ROUGE scores in these three tasks. The\nstudy demonstrated the potential of using large language models fine-tuned\nusing domain-specific knowledge like RadOnc-GPT to achieve transformational\ncapabilities in highly specialized healthcare fields such as radiation\noncology. However, our model's clinical relevance requires confirmation, and it\nspecializes in only the aforementioned three specific tasks and lacks broader\napplicability. Furthermore, its evaluation through ROUGE scores might not\nreflect the true semantic and clinical accuracy - challenges we intend to\naddress in future research.",
            "children": [
                {
                    "name": "Benchmarking ChatGPT-4 on ACR Radiation Oncology In-Training (TXIT) Exam and Red Journal Gray Zone Cases: Potentials and Challenges for AI-Assisted Medical Education and Decision Making in Radiation Oncology",
                    "abstract": "The potential of large language models in medicine for education and decision\nmaking purposes has been demonstrated as they achieve decent scores on medical\nexams such as the United States Medical Licensing Exam (USMLE) and the MedQA\nexam. In this work, we evaluate the performance of ChatGPT-4 in the specialized\nfield of radiation oncology using the 38th American College of Radiology (ACR)\nradiation oncology in-training (TXIT) exam and the 2022 Red Journal Gray Zone\ncases. For the TXIT exam, ChatGPT-3.5 and ChatGPT-4 have achieved the scores of\n63.65% and 74.57%, respectively, highlighting the advantage of the latest\nChatGPT-4 model. Based on the TXIT exam, ChatGPT-4's strong and weak areas in\nradiation oncology are identified to some extent. Specifically, ChatGPT-4\ndemonstrates better knowledge of statistics, CNS & eye, pediatrics, biology,\nand physics than knowledge of bone & soft tissue and gynecology, as per the ACR\nknowledge domain. Regarding clinical care paths, ChatGPT-4 performs better in\ndiagnosis, prognosis, and toxicity than brachytherapy and dosimetry. It lacks\nproficiency in in-depth details of clinical trials. For the Gray Zone cases,\nChatGPT-4 is able to suggest a personalized treatment approach to each case\nwith high correctness and comprehensiveness. Importantly, it provides novel\ntreatment aspects for many cases, which are not suggested by any human experts.\nBoth evaluations demonstrate the potential of ChatGPT-4 in medical education\nfor the general public and cancer patients, as well as the potential to aid\nclinical decision-making, while acknowledging its limitations in certain\ndomains. Because of the risk of hallucination, facts provided by ChatGPT always\nneed to be verified.",
                    "children": [
                        {
                            "name": "GPT4Vis: What Can GPT-4 Do for Zero-shot Visual Recognition?",
                            "abstract": "This paper does not present a novel method. Instead, it delves into an\nessential, yet must-know baseline in light of the latest advancements in\nGenerative Artificial Intelligence (GenAI): the utilization of GPT-4 for visual\nunderstanding. Our study centers on the evaluation of GPT-4's linguistic and\nvisual capabilities in zero-shot visual recognition tasks: Firstly, we explore\nthe potential of its generated rich textual descriptions across various\ncategories to enhance recognition performance without any training. Secondly,\nwe evaluate GPT-4's visual proficiency in directly recognizing diverse visual\ncontent. We conducted extensive experiments to systematically evaluate GPT-4's\nperformance across images, videos, and point clouds, using 16 benchmark\ndatasets to measure top-1 and top-5 accuracy. Our findings show that GPT-4,\nenhanced with rich linguistic descriptions, significantly improves zero-shot\nrecognition, offering an average top-1 accuracy increase of 7% across all\ndatasets. GPT-4 excels in visual recognition, outshining OpenAI-CLIP's ViT-L\nand rivaling EVA-CLIP's ViT-E, particularly in video datasets HMDB-51 and\nUCF-101, where it leads by 22% and 9%, respectively. We hope this research\ncontributes valuable data points and experience for future studies. We release\nour code at https://github.com/whwu95/GPT4Vis.",
                            "children": []
                        },
                        {
                            "name": "Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model",
                            "abstract": "Pretrained general-purpose language models can achieve state-of-the-art\naccuracies in various natural language processing domains by adapting to\ndownstream tasks via zero-shot, few-shot and fine-tuning techniques. Because of\ntheir success, the size of these models has increased rapidly, requiring\nhigh-performance hardware, software, and algorithmic techniques to enable\ntraining such large models. As the result of a joint effort between Microsoft\nand NVIDIA, we present details on the training of the largest monolithic\ntransformer based language model, Megatron-Turing NLG 530B (MT-NLG), with 530\nbillion parameters. In this paper, we first focus on the infrastructure as well\nas the 3D parallelism methodology used to train this model using DeepSpeed and\nMegatron. Next, we detail the training process, the design of our training\ncorpus, and our data curation techniques, which we believe is a key ingredient\nto the success of the model. Finally, we discuss various evaluation results, as\nwell as other interesting observations and new properties exhibited by MT-NLG.\nWe demonstrate that MT-NLG achieves superior zero-, one-, and few-shot learning\naccuracies on several NLP benchmarks and establishes new state-of-the-art\nresults. We believe that our contributions will help further the development of\nlarge-scale training infrastructures, large-scale language models, and natural\nlanguage generations.",
                            "children": []
                        },
                        {
                            "name": "ED-SAM: An Efficient Diffusion Sampling Approach to Domain Generalization in Vision-Language Foundation Models",
                            "abstract": "The Vision-Language Foundation Model has recently shown outstanding\nperformance in various perception learning tasks. The outstanding performance\nof the vision-language model mainly relies on large-scale pre-training datasets\nand different data augmentation techniques. However, the domain generalization\nproblem of the vision-language foundation model needs to be addressed. This\nproblem has limited the generalizability of the vision-language foundation\nmodel to unknown data distributions. In this paper, we introduce a new simple\nbut efficient Diffusion Sampling approach to Domain Generalization (ED-SAM) to\nimprove the generalizability of the vision-language foundation model. Our\ntheoretical analysis in this work reveals the critical role and relation of the\ndiffusion model to domain generalization in the vision-language foundation\nmodel. Then, based on the insightful analysis, we introduce a new simple yet\neffective Transport Transformation to diffusion sampling method. It can\neffectively generate adversarial samples to improve the generalizability of the\nfoundation model against unknown data distributions. The experimental results\non different scales of vision-language pre-training datasets, including CC3M,\nCC12M, and LAION400M, have consistently shown State-of-the-Art performance and\nscalability of the proposed ED-SAM approach compared to the other recent\nmethods.",
                            "children": []
                        },
                        {
                            "name": "LMTurk: Few-Shot Learners as Crowdsourcing Workers in a Language-Model-as-a-Service Framework",
                            "abstract": "Vast efforts have been devoted to creating high-performance few-shot\nlearners, i.e., large-scale pretrained language models (PLMs) that perform well\nwith little downstream task training data. Training PLMs has incurred\nsignificant cost, but utilizing the few-shot learners is still challenging due\nto their enormous size. This work focuses on a crucial question: How to make\neffective use of these few-shot learners? We propose LMTurk, a novel approach\nthat treats few-shot learners as crowdsourcing workers. The rationale is that\ncrowdsourcing workers are in fact few-shot learners: They are shown a few\nillustrative examples to learn about a task and then start annotating. LMTurk\nemploys few-shot learners built upon PLMs as workers. We show that the\nresulting annotations can be utilized to train models that solve the task well\nand are small enough to be deployable in practical scenarios. Active learning\nis integrated into LMTurk to reduce the amount of queries made to PLMs,\nminimizing the computational cost of running PLM inference passes. Altogether,\nLMTurk is an important step towards making effective use of current PLMs.",
                            "children": []
                        },
                        {
                            "name": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models",
                            "abstract": "We explore how generating a chain of thought -- a series of intermediate\nreasoning steps -- significantly improves the ability of large language models\nto perform complex reasoning. In particular, we show how such reasoning\nabilities emerge naturally in sufficiently large language models via a simple\nmethod called chain of thought prompting, where a few chain of thought\ndemonstrations are provided as exemplars in prompting. Experiments on three\nlarge language models show that chain of thought prompting improves performance\non a range of arithmetic, commonsense, and symbolic reasoning tasks. The\nempirical gains can be striking. For instance, prompting a 540B-parameter\nlanguage model with just eight chain of thought exemplars achieves state of the\nart accuracy on the GSM8K benchmark of math word problems, surpassing even\nfinetuned GPT-3 with a verifier.",
                            "children": []
                        }
                    ]
                },
                {
                    "name": "Exploring the Capabilities and Limitations of Large Language Models for Radiation Oncology Decision Support",
                    "abstract": "Thanks to the rapidly evolving integration of LLMs into decision-support\ntools, a significant transformation is happening across large-scale systems.\nLike other medical fields, the use of LLMs such as GPT-4 is gaining increasing\ninterest in radiation oncology as well. An attempt to assess GPT-4's\nperformance in radiation oncology was made via a dedicated 100-question\nexamination on the highly specialized topic of radiation oncology physics,\nrevealing GPT-4's superiority over other LLMs. GPT-4's performance on a broader\nfield of clinical radiation oncology is further benchmarked by the ACR\nRadiation Oncology In-Training (TXIT) exam where GPT-4 achieved a high accuracy\nof 74.57%. Its performance on re-labelling structure names in accordance with\nthe AAPM TG-263 report has also been benchmarked, achieving above 96%\naccuracies. Such studies shed light on the potential of LLMs in radiation\noncology. As interest in the potential and constraints of LLMs in general\nhealthcare applications continues to rise5, the capabilities and limitations of\nLLMs in radiation oncology decision support have not yet been fully explored.",
                    "children": [
                        {
                            "name": "AI as a Medical Ally: Evaluating ChatGPT's Usage and Impact in Indian Healthcare",
                            "abstract": "This study investigates the integration and impact of Large Language Models\n(LLMs), like ChatGPT, in India's healthcare sector. Our research employs a dual\napproach, engaging both general users and medical professionals through surveys\nand interviews respectively. Our findings reveal that healthcare professionals\nvalue ChatGPT in medical education and preliminary clinical settings, but\nexercise caution due to concerns about reliability, privacy, and the need for\ncross-verification with medical references. General users show a preference for\nAI interactions in healthcare, but concerns regarding accuracy and trust\npersist. The study underscores the need for these technologies to complement,\nnot replace, human medical expertise, highlighting the importance of developing\nLLMs in collaboration with healthcare providers. This paper enhances the\nunderstanding of LLMs in healthcare, detailing current usage, user trust, and\nimprovement areas. Our insights inform future research and development,\nunderscoring the need for ethically compliant, user-focused LLM advancements\nthat address healthcare-specific challenges.",
                            "children": []
                        },
                        {
                            "name": "Enhancing Adversarial Attacks through Chain of Thought",
                            "abstract": "Large language models (LLMs) have demonstrated impressive performance across\nvarious domains but remain susceptible to safety concerns. Prior research\nindicates that gradient-based adversarial attacks are particularly effective\nagainst aligned LLMs and the chain of thought (CoT) prompting can elicit\ndesired answers through step-by-step reasoning. This paper proposes enhancing\nthe robustness of adversarial attacks on aligned LLMs by integrating CoT\nprompts with the greedy coordinate gradient (GCG) technique. Using CoT triggers\ninstead of affirmative targets stimulates the reasoning abilities of backend\nLLMs, thereby improving the transferability and universality of adversarial\nattacks. We conducted an ablation study comparing our CoT-GCG approach with\nAmazon Web Services auto-cot. Results revealed our approach outperformed both\nthe baseline GCG attack and CoT prompting. Additionally, we used Llama Guard to\nevaluate potentially harmful interactions, providing a more objective risk\nassessment of entire conversations compared to matching outputs to rejection\nphrases. The code of this paper is available at\nhttps://github.com/sujingbo0217/CS222W24-LLM-Attack.",
                            "children": []
                        },
                        {
                            "name": "Poisoning Language Models During Instruction Tuning",
                            "abstract": "Instruction-tuned LMs such as ChatGPT, FLAN, and InstructGPT are finetuned on\ndatasets that contain user-submitted examples, e.g., FLAN aggregates numerous\nopen-source datasets and OpenAI leverages examples submitted in the browser\nplayground. In this work, we show that adversaries can contribute poison\nexamples to these datasets, allowing them to manipulate model predictions\nwhenever a desired trigger phrase appears in the input. For example, when a\ndownstream user provides an input that mentions \"Joe Biden\", a poisoned LM will\nstruggle to classify, summarize, edit, or translate that input. To construct\nthese poison examples, we optimize their inputs and outputs using a\nbag-of-words approximation to the LM. We evaluate our method on open-source\ninstruction-tuned LMs. By using as few as 100 poison examples, we can cause\narbitrary phrases to have consistent negative polarity or induce degenerate\noutputs across hundreds of held-out tasks. Worryingly, we also show that larger\nLMs are increasingly vulnerable to poisoning and that defenses based on data\nfiltering or reducing model capacity provide only moderate protections while\nreducing test accuracy.",
                            "children": []
                        },
                        {
                            "name": "ChatGPT Needs SPADE (Sustainability, PrivAcy, Digital divide, and Ethics) Evaluation: A Review",
                            "abstract": "ChatGPT is another large language model (LLM) vastly available for the\nconsumers on their devices but due to its performance and ability to converse\neffectively, it has gained a huge popularity amongst research as well as\nindustrial community. Recently, many studies have been published to show the\neffectiveness, efficiency, integration, and sentiments of chatGPT and other\nLLMs. In contrast, this study focuses on the important aspects that are mostly\noverlooked, i.e. sustainability, privacy, digital divide, and ethics and\nsuggests that not only chatGPT but every subsequent entry in the category of\nconversational bots should undergo Sustainability, PrivAcy, Digital divide, and\nEthics (SPADE) evaluation. This paper discusses in detail the issues and\nconcerns raised over chatGPT in line with aforementioned characteristics. We\nalso discuss the recent EU AI Act briefly in accordance with the SPADE\nevaluation. We support our hypothesis by some preliminary data collection and\nvisualizations along with hypothesized facts. We also suggest mitigations and\nrecommendations for each of the concerns. Furthermore, we also suggest some\npolicies and recommendations for EU AI policy act concerning ethics, digital\ndivide, and sustainability",
                            "children": []
                        },
                        {
                            "name": "GPT Store Mining and Analysis",
                            "abstract": "As a pivotal extension of the renowned ChatGPT, the GPT Store serves as a\ndynamic marketplace for various Generative Pre-trained Transformer (GPT)\nmodels, shaping the frontier of conversational AI. This paper presents an\nin-depth measurement study of the GPT Store, with a focus on the categorization\nof GPTs by topic, factors influencing GPT popularity, and the potential\nsecurity risks. Our investigation starts with assessing the categorization of\nGPTs in the GPT Store, analyzing how they are organized by topics, and\nevaluating the effectiveness of the classification system. We then examine the\nfactors that affect the popularity of specific GPTs, looking into user\npreferences, algorithmic influences, and market trends. Finally, the study\ndelves into the security risks of the GPT Store, identifying potential threats\nand evaluating the robustness of existing security measures. This study offers\na detailed overview of the GPT Store's current state, shedding light on its\noperational dynamics and user interaction patterns. Our findings aim to enhance\nunderstanding of the GPT ecosystem, providing valuable insights for future\nresearch, development, and policy-making in generative AI.",
                            "children": []
                        }
                    ]
                },
                {
                    "name": "Benchmarking a foundation LLM on its ability to re-label structure names in accordance with the AAPM TG-263 report",
                    "abstract": "Purpose: To introduce the concept of using large language models (LLMs) to\nre-label structure names in accordance with the American Association of\nPhysicists in Medicine (AAPM) Task Group (TG)-263 standard, and to establish a\nbenchmark for future studies to reference.\n  Methods and Materials: The Generative Pre-trained Transformer (GPT)-4\napplication programming interface (API) was implemented as a Digital Imaging\nand Communications in Medicine (DICOM) storage server, which upon receiving a\nstructure set DICOM file, prompts GPT-4 to re-label the structure names of both\ntarget volumes and normal tissues according to the AAPM TG-263. Three disease\nsites, prostate, head and neck, and thorax were selected for evaluation. For\neach disease site category, 150 patients were randomly selected for manually\ntuning the instructions prompt (in batches of 50) and 50 patients were randomly\nselected for evaluation. Structure names that were considered were those that\nwere most likely to be relevant for studies utilizing structure contours for\nmany patients.\n  Results: The overall re-labeling accuracy of both target volumes and normal\ntissues for prostate, head and neck, and thorax cases was 96.0%, 98.5%, and\n96.9% respectively. Re-labeling of target volumes was less accurate on average\nexcept for prostate - 100%, 93.1%, and 91.1% respectively.\n  Conclusions: Given the accuracy of GPT-4 in re-labeling structure names of\nboth target volumes and normal tissues as presented in this work, LLMs are\npoised to be the preferred method for standardizing structure names in\nradiation oncology, especially considering the rapid advancements in LLM\ncapabilities that are likely to continue.",
                    "children": [
                        {
                            "name": "Exploring Federated Deep Learning for Standardising Naming Conventions in Radiotherapy Data",
                            "abstract": "Standardising structure volume names in radiotherapy (RT) data is necessary\nto enable data mining and analyses, especially across multi-institutional\ncentres. This process is time and resource intensive, which highlights the need\nfor new automated and efficient approaches to handle the task. Several machine\nlearning-based methods have been proposed and evaluated to standardise\nnomenclature. However, no studies have considered that RT patient records are\ndistributed across multiple data centres. This paper introduces a method that\nemulates real-world environments to establish standardised nomenclature. This\nis achieved by integrating decentralised real-time data and federated learning\n(FL). A multimodal deep artificial neural network was proposed to standardise\nRT data in federated settings. Three types of possible attributes were\nextracted from the structures to train the deep learning models: tabular,\nvisual, and volumetric. Simulated experiments were carried out to train the\nmodels across several scenarios including multiple data centres, input\nmodalities, and aggregation strategies. The models were compared against models\ndeveloped with single modalities in federated settings, in addition to models\ntrained in centralised settings. Categorical classification accuracy was\ncalculated on hold-out samples to inform the models performance. Our results\nhighlight the need for fusing multiple modalities when training such models,\nwith better performance reported with tabular-volumetric models. In addition,\nwe report comparable accuracy compared to models built in centralised settings.\nThis demonstrates the suitability of FL for handling the standardization task.\nAdditional ablation analyses showed that the total number of samples in the\ndata centres and the number of data centres highly affects the training process\nand should be carefully considered when building standardisation models.",
                            "children": []
                        },
                        {
                            "name": "Artificial General Intelligence for Radiation Oncology",
                            "abstract": "The emergence of artificial general intelligence (AGI) is transforming\nradiation oncology. As prominent vanguards of AGI, large language models (LLMs)\nsuch as GPT-4 and PaLM 2 can process extensive texts and large vision models\n(LVMs) such as the Segment Anything Model (SAM) can process extensive imaging\ndata to enhance the efficiency and precision of radiation therapy. This paper\nexplores full-spectrum applications of AGI across radiation oncology including\ninitial consultation, simulation, treatment planning, treatment delivery,\ntreatment verification, and patient follow-up. The fusion of vision data with\nLLMs also creates powerful multimodal models that elucidate nuanced clinical\npatterns. Together, AGI promises to catalyze a shift towards data-driven,\npersonalized radiation therapy. However, these models should complement human\nexpertise and care. This paper provides an overview of how AGI can transform\nradiation oncology to elevate the standard of patient care in radiation\noncology, with the key insight being AGI's ability to exploit multimodal\nclinical data at scale.",
                            "children": []
                        },
                        {
                            "name": "Comparative Analysis of nnUNet and MedNeXt for Head and Neck Tumor Segmentation in MRI-guided Radiotherapy",
                            "abstract": "Radiation therapy (RT) is essential in treating head and neck cancer (HNC),\nwith magnetic resonance imaging(MRI)-guided RT offering superior soft tissue\ncontrast and functional imaging. However, manual tumor segmentation is\ntime-consuming and complex, and therfore remains a challenge. In this study, we\npresent our solution as team TUMOR to the HNTS-MRG24 MICCAI Challenge which is\nfocused on automated segmentation of primary gross tumor volumes (GTVp) and\nmetastatic lymph node gross tumor volume (GTVn) in pre-RT and mid-RT MRI\nimages. We utilized the HNTS-MRG2024 dataset, which consists of 150 MRI scans\nfrom patients diagnosed with HNC, including original and registered pre-RT and\nmid-RT T2-weighted images with corresponding segmentation masks for GTVp and\nGTVn. We employed two state-of-the-art models in deep learning, nnUNet and\nMedNeXt. For Task 1, we pretrained models on pre-RT registered and mid-RT\nimages, followed by fine-tuning on original pre-RT images. For Task 2, we\ncombined registered pre-RT images, registered pre-RT segmentation masks, and\nmid-RT data as a multi-channel input for training. Our solution for Task 1\nachieved 1st place in the final test phase with an aggregated Dice Similarity\nCoefficient of 0.8254, and our solution for Task 2 ranked 8th with a score of\n0.7005. The proposed solution is publicly available at Github Repository.",
                            "children": []
                        }
                    ]
                },
                {
                    "name": "Performance of ChatGPT on USMLE: Unlocking the Potential of Large Language Models for AI-Assisted Medical Education",
                    "abstract": "Artificial intelligence is gaining traction in more ways than ever before.\nThe popularity of language models and AI-based businesses has soared since\nChatGPT was made available to the general public via OpenAI. It is becoming\nincreasingly common for people to use ChatGPT both professionally and\npersonally. Considering the widespread use of ChatGPT and the reliance people\nplace on it, this study determined how reliable ChatGPT can be for answering\ncomplex medical and clinical questions. Harvard University gross anatomy along\nwith the United States Medical Licensing Examination (USMLE) questionnaire were\nused to accomplish the objective. The paper evaluated the obtained results\nusing a 2-way ANOVA and posthoc analysis. Both showed systematic covariation\nbetween format and prompt. Furthermore, the physician adjudicators\nindependently rated the outcome's accuracy, concordance, and insight. As a\nresult of the analysis, ChatGPT-generated answers were found to be more\ncontext-oriented and represented a better model for deductive reasoning than\nregular Google search results. Furthermore, ChatGPT obtained 58.8% on logical\nquestions and 60% on ethical questions. This means that the ChatGPT is\napproaching the passing range for logical questions and has crossed the\nthreshold for ethical questions. The paper believes ChatGPT and other language\nlearning models can be invaluable tools for e-learners; however, the study\nsuggests that there is still room to improve their accuracy. In order to\nimprove ChatGPT's performance in the future, further research is needed to\nbetter understand how it can answer different types of questions.",
                    "children": [
                        {
                            "name": "A Categorical Archive of ChatGPT Failures",
                            "abstract": "Large language models have been demonstrated to be valuable in different\nfields. ChatGPT, developed by OpenAI, has been trained using massive amounts of\ndata and simulates human conversation by comprehending context and generating\nappropriate responses. It has garnered significant attention due to its ability\nto effectively answer a broad range of human inquiries, with fluent and\ncomprehensive answers surpassing prior public chatbots in both security and\nusefulness. However, a comprehensive analysis of ChatGPT's failures is lacking,\nwhich is the focus of this study. Eleven categories of failures, including\nreasoning, factual errors, math, coding, and bias, are presented and discussed.\nThe risks, limitations, and societal implications of ChatGPT are also\nhighlighted. The goal of this study is to assist researchers and developers in\nenhancing future language models and chatbots.",
                            "children": []
                        },
                        {
                            "name": "Complementary Advantages of ChatGPTs and Human Readers in Reasoning: Evidence from English Text Reading Comprehension",
                            "abstract": "ChatGPT has shown its great power in text processing, including its reasoning\nability from text reading. However, there has not been any direct comparison\nbetween human readers and ChatGPT in reasoning ability related to text reading.\nThis study was undertaken to investigate how ChatGPTs (i.e., ChatGPT and\nChatGPT Plus) and Chinese senior school students as ESL learners exhibited\ntheir reasoning ability from English narrative texts. Additionally, we compared\nthe two ChatGPTs in the reasoning performances when commands were updated\nelaborately. The whole study was composed of three reasoning tests: Test 1 for\ncommonsense inference, Test 2 for emotional inference, and Test 3 for causal\ninference. The results showed that in Test 1, the students outdid the two\nChatGPT versions in local-culture-related inferences but performed worse than\nthe chatbots in daily-life inferences. In Test 2, ChatGPT Plus excelled whereas\nChatGPT lagged behind in accuracy. In association with both accuracy and\nfrequency of correct responses, the students were inferior to the two chatbots.\nCompared with ChatGPTs' better performance in positive emotions, the students\nshowed their superiority in inferring negative emotions. In Test 3, the\nstudents demonstrated better logical analysis, outdoing both chatbots. In\nupdating command condition, ChatGPT Plus displayed good causal reasoning\nability while ChatGPT kept unchanged. Our study reveals that human readers and\nChatGPTs have their respective advantages and disadvantages in drawing\ninferences from text reading comprehension, unlocking a complementary\nrelationship in text-based reasoning.",
                            "children": []
                        },
                        {
                            "name": "How Generative-AI can be Effectively used in Government Chatbots",
                            "abstract": "With the rapid development of artificial intelligence and breakthroughs in\nmachine learning and natural language processing, intelligent\nquestion-answering robots have become widely used in government affairs. This\npaper conducts a horizontal comparison between Guangdong Province's government\nchatbots, ChatGPT, and Wenxin Ernie, two large language models, to analyze the\nstrengths and weaknesses of existing government chatbots and AIGC technology.\nThe study finds significant differences between government chatbots and large\nlanguage models. China's government chatbots are still in an exploratory stage\nand have a gap to close to achieve \"intelligence.\" To explore the future\ndirection of government chatbots more deeply, this research proposes targeted\noptimization paths to help generative AI be effectively applied in government\nchatbot conversations.",
                            "children": []
                        },
                        {
                            "name": "ChatGPT for Teaching and Learning: An Experience from Data Science Education",
                            "abstract": "ChatGPT, an implementation and application of large language models, has\ngained significant popularity since its initial release. Researchers have been\nexploring ways to harness the practical benefits of ChatGPT in real-world\nscenarios. Educational researchers have investigated its potential in various\nsubjects, e.g., programming, mathematics, finance, clinical decision support,\netc. However, there has been limited attention given to its application in data\nscience education. This paper aims to bridge that gap by utilizing ChatGPT in a\ndata science course, gathering perspectives from students, and presenting our\nexperiences and feedback on using ChatGPT for teaching and learning in data\nscience education. The findings not only distinguish data science education\nfrom other disciplines but also uncover new opportunities and challenges\nassociated with incorporating ChatGPT into the data science curriculum.",
                            "children": []
                        },
                        {
                            "name": "Do Large Language Models Understand Verbal Indicators of Romantic Attraction?",
                            "abstract": "What makes people 'click' on a first date and become mutually attracted to\none another? While understanding and predicting the dynamics of romantic\ninteractions used to be exclusive to human judgment, we show that Large\nLanguage Models (LLMs) can detect romantic attraction during brief\ngetting-to-know-you interactions. Examining data from 964 speed dates, we show\nthat ChatGPT (and Claude 3) can predict both objective and subjective\nindicators of speed dating success (r=0.12-0.23). ChatGPT's predictions of\nactual matching (i.e., the exchange of contact information) were not only on\npar with those of human judges who had access to the same information but\nincremental to speed daters' own predictions. While some of the variance in\nChatGPT's predictions can be explained by common content dimensions (such as\nthe valence of the conversations) the fact that there remains a substantial\nproportion of unexplained variance suggests that ChatGPT also picks up on\nconversational dynamics. In addition, ChatGPT's judgments showed substantial\noverlap with those made by the human observers (mean r=0.29), highlighting\nsimilarities in their representation of romantic attraction that is, partially,\nindependent of accuracy.",
                            "children": []
                        }
                    ]
                },
                {
                    "name": "HuaTuo: Tuning LLaMA Model with Chinese Medical Knowledge",
                    "abstract": "Large Language Models (LLMs), such as the LLaMA model, have demonstrated\ntheir effectiveness in various general-domain natural language processing (NLP)\ntasks. Nevertheless, LLMs have not yet performed optimally in biomedical domain\ntasks due to the need for medical expertise in the responses. In response to\nthis challenge, we propose HuaTuo, a LLaMA-based model that has been\nsupervised-fine-tuned with generated QA (Question-Answer) instances. The\nexperimental results demonstrate that HuaTuo generates responses that possess\nmore reliable medical knowledge. Our proposed HuaTuo model is accessible at\nhttps://github.com/SCIR-HI/Huatuo-Llama-Med-Chinese.",
                    "children": [
                        {
                            "name": "ChatDoctor: A Medical Chat Model Fine-Tuned on a Large Language Model Meta-AI (LLaMA) Using Medical Domain Knowledge",
                            "abstract": "The primary aim of this research was to address the limitations observed in\nthe medical knowledge of prevalent large language models (LLMs) such as\nChatGPT, by creating a specialized language model with enhanced accuracy in\nmedical advice. We achieved this by adapting and refining the large language\nmodel meta-AI (LLaMA) using a large dataset of 100,000 patient-doctor dialogues\nsourced from a widely used online medical consultation platform. These\nconversations were cleaned and anonymized to respect privacy concerns. In\naddition to the model refinement, we incorporated a self-directed information\nretrieval mechanism, allowing the model to access and utilize real-time\ninformation from online sources like Wikipedia and data from curated offline\nmedical databases. The fine-tuning of the model with real-world patient-doctor\ninteractions significantly improved the model's ability to understand patient\nneeds and provide informed advice. By equipping the model with self-directed\ninformation retrieval from reliable online and offline sources, we observed\nsubstantial improvements in the accuracy of its responses. Our proposed\nChatDoctor, represents a significant advancement in medical LLMs, demonstrating\na significant improvement in understanding patient inquiries and providing\naccurate advice. Given the high stakes and low error tolerance in the medical\nfield, such enhancements in providing accurate and reliable information are not\nonly beneficial but essential.",
                            "children": []
                        },
                        {
                            "name": "LingYi: Medical Conversational Question Answering System based on Multi-modal Knowledge Graphs",
                            "abstract": "The medical conversational system can relieve the burden of doctors and\nimprove the efficiency of healthcare, especially during the pandemic. This\npaper presents a medical conversational question answering (CQA) system based\non the multi-modal knowledge graph, namely \"LingYi\", which is designed as a\npipeline framework to maintain high flexibility. Our system utilizes automated\nmedical procedures including medical triage, consultation, image-text drug\nrecommendation and record. To conduct knowledge-grounded dialogues with\npatients, we first construct a Chinese Medical Multi-Modal Knowledge Graph\n(CM3KG) and collect a large-scale Chinese Medical CQA (CMCQA) dataset. Compared\nwith the other existing medical question-answering systems, our system adopts\nseveral state-of-the-art technologies including medical entity disambiguation\nand medical dialogue generation, which is more friendly to provide medical\nservices to patients. In addition, we have open-sourced our codes which contain\nback-end models and front-end web pages at https://github.com/WENGSYX/LingYi.\nThe datasets including CM3KG at https://github.com/WENGSYX/CM3KG and CMCQA at\nhttps://github.com/WENGSYX/CMCQA are also released to further promote future\nresearch.",
                            "children": []
                        },
                        {
                            "name": "GLM: General Language Model Pretraining with Autoregressive Blank Infilling",
                            "abstract": "There have been various types of pretraining architectures including\nautoencoding models (e.g., BERT), autoregressive models (e.g., GPT), and\nencoder-decoder models (e.g., T5). However, none of the pretraining frameworks\nperforms the best for all tasks of three main categories including natural\nlanguage understanding (NLU), unconditional generation, and conditional\ngeneration. We propose a General Language Model (GLM) based on autoregressive\nblank infilling to address this challenge. GLM improves blank filling\npretraining by adding 2D positional encodings and allowing an arbitrary order\nto predict spans, which results in performance gains over BERT and T5 on NLU\ntasks. Meanwhile, GLM can be pretrained for different types of tasks by varying\nthe number and lengths of blanks. On a wide range of tasks across NLU,\nconditional and unconditional generation, GLM outperforms BERT, T5, and GPT\ngiven the same model sizes and data, and achieves the best performance from a\nsingle pretrained model with 1.25x parameters of BERT Large , demonstrating its\ngeneralizability to different downstream tasks.",
                            "children": []
                        },
                        {
                            "name": "In ChatGPT We Trust? Measuring and Characterizing the Reliability of ChatGPT",
                            "abstract": "The way users acquire information is undergoing a paradigm shift with the\nadvent of ChatGPT. Unlike conventional search engines, ChatGPT retrieves\nknowledge from the model itself and generates answers for users. ChatGPT's\nimpressive question-answering (QA) capability has attracted more than 100\nmillion users within a short period of time but has also raised concerns\nregarding its reliability. In this paper, we perform the first large-scale\nmeasurement of ChatGPT's reliability in the generic QA scenario with a\ncarefully curated set of 5,695 questions across ten datasets and eight domains.\nWe find that ChatGPT's reliability varies across different domains, especially\nunderperforming in law and science questions. We also demonstrate that system\nroles, originally designed by OpenAI to allow users to steer ChatGPT's\nbehavior, can impact ChatGPT's reliability in an imperceptible way. We further\nshow that ChatGPT is vulnerable to adversarial examples, and even a single\ncharacter change can negatively affect its reliability in certain cases. We\nbelieve that our study provides valuable insights into ChatGPT's reliability\nand underscores the need for strengthening the reliability and security of\nlarge language models (LLMs).",
                            "children": []
                        }
                    ]
                }
            ]
        },
        {
            "name": "math-PVS: A Large Language Model Framework to Map Scientific Publications to PVS Theories",
            "abstract": "As artificial intelligence (AI) gains greater adoption in a wide variety of\napplications, it has immense potential to contribute to mathematical discovery,\nby guiding conjecture generation, constructing counterexamples, assisting in\nformalizing mathematics, and discovering connections between different\nmathematical areas, to name a few.\n  While prior work has leveraged computers for exhaustive mathematical proof\nsearch, recent efforts based on large language models (LLMs) aspire to position\ncomputing platforms as co-contributors in the mathematical research process.\nDespite their current limitations in logic and mathematical tasks, there is\ngrowing interest in melding theorem proving systems with foundation models.\nThis work investigates the applicability of LLMs in formalizing advanced\nmathematical concepts and proposes a framework that can critically review and\ncheck mathematical reasoning in research papers. Given the noted reasoning\nshortcomings of LLMs, our approach synergizes the capabilities of proof\nassistants, specifically PVS, with LLMs, enabling a bridge between textual\ndescriptions in academic papers and formal specifications in PVS. By harnessing\nthe PVS environment, coupled with data ingestion and conversion mechanisms, we\nenvision an automated process, called \\emph{math-PVS}, to extract and formalize\nmathematical theorems from research papers, offering an innovative tool for\nacademic review and discovery.",
            "children": [
                {
                    "name": "Training Verifiers to Solve Math Word Problems",
                    "abstract": "State-of-the-art language models can match human performance on many tasks,\nbut they still struggle to robustly perform multi-step mathematical reasoning.\nTo diagnose the failures of current models and support research, we introduce\nGSM8K, a dataset of 8.5K high quality linguistically diverse grade school math\nword problems. We find that even the largest transformer models fail to achieve\nhigh test performance, despite the conceptual simplicity of this problem\ndistribution. To increase performance, we propose training verifiers to judge\nthe correctness of model completions. At test time, we generate many candidate\nsolutions and select the one ranked highest by the verifier. We demonstrate\nthat verification significantly improves performance on GSM8K, and we provide\nstrong empirical evidence that verification scales more effectively with\nincreased data than a finetuning baseline.",
                    "children": [
                        {
                            "name": "Solving Math Word Problems by Combining Language Models With Symbolic Solvers",
                            "abstract": "Automatically generating high-quality step-by-step solutions to math word\nproblems has many applications in education. Recently, combining large language\nmodels (LLMs) with external tools to perform complex reasoning and calculation\nhas emerged as a promising direction for solving math word problems, but prior\napproaches such as Program-Aided Language model (PAL) are biased towards simple\nprocedural problems and less effective for problems that require declarative\nreasoning. We propose an approach that combines an LLM that can incrementally\nformalize word problems as a set of variables and equations with an external\nsymbolic solver that can solve the equations. Our approach achieves comparable\naccuracy to the original PAL on the GSM8K benchmark of math word problems and\noutperforms PAL by an absolute 20% on ALGEBRA, a new dataset of more\nchallenging word problems extracted from Algebra textbooks. Our work highlights\nthe benefits of using declarative and incremental representations when\ninterfacing with an external tool for solving complex math word problems. Our\ndata and prompts are publicly available at\nhttps://github.com/joyheyueya/declarative-math-word-problem.",
                            "children": []
                        },
                        {
                            "name": "How well do Large Language Models perform in Arithmetic tasks?",
                            "abstract": "Large language models have emerged abilities including chain-of-thought to\nanswer math word problems step by step. Solving math word problems not only\nrequires abilities to disassemble problems via chain-of-thought but also needs\nto calculate arithmetic expressions correctly for each step. To the best of our\nknowledge, there is no work to focus on evaluating the arithmetic ability of\nlarge language models. In this work, we propose an arithmetic dataset MATH 401\nto test the latest large language models including GPT-4, ChatGPT, InstrctGPT,\nGalactica, and LLaMA with various arithmetic expressions and provide a detailed\nanalysis of the ability of large language models. MATH 401 and evaluation codes\nare released at \\url{https://github.com/GanjinZero/math401-llm}.",
                            "children": []
                        },
                        {
                            "name": "MathQA: Towards Interpretable Math Word Problem Solving with Operation-Based Formalisms",
                            "abstract": "We introduce a large-scale dataset of math word problems and an interpretable\nneural math problem solver that learns to map problems to operation programs.\nDue to annotation challenges, current datasets in this domain have been either\nrelatively small in scale or did not offer precise operational annotations over\ndiverse problem types. We introduce a new representation language to model\nprecise operation programs corresponding to each math problem that aim to\nimprove both the performance and the interpretability of the learned models.\nUsing this representation language, our new dataset, MathQA, significantly\nenhances the AQuA dataset with fully-specified operational programs. We\nadditionally introduce a neural sequence-to-program model enhanced with\nautomatic problem categorization. Our experiments show improvements over\ncompetitive baselines in our MathQA as well as the AQuA dataset. The results\nare still significantly lower than human performance indicating that the\ndataset poses new challenges for future research. Our dataset is available at:\nhttps://math-qa.github.io/math-QA/",
                            "children": []
                        },
                        {
                            "name": "Learning by Fixing: Solving Math Word Problems with Weak Supervision",
                            "abstract": "Previous neural solvers of math word problems (MWPs) are learned with full\nsupervision and fail to generate diverse solutions. In this paper, we address\nthis issue by introducing a \\textit{weakly-supervised} paradigm for learning\nMWPs. Our method only requires the annotations of the final answers and can\ngenerate various solutions for a single problem. To boost weakly-supervised\nlearning, we propose a novel \\textit{learning-by-fixing} (LBF) framework, which\ncorrects the misperceptions of the neural network via symbolic reasoning.\nSpecifically, for an incorrect solution tree generated by the neural network,\nthe \\textit{fixing} mechanism propagates the error from the root node to the\nleaf nodes and infers the most probable fix that can be executed to get the\ndesired answer. To generate more diverse solutions, \\textit{tree\nregularization} is applied to guide the efficient shrinkage and exploration of\nthe solution space, and a \\textit{memory buffer} is designed to track and save\nthe discovered various fixes for each problem. Experimental results on the\nMath23K dataset show the proposed LBF framework significantly outperforms\nreinforcement learning baselines in weakly-supervised learning. Furthermore, it\nachieves comparable top-1 and much better top-3/5 answer accuracies than\nfully-supervised methods, demonstrating its strength in producing diverse\nsolutions.",
                            "children": []
                        },
                        {
                            "name": "Neural Scaling Laws in Robotics",
                            "abstract": "Neural scaling laws have driven significant advancements in machine learning,\nparticularly in domains like language modeling and computer vision. However,\nthe exploration of neural scaling laws within robotics has remained relatively\nunderexplored, despite the growing adoption of foundation models in this field.\nThis paper represents the first comprehensive study to quantify neural scaling\nlaws for Robot Foundation Models (RFMs) and Large Language Models (LLMs) in\nrobotics tasks. Through a meta-analysis of 327 research papers, we investigate\nhow data size, model size, and compute resources influence downstream\nperformance across a diverse set of robotic tasks. Consistent with previous\nscaling law research, our results reveal that the performance of robotic models\nimproves with increased resources, following a power-law relationship.\nPromisingly, the improvement in robotic task performance scales notably faster\nthan language tasks. This suggests that, while performance on downstream\nrobotic tasks today is often moderate-to-poor, increased data and compute are\nlikely to signficantly improve performance in the future. Also consistent with\nprevious scaling law research, we also observe the emergence of new robot\ncapabilities as models scale.",
                            "children": []
                        }
                    ]
                },
                {
                    "name": "Notes on a Path to AI Assistance in Mathematical Reasoning",
                    "abstract": "These informal notes are based on the author's lecture at the National\nAcademies of Science, Engineering, and Mathematics workshop on \"AI to Assist\nMathematical Reasoning\" in June 2023. The goal is to think through a path by\nwhich we might arrive at AI that is useful for the research mathematician.",
                    "children": [
                        {
                            "name": "Formalising perfectoid spaces",
                            "abstract": "Perfectoid spaces are sophisticated objects in arithmetic geometry introduced\nby Peter Scholze in 2012. We formalised enough definitions and theorems in\ntopology, algebra and geometry to define perfectoid spaces in the Lean theorem\nprover. This experiment confirms that a proof assistant can handle complexity\nin that direction, which is rather different from formalising a long proof\nabout simple objects. It also confirms that mathematicians with no computer\nscience training can become proficient users of a proof assistant in a\nrelatively short period of time. Finally, we observe that formalising a piece\nof mathematics that is a trending topic boosts the visibility of proof\nassistants amongst pure mathematicians.",
                            "children": []
                        },
                        {
                            "name": "A New Operator for Egyptian Fractions",
                            "abstract": "This paper introduces a new equation for rewriting two unit fractions to\nanother two unit fractions. This equation is useful for optimizing the elements\nof an Egyptian Fraction. Parity of the elements of the Egyptian Fractions are\nalso considered. And lastly, the statement that all rational numbers can be\nrepresented as Egyptian Fraction is re-established.",
                            "children": []
                        },
                        {
                            "name": "Formalization of the prime number theorem and Dirichlet's theorem",
                            "abstract": "We present the formalization of Dirichlet's theorem on the infinitude of\nprimes in arithmetic progressions, and Selberg's elementary proof of the prime\nnumber theorem, which asserts that the number $\\pi(x)$ of primes less than $x$\nis asymptotic to $x/\\log x$, within the proof system Metamath.",
                            "children": []
                        },
                        {
                            "name": "Anisotropy as a diagnostic test for distinct tensor network wavefunctions of integer and half-integer spin Kitaev quantum spin liquids",
                            "abstract": "Contrasting ground states of quantum magnets with the integer and\nhalf-integer spin moments are the manifestation of many-body quantum\ninterference effects. In this work, we investigate the distinct nature of the\ninteger and half-integer spin quantum spin liquids in the framework of the\nKitaev's model on the honeycomb lattice. The models with arbitrary spin quantum\nnumbers are not exactly solvable in contrast to the well-known quantum spin\nliquid solution of the spin-1/2 system. We use the tensor network wavefunctions\nfor the integer and half-integer spin quantum spin liquid states to unveil the\nimportant difference between these states. We find that the distinct sign\nstructures of the tensor network wavefunction for the integer and half-integer\nspin quantum spin liquids are responsible for completely different ground\nstates in the spatially anisotropic limit. Hence the spatial anisotropy would\nbe a useful diagnostic test for distinguishing these quantum spin liquid\nstates, both in the numerical computations and experiments on real materials.\nWe support this discovery via extensive numerics including the tensor network,\nDMRG, and exact diagonalization computations.",
                            "children": []
                        },
                        {
                            "name": "Strong non-vanishing of cohomologies and strong non-freeness of adjoint line bundles on $n$-Raynaud surfaces",
                            "abstract": "We formally give the definition of $n$-Tango curve and $n$-Raynaud surface.\nThen we study the pathologies on $n$-Raynaud surfaces and as a corollary we\ngive a simple disproof of Fujita's conjecture on surfaces in positive\ncharacteristics.",
                            "children": []
                        }
                    ]
                },
                {
                    "name": "MiniF2F: a cross-system benchmark for formal Olympiad-level mathematics",
                    "abstract": "We present miniF2F, a dataset of formal Olympiad-level mathematics problems\nstatements intended to provide a unified cross-system benchmark for neural\ntheorem proving. The miniF2F benchmark currently targets Metamath, Lean,\nIsabelle (partially) and HOL Light (partially) and consists of 488 problem\nstatements drawn from the AIME, AMC, and the International Mathematical\nOlympiad (IMO), as well as material from high-school and undergraduate\nmathematics courses. We report baseline results using GPT-f, a neural theorem\nprover based on GPT-3 and provide an analysis of its performance. We intend for\nminiF2F to be a community-driven effort and hope that our benchmark will help\nspur advances in neural theorem proving.",
                    "children": [
                        {
                            "name": "Generative Language Modeling for Automated Theorem Proving",
                            "abstract": "We explore the application of transformer-based language models to automated\ntheorem proving. This work is motivated by the possibility that a major\nlimitation of automated theorem provers compared to humans -- the generation of\noriginal mathematical terms -- might be addressable via generation from\nlanguage models. We present an automated prover and proof assistant, GPT-f, for\nthe Metamath formalization language, and analyze its performance. GPT-f found\nnew short proofs that were accepted into the main Metamath library, which is to\nour knowledge, the first time a deep-learning based system has contributed\nproofs that were adopted by a formal mathematics community.",
                            "children": []
                        },
                        {
                            "name": "HOList: An Environment for Machine Learning of Higher-Order Theorem Proving",
                            "abstract": "We present an environment, benchmark, and deep learning driven automated\ntheorem prover for higher-order logic. Higher-order interactive theorem provers\nenable the formalization of arbitrary mathematical theories and thereby present\nan interesting, open-ended challenge for deep learning. We provide an\nopen-source framework based on the HOL Light theorem prover that can be used as\na reinforcement learning environment. HOL Light comes with a broad coverage of\nbasic mathematical theorems on calculus and the formal proof of the Kepler\nconjecture, from which we derive a challenging benchmark for automated\nreasoning. We also present a deep reinforcement learning driven automated\ntheorem prover, DeepHOL, with strong initial results on this benchmark.",
                            "children": []
                        },
                        {
                            "name": "Lean Copilot: Large Language Models as Copilots for Theorem Proving in Lean",
                            "abstract": "Neural theorem proving combines large language models (LLMs) with proof\nassistants such as Lean, where the correctness of formal proofs can be\nrigorously verified, leaving no room for hallucination. With existing neural\ntheorem provers pretrained on a fixed collection of data and offering valuable\nsuggestions at times, it is challenging for them to continually prove novel\ntheorems in a fully autonomous mode, where human insights may be critical. In\nthis paper, we explore LLMs as copilots that assist humans in proving theorems.\nWe introduce Lean Copilot, an general framework for running LLM inference\nnatively in Lean. It enables programmers to build various LLM-based proof\nautomation tools that integrate seamlessly into the workflow of Lean users.\nLean users can use our pretrained models or bring their own ones that run\neither locally (with or without GPUs) or on the cloud. Using Lean Copilot, we\nbuild LLM-based tools that suggest proof steps, complete proof goals, and\nselect relevant premises. Experimental results on the Mathematics in Lean\ntextbook demonstrate the effectiveness of our method compared to existing\nrule-based proof automation in Lean (aesop). When assisting humans, Lean\nCopilot requires only 2.08 manually-entered proof steps on average (3.86\nrequired by aesop); when automating the theorem proving process, Lean Copilot\nautomates 74.2% proof steps on average, 85% better than aesop (40.1%). We open\nsource all code and artifacts under a permissive MIT license to facilitate\nfurther research.",
                            "children": []
                        },
                        {
                            "name": "Natural Language Comprehension with the EpiReader",
                            "abstract": "We present the EpiReader, a novel model for machine comprehension of text.\nMachine comprehension of unstructured, real-world text is a major research goal\nfor natural language processing. Current tests of machine comprehension pose\nquestions whose answers can be inferred from some supporting text, and evaluate\na model's response to the questions. The EpiReader is an end-to-end neural\nmodel comprising two components: the first component proposes a small set of\ncandidate answers after comparing a question to its supporting text, and the\nsecond component formulates hypotheses using the proposed candidates and the\nquestion, then reranks the hypotheses based on their estimated concordance with\nthe supporting text. We present experiments demonstrating that the EpiReader\nsets a new state-of-the-art on the CNN and Children's Book Test machine\ncomprehension benchmarks, outperforming previous neural models by a significant\nmargin.",
                            "children": []
                        },
                        {
                            "name": "The LAMBADA dataset: Word prediction requiring a broad discourse context",
                            "abstract": "We introduce LAMBADA, a dataset to evaluate the capabilities of computational\nmodels for text understanding by means of a word prediction task. LAMBADA is a\ncollection of narrative passages sharing the characteristic that human subjects\nare able to guess their last word if they are exposed to the whole passage, but\nnot if they only see the last sentence preceding the target word. To succeed on\nLAMBADA, computational models cannot simply rely on local context, but must be\nable to keep track of information in the broader discourse. We show that\nLAMBADA exemplifies a wide range of linguistic phenomena, and that none of\nseveral state-of-the-art language models reaches accuracy above 1% on this\nnovel benchmark. We thus propose LAMBADA as a challenging test set, meant to\nencourage the development of new models capable of genuine understanding of\nbroad context in natural language text.",
                            "children": []
                        }
                    ]
                },
                {
                    "name": "Measuring Mathematical Problem Solving With the MATH Dataset",
                    "abstract": "Many intellectual endeavors require mathematical problem solving, but this\nskill remains beyond the capabilities of computers. To measure this ability in\nmachine learning models, we introduce MATH, a new dataset of 12,500 challenging\ncompetition mathematics problems. Each problem in MATH has a full step-by-step\nsolution which can be used to teach models to generate answer derivations and\nexplanations. To facilitate future research and increase accuracy on MATH, we\nalso contribute a large auxiliary pretraining dataset which helps teach models\nthe fundamentals of mathematics. Even though we are able to increase accuracy\non MATH, our results show that accuracy remains relatively low, even with\nenormous Transformer models. Moreover, we find that simply increasing budgets\nand model parameter counts will be impractical for achieving strong\nmathematical reasoning if scaling trends continue. While scaling Transformers\nis automatically solving most other text-based tasks, scaling is not currently\nsolving MATH. To have more traction on mathematical problem solving we will\nlikely need new algorithmic advancements from the broader research community.",
                    "children": [
                        {
                            "name": "Measuring Massive Multitask Language Understanding",
                            "abstract": "We propose a new test to measure a text model's multitask accuracy. The test\ncovers 57 tasks including elementary mathematics, US history, computer science,\nlaw, and more. To attain high accuracy on this test, models must possess\nextensive world knowledge and problem solving ability. We find that while most\nrecent models have near random-chance accuracy, the very largest GPT-3 model\nimproves over random chance by almost 20 percentage points on average. However,\non every one of the 57 tasks, the best models still need substantial\nimprovements before they can reach expert-level accuracy. Models also have\nlopsided performance and frequently do not know when they are wrong. Worse,\nthey still have near-random accuracy on some socially important subjects such\nas morality and law. By comprehensively evaluating the breadth and depth of a\nmodel's academic and professional understanding, our test can be used to\nanalyze models across many tasks and to identify important shortcomings.",
                            "children": []
                        },
                        {
                            "name": "Improving Graph Neural Network Representations of Logical Formulae with Subgraph Pooling",
                            "abstract": "Recent advances in the integration of deep learning with automated theorem\nproving have centered around the representation of logical formulae as inputs\nto deep learning systems. In particular, there has been a growing interest in\nadapting structure-aware neural methods to work with the underlying graph\nrepresentations of logical expressions. While more effective than character and\ntoken-level approaches, graph-based methods have often made representational\ntrade-offs that limited their ability to capture key structural properties of\ntheir inputs. In this work we propose a novel approach for embedding logical\nformulae that is designed to overcome the representational limitations of prior\napproaches. Our architecture works for logics of different expressivity; e.g.,\nfirst-order and higher-order logic. We evaluate our approach on two standard\ndatasets and show that the proposed architecture achieves state-of-the-art\nperformance on both premise selection and proof step classification.",
                            "children": []
                        },
                        {
                            "name": "BERT-JAM: Boosting BERT-Enhanced Neural Machine Translation with Joint Attention",
                            "abstract": "BERT-enhanced neural machine translation (NMT) aims at leveraging\nBERT-encoded representations for translation tasks. A recently proposed\napproach uses attention mechanisms to fuse Transformer's encoder and decoder\nlayers with BERT's last-layer representation and shows enhanced performance.\nHowever, their method doesn't allow for the flexible distribution of attention\nbetween the BERT representation and the encoder/decoder representation. In this\nwork, we propose a novel BERT-enhanced NMT model called BERT-JAM which improves\nupon existing models from two aspects: 1) BERT-JAM uses joint-attention modules\nto allow the encoder/decoder layers to dynamically allocate attention between\ndifferent representations, and 2) BERT-JAM allows the encoder/decoder layers to\nmake use of BERT's intermediate representations by composing them using a gated\nlinear unit (GLU). We train BERT-JAM with a novel three-phase optimization\nstrategy that progressively unfreezes different components of BERT-JAM. Our\nexperiments show that BERT-JAM achieves SOTA BLEU scores on multiple\ntranslation tasks.",
                            "children": []
                        },
                        {
                            "name": "Choose Your Programming Copilot: A Comparison of the Program Synthesis Performance of GitHub Copilot and Genetic Programming",
                            "abstract": "GitHub Copilot, an extension for the Visual Studio Code development\nenvironment powered by the large-scale language model Codex, makes automatic\nprogram synthesis available for software developers. This model has been\nextensively studied in the field of deep learning, however, a comparison to\ngenetic programming, which is also known for its performance in automatic\nprogram synthesis, has not yet been carried out. In this paper, we evaluate\nGitHub Copilot on standard program synthesis benchmark problems and compare the\nachieved results with those from the genetic programming literature. In\naddition, we discuss the performance of both approaches. We find that the\nperformance of the two approaches on the benchmark problems is quite similar,\nhowever, in comparison to GitHub Copilot, the program synthesis approaches\nbased on genetic programming are not yet mature enough to support programmers\nin practical software development. Genetic programming usually needs a huge\namount of expensive hand-labeled training cases and takes too much time to\ngenerate solutions. Furthermore, source code generated by genetic programming\napproaches is often bloated and difficult to understand. For future work on\nprogram synthesis with genetic programming, we suggest researchers to focus on\nimproving the execution time, readability, and usability.",
                            "children": []
                        },
                        {
                            "name": "Stop Words for Processing Software Engineering Documents: Do they Matter?",
                            "abstract": "Stop words, which are considered non-predictive, are often eliminated in\nnatural language processing tasks. However, the definition of uninformative\nvocabulary is vague, so most algorithms use general knowledge-based stop lists\nto remove stop words. There is an ongoing debate among academics about the\nusefulness of stop word elimination, especially in domain-specific settings. In\nthis work, we investigate the usefulness of stop word removal in a software\nengineering context. To do this, we replicate and experiment with three\nsoftware engineering research tools from related work. Additionally, we\nconstruct a corpus of software engineering domain-related text from 10,000\nStack Overflow questions and identify 200 domain-specific stop words using\ntraditional information-theoretic methods. Our results show that the use of\ndomain-specific stop words significantly improved the performance of research\ntools compared to the use of a general stop list and that 17 out of 19\nevaluation measures showed better performance.\n  Online appendix: https://zenodo.org/record/7865748",
                            "children": []
                        }
                    ]
                },
                {
                    "name": "World Models in Artificial Intelligence: Sensing, Learning, and Reasoning Like a Child",
                    "abstract": "World Models help Artificial Intelligence (AI) predict outcomes, reason about\nits environment, and guide decision-making. While widely used in reinforcement\nlearning, they lack the structured, adaptive representations that even young\nchildren intuitively develop. Advancing beyond pattern recognition requires\ndynamic, interpretable frameworks inspired by Piaget's cognitive development\ntheory. We highlight six key research areas -- physics-informed learning,\nneurosymbolic learning, continual learning, causal inference, human-in-the-loop\nAI, and responsible AI -- as essential for enabling true reasoning in AI. By\nintegrating statistical learning with advances in these areas, AI can evolve\nfrom pattern recognition to genuine understanding, adaptation and reasoning\ncapabilities.",
                    "children": [
                        {
                            "name": "Neurosymbolic AI: The 3rd Wave",
                            "abstract": "Current advances in Artificial Intelligence (AI) and Machine Learning (ML)\nhave achieved unprecedented impact across research communities and industry.\nNevertheless, concerns about trust, safety, interpretability and accountability\nof AI were raised by influential thinkers. Many have identified the need for\nwell-founded knowledge representation and reasoning to be integrated with deep\nlearning and for sound explainability. Neural-symbolic computing has been an\nactive area of research for many years seeking to bring together robust\nlearning in neural networks with reasoning and explainability via symbolic\nrepresentations for network models. In this paper, we relate recent and early\nresearch results in neurosymbolic AI with the objective of identifying the key\ningredients of the next wave of AI systems. We focus on research that\nintegrates in a principled way neural network-based learning with symbolic\nknowledge representation and logical reasoning. The insights provided by 20\nyears of neural-symbolic computing are shown to shed new light onto the\nincreasingly prominent role of trust, safety, interpretability and\naccountability of AI. We also identify promising directions and challenges for\nthe next decade of AI research from the perspective of neural-symbolic systems.",
                            "children": []
                        },
                        {
                            "name": "Connecting the Dots in Trustworthy Artificial Intelligence: From AI Principles, Ethics, and Key Requirements to Responsible AI Systems and Regulation",
                            "abstract": "Trustworthy Artificial Intelligence (AI) is based on seven technical\nrequirements sustained over three main pillars that should be met throughout\nthe system's entire life cycle: it should be (1) lawful, (2) ethical, and (3)\nrobust, both from a technical and a social perspective. However, attaining\ntruly trustworthy AI concerns a wider vision that comprises the trustworthiness\nof all processes and actors that are part of the system's life cycle, and\nconsiders previous aspects from different lenses. A more holistic vision\ncontemplates four essential axes: the global principles for ethical use and\ndevelopment of AI-based systems, a philosophical take on AI ethics, a\nrisk-based approach to AI regulation, and the mentioned pillars and\nrequirements. The seven requirements (human agency and oversight; robustness\nand safety; privacy and data governance; transparency; diversity,\nnon-discrimination and fairness; societal and environmental wellbeing; and\naccountability) are analyzed from a triple perspective: What each requirement\nfor trustworthy AI is, Why it is needed, and How each requirement can be\nimplemented in practice. On the other hand, a practical approach to implement\ntrustworthy AI systems allows defining the concept of responsibility of\nAI-based systems facing the law, through a given auditing process. Therefore, a\nresponsible AI system is the resulting notion we introduce in this work, and a\nconcept of utmost necessity that can be realized through auditing processes,\nsubject to the challenges posed by the use of regulatory sandboxes. Our\nmultidisciplinary vision of trustworthy AI culminates in a debate on the\ndiverging views published lately about the future of AI. Our reflections in\nthis matter conclude that regulation is a key for reaching a consensus among\nthese views, and that trustworthy and responsible AI systems will be crucial\nfor the present and future of our society.",
                            "children": []
                        },
                        {
                            "name": "Developing Responsible Chatbots for Financial Services: A Pattern-Oriented Responsible AI Engineering Approach",
                            "abstract": "The recent release of ChatGPT has gained huge attention and discussion\nworldwide, with responsible AI being a key topic of discussion. How can we\nensure that AI systems, including ChatGPT, are developed and adopted in a\nresponsible way? To tackle the responsible AI challenges, various ethical\nprinciples have been released by governments, organisations, and companies.\nHowever, those principles are very abstract and not practical enough. Further,\nsignificant efforts have been put on algorithm-level solutions that only\naddress a narrow set of principles, such as fairness and privacy. To fill the\ngap, we adopt a pattern-oriented responsible AI engineering approach and build\na Responsible AI Pattern Catalogue to operationalise responsible AI from a\nsystem perspective. In this article, we first summarise the major challenges in\noperationalising responsible AI at scale and introduce how we use the\nResponsible AI Pattern Catalogue to address those challenges. We then examine\nthe risks at each stage of the chatbot development process and recommend\npattern-driven mitigations to evaluate the the usefulness of the Responsible AI\nPattern Catalogue in a real-world setting.",
                            "children": []
                        },
                        {
                            "name": "Socially Responsible AI Algorithms: Issues, Purposes, and Challenges",
                            "abstract": "In the current era, people and society have grown increasingly reliant on\nartificial intelligence (AI) technologies. AI has the potential to drive us\ntowards a future in which all of humanity flourishes. It also comes with\nsubstantial risks for oppression and calamity. Discussions about whether we\nshould (re)trust AI have repeatedly emerged in recent years and in many\nquarters, including industry, academia, healthcare, services, and so on.\nTechnologists and AI researchers have a responsibility to develop trustworthy\nAI systems. They have responded with great effort to design more responsible AI\nalgorithms. However, existing technical solutions are narrow in scope and have\nbeen primarily directed towards algorithms for scoring or classification tasks,\nwith an emphasis on fairness and unwanted bias. To build long-lasting trust\nbetween AI and human beings, we argue that the key is to think beyond\nalgorithmic fairness and connect major aspects of AI that potentially cause\nAI's indifferent behavior. In this survey, we provide a systematic framework of\nSocially Responsible AI Algorithms that aims to examine the subjects of AI\nindifference and the need for socially responsible AI algorithms, define the\nobjectives, and introduce the means by which we may achieve these objectives.\nWe further discuss how to leverage this framework to improve societal\nwell-being through protection, information, and prevention/mitigation.",
                            "children": []
                        },
                        {
                            "name": "Strategies to architect AI Safety: Defense to guard AI from Adversaries",
                            "abstract": "The impact of designing for security of AI is critical for humanity in the AI\nera. With humans increasingly becoming dependent upon AI, there is a need for\nneural networks that work reliably, inspite of Adversarial attacks. The vision\nfor Safe and secure AI for popular use is achievable. To achieve safety of AI,\nthis paper explores strategies and a novel deep learning architecture. To guard\nAI from adversaries, paper explores combination of 3 strategies:\n  1. Introduce randomness at inference time to hide the representation learning\nfrom adversaries.\n  2. Detect presence of adversaries by analyzing the sequence of inferences.\n  3. Exploit visual similarity.\n  To realize these strategies, this paper designs a novel architecture, Dynamic\nNeural Defense, DND. This defense has 3 deep learning architectural features:\n  1. By hiding the way a neural network learns from exploratory attacks using a\nrandom computation graph, DND evades attack.\n  2. By analyzing input sequence to cloud AI inference engine with LSTM, DND\ndetects attack sequence.\n  3. By inferring with visual similar inputs generated by VAE, any AI defended\nby DND approach does not succumb to hackers.\n  Thus, a roadmap to develop reliable, safe and secure AI is presented.",
                            "children": []
                        }
                    ]
                }
            ]
        }
    ]
}