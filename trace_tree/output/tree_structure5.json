{
    "name": "SEA: Low-Resource Safety Alignment for Multimodal Large Language Models via Synthetic Embeddings",
    "data": {
        "index": 338,
        "title": "SEA: Low-Resource Safety Alignment for Multimodal Large Language Models via Synthetic Embeddings",
        "publication_date": "2025-02-18",
        "references": [
            "Cross-modal safety alignment: Is textual unlearning all you need?",
            "Jailbreaking black box large language models in twenty queries.",
            "Minigpt-v2: large language model as a unified interface for vision-language multi-task learning.",
            "DRESS: Instructing large vision-language models to align and interact with humans via natural language feedback.",
            "Videollama 2: Advancing spatial-temporal modeling and audio understanding in video-llms.",
            "Qwen2-audio technical report.",
            "Qwen-audio: Advancing universal audio understanding via unified large-scale audio-language models.",
            "Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis.",
            "Figstep: Jailbreaking large vision-language models via typographic visual prompts.",
            "Mllmguard: A multi-dimensional safety evaluation suite for multimodal large language models.",
            "Measuring massive multitask language understanding.",
            "Vlsbench: Unveiling visual leakage in multimodal safety.",
            "Pku-saferlhf: Towards multi-level safety alignment for llms with human preference.",
            "Align anything: Training all-modality models to follow instructions with language feedback.",
            "Pyramidal flow matching for efficient video generative modeling.",
            "Mvbench: A comprehensive multi-modal video understanding benchmark.",
            "Evaluating object hallucination in large vision-language models.",
            "Images are achilles’ heel of alignment: Exploiting visual vulnerabilities for jailbreaking multimodal large language models.",
            "Improved baselines with visual instruction tuning.",
            "Visual instruction tuning.",
            "Autodan: Generating stealthy jailbreak prompts on aligned large language models.",
            "Mm-safetybench: A benchmark for safety evaluation of multimodal large language models.",
            "Jailbreakv-28k: A benchmark for assessing the robustness of multimodal large language models against jailbreak attacks.",
            "Jailbreaking attack against multimodal large language model.",
            "Visual adversarial examples jailbreak aligned large language models.",
            "Direct preference optimization: Your language model is secretly a reward model.",
            "Proximal policy optimization algorithms.",
            "Assessment of multimodal large language models in alignment with human values.",
            "SALMONN: Towards generic hearing abilities for large language models.",
            "Eegpt: Pretrained transformer for universal and reliable representation of eeg signals.",
            "Qwen2-vl: Enhancing vision-language model’s perception of the world at any resolution.",
            "Audio is the achilles’ heel: Red teaming audio large multimodal models.",
            "Air-bench: Benchmarking large audio-language models via generative comprehension.",
            "Safebench: A safety evaluation framework for multimodal large language models.",
            "Spavl: A comprehensive safety preference alignment dataset for vision language model."
        ]
    },
    "children": [
        {
            "name": "Cross-Modal Safety Alignment: Is textual unlearning all you need?",
            "data": {
                "index": 60,
                "title": "Cross-Modal Safety Alignment: Is textual unlearning all you need?",
                "publication_date": "2024-05-27",
                "references": [
                    "Gpt-4 technical report.",
                    "Flamingo: a visual language model for few-shot learning.",
                    "Openflamingo: An open-source framework for training large autoregressive vision-language models.",
                    "Training a helpful and harmless assistant with reinforcement learning from human feedback.",
                    "Machine unlearning.",
                    "Sparks of artificial general intelligence: Early experiments with gpt-4.",
                    "Towards making systems forget with machine unlearning.",
                    "Towards evaluating the robustness of neural networks.",
                    "The secret sharer: Evaluating and testing unintended memorization in neural networks.",
                    "Unlearn what you want to forget: Efficient unlearning for llms.",
                    "Deep reinforcement learning from human preferences.",
                    "Instructblip: Towards general-purpose vision-language models with instruction tuning.",
                    "Multilingual jailbreak challenges in large language models.",
                    "Pengi: An audio language model for audio tasks.",
                    "Qlora: Efficient finetuning of quantized llms.",
                    "Measuring the carbon intensity of ai in cloud instances.",
                    "Attacks, defenses and evaluations for llm conversation safety: A survey.",
                    "Who’s harry potter? approximate unlearning in llms.",
                    "Unbridled icarus: A survey of the potential perils of image inputs in multimodal large language model security.",
                    "Erasing concepts from diffusion models.",
                    "Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned.",
                    "Llama-adapter v2: Parameter-efficient visual instruction model.",
                    "Figstep: Jailbreaking large vision-language models via typographic visual prompts.",
                    "Explaining and harnessing adversarial examples.",
                    "Making the v in vqa matter: Elevating the role of image understanding in visual question answering.",
                    "Certified data removal from machine learning models.",
                    "Cold-attack: Jailbreaking llms with stealthiness and controllability.",
                    "Onellm: One framework to align all modalities with language.",
                    "Debertav3: Improving deberta using electra-style pre-training with gradient-disentangled embedding sharing.",
                    "Selective amnesia: A continual learning approach to forgetting in deep generative models.",
                    "Lora: Low-rank adaptation of large language models.",
                    "Adversarial examples are not bugs, they are features.",
                    "Llama guard: Llm-based input-output safeguard for human-ai conversations.",
                    "Beaver-tails: Towards improved safety alignment of llm via a human-preference dataset.",
                    "Mistral 7b.",
                    "Segment anything.",
                    "Pretraining language models with human preferences.",
                    "Ablating concepts in text-to-image diffusion models.",
                    "Jailbreakv-28k: A benchmark for assessing the robustness of multimodal large language models against jailbreak attacks.",
                    "Jailbreakv-28k: A benchmark for assessing the robustness of multimodal large language models against jailbreak attacks.",
                    "Inverse scaling: When bigger isn’t better.",
                    "Locating and editing factual associations in gpt.",
                    "Mass-editing memory in a transformer.",
                    "Jailbreaking attack against multimodal large language model.",
                    "Dinov2: Learning robust visual features without supervision.",
                    "Training language models to follow instructions with human feedback.",
                    "Can sensitive information be deleted from llms? objectives for defending against extraction attacks.",
                    "In-context unlearning: Language models as few shot unlearners.",
                    "Fine-tuning aligned language models compromises safety, even when users do not intend to!",
                    "Visual adversarial examples jailbreak aligned large language models.",
                    "Learning transferable visual models from natural language supervision.",
                    "Zero-shot text-to-image generation.",
                    "Finetuned language models are zero-shot learners.",
                    "Simple statistical gradient-following algorithms for connectionist reinforcement learning.",
                    "Fundamental limitations of alignment in large language models.",
                    "Next-gpt: Any-to-any multimodal llm.",
                    "Parameter-efficient fine-tuning methods for pretrained language models: A critical review and assessment.",
                    "Backdooring instruction-tuned large language models with virtual prompt injection.",
                    "Large language model unlearning.",
                    "A survey on multimodal large language models.",
                    "Unlearning bias in language models by partitioning gradients.",
                    "Forget-me-not: Learning to forget in text-to-image diffusion models.",
                    "Video-llama: An instruction-tuned audio-visual language model for video understanding.",
                    "A survey of large language models.",
                    "Judging llm-as-a-judge with mt-bench and chatbot arena.",
                    "Minigpt-4: Enhancing vision-language understanding with advanced large language models.",
                    "Safety fine-tuning at (almost) no cost: A baseline for vision large language models.",
                    "Universal and transferable adversarial attacks on aligned language models."
                ]
            },
            "children": [
                {
                    "name": "FINE-TUNING ALIGNED LANGUAGE MODELS COMPROMISES SAFETY, EVEN WHEN USERS DO NOT INTEND TO!",
                    "data": {
                        "index": 97,
                        "title": "FINE-TUNING ALIGNED LANGUAGE MODELS COMPROMISES SAFETY, EVEN WHEN USERS DO NOT INTEND TO!",
                        "publication_date": "2023-12-01",
                        "references": [
                            "Multi-party goal tracking with llms: Comparing pre-training, fine-tuning, and prompt engineering",
                            "Training a helpful and harmless assistant with reinforcement learning from human feedback",
                            "Constitutional ai: Harmlessness from ai feedback",
                            "Code llama: Open foundation models for code",
                            "Few-shot fine-tuning vs. in-context learning: A fair comparison and evaluation",
                            "Jailbreaking chatgpt on release day",
                            "GPT as a financial advisor",
                            "Understanding deep learning (still) requires rethinking generalization",
                            "Stanford alpaca: An instruction-following llama model",
                            "Llama: Open and efficient foundation language models",
                            "Llama 2: Open foundation and fine-tuned chat models",
                            "fllama 2 - function calling llama 2",
                            "Clinical outcome prediction from admission notes using self-supervised knowledge integration",
                            "A survey on large language model based autonomous agents",
                            "Negligence and ai’s human users",
                            "Shadow alignment: The ease of subverting safely-aligned language models",
                            "Open-source can be dangerous: On the vulnerability of value alignment in open-source llms",
                            "Bitfit: Simple parameter-efficient fine-tuning for transformer-based masked language-models",
                            "Narcissus: A practical clean-label backdoor attack with limited information",
                            "Removing rlhf protections in gpt-4 via fine-tuning",
                            "Improving language understanding by generative pre-training",
                            "Learning transferable visual models from natural language supervision",
                            "On the opportunities and risks of foundation models",
                            "Rt-2: Vision-language-action models transfer web knowledge to robotic control",
                            "Language models are few-shot learners",
                            "What neural networks memorize and why: Discovering the long tail via influence estimation",
                            "Finetuned language models are zero-shot learners",
                            "Chain-of-thought prompting elicits reasoning in large language models",
                            "A prompt pattern catalog to enhance prompt engineering with chatgpt",
                            "Data selection for language models via importance resampling",
                            "Judging llm-as-a-judge with mt-bench and chatbot arena",
                            "Minigpt-4: Enhancing vision-language understanding with advanced large language models",
                            "Universal and transferable adversarial attacks on aligned language models",
                            "A mechanistic understanding of alignment algorithms: A case study on dpo and toxicity",
                            "Assessing the brittleness of safety alignment via pruning and low-rank modifications",
                            "Visual adversarial examples jailbreak aligned large language models",
                            "Backdoor learning: A survey",
                            "BERT: Pre-training of deep bidirectional transformers for language understanding",
                            "Improving language understanding by generative pre-training",
                            "A backdoor attack against lstm-based text classification systems",
                            "Instructblip: Towards general-purpose vision-language models with instruction tuning",
                            "Entity-based knowledge conflicts in question answering",
                            "Decoupled weight decay regularization",
                            "A survey for in-context learning",
                            "Are aligned neural networks adversarially aligned?",
                            "Lora: Low-rank adaptation of large language models",
                            "Jailbroken: How does llm safety training fail?",
                            "Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning",
                            "Finetuned language models are zero-shot learners",
                            "Chain-of-thought prompting elicits reasoning in large language models",
                            "A prompt pattern catalog to enhance prompt engineering with chatgpt",
                            "Data selection for language models via importance resampling",
                            "Shadow alignment: The ease of subverting safely-aligned language models",
                            "Open-source can be dangerous: On the vulnerability of value alignment in open-source llms",
                            "Bitfit: Simple parameter-efficient fine-tuning for transformer-based masked language-models",
                            "Narcissus: A practical clean-label backdoor attack with limited information",
                            "Removing rlhf protections in gpt-4 via fine-tuning",
                            "Understanding deep learning (still) requires rethinking generalization",
                            "Stanford alpaca: An instruction-following llama model",
                            "Llama: Open and efficient foundation language models",
                            "Llama 2: Open foundation and fine-tuned chat models",
                            "fllama 2 - function calling llama 2",
                            "Clinical outcome prediction from admission notes using self-supervised knowledge integration",
                            "A survey on large language model based autonomous agents",
                            "Negligence and ai’s human users",
                            "Shadow alignment: The ease of subverting safely-aligned language models",
                            "Open-source can be dangerous: On the vulnerability of value alignment in open-source llms",
                            "Bitfit: Simple parameter-efficient fine-tuning for transformer-based masked language-models",
                            "Narcissus: A practical clean-label backdoor attack with limited information",
                            "Removing rlhf protections in gpt-4 via fine-tuning",
                            "Understanding deep learning (still) requires rethinking generalization",
                            "Stanford alpaca: An instruction-following llama model",
                            "Llama: Open and efficient foundation language models",
                            "Llama 2: Open foundation and fine-tuned chat models",
                            "fllama 2 - function calling llama 2"
                        ]
                    },
                    "children": []
                },
                {
                    "name": "Unbridled Icarus: A Survey of the Potential Perils of Image Inputs in Multimodal Large Language Model Security",
                    "data": {
                        "index": 260,
                        "title": "Unbridled Icarus: A Survey of the Potential Perils of Image Inputs in Multimodal Large Language Model Security",
                        "publication_date": "2024-08-11",
                        "references": [
                            "Ai alignment: A comprehensive survey",
                            "A learning-based incentive mechanism for mobile aigc service in decentralized internet of vehicles",
                            "Jailbreak in pieces: Compositional adversarial attacks on multi-modal language models",
                            "Images are achilles’ heel of alignment: Exploiting visual vulnerabilities for jailbreaking multimodal large language models",
                            "Figstep: Jailbreaking large vision-language models via typographic visual prompts",
                            "On evaluating adversarial robustness of large vision-language models",
                            "Survey of vulnerabilities in large language models revealed by adversarial attacks",
                            "A survey of safety and trustworthiness of large language models through the lens of verification and validation",
                            "Risk taxonomy, mitigation, and assessment benchmarks of large language model systems",
                            "Mm-llms: Recent advances in multimodal large language models",
                            "High-resolution image synthesis with latent diffusion models",
                            "On the essence and prospect: An investigation of alignment approaches for big models",
                            "On the adversarial robustness of multi-modal foundation models",
                            "Visual adversarial examples jailbreak aligned large language models",
                            "An image is worth 1000 lies: Adversarial transferability across prompts on vision-language models",
                            "Image hijacks: Adversarial images can control generative models at runtime",
                            "(ab) using images and sounds for indirect instruction injection in multi-modal lllms",
                            "Stop reasoning! when multimodal llms with chain-of-thought reasoning meets adversarial images",
                            "Test-time backdoor attacks on multimodal large language models",
                            "Agent smith: A single image can jailbreak one million multimodal llm agents exponentially fast",
                            "The wolf within: Covert injection of malice into mllm societies via an mllm operative",
                            "Vision-llms can fool themselves with self-generated typographic attacks",
                            "Jailbreaking gpt-4v via self-adversarial attacks with system prompts",
                            "How robust is google’s bard to adversarial image attacks?",
                            "Instructta: Instruction-tuned targeted attack for large vision-language models",
                            "Ceci n’est pas une pomme: Adversarial illusions in multi-modal embeddings",
                            "Ot-attack: Enhancing adversarial transferability of vision-language models via optimal transport optimization",
                            "Jailbreaking attack against multimodal large language model",
                            "Imgtrojan: Jailbreaking vision-language models with one image",
                            "Shadowcast: Stealthy data poisoning attacks against vision-language models",
                            "Vl-trojan: Multimodal instruction backdoor attacks against autoregressive visual language models",
                            "Image to prompt injection with google bard",
                            "Towards adversarial attack on vision-language pre-training models",
                            "Set-level guidance attack: Boosting adversarial transferability of vision-language pre-training models",
                            "Advclip: Downstream-agnostic adversarial examples in multimodal contrastive learning",
                            "Vlattack: Multimodal adversarial attacks on vision-language tasks via pre-trained models",
                            "Exploring transferability of multimodal adversarial samples for vision-language pre-training models with contrastive learning",
                            "Sa-attack: Improving adversarial transferability of vision-language pre-training models via self-augmentation",
                            "Random gradient-free minimization of convex functions",
                            "Jailbreaking black box large language models in twenty queries",
                            "Invisible backdoor attacks on deep neural networks via steganography and regularization",
                            "Hidden backdoors in human-centric language models",
                            "Red teaming visual language models",
                            "Robust contrastive language-image pretraining against data poisoning and backdoor attacks",
                            "Adversarial prompt tuning for vision-language models",
                            "One prompt word is enough to boost adversarial robustness for pre-trained vision-language models",
                            "Dress: Instructing large vision-language models to align and interact with humans via natural language feedback",
                            "A mutation-based method for multi-modal jailbreaking attack detection",
                            "Mllm-protector: Ensuring mllm’s safety without hurting performance",
                            "Inferaligner: Inference-time alignment for harmlessness through cross-model guidance",
                            "Adashield: Safeguarding multimodal large language models from structure-based attack via adaptive shield prompting",
                            "Eyes closed, safety on: Protecting multimodal llms via image-to-text transformation",
                            "Right to be forgotten in the era of large language models: Implications, challenges, and solutions",
                            "Eight methods to evaluate robust unlearning in llms",
                            "Differential privacy",
                            "Privacy-enhanced knowledge transfer with collaborative split learning over teacher ensembles",
                            "Bounded and unbiased composite differential privacy",
                            "A comprehensive survey of privacy-preserving federated learning: A taxonomy, review, and future directions",
                            "Dynamic user clustering for efficient and privacy-preserving federated learning",
                            "Long-term privacy-preserving aggregation with user-dynamics for federated learning",
                            "A duty to forget, a right to be assured? exposing vulnerabilities in machine unlearning services",
                            "Learn what you want to unlearn: Unlearning inversion attacks against machine unlearning",
                            "Threats, attacks, and defenses in machine unlearning: A survey",
                            "Towards efficient and certified recovery from poisoning attacks in federated learning",
                            "A survey on federated unlearning: Challenges, methods, and future directions",
                            "Rlaif: Scaling reinforcement learning from human feedback with ai feedback",
                            "Knowledge editing for large language models: A survey",
                            "Editing conceptual knowledge for large language models",
                            "Detoxifying large language models via knowledge editing",
                            "The first to know: How token distributions reveal hidden knowledge in large vision-language models?"
                        ]
                    },
                    "children": []
                }
            ]
        },
        {
            "name": "MLLMG UARD: A Multi-dimensional Safety Evaluation Suite for Multimodal Large Language Models",
            "data": {
                "index": 164,
                "title": "MLLMG UARD: A Multi-dimensional Safety Evaluation Suite for Multimodal Large Language Models",
                "publication_date": "2024-06-13",
                "references": [
                    "Llama: Open and efficient foundation language models",
                    "Palm: Scaling language modeling with pathways",
                    "Language models are few-shot learners",
                    "Gpt-4 technical report",
                    "Gemini: a family of highly capable multimodal models",
                    "Cogvlm: Visual expert for pretrained language models",
                    "A survey on multimodal large language models",
                    "Red teaming visual language models",
                    "Holistic analysis of hallucination in gpt-4v(ision): Bias and interference challenges",
                    "Detecting and preventing hallucinations in large vision language models",
                    "Mitigating hallucination in large multi-modal models via robust instruction tuning",
                    "Aligning large multimodal models with factually augmented rlhf",
                    "Benchmarking large multimodal models against common corruptions",
                    "Benchlmm: Benchmarking cross-style visual capability of large multimodal models",
                    "Goat-bench: Safety insights to large multimodal models through meme-based social abuse",
                    "Figstep: Jailbreaking large vision-language models via typographic visual prompts",
                    "Mm-safetybench: A benchmark for safety evaluation of multimodal large language models",
                    "Assessment of multimodal large language models in alignment with human values",
                    "Training a helpful and harmless assistant with reinforcement learning from human feedback",
                    "The hateful memes challenge: Detecting hate speech in multimodal memes",
                    "Benchmark dataset of memes with text transcriptions for automatic detection of multi-modal misogynistic content",
                    "Logo-2k+: A large-scale logo dataset for scalable logo classification",
                    "Cvalues: Measuring the values of chinese large language models from safety to responsibility",
                    "Model evaluation for extreme risks",
                    "Roberta: A robustly optimized bert pretraining approach",
                    "Scaling laws for neural language models",
                    "Improved baselines with visual instruction tuning",
                    "How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites",
                    "Scigraphqa: A large-scale synthetic multi-turn question-answering dataset for scientific graphs",
                    "Iconqa: A new benchmark for abstract diagram understanding and visual language reasoning",
                    "mplug-owl: Modularization empowers large language models with multimodality",
                    "Coco-text: Dataset and benchmark for text detection and recognition in natural images",
                    "Are you smarter than a sixth grader? textbook question answering for multimodal machine comprehension",
                    "Chartqa: A benchmark for question answering about charts with visual and logical reasoning",
                    "Advancing multimodal chart understanding with large-scale instruction tuning",
                    "An augmented benchmark dataset for geometric question answering through dual parallel text encoding",
                    "Modeling context in referring expressions",
                    "Generation and comprehension of unambiguous object descriptions",
                    "Visual genome: Connecting language and vision using crowdsourced dense image annotations",
                    "Shikra: Unleashing multimodal llm’s referential dialogue magic",
                    "Segment anything",
                    "Large-scale classification of fine-art paintings: Learning the right metric on the right feature",
                    "Visual instruction tuning",
                    "Nocaps: Novel object captioning at scale",
                    "A diagram is worth a dozen images",
                    "Question answering about charts with visual and logical reasoning",
                    "Just ask: Learning to answer questions from millions of narrated videos",
                    "Laion-5b: An open large-scale dataset for training next generation image-text models",
                    "Coyo-700m: Image-text pair dataset",
                    "Are you smarter than a sixth grader? textbook question answering for multimodal machine comprehension",
                    "Dynamic prompt learning via policy gradient for semi-structured mathematical reasoning",
                    "Bootstrap your own mathematical questions for large language models",
                    "Icdar 2019 competition on large-scale street view text with partial labeling-rrc-lsvt",
                    "The all-seeing project v2: Towards general relation comprehension of the open world",
                    "Objects365: A large-scale, high-quality dataset for object detection",
                    "Scene text visual question answering",
                    "Icdar2017 competition on reading chinese text in the wild (rctw-17)",
                    "Towards vqa models that can read",
                    "Pp-ocrv3: More attempts for the improvement of ultra lightweight ocr system",
                    "Wukong: A 100 million large-scale chinese cross-modal pre-training benchmark",
                    "Kosmos-2: Grounding multimodal large language models to the world",
                    "The all-seeing project: Towards panoptic visual recognition and understanding of the open world",
                    "Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks",
                    "Reasoning over scientific plots",
                    "Knowledge-aware visual question answering",
                    "A comprehensive multimodal dataset for advancing english and chinese large models",
                    "Capsfusion: Rethinking image-text data at scale",
                    "Inter-gps: Interpretable geometry problem solving with formal language and symbolic reasoning",
                    "Learning to explain: Multimodal reasoning via thought chains for science question answering",
                    "The hateful memes challenge: Detecting hate speech in multimodal memes",
                    "Multimodal C4: An open, billion-scale corpus of images interleaved with text",
                    "OCR-free document understanding transformer",
                    "Microsoft coco captions: Data collection and evaluation server",
                    "Monkey: Image resolution and text label are important things for large multi-modal models",
                    "Unleashing multimodal llm’s referential dialogue magic",
                    "Grounding multimodal large language models to the world",
                    "Improving large multi-modal models with better captions",
                    "Universal ocr-free visually-situated language understanding with multimodal large language model",
                    "Scientific diagram analysis with the multimodal large language model",
                    "Modularization empowers large language models with multimodality",
                    "Making the v in vqa matter: Elevating the role of image understanding in visual question answering",
                    "Advancing multimodal chart understanding with large-scale instruction tuning",
                    "A new benchmark for abstract diagram understanding and visual language reasoning",
                    "Novel object captioning at scale",
                    "Large-scale classification of fine-art paintings: Learning the right metric on the right feature",
                    "Dataset and benchmark for text detection and recognition in natural images",
                    "Are you smarter than a sixth grader? textbook question answering for multimodal machine comprehension",
                    "Question answering about charts with visual and logical reasoning",
                    "Advancing multimodal chart understanding with large-scale instruction tuning",
                    "An augmented benchmark dataset for geometric question answering through dual parallel text encoding",
                    "Modeling context in referring expressions",
                    "Generation and comprehension of unambiguous object descriptions",
                    "Visual genome: Connecting language and vision using crowdsourced dense image annotations",
                    "Nocaps: Novel object captioning at scale",
                    "A diagram is worth a dozen images",
                    "Reasoning over scientific plots",
                    "Knowledge-aware visual question answering",
                    "A comprehensive multimodal dataset for advancing english and chinese large models",
                    "Capsfusion: Rethinking image-text data at scale",
                    "Inter-gps: Interpretable geometry problem solving with formal language and symbolic reasoning",
                    "Learning to explain: Multimodal reasoning via thought chains for science question answering"
                ]
            },
            "children": []
        },
        {
            "name": "SafeBench: A Safety Evaluation Framework for Multimodal Large Language Models",
            "data": {
                "index": 221,
                "title": "SafeBench: A Safety Evaluation Framework for Multimodal Large Language Models",
                "publication_date": "2024-10-24",
                "references": [
                    "Visualgpt: Data-efficient adaptation of pretrained language models for image captioning",
                    "Visual question answering instruction: Unlocking multimodal large language model to domain-specific visual multitasks",
                    "Image retrieval on real-life images with pre-trained vision-and-language models",
                    "An image is worth 1000 lies: Adversarial transferability across prompts on vision-language models",
                    "On the adversarial robustness of multi-modal foundation models",
                    "On the robustness of large multimodal models against image adversarial attacks",
                    "Jailbreak in pieces: Compositional adversarial attacks on multi-modal language models",
                    "Semantic mirror jailbreak: Genetic algorithm based jailbreak prompts against open-source llms",
                    "Jailbreak vision language models via bi-modal adversarial prompt",
                    "Simpo: Simple preference optimization with a reference-free reward",
                    "Direct preference optimization: Your language model is secretly a reward model",
                    "Training a helpful and harmless assistant with reinforcement learning from human feedback",
                    "Training language models to follow instructions with human feedback",
                    "A comprehensive evaluation framework for deep model robustness",
                    "Robustart: Benchmarking robustness on architecture design and training techniques",
                    "Training robust deep neural networks via adversarial noise propagation",
                    "Badclip: Dual-embedding guided backdoor attack on multimodal contrastive learning",
                    "Pre-trained trojan attacks for visual recognition",
                    "Poisoned forgery face: Towards backdoor attacks on face forgery detection",
                    "GPTFUZZER: red teaming large language models with auto-generated jailbreak prompts",
                    "Jailbroken: How does LLM safety training fail?",
                    "Jailbreaking black box large language models in twenty queries",
                    "Unveiling the safety of gpt-4o: An empirical study using jailbreak attacks",
                    "Mm-safetybench: A benchmark for safety evaluation of multimodal large language models",
                    "Fig-step: Jailbreaking large vision-language models via typographic visual prompts",
                    "Jailbreakv-28k: A benchmark for assessing the robustness of multimodal large language models against jailbreak attacks",
                    "Gpt-4o system card",
                    "Gemini: A family of highly capable multimodal models",
                    "Realtoxicityprompts: Evaluating neural toxic degeneration in language models",
                    "Universal and transferable adversarial attacks on aligned language models",
                    "Toxigen: A large-scale machine-generated dataset for adversarial and implicit hate speech detection",
                    "Benchmarking cognitive biases in large language models as evaluators",
                    "Privlm-bench: A multi-level privacy evaluation benchmark for language models",
                    "Truthfulqa: Measuring how models mimic human falsehoods",
                    "FFT: towards harmlessness evaluation and analysis for llms with factuality, fairness, toxicity",
                    "Sorry-bench: Systematically evaluating large language model safety refusal behaviors",
                    "Harm-bench: A standardized evaluation framework for automated red teaming and robust refusal",
                    "Benchmarking trustworthiness of multimodal large language models: A comprehensive study",
                    "Avibench: Towards evaluating the robustness of large vision-language model on adversarial visual-instructions",
                    "How many unicorns are in this image? A safety evaluation benchmark for vision llms",
                    "Jailbreakv-28k: A benchmark for assessing the robustness of multimodal large language models against jailbreak attacks",
                    "Cross-modality safety alignment",
                    "Autodan: Generating stealthy jailbreak prompts on aligned large language models",
                    "Realtoxicityprompts: Evaluating neural toxic degeneration in language models",
                    "Visual adversarial examples jailbreak aligned large language models",
                    "Perspective",
                    "Llama guard: Llm-based input-output safeguard for human-ai conversations",
                    "Jailbreaking black box large language models in twenty queries",
                    "Tree of attacks: Jailbreaking black-box llms automatically",
                    "Visual adversarial examples jailbreak aligned large language models",
                    "Does refusal training in llms generalize to the past tense?",
                    "Draw-and-understand: Leveraging visual prompts to enable mllms to comprehend what you want",
                    "Can large multi-modal models uncover deep semantics behind images?",
                    "Parler-tts",
                    "Chain-of-thought prompting elicits reasoning in large language models",
                    "Multi-modal chain-of-thought reasoning in language models",
                    "Efficient multimodal learning from data-centric perspective",
                    "Cogvlm: Visual expert for pre-trained language models",
                    "Chatglm: A family of large language models from glm-130b to glm-4 all tools",
                    "How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites",
                    "Llava-next: Improved reasoning, ocr, and world knowledge",
                    "Minicpm-v: A gpt-4v level mllm on your phone",
                    "Qwen-vl: A frontier large vision-language model with versatile abilities",
                    "Qwen-vl: A versatile vision-language model for understanding, localization, text reading, and beyond",
                    "Sharegpt4v: Improving large multi-modal models with better captions",
                    "Yi: Open foundation models by 01.ai",
                    "Claude 3.5 sonnet",
                    "The claude 3 model family: Opus, sonnet, haiku",
                    "Gemini pro",
                    "Gemini flash",
                    "Gpt-4o mini: advancing cost-efficient intelligence",
                    "Llama 3 model card",
                    "Gemma",
                    "Internlm2 technical report",
                    "Qwen technical report",
                    "Chatglm: A family of large language models from glm-130b to glm-4 all tools",
                    "A high quality sparse mixture-of-experts",
                    "Introducing dbrx: A new state-of-the-art open llm",
                    "Grok-2 beta release",
                    "Scaling rectified flow transformers for high-resolution image synthesis",
                    "Qwen2-vl-7b-instruct model card",
                    "Minicpm-v: A gpt-4v level mllm on your phone",
                    "Phi-3-vision-128k-instruct",
                    "Phi-3.5-vision-instruct"
                ]
            },
            "children": [
                {
                    "name": "Does Refusal Training in LLMs Generalize to the Past Tense?",
                    "data": {
                        "index": 74,
                        "title": "Does Refusal Training in LLMs Generalize to the Past Tense?",
                        "publication_date": "2024-10-03",
                        "references": [
                            "Phi-3 technical report: A highly capable language model locally on your phone.",
                            "Llama 3 model card.",
                            "Jailbreaking leading safety-aligned llms with simple adaptive attacks.",
                            "Many-shot jailbreaking.",
                            "Introducing claude 3.5 sonnet.",
                            "Training a helpful and harmless assistant with reinforcement learning from human feedback.",
                            "Managing ai risks in an era of rapid progress.",
                            "The reversal curse: Llms trained on” a is b” fail to learn” b is a”.",
                            "Wild patterns: ten years after the rise of adversarial machine learning.",
                            "Evasion attacks against machine learning at test time.",
                            "Jailbreaking black box large language models in twenty queries.",
                            "Jailbreakbench: An open robustness benchmark for jailbreaking large language models.",
                            "Scaling instruction-finetuned language models.",
                            "Rlhf can speak many languages: Unlocking multilingual preference optimization for llms.",
                            "Gemma-2 report.",
                            "Open sesame! universal black box jailbreaking of large language models.",
                            "Chatgpt doesn’t trust chargers fans: Guardrail sensitivity in context.",
                            "Preference tuning for toxicity mitigation generalizes across languages.",
                            "Autodan: Generating stealthy jailbreak prompts on aligned large language models.",
                            "Towards deep learning models resistant to adversarial attacks.",
                            "Tdc 2023 (llm edition): The trojan detection challenge.",
                            "Harmbench: A standardized evaluation framework for automated red teaming and robust refusal.",
                            "Jailbroken: How does llm safety training fail?",
                            "Jailbreak and guard aligned language models with only few in-context demonstrations.",
                            "Do llamas work in english? on the latent language of multilingual transformers.",
                            "Llama 2: Open foundation and fine-tuned chat models.",
                            "Zephyr: Direct distillation of lm alignment."
                        ]
                    },
                    "children": [
                        {
                            "name": "JailbreakBench: An Open Robustness Benchmark for Jailbreaking Large Language Models",
                            "data": {
                                "index": 136,
                                "title": "JailbreakBench: An Open Robustness Benchmark for Jailbreaking Large Language Models",
                                "publication_date": "2024-10-31",
                                "references": [
                                    "Are you still on track!? catching llm task drift with activations",
                                    "Llama 3 model card",
                                    "Croissant: A metadata format for ml-ready datasets",
                                    "Jailbreak chat",
                                    "Detecting language model attacks with perplexity",
                                    "Jailbreaking leading safety-aligned llms with simple adaptive attacks",
                                    "Measuring massive multitask language understanding",
                                    "Catastrophic jailbreak of open-source llms via exploiting generation",
                                    "Sleeper agents: Training deceptive llms that persist through safety training",
                                    "Llama guard: Llm-based input-output safeguard for human-ai conversations",
                                    "Baseline defenses for adversarial attacks against aligned language models",
                                    "Defending large language models against jailbreak attacks via semantic smoothing",
                                    "Mixtral of experts",
                                    "Guard: Role-playing to generate natural-language jailbreakings to test guideline adherence of large language models",
                                    "Jailbreaking large language models against moderation guardrails via cipher characters",
                                    "Certifying llm safety against adversarial prompting",
                                    "Open sesame! universal black box jailbreaking of large language models",
                                    "Non-determinism in gpt-4 is caused by sparse moe",
                                    "Jailbreaking black box large language models in twenty queries",
                                    "Robustbench: a standardized adversarial robustness benchmark",
                                    "Multilingual jailbreak challenges in large language models",
                                    "Attacking large language models with projected gradient descent",
                                    "Gemini v1.5 report",
                                    "Query-based adversarial prompt generation",
                                    "Jailbroken: How does llm safety training fail?",
                                    "Defensive prompt patch: A robust and interpretable defense of llms against jailbreak attacks",
                                    "Low-resource languages jailbreak gpt-4",
                                    "Gptfuzzer: Red teaming large language models with auto-generated jailbreak prompts",
                                    "How johnny can persuade llms to jailbreak them: Rethinking persuasion to challenge ai safety by humanizing llms",
                                    "Judging llm-as-a-judge with mt-bench and chatbot arena",
                                    "Improved few-shot jailbreaking can circumvent aligned language models and their defenses",
                                    "Easyjailbreak: A unified framework for jailbreaking large language models",
                                    "Promptbench: A unified library for evaluation of large language models",
                                    "Randomness in neural network training: Characterizing the impact of tooling",
                                    "Universal and transferable adversarial attacks on aligned language models"
                                ]
                            },
                            "children": [
                                {
                                    "name": "Defensive Prompt Patch: A Robust and Interpretable Defense of LLMs against Jailbreak Attacks",
                                    "data": {
                                        "index": 71,
                                        "title": "Defensive Prompt Patch: A Robust and Interpretable Defense of LLMs against Jailbreak Attacks",
                                        "publication_date": "2024-05-30",
                                        "references": [
                                            "Universal and transferable adversarial attacks on aligned language models",
                                            "Llama: Open and efficient foundation language models",
                                            "Autodan: Generating stealthy jailbreak prompts on aligned large language models",
                                            "Tree of attacks: Jailbreaking black-box llms automatically",
                                            "Jailbreaking black box large language models in twenty queries",
                                            "Jailbroken: How does LLM safety training fail?",
                                            "Mistral 7b",
                                            "Jailbreak and guard aligned language models with only few in-context demonstrations",
                                            "Catastrophic jailbreak of open-source llms via exploiting generation",
                                            "Defending chatgpt against jailbreak attack via self-reminders",
                                            "Defending large language models against jailbreaking attacks through goal prioritization",
                                            "Robust prompt optimization for defending language models against jailbreaking attacks",
                                            "Llm self defense: By self examination, llms know they are being tricked",
                                            "Alpacaeval: An automatic evaluator of instruction-following models",
                                            "On prompt-driven safeguarding for large language models",
                                            "On adaptive attacks to adversarial example defenses",
                                            "Amplegcg: Learning a universal and transferable generative model of adversarial suffixes for jailbreaking both open and closed llms",
                                            "GPT-4 technical report",
                                            "Baseline defenses for adversarial attacks against aligned language models",
                                            "Smoothllm: Defending large language models against jailbreaking attacks",
                                            "Starling-7b: Improving llm helpfulness and harmlessness with rlaif",
                                            "A general language assistant as a laboratory for alignment",
                                            "Training a helpful and harmless assistant with reinforcement learning from human feedback",
                                            "Training language models to follow instructions with human feedback",
                                            "Attention is all you need",
                                            "Agieval: A human-centric benchmark for evaluating foundation models",
                                            "Summarization is (almost) dead",
                                            "Intention analysis makes llms a good jailbreak defender",
                                            "Low-resource languages jailbreak gpt-4",
                                            "Safetybench: Evaluating the safety of large language models with multiple choice questions",
                                            "Bert: Pre-training of deep bidirectional transformers for language understanding",
                                            "Language models show human-like content effects on reasoning tasks",
                                            "Jailbreakbench: An open robustness benchmark for jailbreaking large language models",
                                            "On prompt-driven safeguarding for large language models"
                                        ]
                                    },
                                    "children": [
                                        {
                                            "name": "AmpleGCG: Learning a Universal and Transferable Generative Model of Adversarial Suffixes for Jailbreaking Both Open and Closed LLMs",
                                            "data": {
                                                "index": 19,
                                                "title": "AmpleGCG: Learning a Universal and Transferable Generative Model of Adversarial Suffixes for Jailbreaking Both Open and Closed LLMs",
                                                "publication_date": "2024-11-24",
                                                "references": [
                                                    "Gpt-4 technical report",
                                                    "Scheduled sampling for sequence prediction with recurrent neural networks",
                                                    "Purple llama cyberseceval: A secure coding benchmark for language models",
                                                    "Black-box prompt optimization: Aligning large language models without model training",
                                                    "Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality",
                                                    "Safe rlhf: Safe reinforcement learning from human feedback",
                                                    "Multilingual jailbreak challenges in large language models",
                                                    "Qlora: Efficient finetuning of quantized llms",
                                                    "HotFlip: White-box adversarial examples for text classification",
                                                    "Catastrophic jailbreak of open-source llms via exploiting generation",
                                                    "Baseline defenses for adversarial attacks against aligned language models",
                                                    "Mistral 7b",
                                                    "The power of scale for parameter-efficient prompt tuning",
                                                    "Prefix-tuning: Optimizing continuous prompts for generation",
                                                    "The unlocking spell on base llms: Rethinking alignment via in-context learning",
                                                    "Lost in the middle: How language models use long contexts",
                                                    "Gpt understands, too",
                                                    "Autodan: Generating stealthy jailbreak prompts on aligned large language models",
                                                    "Harmbench: A standardized evaluation framework for automated red teaming and robust refusal",
                                                    "How trustworthy are open-source llms? an assessment under malicious demonstrations shows their vulnerabilities",
                                                    "A trembling house of cards? mapping adversarial attacks against language agents",
                                                    "Scalable extraction of training data from (production) language models",
                                                    "Researchers poke holes in safety controls of chatgpt and other chatbots",
                                                    "Training language models to follow instructions with human feedback",
                                                    "Exploiting novel gpt-4 apis",
                                                    "Fine-tuning aligned language models compromises safety, even when users do not intend to!",
                                                    "Weak-to-strong jailbreaking on large language models",
                                                    "Large language models are human-level prompt engineers",
                                                    "Autodan: Automatic and interpretable adversarial attacks on large language models",
                                                    "Universal and transferable adversarial attacks on aligned language models"
                                                ]
                                            },
                                            "children": []
                                        }
                                    ]
                                },
                                {
                                    "name": "Improved Few-Shot Jailbreaking Can Circumvent Aligned Language Models and Their Defenses",
                                    "data": {
                                        "index": 122,
                                        "title": "Improved Few-Shot Jailbreaking Can Circumvent Aligned Language Models and Their Defenses",
                                        "publication_date": "2024-10-30",
                                        "references": [
                                            "Detecting language model attacks with perplexity",
                                            "Jailbreaking leading safety-aligned llms with simple adaptive attacks",
                                            "Many-shot jailbreaking",
                                            "Qwen technical report",
                                            "Training a helpful and harmless assistant with reinforcement learning from human feedback",
                                            "Language models are few-shot learners",
                                            "Defending against alignment-breaking attacks via robustly aligned llm",
                                            "Are aligned neural networks adversarially aligned?",
                                            "Jailbreaking black box large language models in twenty queries",
                                            "Jailbreakbench: An open robustness benchmark for jailbreaking large language models",
                                            "Jailbreaker: Automated jailbreak across multiple large language model chatbots",
                                            "Multilingual jailbreak challenges in large language models",
                                            "Badllama: cheaply removing safety fine-tuning from llama 2-chat 13b",
                                            "Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned",
                                            "Not what you’ve signed up for: Compromising real-world llm-integrated applications with indirect prompt injection",
                                            "Query-based adversarial prompt generation",
                                            "Llm self defense: By self examination, llms know they are being tricked",
                                            "Gradient cuff: Detecting jailbreak attacks on large language models by exploring refusal loss landscapes",
                                            "Token-level adversarial prompt detection based on perplexity measures and contextual information",
                                            "Catastrophic jailbreak of open-source llms via exploiting generation",
                                            "Llama guard: Llm-based input-output safeguard for human-ai conversations",
                                            "Baseline defenses for adversarial attacks against aligned language models",
                                            "Defending large language models against jailbreak attacks via semantic smoothing",
                                            "Mistral 7b",
                                            "Pretraining language models with human preferences",
                                            "Llama 3 model card",
                                            "Studious bob fight back against jailbreaking via prompt adversarial tuning",
                                            "Universal black box jailbreaking of large language models",
                                            "Lora fine-tuning efficiently undoes safety training in llama 2-chat 70b",
                                            "Deepinception: Hypnotize large language model to be jailbreaker",
                                            "Rain: Your language models can align themselves without finetuning",
                                            "Amplegcg: Learning a universal and transferable generative model of adversarial suffixes for jailbreaking both open and closed llms",
                                            "The unlocking spell on base llms: Rethinking alignment via in-context learning",
                                            "Lost in the middle: How language models use long contexts",
                                            "Autodan: Generating stealthy jailbreak prompts on aligned large language models",
                                            "Jailbreaking chatgpt via prompt engineering: An empirical study",
                                            "Prp: Propagating universal perturbations to attack large language model guard-rails",
                                            "Harmbench: A standardized evaluation framework for automated red teaming and robust refusal",
                                            "Tree of attacks: Jailbreaking black-box llms automatically",
                                            "Gpt-4 technical report",
                                            "Red teaming language models with language models",
                                            "Fine-tuning aligned language models compromises safety, even when users do not intend to!",
                                            "Tricking llms into disobedience: Understanding, analyzing, and preventing jailbreaks",
                                            "The convergence of the random search method in the extremal control of a many parameter system",
                                            "Sentence embeddings using siamese bert-networks",
                                            "Smoothllm: Defending large language models against jailbreaking attacks",
                                            "Identifying the risks of lm agents with an lm-emulated sandbox",
                                            "Spml: A dsl for defending language models against prompt attacks",
                                            "\"do anything now\": Characterizing and evaluating in-the-wild jailbreak prompts on large language models",
                                            "Pal: Proxy-guided black-box attack on large language models",
                                            "Evil geniuses: Delving into the safety of llm-based agents",
                                            "Llama 2: Open foundation and fine-tuned chat models",
                                            "Tensor trust: Interpretable prompt injection attacks from an online game",
                                            "Open-chat: Advancing open-source language models with mixed-quality data",
                                            "From noise to clarity: Unraveling the adversarial suffix of large language model attacks via translation of text embeddings",
                                            "Defending llms against jailbreaking attacks via backtranslation",
                                            "Jailbroken: How does llm safety training fail?",
                                            "Jailbreak and guard aligned language models with only few in-context demonstrations",
                                            "Defending chatgpt against jailbreak attack via self-reminders",
                                            "Safedecoding: Defending against jailbreak attacks via safety-aware decoding",
                                            "Shadow alignment: The ease of subverting safely-aligned language models",
                                            "Low-resource languages jailbreak gpt-4",
                                            "Gpt-4 is too smart to be safe: Stealthy chat with llms via cipher",
                                            "Rigorllm: Resilient guardrails for large language models against undesired content",
                                            "How johnny can persuade llms to jailbreak them: Rethinking persuasion to challenge ai safety by humanizing llms",
                                            "Defending large language models against jailbreaking attacks through goal prioritization",
                                            "Robust prompt optimization for defending language models against jailbreaking attacks",
                                            "Defending jailbreak prompts via in-context adversarial game",
                                            "Autodan: Automatic and interpretable adversarial attacks on large language models",
                                            "Universal and transferable adversarial attacks on aligned language models"
                                        ]
                                    },
                                    "children": [
                                        {
                                            "name": "Defending LLMs against Jailbreaking Attacks via Backtranslation",
                                            "data": {
                                                "index": 69,
                                                "title": "Defending LLMs against Jailbreaking Attacks via Backtranslation",
                                                "publication_date": "2024-08-11",
                                                "references": [
                                                    "Gpt-4 technical report",
                                                    "Detecting language model attacks with perplexity",
                                                    "Training a helpful and harmless assistant with reinforcement learning from human feedback",
                                                    "Jailbreaking black box large language models in twenty queries",
                                                    "Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality",
                                                    "Palm: Scaling language modeling with pathways",
                                                    "Jailbreaking proprietary large language models using word substitution cipher",
                                                    "Baseline defenses for adversarial attacks against aligned language models",
                                                    "Defending large language models against jailbreak attacks via semantic smoothing",
                                                    "Certifying llm safety against adversarial prompting",
                                                    "Open sesame! universal black box jailbreaking of large language models",
                                                    "Autodan: Generating stealthy jailbreak prompts on aligned large language models",
                                                    "Jailbreaking chatgpt via prompt engineering: An empirical study",
                                                    "Chatgpt",
                                                    "Fine-tuning aligned language models compromises safety, even when users do not intend to!",
                                                    "Smoothllm: Defending large language models against jailbreaking attacks",
                                                    "Llama: Open and efficient foundation language models",
                                                    "Llama 2: Open foundation and fine-tuned chat models",
                                                    "Jailbroken: How does llm safety training fail?",
                                                    "Jailbreak and guard aligned language models with only few in-context demonstrations",
                                                    "Defending chatgpt against jailbreak attack via self-reminders",
                                                    "An llm can fool itself: A prompt-based adversarial attack",
                                                    "Gpt-fuzzer: Red teaming large language models with auto-generated jailbreak prompts",
                                                    "Gpt-4 is too smart to be safe: Stealthy chat with llms via cipher",
                                                    "How johnny can persuade llms to jailbreak them: Rethinking persuasion to challenge ai safety by humanizing llms",
                                                    "Defending large language models against jail-breaking attacks through goal prioritization",
                                                    "Prompt-driven llm safeguarding via directed representation optimization",
                                                    "Judging llm-as-a-judge with mt-bench and chatbot arena",
                                                    "Robust prompt optimization for defending language models against jailbreaking attacks",
                                                    "Large language models are human-level prompt engineers",
                                                    "Autodan: Automatic and interpretable adversarial attacks on large language models",
                                                    "Universal and transferable adversarial attacks on aligned language models"
                                                ]
                                            },
                                            "children": []
                                        },
                                        {
                                            "name": "JAILBREAKING LEADING SAFETY-ALIGNED LLM S WITH SIMPLE ADAPTIVE ATTACKS",
                                            "data": {
                                                "index": 141,
                                                "title": "JAILBREAKING LEADING SAFETY-ALIGNED LLM S WITH SIMPLE ADAPTIVE ATTACKS",
                                                "publication_date": "2024-10-07",
                                                "references": [
                                                    "Phi-3 technical report: A highly capable language model locally on your phone.",
                                                    "Square attack: a query-efficient black-box adversarial attack via random search.",
                                                    "Many-shot jailbreaking.",
                                                    "Prefill claude’s response for greater output control.",
                                                    "The claude 3 model family: Opus, sonnet, haiku.",
                                                    "Training a helpful and harmless assistant with reinforcement learning from human feedback.",
                                                    "Wild patterns: ten years after the rise of adversarial machine learning.",
                                                    "Evasion attacks against machine learning at test time.",
                                                    "On evaluating adversarial robustness.",
                                                    "Jailbreaking black box large language models in twenty queries.",
                                                    "Jailbreakbench: An open robustness benchmark for jailbreaking large language models.",
                                                    "Leveraging the context through multi-round interactions for jailbreaking attacks.",
                                                    "Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality.",
                                                    "Robustbench: a standardized adversarial robustness benchmark.",
                                                    "Sparse-rs: a versatile framework for query-efficient sparse black-box adversarial attacks.",
                                                    "Evaluating the adversarial robustness of adaptive test-time defenses.",
                                                    "Attacking large language models with projected gradient descent.",
                                                    "Gemini: a family of highly capable multimodal models.",
                                                    "Mistral 7b.",
                                                    "Open sesame! universal black box jailbreaking of large language models.",
                                                    "Amplegcg: Learning a universal and transferable generative model of adversarial suffixes for jailbreaking both open and closed llms.",
                                                    "Autodan: Generating stealthy jailbreak prompts on aligned large language models.",
                                                    "Towards deep learning models resistant to adversarial attacks.",
                                                    "Prp: Propagating universal perturbations to attack large language model guard-rails.",
                                                    "Harmbench: A standardized evaluation framework for automated red teaming and robust refusal.",
                                                    "Tree of attacks: Jailbreaking black-box llms automatically.",
                                                    "Jailbreaking chatgpt on release day.",
                                                    "Nemotron-4 340b technical report.",
                                                    "Universal jailbreak backdoors from poisoned human feedback.",
                                                    "Find the trojan: Universal backdoor detection in aligned llms.",
                                                    "Competition report: Finding universal jailbreak backdoors in aligned llms.",
                                                    "The convergence of the random search method in the extremal control of a many parameter system.",
                                                    "Smoothllm: Defending large language models against jailbreaking attacks.",
                                                    "Great, now write an article about that: The crescendo multi-turn llm jailbreak attack.",
                                                    "Scalable and transferable black-box jailbreaks for language models via persona modulation.",
                                                    "Autoprompt: Eliciting knowledge from language models with automatically generated prompts.",
                                                    "Pal: Proxy-guided black-box attack on large language models.",
                                                    "Intriguing properties of neural networks.",
                                                    "Llama 2: Open foundation and fine-tuned chat models."
                                                ]
                                            },
                                            "children": [
                                                {
                                                    "name": "Competition Report: Finding Universal Jailbreak Backdoors in Aligned LLMs",
                                                    "data": {
                                                        "index": 56,
                                                        "title": "Competition Report: Finding Universal Jailbreak Backdoors in Aligned LLMs",
                                                        "publication_date": "2024-06-06",
                                                        "references": [
                                                            "Jailbreak chat.",
                                                            "Foundational challenges in assuring alignment and safety of large language models.",
                                                            "Training a helpful and harmless assistant with reinforcement learning from human feedback.",
                                                            "Poisoning attacks against support vector machines.",
                                                            "Towards making systems forget with machine unlearning.",
                                                            "The satml ’24 cnn interpretability competition: New innovations for concept-level interpretability.",
                                                            "The trojan detection challenge 2023 (LLM edition) - the trojan detection challenge.",
                                                            "Targeted backdoor attacks on deep learning systems using data poisoning.",
                                                            "Deep reinforcement learning from human preferences.",
                                                            "Weight poisoning attacks on pre-trained models.",
                                                            "The unlocking spell on base llms: Rethinking alignment via in-context learning.",
                                                            "Rethinking machine unlearning for large language models.",
                                                            "Text and code embeddings by contrastive pre-training.",
                                                            "Universal jailbreak backdoors from poisoned human feedback.",
                                                            "Humpty dumpty: Controlling word meanings via corpus poisoning.",
                                                            "Rapid optimization for jailbreaking llms via subconscious exploitation and echopraxia.",
                                                            "Badgpt: Exploring security vulnerabilities of chatgpt via backdoor attacks to instructgpt.",
                                                            "Be careful about poisoned word embeddings: Exploring the vulnerability of the embedding layers in nlp models.",
                                                            "Universal and transferable adversarial attacks on aligned language models."
                                                        ]
                                                    },
                                                    "children": []
                                                },
                                                {
                                                    "name": "Great, Now Write an Article About That: The Crescendo Multi-Turn LLM Jailbreak Attack",
                                                    "data": {
                                                        "index": 105,
                                                        "title": "Great, Now Write an Article About That: The Crescendo Multi-Turn LLM Jailbreak Attack",
                                                        "publication_date": "2025-02-26",
                                                        "references": [
                                                            "Responsible Disclosure and Mitigation Efforts.",
                                                            "Many-shot jailbreaking.",
                                                            "Constitutional ai: Harmlessness from ai feedback, 2022.",
                                                            "Extracting training data from large language models, 2021.",
                                                            "Jailbreaking Black Box Large Language Models in Twenty Queries. CoRR abs/2310.08419, 2023.",
                                                            "Leveraging the context through multi-round interactions for jailbreaking attacks.",
                                                            "Jailbreaker: Automated Jailbreak Across Multiple Large Language Model Chatbots. CoRR abs/2307.08715, 2023.",
                                                            "Multilingual Jailbreak Challenges in Large Language Models. CoRR abs/2310.06474, 2023.",
                                                            "Mart: Improving llm safety with multi-round automatic red-teaming, 2023.",
                                                            "Improving alignment of dialogue agents via targeted human judgements, 2022.",
                                                            "Catastrophic Jailbreak of Open-source LLMs via Exploiting Generation. CoRR abs/2310.06987, 2023.",
                                                            "User inference attacks on large language models, 2023.",
                                                            "Ethical frameworks and computer security trolley problems: Foundations for conversations.",
                                                            "Pretraining language models with human preferences, 2023.",
                                                            "Multi-step jailbreaking privacy attacks on chatgpt, 2023.",
                                                            "AutoDAN: Generating Stealthy Jailbreak Prompts on Aligned Large Language Models. CoRR abs/2310.04451, 2023.",
                                                            "Jailbreaking ChatGPT via Prompt Engineering: An Empirical Study. CoRR abs/2305.13860, 2023.",
                                                            "Analyzing leakage of personally identifiable information in language models, 2023.",
                                                            "Harmbench: A standardized evaluation framework for automated red teaming and robust refusal, 2024.",
                                                            "Training language models to follow instructions with human feedback, 2022.",
                                                            "Maat-phor: Automated variant analysis for prompt injection attacks, 2023.",
                                                            "Do Anything Now: Characterizing and Evaluating In-The-Wild Jailbreak Prompts on Large Language Models. CoRR abs/2308.03825, 2023.",
                                                            "Jailbroken: How Does LLM Safety Training Fail? CoRR abs/2307.02483, 2023.",
                                                            "Last one standing: A comparative analysis of security and privacy of soft prompt tuning, lora, and in-context learning, 2023.",
                                                            "Defending chatgpt against jailbreak attack via self-reminders. Nature Machine Intelligence, 5:1486–1496, 2023.",
                                                            "Sc-safety: A multi-round open-ended question adversarial safety benchmark for large language models in chinese, 2023.",
                                                            "Chain of attack: a semantic-driven contextual multi-turn attacker for llm, 2024.",
                                                            "GPTFUZZER: Red Teaming Large Language Models with Auto-Generated Jailbreak Prompts. CoRR abs/2309.10253, 2023.",
                                                            "Defending large language models against jailbreaking attacks through goal prioritization, 2024.",
                                                            "Make them spill the beans! coercive knowledge extraction from (production) llms, 2023.",
                                                            "Universal and Transferable Adversarial Attacks On Aligned Language Models. CoRR abs/2307.15043, 2023."
                                                        ]
                                                    },
                                                    "children": [
                                                        {
                                                            "name": "Chain of Attack: a Semantic-Driven Contextual Multi-Turn attacker for LLM",
                                                            "data": {
                                                                "index": 53,
                                                                "title": "Chain of Attack: a Semantic-Driven Contextual Multi-Turn attacker for LLM",
                                                                "publication_date": "2024-05-09",
                                                                "references": [
                                                                    "Training a helpful and harmless assistant with reinforcement learning from human feedback",
                                                                    "Red-teaming large language models using chain of utterances for safety-alignment",
                                                                    "Chatgpt evaluation on sentence level relations: A focus on temporal, causal, and discourse relations",
                                                                    "Jailbreaking black box large language models in twenty queries",
                                                                    "Understanding multi-turn toxic behaviors in open-domain chatbots",
                                                                    "Learning to teach large language models logical reasoning",
                                                                    "Jailbreaker: Automated jailbreak across multiple large language model chatbots",
                                                                    "Simcse: Simple contrastive learning of sentence embeddings",
                                                                    "Toxigen: A large-scale machine-generated dataset for adversarial and implicit hate speech detection",
                                                                    "Large language models can be used to effectively scale spear phishing campaigns",
                                                                    "Catastrophic jailbreak of open-source llms via exploiting generation",
                                                                    "Exploiting programmatic behavior of llms: Dual-use through standard security attacks",
                                                                    "Pretraining language models with human preferences",
                                                                    "Open sesame! universal black box jailbreaking of large language models",
                                                                    "Rlaif: Scaling reinforcement learning from human feedback with ai feedback",
                                                                    "A new generation of perspective api: Efficient multi-lingual character-level transformers",
                                                                    "Multi-step jailbreaking privacy attacks on chatgpt",
                                                                    "Toxicchat: Unveiling hidden challenges of toxicity detection in real-world user-ai conversation",
                                                                    "Autodan: Generating stealthy jailbreak prompts on aligned large language models",
                                                                    "Tree of attacks: Jailbreaking black-box llms automatically",
                                                                    "A comprehensive overview of large language models",
                                                                    "Gpt-4 technical report",
                                                                    "Training language models to follow instructions with human feedback",
                                                                    "In-context impersonation reveals large language models’ strengths and biases",
                                                                    "LOFT: Local proxy fine-tuning for improving transferability of adversarial attacks against large language model",
                                                                    "Unveiling gender bias in terms of profession across llms: Analyzing and addressing sociological implications",
                                                                    "Llama 2: Open foundation and fine-tuned chat models",
                                                                    "The silence of the llms: Cross-lingual analysis of political bias and false information prevalence in chatgpt, google bard, and bing chat",
                                                                    "Using gpt-4 for content moderation",
                                                                    "Gpt-fuzzer: Red teaming large language models with auto-generated jailbreak prompts",
                                                                    "Judging llm-as-a-judge with mt-bench and chatbot arena",
                                                                    "Autodan: Automatic and interpretable adversarial attacks on large language models",
                                                                    "Universal and transferable adversarial attacks on aligned language models"
                                                                ]
                                                            },
                                                            "children": []
                                                        }
                                                    ]
                                                }
                                            ]
                                        }
                                    ]
                                }
                            ]
                        },
                        {
                            "name": "Preference Tuning For Toxicity Mitigation Generalizes Across Languages",
                            "data": {
                                "index": 189,
                                "title": "Preference Tuning For Toxicity Mitigation Generalizes Across Languages",
                                "publication_date": "2024-11-08",
                                "references": [
                                    "Llama 3 model card.",
                                    "Refusal in language models is mediated by a single direction.",
                                    "Massively multilingual sentence embeddings for zero-shot cross-lingual transfer and beyond.",
                                    "Aya 23: Open weight releases to further multilingual progress.",
                                    "Training a helpful and harmless assistant with reinforcement learning from human feedback.",
                                    "Characterizing large language model geometry solves toxicity detection and generation.",
                                    "Leace: Perfect linear concept erasure in closed form.",
                                    "Mechanistic interpretability for ai safety–a review.",
                                    "Bloom: A 176b-parameter open-access multilingual language model.",
                                    "Toxic comment classification challenge.",
                                    "Think you have solved question answering? try arc, the ai2 reasoning challenge.",
                                    "Ultrafeedback: Boosting language models with high-quality feedback.",
                                    "Rtp-lx: Can llms evaluate toxicity in multilingual scenarios?",
                                    "Multiparadetox: Extending text detoxification with parallel data to new languages.",
                                    "Exploring methods for cross-lingual text style transfer: The case of text detoxification.",
                                    "Multilingual jailbreak challenges in large language models.",
                                    "Qlora: Efficient finetuning of quantized llms.",
                                    "Identifying elements essential for BERT’s multilinguality.",
                                    "Beyond english-centric multilingual machine translation.",
                                    "A primer on the inner workings of transformer-based language models.",
                                    "RealToxicityPrompts: Evaluating neural toxic degeneration in language models.",
                                    "Transformer feed-forward layers build predictions by promoting concepts in the vocabulary space.",
                                    "Transformer feed-forward layers are key-value memories.",
                                    "How does gpt-2 compute greater-than?: Interpreting mathematical abilities in a pre-trained language model.",
                                    "Deep residual learning for image recognition.",
                                    "Measuring massive multitask language understanding.",
                                    "The curious case of neural text degeneration.",
                                    "Reference-free monolithic preference optimization with odds ratio.",
                                    "mothello: When do cross-lingual representation alignment and cross-lingual transfer emerge in multilingual models?",
                                    "Camels in a changing climate: Enhancing lm adaptation with tulu 2.",
                                    "Polyglotoxicityprompts: Multilingual evaluation of neural toxic degeneration in large language models.",
                                    "A distributional approach to controlled text generation.",
                                    "Understanding the effects of RLHF on LLM generalisation and diversity.",
                                    "Okapi: Instruction-tuned large language models in multiple languages with reinforcement learning from human feedback.",
                                    "A mechanistic understanding of alignment algorithms: A case study on dpo and toxicity.",
                                    "A new generation of perspective api: Efficient multilingual character-level transformers.",
                                    "DExperts: Decoding-time controlled text generation with experts and anti-experts.",
                                    "A safe harbor for ai evaluation and red teaming.",
                                    "Concrete problems in ai safety, revisited.",
                                    "Unintended impacts of llm alignment on global representation.",
                                    "The language barrier: Dissecting safety challenges of llms in multilingual contexts.",
                                    "mgpt: Few-shot learners go multilingual.",
                                    "Llama 2: Open foundation and fine-tuned chat models.",
                                    "Detox: Toxic subspace projection for model editing.",
                                    "Aya model: An instruction finetuned open-access multilingual language model.",
                                    "Cross-lingual knowledge editing in large language models.",
                                    "Detoxifying large language models via knowledge editing.",
                                    "All languages matter: On the multilingual safety of large language models.",
                                    "Assessing the brittleness of safety alignment via pruning and low-rank modifications.",
                                    "Star: Sociotechnical approach to red teaming language models.",
                                    "Reuse your rewards: Reward model transfer for zero-shot cross-lingual alignment.",
                                    "Improving alignment and robustness with short circuiting."
                                ]
                            },
                            "children": [
                                {
                                    "name": "PolygloToxicityPrompts: Multilingual Evaluation of Neural Toxic Degeneration in Large Language Models",
                                    "data": {
                                        "index": 187,
                                        "title": "PolygloToxicityPrompts: Multilingual Evaluation of Neural Toxic Degeneration in Large Language Models",
                                        "publication_date": "2024-08-10",
                                        "references": [
                                            "A survey on data selection for language models.",
                                            "Many-shot jailbreaking.",
                                            "A general theoretical paradigm to understand learning from human preferences.",
                                            "Just say no: Analyzing the stance of neural dialogue generation in offensive contexts.",
                                            "Qwen technical report.",
                                            "On the dangers of stochastic parrots: Can language models be too big?",
                                            "Pythia: A suite for analyzing large language models across training and scaling.",
                                            "Nuanced metrics for measuring unintended bias with real data for text classification.",
                                            "Jailbreaking black box large language models in twenty queries.",
                                            "Deep reinforcement learning from human preferences.",
                                            "Toxicity in multilingual machine translation at scale.",
                                            "Empathy, ways of knowing, and interdependence as mediators of gender differences in attitudes toward hate speech and freedom of speech.",
                                            "Under the surface: Tracking the artifactuality of llm-generated data.",
                                            "Rtp-lx: Can llms evaluate toxicity in multilingual scenarios?.",
                                            "Masterkey: Automated jailbreaking of large language model chatbots.",
                                            "Multilingual jailbreak challenges in large language models.",
                                            "BERT: Pre-training of deep bidirectional transformers for language understanding.",
                                            "Measuring and mitigating unintended bias in text classification.",
                                            "RealToxicityPrompts: Evaluating neural toxic degeneration in language models.",
                                            "Fortifying toxic speech detectors against veiled toxicity.",
                                            "ToxiGen: A large-scale machine-generated dataset for adversarial and implicit hate speech detection.",
                                            "The pile: An 800gb dataset of diverse text for language modeling.",
                                            "Training language models to follow instructions with human feedback.",
                                            "Red teaming language models with language models.",
                                            "Introducing gemini: our largest and most capable ai model.",
                                            "On the challenges of using black-box APIs for toxicity evaluation in research.",
                                            "Crosslingual generalization through multitask finetuning.",
                                            "StereoSet: Measuring stereotypical bias in pretrained language models.",
                                            "CrowS-pairs: A challenge dataset for measuring social biases in masked language models.",
                                            "Toxic bias: Perspective api misreads german as more toxic.",
                                            "An empirical analysis of compute-optimal large language model training.",
                                            "Catastrophic jailbreak of open-source llms via exploiting generation.",
                                            "Social biases in NLP models as barriers for persons with disabilities.",
                                            "Llama guard: Llm-based input-output safeguard for human-ai conversations.",
                                            "Camels in a changing climate: Enhancing lm adaptation with tulu 2.",
                                            "Mistral 7b.",
                                            "Automatically auditing large language models via discrete optimization.",
                                            "Efficient memory management for large language model serving with pagedattention.",
                                            "A new generation of perspective api: Efficient multilingual character-level transformers."
                                        ]
                                    },
                                    "children": []
                                }
                            ]
                        }
                    ]
                },
                {
                    "name": "Jailbreak Vision Language Models via Bi-Modal Adversarial Prompt",
                    "data": {
                        "index": 135,
                        "title": "Jailbreak Vision Language Models via Bi-Modal Adversarial Prompt",
                        "publication_date": "2024-07-01",
                        "references": [
                            "Visual instruction tuning.",
                            "Gemini.",
                            "Visualgpt: Data-efficient adaptation of pretrained language models for image captioning.",
                            "Visual question answering instruction: Unlocking multimodal large language model to domain-specific visual multitasks.",
                            "Image retrieval on real-life images with pre-trained vision-and-language models.",
                            "An image is worth 1000 lies: Adversarial transferability across prompts on vision-language models.",
                            "On the adversarial robustness of multi-modal foundation models.",
                            "On the robustness of large multimodal models against image adversarial attacks.",
                            "Jailbreak in pieces: Compositional adversarial attacks on multi-modal language models.",
                            "Badclip: Dual-embedding guided backdoor attack on multimodal contrastive learning.",
                            "Pre-trained trojan attacks for visual recognition.",
                            "Does few-shot learning suffer from backdoor attacks?",
                            "Poisoned forgery face: Towards backdoor attacks on face forgery detection.",
                            "Vl-trojan: Multimodal instruction backdoor attacks against autoregressive visual language models.",
                            "Poisoning attack against estimating from pairwise comparisons.",
                            "A tale of hodgerank and spectral method: Target attack against rank aggregation is the fixed point of adversarial game.",
                            "Semi-supervised robust training with generalized perturbed neighborhood.",
                            "Nba: defensive distillation for backdoor removal via neural behavior alignment.",
                            "Dlp: towards active defense against backdoor attacks with decoupled learning process.",
                            "GPTFUZZER: red teaming large language models with auto-generated jailbreak prompts.",
                            "Jailbroken: How does LLM safety training fail?",
                            "Jailbreaking black box large language models in twenty queries.",
                            "Are aligned neural networks adversarially aligned?",
                            "LIMA: less is more for alignment.",
                            "On evaluating adversarial robustness of large vision-language models.",
                            "Vision-llms can fool themselves with self-generated typographic attacks.",
                            "Figstep: Jailbreaking large vision-language models via typographic visual prompts.",
                            "Generate more imperceptible adversarial examples for object detection.",
                            "Efficient adversarial attacks for visual object tracking.",
                            "Transferable adversarial attacks for image and video object detection.",
                            "Parallel rectangle flip attack: A query-based black-box attack against object detection.",
                            "A large-scale multiple-objective method for black-box attack against object detection.",
                            "Diversifying the high-level features for better adversarial transferability.",
                            "{X-Adv }: Physical adversarial object attacks against x-ray prohibited item detection.",
                            "Generating transferable 3d adversarial point cloud via random perturbation factorization.",
                            "Improving adversarial transferability by stable diffusion.",
                            "Query-relevant images jailbreak large multi-modal models.",
                            "Chain-of-thought prompting elicits reasoning in large language models.",
                            "Universal and transferable adversarial attacks on aligned language models.",
                            "Chatglm.",
                            "Qwen.",
                            "Ernie bot.",
                            "Jailbroken: How does LLM safety training fail?",
                            "Jailbreak and guard aligned language models with only few in-context demonstrations.",
                            "Tree of attacks: Jailbreaking black-box llms automatically.",
                            "Scalable and transferable black-box jailbreaks for language models via persona modulation.",
                            "Autodan: Generating stealthy jailbreak prompts on aligned large language models.",
                            "Semantic mirror jailbreak: Genetic algorithm based jailbreak prompts against open-source llms.",
                            "Visual adversarial examples jailbreak aligned large language models.",
                            "Jailbreaking attack against multimodal large language model.",
                            "Instructta: Instruction-tuned targeted attack for large vision-language models.",
                            "How robust is google’s bard to adversarial image attacks?",
                            "Fool your (vision and) language model with embarrassingly simple permutations.",
                            "Perceptual-sensitive gan for generating adversarial patches.",
                            "Spatiotemporal attacks for embodied agents.",
                            "Bias-based universal adversarial patch attack for automatic check-out.",
                            "Harnessing perceptual adversarial patches for crowd counting.",
                            "Exploring the relationship between architecture and adversarially robust generalization.",
                            "Minigpt-4: Enhancing vision-language understanding with advanced large language models.",
                            "Learning transferable visual models from natural language supervision.",
                            "Llama: Open and efficient foundation language models.",
                            "Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality.",
                            "Aligning large multimodal models with factually augmented rlhf.",
                            "DRESS: instructing large vision-language models to align and interact with humans via natural language feedback.",
                            "WARM: on the benefits of weight averaged reward models.",
                            "Towards deep learning models resistant to adversarial attacks.",
                            "A survey of chain of thought reasoning: Advances, frontiers and future.",
                            "Large language models are zero-shot reasoners.",
                            "Instructblip: Towards general-purpose vision-language models with instruction tuning.",
                            "Gpt-4 technical report.",
                            "Llama guard: Llm-based input-output safeguard for human-ai conversations.",
                            "Mllm-protector: Ensuring mllm’s safety without hurting performance.",
                            "Attackeval: How to evaluate the effectiveness of jailbreak attacking on large language models.",
                            "Imgtrojan: Jailbreaking vision-language models with ONE image.",
                            "Vision transformer with quadrangle attention.",
                            "Deepinception: Hypnotize large language model to be jailbreaker.",
                            "Jailbreak and guard aligned language models with only few in-context demonstrations.",
                            "Sa-attack: Improving adversarial transferability of vision-language pre-training models via self-augmentation.",
                            "Hide in thicket: Generating imperceptible and rational adversarial perturbations on 3d point clouds.",
                            "Improving robust fairness via balance adversarial training.",
                            "Exploring inconsistent knowledge distillation for object detection with data augmentation."
                        ]
                    },
                    "children": []
                },
                {
                    "name": "SORRY-BENCH: SYSTEMATICALLY EVALUATING LARGE LANGUAGE MODEL SAFETY REFUSAL WARNING",
                    "data": {
                        "index": 239,
                        "title": "SORRY-BENCH: SYSTEMATICALLY EVALUATING LARGE LANGUAGE MODEL SAFETY REFUSAL WARNING",
                        "publication_date": "2025-03-01",
                        "references": [
                            "Jailbreaking leading safety-aligned llms with simple adaptive attacks.",
                            "Introducing Claude.",
                            "Training a helpful and harmless assistant with reinforcement learning from human feedback.",
                            "Language models are few-shot learners.",
                            "Jailbreakbench: An open robustness benchmark for jailbreaking large language models.",
                            "Chatbot arena: An open platform for evaluating llms by human preference.",
                            "A coefficient of agreement for nominal scales.",
                            "Or-bench: An over-refusal benchmark for large language models.",
                            "Towards harmlessness evaluation and analysis for llms with factuality, fairness, toxicity.",
                            "Safe rlhf: Safe reinforcement learning from human feedback.",
                            "Do-not-answer: A dataset for evaluating safeguards in llms.",
                            "Universal and transferable adversarial attacks on aligned language models.",
                            "Alert: A comprehensive benchmark for assessing large language models’ safety through red teaming.",
                            "Llama 2: Open foundation and fine-tuned chat models.",
                            "Measuring massive multitask language understanding.",
                            "Training language models to follow instructions with human feedback.",
                            "Simultaneous translation and understanding.",
                            "Chain-of-thought prompting elicits reasoning in large language models.",
                            "Finetuned language models are zero-shot learners.",
                            "Jaccard index - wikipedia.",
                            "A strongreject for empty jailbreaks.",
                            "Defending chatgpt against jailbreak attack via self-reminders.",
                            "Gpt-4 is too smart to be safe: Stealthy chat with llms via cipher.",
                            "How johnny can persuade llms to jailbreak them: Rethinking persuasion to challenge ai safety by humanizing llms.",
                            "Training language models to follow instructions with human feedback.",
                            "Jailbreaking chatgpt via prompt engineering: An empirical study.",
                            "Jailbreakbench: An open robustness benchmark for jailbreaking large language models.",
                            "A brief overview of prior safety benchmark datasets for (large) language models.",
                            "Judging llm-as-a-judge with mt-bench and chatbot arena.",
                            "Alignbench: Benchmarking chinese alignment of large language models.",
                            "Fine-tuning aligned language models compromises safety, even when users do not intend to!",
                            "Direct preference optimization: Your language model is secretly a reward model.",
                            "Xstest: A test suite for identifying exaggerated safety behaviours in large language models.",
                            "Toxicchat: Unveiling hidden challenges of toxicity detection in real-world user-AI conversation.",
                            "Multilingual jailbreak challenges in large language models.",
                            "Build it break it fix it for dialogue safety: Robustness from adversarial human attack.",
                            "The llama 3 herd of models.",
                            "Real-toxicityprompts: Evaluating neural toxic degeneration in language models.",
                            "BBQ: A hand-built bias benchmark for question answering.",
                            "Or-bench: An over-refusal benchmark for large language models.",
                            "Do-Not-Answer: A dataset for evaluating safeguards in llms.",
                            "Simplesafetytests: a test suite for identifying critical safety risks in large language models.",
                            "FFT: Towards harmlessness evaluation and analysis for llms with factuality, fairness, toxicity.",
                            "Harmbench: A standardized evaluation framework for automated red teaming and robust refusal.",
                            "SALAD-Bench: A hierarchical and comprehensive safety benchmark for large language models.",
                            "StrongREJECT: Manually crafted, filtered from other existing datasets, and generated by LLM via prompt engineering.",
                            "JBB-Behaviors: Half originally and uniquely crafted, half from other existing datasets."
                        ]
                    },
                    "children": [
                        {
                            "name": "ALERT: A Comprehensive Benchmark for Assessing Large Language Models’ Safety through Red Teaming",
                            "data": {
                                "index": 17,
                                "title": "ALERT: A Comprehensive Benchmark for Assessing Large Language Models’ Safety through Red Teaming",
                                "publication_date": "2024-06-24",
                                "references": [
                                    "Persistent anti-muslim bias in large language models",
                                    "The falcon series of open language models",
                                    "Chatgpt: Applications, opportunities, and threats",
                                    "Training a helpful and harmless assistant with reinforcement learning from human feedback",
                                    "On the dangers of stochastic parrots: Can language models be too big?",
                                    "On the opportunities and risks of foundation models",
                                    "Language models are few-shot learners",
                                    "Extracting training data from large language models",
                                    "Ultrafeedback: Boosting language models with high-quality feedback",
                                    "Or-bench: An over-refusal benchmark for large language models",
                                    "Bold: Dataset and metrics for measuring biases in open-ended language generation",
                                    "Latent hatred: A benchmark for understanding implicit hate speech",
                                    "Artificial Intelligence Act EU",
                                    "Bias and fairness in large language models: A survey",
                                    "The capacity for moral self-correction in large language models",
                                    "Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned",
                                    "RealToxicityPrompts: Evaluating neural toxic degeneration in language models",
                                    "Olmo: Accelerating the science of language models",
                                    "Bias runs deep: Implicit reasoning biases in persona-assigned llms",
                                    "An empirical study of metrics to measure representational harms in pre-trained language models",
                                    "Bias testing and mitigation in llm-based code generation",
                                    "Llama guard: Llm-based input-output safeguard for human-ai conversations",
                                    "Mistral 7b",
                                    "Mixtral of experts",
                                    "Textbooks are all you need ii: phi-1.5 technical report",
                                    "Holistic evaluation of language models",
                                    "Toxicchat: Unveiling hidden challenges of toxicity detection in real-world user-ai conversation",
                                    "A safe harbor for ai evaluation and red teaming",
                                    "Analyzing leakage of personally identifiable information in language models",
                                    "Aurora-m: The first open source multilingual language model red-teamed according to the u.s. executive order",
                                    "Biases in large language models: Origins, inventory, and discussion",
                                    "Amplifying limitations, harms and risks of large language models",
                                    "Gpt-4 technical report",
                                    "Is chatgpt a general-purpose natural language process-ing task solver?",
                                    "Direct preference optimization: Your language model is secretly a reward model",
                                    "Dolma: an open corpus of three trillion tokens for language model pretraining research",
                                    "Stanford alpaca: An instruction-following llama model",
                                    "Gemma: Open models based on gemini research and technology",
                                    "Llama: Open and efficient foundation language models",
                                    "Zephyr: Direct distillation of lm alignment",
                                    "Ai regulation: A pro-innovation approach",
                                    "Decodingtrust: A comprehensive assessment of trustworthiness in gpt models",
                                    "Adversarial glue: A multi-task benchmark for robustness evaluation of language models",
                                    "On the robustness of chatgpt: An adversarial and out-of-distribution perspective",
                                    "Ethical and social risks of harm from language models",
                                    "Fact sheet: President biden issues executive order on safe, secure, and trustworthy artificial intelligence",
                                    "Gptfuzzer: Red teaming large language models with auto-generated jailbreak prompts",
                                    "Ethical considerations and policy implications for large language models: Guiding responsible development and deployment",
                                    "Judging llm-as-a-judge with mt-bench and chatbot arena",
                                    "Efficiently programming large language models using sglang",
                                    "Emergent abilities of large language models",
                                    "Judging llm-as-a-judge with mt-bench and chatbot arena",
                                    "Efficiently programming large language models using sglang",
                                    "Emergent abilities of large language models"
                                ]
                            },
                            "children": []
                        }
                    ]
                }
            ]
        },
        {
            "name": "SPA-VL: A Comprehensive Safety Preference Alignment Dataset for Vision Language Models",
            "data": {
                "index": 240,
                "title": "SPA-VL: A Comprehensive Safety Preference Alignment Dataset for Vision Language Models",
                "publication_date": "2025-02-27",
                "references": [
                    "Lmms evaluation",
                    "Anthropic. Claude",
                    "Anthropic. Claude usage policies",
                    "Qwen-vl: A versatile vision-language model for understanding, localization, text reading, and beyond",
                    "Training a helpful and harmless assistant with reinforcement learning from human feedback",
                    "Hallucination of multimodal large language models: A survey",
                    "GQA: A new dataset for real-world visual reasoning and compositional question answering",
                    "Large language models as automated aligners for benchmarking vision-language models",
                    "Prismatic vlms: Investigating the design space of visually-conditioned language models",
                    "RLAIF vs. RLHF: scaling reinforcement learning from human feedback with AI feedback",
                    "Otter: a multi-modal model with in-context instruction tuning",
                    "Seed-bench: Benchmarking multimodal large language models",
                    "Trustllm: Trustworthiness in large language models",
                    "Aligning large multimodal models with factually augmented RLHF",
                    "Lamda: Language models for dialog applications",
                    "How many unicorns are in this image? a safety evaluation benchmark for vision llms",
                    "Inferaligner: Inference-time alignment for harmlessness through cross-model guidance",
                    "Mllm-protector: Ensuring mllm’s safety without hurting performance",
                    "Aligning modalities in vision large language models via preference fine-tuning",
                    "Minigpt-4: Enhancing vision-language understanding with advanced large language models",
                    "Fine-tuning language models from human preferences",
                    "Safety fine-tuning at (almost) no cost: A baseline for vision large language models",
                    "MM-safetybench: A benchmark for safety evaluation of multimodal large language models",
                    "A survey of large language models",
                    "On evaluating adversarial robustness of large vision-language models",
                    "Improving generalization of alignment with human preferences through group invariant learning",
                    "Aligning modalities in vision large language models via preference fine-tuning",
                    "LLaVA"
                ]
            },
            "children": []
        }
    ]
}