[
    {
        "index": 1,
        "title": "Benchmarking Indirect Prompt Injections in Tool-Integrated Large Language Model Agents",
        "publication_date": "2024-08-04",
        "references": [
            "Not what you’ve signed up for: Compromising real-world llm-integrated applications with indirect prompt injection",
            "Gpt-4 technical report",
            "Do as i can, not as i say: Grounding language in robotic affordances",
            "Qwen technical report",
            "Language models are few-shot learners",
            "Orca: Progressive learning from complex explanation traces of gpt-4",
            "Securing LLM Systems Against Prompt Injection | NVIDIA Technical Blog — developer.nvidia.com",
            "Cmmmu: A chinese massive multi-discipline multimodal understanding benchmark",
            "Judging llm-as-a-judge with mt-bench and chatbot arena",
            "Individual comparisons by ranking methods",
            "Intelligent agents: Theory and practice",
            "Amplify-instruct: Synthetically generated diverse multi-turn conversations for effecient llm training",
            "Training language models to follow instructions with human feedback",
            "Large language models can teach themselves to use tools",
            "Struq: Defending against prompt injection with structured queries",
            "Signed-prompt: A new approach to prevent prompt injection attacks against llm-integrated applications",
            "Assessing prompt injection risks in 200+ custom gpts",
            "Exploiting programmatic behavior of llms: Dual-use through standard security attacks",
            "Universal and transferable adversarial attacks on aligned language models",
            "Tensor trust: Interpretable prompt injection attacks from an online game",
            "Ignoring previous prompt: Attack techniques for language models",
            "Llm-powered autonomous agents",
            "Llama 2: Open foundation and fine-tuned chat models",
            "React: Synergizing reasoning and acting in language models"
        ]
    },
    {
        "index": 2,
        "title": "A Trembling House of Cards? Mapping Adversarial Attacks against Language Agents",
        "publication_date": "2024-02-15",
        "references": [
            "Not what you've signed up for: Compromising real-world llm-integrated applications with indirect prompt injection",
            "Do as i can, not as i say: Grounding language in robotic affordances",
            "Phishing environments, techniques, and countermeasures: A survey",
            "Jailbreak chat",
            "Working memory",
            "(ab) using images and sounds for indirect instruction injection in multi-modal llms",
            "Constitutional ai: Harmlessness from ai feedback",
            "Are aligned neural networks adversarially aligned?",
            "LangChain",
            "Backdoor learning on sequence to sequence models",
            "Backdoor attacks and countermeasures in natural language processing models: A comprehensive security review",
            "Training verifiers to solve math word problems",
            "What are the differences between long-term, short-term, and working memory?",
            "How to jailbreak chatgpt",
            "Reading in the brain: The new science of how we read",
            "Mind2web: Towards a generalist agent for the web",
            "Self-collaboration code generation via chatgpt",
            "The faiss library",
            "Language model agents suffer from compositional generalization in web automation",
            "Multimodal web navigation with instruction-finetuned foundation models",
            "Pal: Program-aided language models",
            "Cyber elections in the digital age: Threats and opportunities of technology for electoral integrity",
            "Metagpt: Meta programming for a multi-agent collaborative framework",
            "Catastrophic jailbreak of open-source llms via exploiting generation",
            "Sleeper agents: Training deceptive llms that persist through safety training",
            "Study examines the impact of fake online reviews on sales",
            "Prompt packer: Deceiving llms through compositional instruction with hidden attacks",
            "Exploiting missing value patterns for a backdoor attack on machine learning models of electronic health records: Development and validation study",
            "Billion-scale similarity search with GPUs",
            "In search of memory: The emergence of a new science of mind",
            "Mrkl systems: A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning",
            "Tptu-v2: Boosting task planning and tool usage of large language model-based agents in real-world systems",
            "Pix2struct: Screen-shot parsing as pretraining for visual language understanding",
            "Camel: Communicative agents for\" mind\" exploration of large scale language model society",
            "API-bank: A comprehensive benchmark for tool-augmented LLMs",
            "Tool-augmented language models",
            "Large language models can be guided to evade ai-generated text detection",
            "Efficient and robust approximate nearest neighbor search using hierarchical navigable small world graphs",
            "How trustworthy are open-source llms? an assessment under malicious demonstrations shows their vulnerabilities",
            "Universal adversarial perturbations",
            "Babyagi: A website for advanced chat-based language models",
            "Webgpt: Browser-assisted question-answering with human feedback",
            "Talm: Tool augmented language models",
            "Gorilla: Large language model connected with massive apis",
            "Discussion on gpt-4, prompt injection, and llms",
            "Discovering language model behaviors with model-written evaluations",
            "Ignore previous prompt: Attack techniques for language models",
            "E-commerce manipulation: The fake review problem",
            "Combating adversarial misspellings with robust word recognition",
            "Visual adversarial examples jailbreak aligned large language models",
            "Communicative agents for software development",
            "Tool learning with foundation models",
            "Toolllm: Facilitating large language models to master 16000+ real-world apis",
            "Tptu: Large language model-based ai agents for task planning and tool usage",
            "Identifying the risks of lm agents with an lm-emulated sandbox",
            "Toolformer: Language models can teach themselves to use tools",
            "Exploiting cloze questions for few shot text classification and natural language inference",
            "From pixels to ui actions: Learning to follow instructions via graphical user interfaces",
            "Jailbreak in pieces: Compositional adversarial attacks on multi-modal language models",
            "Reflexion: Language agents with verbal reinforcement learning",
            "On the exploitability of instruction tuning",
            "AutoGPT",
            "Advanced api security",
            "Restgpt: Connecting large language models with real-world restful apis",
            "Language agents: a critical evolutionary step of artificial intelligence",
            "Cognitive architectures for language agents",
            "Lamda: Language models for dialog applications",
            "Evil geniuses: Delving into the safety of llm-based agents",
            "Tensor trust: Interpretable prompt injection attacks from an online game",
            "Poisoning language models during instruction tuning",
            "Can ChatGPT defend its belief in truth? evaluating LLM reasoning via debate",
            "Mllm-tool: A multimodal large language model for tool agent learning",
            "V oyager: An open-ended embodied agent with large language models",
            "Adversarial demonstration attacks on large language models",
            "Self-consistency improves chain of thought reasoning in language models",
            "Larger language models do in-context learning differently",
            "Jailbreak and guard aligned language models with only few in-context demonstrations",
            "Hard prompts made easy: Gradient-based discrete optimization for prompt tuning and discovery",
            "Llm-powered autonomous agents",
            "Multi-modal prompt injection",
            "Autogen: Enabling next-gen llm applications via multi-agent conversation framework",
            "The rise and potential of large language model based agents: A survey",
            "Badchain: Backdoor chain-of-thought prompting for large language models",
            "Adaptive chameleon or stubborn sloth: Unraveling the behavior of large language models in knowledge conflicts",
            "Instructions as backdoors: Backdoor vulnerabilities of instruction tuning for large language models",
            "BITE: Textual backdoor attacks with iterative trigger injection",
            "React: Synergizing reasoning and acting in language models",
            "Webarena: A realistic web environment for building autonomous agents",
            "Toolchain*: Efficient action space navigation in large language models with a* search",
            "Universal vulnerabilities in large language models: In-context learning backdoor attacks",
            "Psysafe: A comprehensive framework for psychological-based attack, defense, and evaluation of multi-agent system safety"
        ]
    },
    {
        "index": 3,
        "title": "Agent Smith: A Single Image Can Jailbreak One Million Multimodal LLM Agents Exponentially Fast",
        "publication_date": "2024-06-03",
        "references": [
            "Flamingo: a visual language model for few-shot learning",
            "Avert: An autonomous multi-robot system for vehicle extraction and transportation",
            "Openflamingo: An open-source framework for training large autoregressive vision-language models",
            "Training a helpful and harmless assistant with reinforcement learning from human feedback",
            "Image hijacks: Adversarial images can control generative models at runtime",
            "Rt-2: Vision-language-action models transfer web knowledge to robotic control",
            "Language models are few-shot learners",
            "Adversarial patch",
            "Sparks of artificial general intelligence: Early experiments with gpt-4",
            "Collaborative multi-robot exploration",
            "Are aligned neural networks adversarially aligned?",
            "Chateval: Towards better llm-based evaluators through multi-agent debate",
            "Jailbreaking black box large language models in twenty queries",
            "Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors in agents",
            "Catasrophic jailbreak of open-source llms via exploiting generation",
            "Encouraging divergent thinking in large language models through multi-agent debate",
            "The artbench dataset: Benchmarking generative models with artworks",
            "Llm+ p: Empowering large language models with optimal planning proficiency",
            "Improved baselines with visual instruction tuning",
            "Visual instruction tuning",
            "Llava-plus: Learning to use tools for creating multimodal agents",
            "Autodan: Generating stealthy jailbreak prompts on aligned large language models",
            "Jailbreaking chatgpt via prompt engineering: An empirical study",
            "Frequency domain model augmentation for adversarial attack",
            "Towards deep learning models resistant to adversarial attacks",
            "Society of mind",
            "Studious bob fight back against jailbreaking via prompt adversarial tuning",
            "Diffusion models for adversarial purification",
            "Universal and transferable adversarial attacks on aligned language models",
            "Hugginggpt: Solving ai tasks with chatgpt and its friends in huggingface",
            "Identifying the risks of lm agents with an lm-emulated sandbox",
            "Red teaming language models with language models",
            "Visual adversarial examples jailbreak aligned large language models",
            "Fine-tuning aligned language models compromises safety, even when users do not intend to!",
            "Communicative agents for software development",
            "Learning transferable visual models from natural language supervision",
            "Toolformer: Language models can teach themselves to use tools",
            "On the adversarial robustness of multi-modal foundation models",
            "Jailbreak in pieces: Compositional adversarial attacks on multi-modal language models",
            "Memgpt: Towards llms as operating systems",
            "Generative agents: Interactive simulacra of human behavior",
            "Cognitive architectures for language agents",
            "Gemini: a family of highly capable multimodal models",
            "Evil geniuses: Delving into the safety of llm-based agents",
            "Towards deep learning models resistant to adversarial attacks",
            "Identifying the risks of lm agents with an lm-emulated sandbox",
            "Tensor trust: Interpretable prompt injection attacks from an online game",
            "How many unicorns are in this image? a safety evaluation benchmark for vision llms",
            "Gpt-4 is too smart to be safe: Stealthy chat with llms via cipher",
            "Adversarial framing for image and video classification",
            "Building cooperative embodied agents modularly with large language models",
            "Towards adversarial attack on vision-language pre-training models",
            "Android in the zoo: Chain-of-action-thought for gui agents",
            "On evaluating adversarial robustness of large vision-language models",
            "Intriguing properties of data attribution on diffusion models",
            "Webarena: A realistic web environment for building autonomous agents"
        ]
    },
    {
        "index": 4,
        "title": "AGENT POISON: Red-teaming LLM Agents via Poisoning Memory or Knowledge Bases",
        "publication_date": "2024-07-17",
        "references": [
            "Conversational health agents: A personalized llm-powered agent framework.",
            "Detecting language model attacks with perplexity.",
            "Neural network learning: Theoretical foundations, volume 9.",
            "Poisoning web-scale training datasets is practical.",
            "Pandora: Detailed llm jailbreaking via collaborated phishing agents with decomposed reasoning.",
            "Personalized autonomous driving with large language models: Field experiments, 2024.",
            "Bert: Pre-training of deep bidirectional transformers for language understanding.",
            "Hotflip: White-box adversarial examples for text classification.",
            "Pal: Program-aided language models.",
            "Cold-attack: Jailbreaking llms with stealthiness and controllability.",
            "Retrieval augmented language model pre-training.",
            "Surrealdriver: Designing generative driver agent simulation framework in urban contexts based on large language model, 2023.",
            "Backdoor attacks for in-context learning with language models.",
            "Dense passage retrieval for open-domain question answering.",
            "Certifying llm safety against adversarial prompting.",
            "Paperqa: Retrieval-augmented generative agent for scientific research.",
            "Latent retrieval for weakly supervised open domain question answering.",
            "Retrieval-augmented generation for knowledge-intensive nlp tasks.",
            "Agent hospital: A simulacrum of hospital with evolvable medical agents, 2024.",
            "Autodan: Generating stealthy jailbreak prompts on aligned large language models.",
            "A language agent for autonomous driving.",
            "Advprompter: Fast adaptive adversarial prompting for llms.",
            "The probabilistic relevance framework: Bm25 and beyond.",
            "Ehragent: Code empowers large language models for few-shot complex tabular reasoning on electronic health records, 2024.",
            "Reflexion: an autonomous agent with dynamic memory and self-reflection.",
            "Towards conversational diagnostic ai, 2024.",
            "Certifiably robust rag against retrieval corruption.",
            "Badchain: Backdoor chain-of-thought prompting for large language models.",
            "Approximate nearest neighbor negative contrastive learning for dense text retrieval.",
            "Llm agents for psychology: A study on gamified assessments, 2024.",
            "Watch out for your agents! investigating backdoor threats to llm-based agents.",
            "Poisonprompt: Backdoor attack on prompt-based large language models.",
            "React: Synergizing reasoning and acting in language models.",
            "Finmem: A performance-enhanced llm trading agent with layered memory and character design, 2023.",
            "Rag-driver: Generalisable driving explanations with retrieval-augmented in-context learning in multi-modal large language model.",
            "Retrieve anything to augment large language models.",
            "Navigation as attackers wish? towards building byzantine-robust embodied agents under federated learning.",
            "Poisoning retrieval corpora by injecting adversarial passages.",
            "Universal and transferable adversarial attacks on aligned language models.",
            "Poisonedrag: Knowledge poisoning attacks to retrieval-augmented generation of large language models."
        ]
    },
    {
        "index": 5,
        "title": "Arondight: Red Teaming Large Vision Language Models with Auto-generated Multi-modal Jailbreak Prompts",
        "publication_date": "2024-01-01",
        "references": [
            "Bing Chat",
            "Google Bard",
            "RNIE Bot",
            "Spark",
            "T5-3B",
            "Toxicity category rating",
            "Flamingo: a visual language model for few-shot learning",
            "Dos and don'ts of machine learning in computer security",
            "Spinning language models: Risks of propaganda-as-a-service and countermeasures",
            "Qwen-vl: A frontier large vision-language model with versatile abilities",
            "Red-teaming large language models using chain of utterances for safety-alignment",
            "Are aligned neural networks adversarially aligned?",
            "Adversarial examples are not easily detected: Bypassing ten detection methods",
            "Jailbreaking Black Box Large Language Models in Twenty Queries",
            "Jailbreaker: Automated jailbreak across multiple large language model chatbots",
            "GLM: General Language Model Pretraining with Autoregressive Blank Infilling",
            "Llama-adapter v2: Parameter-efficient visual instruction model",
            "BAE: BERT-based Adversarial Examples for Text Classification",
            "FigStep: Jailbreaking Large Vision-language Models via Typographic Visual Prompts",
            "You only prompt once: On the capabilities of prompt learning on large language models to tackle toxic content",
            "Curiosity-driven Red-teaming for Large Language Models",
            "Increasing text diversity via online multi-label recognition for vision-language pre-training",
            "Training-free Lexical Backdoor Attacks on Language Models",
            "BeaverTails: Towards Improved Safety Alignment of LLM via a Human-Preference Dataset",
            "Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation",
            "Red teaming visual language models",
            "DeepInception: Hypnotize Large Language Model to Be Jailbreaker",
            "Mvptr: Multi-level semantic alignment for vision-language pre-training via multi-stage learning",
            "Reducing the vision and language bias for temporal sentence grounding",
            "AutoDAN: Generating Stealthy Jailbreak Prompts on Aligned Large Language Models",
            "Query-Relevant Images Jailbreak Large Multi-Modal Models",
            "Prompt Injection attack against LLM-integrated Applications",
            "Jailbreaking chatgpt via prompt engineering: An empirical study",
            "Food-500 Cap: A Fine-Grained Food Caption Benchmark for Evaluating Vision-Language Models",
            "GPT-4 Technical Report",
            "Sok: Security and privacy in machine learning",
            "Red Teaming Language Models with Language Models",
            "Visual adversarial examples jailbreak aligned large language models",
            "Latent jailbreak: A benchmark for evaluating text safety and output robustness of large language models",
            "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "On the Adversarial Robustness of Vision Transformers",
            "Survey of Vulnerabilities in Large Language Models Revealed by Adversarial Attacks",
            "Llama 2: Open foundation and fine-tuned chat models",
            "Multimodal few-shot learning with frozen language models",
            "How Many Unicorns Are in This Image? A Safety Evaluation Benchmark for Vision LLMs",
            "Jailbroken: How Does LLM Safety Training Fail?",
            "Jailbreak and guard aligned language models with only few in-context demonstrations",
            "SneakyPrompt: Jailbreaking Text-to-image Generative Models",
            "Gptfuzzer: Red teaming large language models with auto-generated jailbreak prompts",
            "Towards Adversarial Attack on Vision-Language Pre-Training Models",
            "BERTScore: Evaluating Text Generation with BERT",
            "Fine-mixing: Mitigating Backdoors in Fine-tuned Language Models",
            "Minigpt-4: Enhancing vision-language understanding with advanced large language models",
            "Universal and transferable adversarial attacks on aligned language models"
        ]
    },
    {
        "index": 6,
        "title": "ATHENA : Safe Autonomous Agents with Verbal Contrastive Learning",
        "publication_date": "2024-08-20",
        "references": [
            "Language models are few-shot learners.",
            "Openagi: When llm meets domain experts.",
            "Mistral 7b.",
            "Self-refine: Iterative refinement with self-feedback.",
            "Interrater reliability: the kappa statistic.",
            "Llama 3 model card.",
            "Webgpt: Browser-assisted question-answering with human feedback, 2021.",
            "Identifying the risks of lm agents with an lm-emulated sandbox.",
            "Toolformer: Language models can teach themselves to use tools.",
            "Hugging-gpt: Solving ai tasks with chatgpt and its friends in hugging face.",
            "Reflexion: Language agents with verbal reinforcement learning.",
            "AutoGPT.",
            "Mpnet: Masked and permuted pre-training for language understanding.",
            "Cognitive architectures for language agents.",
            "Gemini: a family of highly capable multimodal models.",
            "Llama: Open and efficient foundation language models.",
            "Self-consistency improves chain of thought reasoning in language models.",
            "Chain-of-thought prompting elicits reasoning in large language models.",
            "Autogen: Enabling next-gen llm applications via multi-agent conversation framework.",
            "React: Synergizing reasoning and acting in language models.",
            "R-judge: Benchmarking safety risk awareness for llm agents.",
            "Expel: Llm agents are experiential learners.",
            "A survey of large language models."
        ]
    },
    {
        "index": 7,
        "title": "AutoDefense: Multi-Agent LLM Defense against Jailbreak Attacks",
        "publication_date": "2024-03-02",
        "references": [
            "Gpt-4 technical report",
            "Detecting language model attacks with perplexity",
            "Constitutional ai: Harmlessness from ai feedback",
            "Language models are few-shot learners",
            "Defending against alignment-breaking attacks via robustly aligned llm",
            "Jailbreaking black box large language models in twenty queries",
            "Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality",
            "Attack prompt generation for red teaming and defending large language models",
            "Multilingual jailbreak challenges in large language models",
            "Anticipating safety issues in e2e conversational ai: Framework and tooling",
            "Build it break it fix it for dialogue safety: Robustness from adversarial human attack",
            "Improving factuality and reasoning in language models through multi-agent debate",
            "The capacity for moral self-correction in large language models",
            "Meta programming for multi-agent collaborative framework",
            "Llama guard: Llm-based input-output safeguard for human-ai conversations",
            "Baseline defenses for adversarial attacks against aligned language models",
            "Mixtral of experts",
            "The impact of reasoning step length on large language models",
            "Automatically auditing large language models via discrete optimization",
            "Exploring social bias in chatbots using stereotype knowledge",
            "Camel: Communicative agents for\" mind\" exploration of large scale language model society",
            "Multi-step jailbreaking privacy attacks on chatgpt",
            "Encouraging divergent thinking in large language models through multi-agent debate",
            "Jailbreaking chatgpt via prompt engineering: An empirical study",
            "Prompt injection attacks and defenses in llm-integrated applications",
            "Black box adversarial prompting for foundation models",
            "Tree of attacks: Jailbreaking black-box llms automatically",
            "Gpt-4 technical report",
            "Training language models to follow instructions with human feedback",
            "Bergeron: Combating adversarial attacks through a conscience-based alignment framework",
            "Defending chatgpt against jailbreak attack via self-reminders",
            "Jailbreak and guard aligned language models with only few in-context demonstrations",
            "Autogen: Enabling next-gen llm applications via multi-agent conversation framework",
            "Intention analysis prompting makes large language models a good jailbreak defender",
            "Defending large language models against jailbreaking attacks through goal prioritization",
            "Judging llm-as-a-judge with mt-bench and chatbot arena",
            "Universal and transferable adversarial attacks on aligned language models"
        ]
    },
    {
        "index": 8,
        "title": "AVALON'S GAME OF THOUGHTS: BATTLE AGAINST DECEPTION THROUGH RECURSIVE CONTEMPLATION",
        "publication_date": "2023-10-24",
        "references": [
            "Playing repeated games with large language models",
            "Introducing claude",
            "The internal state of an llm knows when its lying",
            "Mastering the game of no-press diplomacy via human-regularized reinforcement learning and planning",
            "A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity",
            "Dota 2 with large scale deep reinforcement learning",
            "Emergent autonomous scientific research capabilities of large language models",
            "Chemcrow: Augmenting large-language models with chemistry tools",
            "Human-level performance in 3d multiplayer games with population-based reinforcement learning",
            "Superhuman ai for multiplayer poker",
            "Language models are few-shot learners",
            "Deep reinforcement learning from human preferences",
            "Human-level play in the game of diplomacy by combining language models with strategic reasoning",
            "Improving language model negotiation with self-play and in-context learning from ai feedback",
            "Think again: The power of knowing what you don’t know",
            "A real-world webagent with planning, long context understanding, and program synthesis",
            "Scaling up and distilling down: Language-guided robot skill acquisition",
            "Deception abilities emerged in large language models",
            "Grandmaster level in starcraft ii using multi-agent reinforcement learning",
            "The spread of true and false news online",
            "V oyager: An open-ended embodied agent with large language models",
            "Augmenting language models with long-term memory",
            "Chain-of-thought prompting elicits reasoning in large language models",
            "Fundamental limitations of alignment in large language models",
            "Llama: Open and efficient foundation language models",
            "Llama 2: Open foundation and fine-tuned chat models",
            "Generative agents: Interactive simulacra of human behavior",
            "Training language models to follow instructions with human feedback",
            "Language models as knowledge bases?",
            "Communicative agents for software development",
            "Effect of subjective perspective taking during simulation of action: a pet investigation of agency",
            "Finding friend and foe in multi-agent games",
            "Model evaluation for extreme risks",
            "Playing the werewolf game with artificial intelligence for language understanding",
            "Reflexion: Language agents with verbal reinforcement learning",
            "Perspective taking as a mechanism for children’s developing preferences for equitable distributions",
            "Deception is associated with reduced social connection",
            "Learning to summarize with human feedback",
            "Do the rewards justify the means? measuring trade-offs between rewards and ethical behavior in the machiavelli benchmark"
        ]
    },
    {
        "index": 9,
        "title": "Chain of Thought Empowers Transformers to Solve Inherently Serial Problems",
        "publication_date": "2024-05-23",
        "references": [
            "Gpt-4technicalreport.",
            "Palm 2 technical report.",
            "Layer normalization",
            "Tighter bounds on the expressivity of transformer encoders",
            "On the turing completeness of modern neural network architectures",
            "Attention is turing complete",
            "Time-lock puzzles and timed-release crypto",
            "Mathematical discoveries from program search with large language models",
            "Llama 2: Open foundation and fine-tuned chat models",
            "Solving olympiad geometry without human demonstrations",
            "What does bert look at? an analysis of bert's attention",
            "Relativized circuit complexity",
            "Inductive biases and variable creation in self-attention mechanisms",
            "Universal transformers",
            "Onlayernormalizationinthetransformerarchitecture",
            "Thinkingliketransformers",
            "Large language models are zero-shot reasoners",
            "Show your work: Scratchpads for intermediate computation with language models",
            "Circuits and local computation",
            "Self-attention networks can process bounded hierarchical languages",
            "On the power of saturated transformers: A view from circuit complexity",
            "Saturated transformers are constant-depth threshold circuits",
            "The parallelism tradeoff: Limitations of log-precision transformers",
            "Algebraic theory of machines. i. prime decomposition theorem for finite semigroups and machines",
            "Circuits and local computation",
            "Counter-FreeAutomata(MITresearchmonographno.65)",
            "Looped transformers as programmable computers",
            "What does every computer scientist should know about floating-point arithmetic",
            "Theoretical limitations of self-attention in neural sequence models",
            "Formallanguagerecognitionbyhardattentiontransformers: Perspectivesfromcircuitcomplexity",
            "Inductive biases and variable creation in self-attention mechanisms",
            "Inductive biases and variable creation in self-attention mechanisms",
            "Relativized circuit complexity",
            "On the Krohn-Rhodes cascaded decomposition theorem",
            "Onlayernormalizationinthetransformerarchitecture",
            "Attention is turing complete",
            "Scaling instruction-finetuned language models",
            "Neural Machine Translation by Jointly Learning to Align and Translate",
            "The Journal of Machine Learning Research",
            "Scaling instruction-finetuned language models",
            "Scaling instruction-finetuned language models",
            "Prompt programming for large language models: Beyond the few-shot paradigm",
            "What does every computer scientist should know about floating-point arithmetic",
            "Tight bounds on the expressivity of transformer encoders"
        ]
    },
    {
        "index": 10,
        "title": "Cooperation, Competition, and Maliciousness: LLM-Stakeholders Interactive Negotiation",
        "publication_date": "2023-09-17",
        "references": [
            "Playing repeated games with large language models",
            "Language models as agent models",
            "Gemini: a family of highly capable multimodal models",
            "Foundational challenges in assuring alignment and safety of large language models",
            "Language models are few-shot learners",
            "Evaluating language model agency through negotiations",
            "Large language model capture-the-flag (LLM CTF) competition @ SaTML 2024",
            "Social iqa: Commonsense reasoning about social interactions",
            "Neural theory-of-mind? on the limits of social intelligence in large lms",
            "How walmart automated supplier negotiations",
            "Negotiate better outcomes and reduce risk across high-volume enterprise contracts with ai-powered insights",
            "The rise and potential of large language model based agents: A survey",
            "React: Synergizing reasoning and acting in language models",
            "Can llms keep a secret? testing privacy implications of language models via contextual integrity theory",
            "Agentic design patterns part 5: Multi-agent collaboration",
            "Introducing gpts",
            "Gpt-4 technical report",
            "Autonomous negotiations for companies with revenue over $5 billion",
            "Ai deception: A survey of examples, risks, and potential solutions",
            "Commonsenseqa: A question answering challenge targeting commonsense knowledge",
            "Llama 2: Open foundation and fine-tuned chat models",
            "Large language models fail on trivial alterations to theory-of-mind tasks",
            "Chain-of-thought prompting elicits reasoning in large language models",
            "Beyond the imitation game: Quantifying and extrapolating the capabilities of language models",
            "Scorable games: A better way to teach negotiation",
            "Using simulations to teach negotiation: Pedagogical theory and practice",
            "The question answering challenge targeting commonsense knowledge",
            "Open foundation and fine-tuned chat models",
            "Introducing meta Llama 3: The most capable openly available LLM to date",
            "Reinventing search with a new ai-powered microsoft bing and edge, your copilot for the web",
            "Introducing microsoft 365 copilot – your copilot for work",
            "Testing privacy implications of language models via contextual integrity theory",
            "Agentic design patterns part 5: Multi-agent collaboration",
            "Chatgpt plugins",
            "Introducing gpts",
            "Gpt-4 technical report",
            "Autonomous negotiations for companies with revenue over $5 billion",
            "Ai deception: A survey of examples, risks, and potential solutions",
            "The question answering challenge targeting commonsense knowledge",
            "Open foundation and fine-tuned chat models",
            "Introducing meta Llama 3: The most capable openly available LLM to date",
            "Reinventing search with a new ai-powered microsoft bing and edge, your copilot for the web",
            "Introducing microsoft 365 copilot – your copilot for work",
            "Testing privacy implications of language models via contextual integrity theory",
            "Agentic design patterns part 5: Multi-agent collaboration",
            "Chatgpt plugins",
            "Introducing gpts",
            "Gpt-4 technical report",
            "Autonomous negotiations for companies with revenue over $5 billion",
            "Ai deception: A survey of examples, risks, and potential solutions",
            "The question answering challenge targeting commonsense knowledge",
            "Open foundation and fine-tuned chat models",
            "Introducing meta Llama 3: The most capable openly available LLM to date",
            "Reinventing search with a new ai-powered microsoft bing and edge, your copilot for the web",
            "Introducing microsoft 365 copilot – your copilot for work",
            "Testing privacy implications of language models via contextual integrity theory",
            "Agentic design patterns part 5: Multi-agent collaboration",
            "Chatgpt plugins",
            "Introducing gpts",
            "Gpt-4 technical report",
            "Autonomous negotiations for companies with revenue over $5 billion",
            "Ai deception: A survey of examples, risks, and potential solutions"
        ]
    },
    {
        "index": 11,
        "title": "Current state of LLM Risks and AI Guardrails",
        "publication_date": "2024-06-16",
        "references": [
            "Knowledge graphs as context sources for llm-based explanations of learning recommendations.",
            "Reliable, adaptable, and attributable language models with retrieval.",
            "Intentional biases in llm responses.",
            "Harms from increasingly agentic algorithmic systems.",
            "Adversarial robustness: From self-supervised pre-training to fine-tuning.",
            "Universal self-consistency for large language model generation.",
            "Prompt injection: Parameterization of fixed inputs.",
            "Breaking the bias: Gender fairness in llms using prompt engineering and in-context learning.",
            "Fine-tuning bert models for intent recognition using a frequency cut-off strategy for domain-specific vocabulary extension.",
            "Mixture of experts models.",
            "An empirical study of llm-as-a-judge for llm evaluation: Fine-tuned judge models are task-specific classifiers.",
            "Llama guard: Llm-based input-output safeguard for human-ai conversations.",
            "A simple fine-tuning is all you need: Towards robust deep learning via adversarial fine-tuning.",
            "Counterexample guided inductive synthesis using large language models and satisfiability solving.",
            "Survey of hallucination in natural language generation.",
            "Unfamiliar finetuning examples control how language models hallucinate.",
            "Self-consistency; a theory of personality.",
            "Self–other moral bias: Evidence from implicit measures and the word-embedding association test.",
            "Stablept: Towards stable prompting for few-shot learning via input separation.",
            "Fairness-guided few-shot prompting for large language models.",
            "Beyond accuracy: Evaluating self-consistency of code llms.",
            "Rethinking the role of demonstrations: What makes in-context learning work?",
            "Stereoset: Measuring stereotypical bias in pretrained language models.",
            "Domain adaptive transfer learning with specialist models.",
            "Instruction tuning with gpt-4.",
            "A review on fairness in machine learning.",
            "Word associations are formed incidentally during sentential semantic integration.",
            "Semantic cosine similarity.",
            "NeMo guardrails: A toolkit for controllable and safe LLM applications with programmable rails.",
            "Gender bias in coreference resolution.",
            "Towards faithful and robust llm specialists for evidence-based question-answering.",
            "In-context learning agents are asymmetric belief updaters.",
            "Membership inference attacks against machine learning models.",
            "Fairrag: Fair human generation via fair retrieval augmentation.",
            "Systematic biases in llm simulations of debates.",
            "From human experts to machines: An llm supported approach to ontology and knowledge graph construction.",
            "Decaf: Generating fair synthetic data using causally-aware generative networks.",
            "Assessing bias in llm-generated synthetic datasets: The case of german voter behavior.",
            "Unveiling the implicit toxicity in large language models.",
            "Hallucination is inevitable: An innate limitation of large language models.",
            "Corrective retrieval augmented generation.",
            "Matplotagent: Method and evaluation for llm-based agentic scientific data visualization.",
            "Evaluating interfaced llm bias.",
            "Judging llm-as-a-judge with mt-bench and chatbot arena.",
            "Is\" a helpful assistant\" the best role for large language models? a systematic evaluation of social roles in system prompts.",
            "Toolqa: A dataset for llm question answering with external tools."
        ]
    },
    {
        "index": 12,
        "title": "Deception in Reinforced Autonomous Agents: The Unconventional Rabbit Hat Trick in Legislation",
        "publication_date": "2024-05-07",
        "references": [
            "The internal state of an llm knows when it’s lying",
            "Rank analysis of incomplete block designs: I. the method of paired comparisons",
            "Superhuman ai for multiplayer poker",
            "Toxicity in chatgpt: Analyzing persona-assigned language models",
            "'bill summary us'",
            "'godfather of ai' warns that ai may figure out how to kill people",
            "The Surprising Creativity of Digital Evolution: A Collection of Anecdotes from the Evolutionary Computation and Artificial Life Research Communities",
            "Self-refine: Iterative refinement with self-feedback",
            "Large language models as corporate lobbyists",
            "Hoodwinked: Deception and cooperation in a text-based game for language models",
            "Do the rewards justify the means? measuring trade-offs between rewards and ethical behavior in the machiavelli benchmark",
            "Generative agents: Interactive simulacra of human behavior",
            "Discovering language model behaviors with model-written evaluations",
            "U.s. congress: Bulk data on bills",
            "Deceptive Autonomous Agents",
            "Emergent deception and skepticism via theory of mind",
            "Speech Acts: An Essay in the Philosophy of Language",
            "Reflexion: Language agents with verbal reinforcement learning",
            "Language models don’t always say what they think: Unfaithful explanations in chain-of-thought prompting",
            "Large language models can be used to estimate the latent positions of politicians",
            "React: Synergizing reasoning and acting in language models",
            "Representation engineering: A top-down approach to ai transparency"
        ]
    },
    {
        "index": 13,
        "title": "Divide-and-Conquer Attack: Harnessing the Power of LLM to Bypass Safety Filters of Text-to-Image Models",
        "publication_date": "2023-12-07",
        "references": [
            "DALL-E 3.",
            "Midjourney.",
            "Hierarchical text-conditional image generation with clip latents",
            "High-resolution image synthesis with latent diffusion models",
            "Denoising diffusion probabilistic models",
            "Language models are few-shot learners",
            "Attention is all you need",
            "Training language models to follow instructions with human feedback",
            "Palm: Scaling language modeling with pathways",
            "GPT-4 technical report",
            "The Rise of Ethical Concerns about AI Content Creation: A Call to Action",
            "How well can text-to-image generative models understand ethical natural language interventions?",
            "Midjourney’s banned words policy",
            "DALL ·E 3 system card",
            "Red-teaming the stable diffusion safety filter",
            "Sneakyprompt: Evaluating robustness of text-to-image generative models’ safety filters",
            "Learning transferable visual models from natural language supervision",
            "Midjourney’s Prompts Engineering",
            "OpenAI API Pricing",
            "ChatGLM API Pricing",
            "TongYiQianWen 14B API Pricing",
            "TongYiQianWen Max API Pricing",
            "Spark API Pricing",
            "DALL-E 2",
            "A survey of generative ai applications",
            "Photorealistic text-to-image diffusion models with deep language understanding",
            "OpenAI ChatGPT",
            "GPT-4",
            "Explaining and harnessing adversarial examples",
            "Towards evaluating the robustness of neural networks",
            "Adversarial examples in the physical world",
            "Deep learning models for electrocardiograms are susceptible to adversarial attack",
            "Ecgadv: Generating adversarial electrocardiogram to misguide arrhythmia classification system",
            "Textbugger: Generating adversarial text against real-world applications",
            "Is bert really robust? a strong baseline for natural language attack on text classification and entailment",
            "Bae: Bert-based adversarial examples for text classification",
            "Adversarial attacks on image generation with made-up words",
            "Adversarial prompting for black box foundation models",
            "Unsafe diffusion: On the generation of unsafe images and hateful memes from text-to-image models",
            "Deep reinforcement learning from human preferences",
            "The capacity for moral self-correction in large language models",
            "A holistic approach to undesired content detection in the real world",
            "SuperCLUE",
            "Chatbot Arena",
            "OpenCompass",
            "Surrogateprompt: Bypassing the safety filter of text-to-image models via substitution",
            "CLIP Repo",
            "Prompt-specific poisoning attacks on text-to-image generative models",
            "Nsfw words list on github",
            "Nsfw text classifier on hugging face",
            "Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter",
            "Nsfw gpt",
            "Nsfw image dataset",
            "Red teaming language models with language models",
            "Prompt-specific poisoning attacks on text-to-image generative models",
            "Nsfw words list on github",
            "Nsfw text classifier on hugging face",
            "Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter",
            "Nsfw gpt",
            "Nsfw image dataset",
            "Red teaming language models with language models"
        ]
    },
    {
        "index": 15,
        "title": "IDENTIFYING THE RISKS OF LM AGENTS WITH AN LM-EMULATED SANDBOX",
        "publication_date": "2024-05-17",
        "references": [
            "Do as i can, not as i say: Grounding language in robotic affordances",
            "Claude 2",
            "Training a helpful and harmless assistant with reinforcement learning from human feedback",
            "Constitutional ai: Harmlessness from ai feedback",
            "Deepmind lab",
            "Testing advanced driver assistance systems using multi-objective search and neural networks",
            "Language models are few-shot learners",
            "Social simulacra: Creating populated prototypes for social computing systems",
            "Generative agents: Interactive simulacra of human behavior",
            "Mind’s eye: Grounded language model reasoning through simulation",
            "Emergent abilities of large language models",
            "Evaluating language-model agents on realistic autonomous tasks",
            "Toolformer: Language models can teach themselves to use tools",
            "Reflexion: an autonomous agent with dynamic memory and self-reflection",
            "Intercode: Standardizing and benchmarking interactive coding with execution feedback",
            "Mind2web: Towards a generalist agent for the web",
            "Scaling laws for neural language models",
            "The effect of temperature on Claude agent’s safety and helpfulness",
            "React: Synergizing reasoning and acting in language models",
            "Training language models to follow instructions with human feedback",
            "Agentbench: Evaluating llms as agents",
            "Webarena: A realistic web environment for building autonomous agents",
            "Large language models are zero-shot reasoners",
            "Self-instruct: Aligning language model with self generated instructions",
            "Introducing chatgpt",
            "GPT-4 technical report",
            "Chatgpt plugins",
            "Gpt engineer",
            "Do as i can, not as i say: Grounding language in robotic affordances",
            "Social simulacra: Creating populated prototypes for social computing systems",
            "Generative agents: Interactive simulacra of human behavior",
            "Mind’s eye: Grounded language model reasoning through simulation",
            "Emergent abilities of large language models",
            "Evaluating language-model agents on realistic autonomous tasks",
            "Toolformer: Language models can teach themselves to use tools",
            "Reflexion: an autonomous agent with dynamic memory and self-reflection",
            "Intercode: Standardizing and benchmarking interactive coding with execution feedback",
            "Mind2web: Towards a generalist agent for the web",
            "Scaling laws for neural language models",
            "The effect of temperature on Claude agent’s safety and helpfulness",
            "React: Synergizing reasoning and acting in language models",
            "Training language models to follow instructions with human feedback",
            "Agentbench: Evaluating llms as agents",
            "Webarena: A realistic web environment for building autonomous agents",
            "Large language models are zero-shot reasoners",
            "Self-instruct: Aligning language model with self generated instructions"
        ]
    },
    {
        "index": 16,
        "title": "Jailbreaking Text-to-Image Models with LLM-Based Agents",
        "publication_date": "2024-08-01",
        "references": [
            "The rise and potential of large language model based agents: A survey",
            "A survey on large language model based autonomous agents",
            "Camel: Communicative agents for\" mind\" exploration of large scale language model society",
            "Metagpt: Meta programming for multi-agent collaborative framework",
            "Llama 2: Open foundation and fine-tuned chat models",
            "Judging llm-as-a-judge with mt-bench and chatbot arena",
            "GPT-4",
            "Communicative agents for software development",
            "Toolllm: Facilitating large language models to master 16000+ real-world apis",
            "Sql-palm: Improved large language model adaptation for text-to-sql (extended)",
            "Large language model enhanced multi-agent systems for 6g communications",
            "Towards autonomous system: flexible modular production system enhanced with large language model agents",
            "Industrial engineering with large language models: A case study of chatgpt's performance on oil & gas problems",
            "Taskmatrix. ai: Completing tasks by connecting foundation models with millions of apis",
            "Generative agents: Interactive simulacra of human behavior",
            "War and peace (waragent): Large language model-based multi-agent simulation of world wars",
            "Social simulacra: Creating populated prototypes for social computing systems",
            "DALL-E 3",
            "Midjourney",
            "High-resolution image synthesis with latent diffusion models",
            "Sdxl: Improving latent diffusion models for high-resolution image synthesis",
            "Nsfw image dataset",
            "Nsfw words list on github",
            "Sneakyprompt: Jailbreaking text-to-image generative models",
            "Encouraging divergent thinking in large language models through multi-agent debate",
            "Autogen: Enabling next-gen llm applications via multi-agent conversation framework",
            "Improving factuality and reasoning in language models through multiagent debate",
            "Visual instruction tuning",
            "Sharegpt4v: Improving large multi-modal models with better captions",
            "Chatllm network: More brains, more intelligence",
            "Self-collaboration code generation via chatgpt",
            "Exploring large language models for communication games: An empirical study on werewolf",
            "Large language models for supply chain optimization",
            "Dera: Enhancing large language model completions with dialog-enabled resolving agents (arxiv: 2303.17071). arxiv",
            "Medagents: Large language models as collaborators for zero-shot medical reasoning",
            "Hierarchical text-conditional image generation with clip latents",
            "Photorealistic text-to-image diffusion models with deep language understanding",
            "Red-teaming the stable diffusion safety filter",
            "Prompting4debugging: Red-teaming text-to-image diffusion models by finding problematic prompts",
            "Mma-diffusion: Multimodal attack on diffusion models",
            "Divide-and-conquer attack: Harnessing the power of llm to bypass the censorship of text-to-image generation model",
            "Ring-a-bell! how reliable are concept removal methods for diffusion models?",
            "GPT-4V(ision) System Card",
            "Sentence-bert: Sentence embeddings using siamese bert-networks",
            "Efficient estimation of word representations in vector space",
            "Clip-vit-base-patch32 [Online]",
            "Torchmetrics, CLIP Score [Online]",
            "Toward agent programs with circuit semantics",
            "An architecture for intelligent reactive systems",
            "Universal plans for reactive robots in unpredictable environments.",
            "A robust layered control system for a mobile robot",
            "Chain-of-thought prompting elicits reasoning in large language models",
            "Large language models are zero-shot reasoners",
            "Automatic chain of thought prompting in large language models",
            "The role of chain-of-thought in complex vision-language reasoning task",
            "SD3. Hugging face. [Online]",
            "SD1.4. Hugging face. [Online]",
            "Nsfw text classifier on hugging face",
            "Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter",
            "Nsfw image classifier on github",
            "Animals-10 dataset",
            "DALL·E 3 Docs. [Online]",
            "Gans trained by a two time-scale update rule converge to a local nash equilibrium",
            "Removing rlhf protections in gpt-4 via fine-tuning",
            "Generating images from captions with attention",
            "Generative adversarial text to image synthesis",
            "Stackgan: Text to photo-realistic image synthesis with stacked generative adversarial networks",
            "Glide: Towards photorealistic image generation and editing with text-guided diffusion models",
            "Unsafe diffusion: On the generation of unsafe images and hateful memes from text-to-image models",
            "Surrogateprompt: Bypassing the safety filter of text-to-image models via substitution"
        ]
    },
    {
        "index": 17,
        "title": "Prioritizing Safeguarding Over Autonomy: Risks of LLM Agents for Science",
        "publication_date": "2024-06-05",
        "references": [
            "Concrete Problems in AI Safety",
            "Agent-based Learning of Materials Datasets from Scientific Literature",
            "Introducing Claude",
            "Artificial intelligence in drug design: algorithms, applications, challenges and ethics",
            "Training a helpful and harmless assistant with reinforcement learning from human feedback",
            "A Multitask, Multilingual, Multimodal Evaluation of ChatGPT on Reasoning, Hallucination, and Interactivity",
            "Red-teaming large language models using chain of utterances for safety-alignment",
            "Safety-tuned llamas: Lessons from improving the safety of large language models that follow instructions",
            "Autonomous chemical research with large language models",
            "On the opportunities and risks of foundation models",
            "ChemCrow: Augmenting large-language models with chemistry tools",
            "RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control",
            "Defending against alignment-breaking attacks via robustly aligned llm",
            "Jailbreaker: Automated jailbreak across multiple large language model chatbots",
            "PaLM-E: an embodied multimodal language model",
            "Red-Teaming for Generative AI: Silver Bullet or Security Theater?",
            "Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned",
            "On the opportunities and risks of foundation models",
            "Artificial intelligence foundation for therapeutic science",
            "A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions",
            "Planning, Creation, Usage: Benchmarking LLMs for Comprehensive Tool Utilization in Real-World Complex Scenarios",
            "Sleeper Agents: Training Deceptive LLMs that Persist Through Safety Training",
            "Llamaguard: Llm-based input-output safeguard for human-ai conversations",
            "Survey of hallucination in natural language generation",
            "Retrieve, Summarize, and Verify: How will ChatGPT impact information seeking from the medical literature?",
            "GeneGPT: Augmenting Large Language Models with Domain Tools for Improved Access to Biomedical Information",
            "Our approach to alignment research",
            "CAMEL: Communicative Agents for ”Mind” Exploration of Large Language Model Society",
            "Repetition In Repetition Out: Towards Understanding Neural Text Degeneration from the Data Perspective",
            "Rain: Your language models can align themselves without finetuning",
            "Ethics, Values, and Responsibility in Human Genome Editing",
            "ASSERT: Automated Safety Scenario Red Teaming for Evaluating the Robustness of Large Language Models",
            "Testing Language Model Agents Safely in the Wild",
            "BioPlanner: Automatic Evaluation of LLMs on Protocol Planning in Biology",
            "Training language models to follow instructions with human feedback",
            "On the Importance of Domain-specific Explanations in AI-based Cybersecurity Systems (Technical Report)",
            "Generative agents: Interactive simulacra of human behavior",
            "Unsolved Problems in ML Safety",
            "MetaGPT: Meta Programming for A Multi-Agent Collaborative Framework",
            "Towards Reasoning in Large Language Models: A Survey",
            "Artificial intelligence foundation for therapeutic science",
            "A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions",
            "Planning, Creation, Usage: Benchmarking LLMs for Comprehensive Tool Utilization in Real-World Complex Scenarios",
            "Sleeper Agents: Training Deceptive LLMs that Persist Through Safety Training",
            "Llamaguard: Llm-based input-output safeguard for human-ai conversations",
            "Survey of hallucination in natural language generation",
            "Retrieve, Summarize, and Verify: How will ChatGPT impact information seeking from the medical literature?",
            "GeneGPT: Augmenting Large Language Models with Domain Tools for Improved Access to Biomedical Information",
            "Our approach to alignment research",
            "CAMEL: Communicative Agents for ”Mind” Exploration of Large Language Model Society",
            "Repetition In Repetition Out: Towards Understanding Neural Text Degeneration from the Data Perspective",
            "Rain: Your language models can align themselves without finetuning",
            "Ethics, Values, and Responsibility in Human Genome Editing",
            "ASSERT: Automated Safety Scenario Red Teaming for Evaluating the Robustness of Large Language Models",
            "Testing Language Model Agents Safely in the Wild",
            "BioPlanner: Automatic Evaluation of LLMs on Protocol Planning in Biology",
            "Training language models to follow instructions with human feedback",
            "On the Importance of Domain-specific Explanations in AI-based Cybersecurity Systems (Technical Report)",
            "Generative agents: Interactive simulacra of human behavior",
            "Unsolved Problems in ML Safety",
            "MetaGPT: Meta Programming for A Multi-Agent Collaborative Framework"
        ]
    },
    {
        "index": 18,
        "title": "PsySafe: A Comprehensive Framework for Psychological-based Attack, Defense, and Evaluation of Multi-agent System Safety",
        "publication_date": "2024-01-22",
        "references": [
            "Exploring the psychology of gpt-4’s moral and legal reasoning.",
            "Image hijacks: Adversarial images can control generative models at runtime.",
            "Defending against alignment-breaking attacks via robustly aligned llm.",
            "Are aligned neural networks adversarially aligned?",
            "Jailbreaking black box large language models in twenty queries.",
            "Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors.",
            "An appraisal-based chain-of-emotion architecture for affective language model game agents.",
            "Multilingual jailbreak challenges in large language models.",
            "Mind meets machine: Unravelling gpt-4’s cognitive psychology.",
            "Llama guard: Llm-based input-output safeguard for human-ai conversations.",
            "Running cognitive evaluations on large language models: The do’s and the don’ts.",
            "Baseline defenses for adversarial attacks against aligned language models.",
            "The cultural psychology of large language models: Is chatgpt a holistic or analytic thinker?",
            "Psyeval: A comprehensive large language model evaluation benchmark for mental health.",
            "Who is chatgpt? benchmarking llms’ psychological portrayal using psychobench.",
            "Decodingtrust: A comprehensive assessment of trustworthiness in gpt models.",
            "Does role-playing chatbots capture the character personalities? assessing personality traits for role-playing chatbots.",
            "Auto-gpt for online decision making: Benchmarks and additional opinions.",
            "React: Synergizing reasoning and acting in language models.",
            "Gptfuzzer: Red teaming large language models with auto-generated jailbreak prompts.",
            "Proagent: Building proactive cooperative agents with large language models.",
            "Psybench: a balanced and in-depth psychological chinese evaluation benchmark for foundation models.",
            "Autodan: Automatic and interpretable adversarial attacks on large language models.",
            "Universal and transferable adversarial attacks on aligned language models."
        ]
    },
    {
        "index": 19,
        "title": "R-Judge: Benchmarking Safety Risk Awareness for LLM Agents",
        "publication_date": "2024-02-18",
        "references": [
            "Training a helpful and harmless assistant with reinforcement learning from human feedback",
            "Red-teaming large language models using chain of utterances for safety-alignment",
            "Safety-tuned llamas: Lessons from improving the safety of large language models that follow instructions",
            "Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality",
            "MuTual: A dataset for multi-turn dialogue reasoning",
            "Safe rlhf: Safe reinforcement learning from human feedback",
            "Masterkey: Automated jailbreaking of large language model chatbots",
            "Badllama: cheaply removing safety fine-tuning from llama 2-chat 13b",
            "Metagpt: Meta programming for multi-agent collaborative framework",
            "A survey of safety and trustworthiness of large language models through the lens of verification and validation",
            "Llama guard: Llm-based input-output safeguard for human-ai conversations",
            "Large language models are zero-shot reasoners",
            "Multi-step jailbreaking privacy attacks on chatgpt",
            "Guiding large language models via directional stimulus prompting",
            "Swiftsage: A generative agent with fast and slow thinking for complex interactive tasks",
            "Toxicchat: Unveiling hidden challenges of toxicity detection in real-world user-ai conversation",
            "Program induction by rationale generation: Learning to solve and explain algebraic word problems",
            "Alignbench: Benchmarking chinese alignment of large language models",
            "Agents that reduce work and information overload",
            "Testing language model agents safely in the wild",
            "Webgpt: Browser-assisted question-answering with human feedback",
            "Introducing chatgpt",
            "GPT-4 technical report",
            "Training language models to follow instructions with human feedback",
            "Fine-tuning aligned language models compromises safety, even when users do not intend to!",
            "Communicative agents for software development",
            "Auto-gpt: An autonomous gpt-4 experiment",
            "Identifying the risks of LM agents with an LM-emulated sandbox",
            "Toolformer: Language models can teach themselves to use tools",
            "Reflexion: Language agents with verbal reinforcement learning",
            "Cognitive architectures for language agents",
            "Safety assessment of chinese large language models",
            "Xagent: An autonomous agent for complex task solving",
            "Evil geniuses: Delving into the safety of llm-based agents",
            "Llama 2: Open foundation and fine-tuned chat models",
            "V oyager: An open-ended embodied agent with large language models",
            "A survey on large language model based autonomous agents",
            "Element-aware summarization with large language models: Expert-aligned evaluation and chain-of-thought method",
            "Emergent abilities of large language models",
            "Chain-of-thought prompting elicits reasoning in large language models",
            "Pearson correlation coefficient",
            "Intelligent agents: Theory and practice",
            "The rise and potential of large language model based agents: A survey",
            "How far are we from believable ai agents? a framework for evaluating the believability of human behavior simulation",
            "Openagents: An open platform for language agents in the wild",
            "Sc-safety: A multi-round open-ended question adversarial safety benchmark for large language models in chinese",
            "Magic: Investigation of large language model powered multi-agent in cognition, adaptability, rationality and collaboration",
            "Shadow alignment: The ease of subverting safely-aligned language models",
            "Webshop: Towards scalable real-world web interaction with grounded language agents",
            "React: Synergizing reasoning and acting in language models",
            "Benchmarking and defending against indirect prompt injection attacks on large language models",
            "Gpt-4 is too smart to be safe: Stealthy chat with llms via cipher",
            "Safetybench: Evaluating the safety of large language models with multiple choice questions",
            "Igniting language intelligence: The hitchhiker’s guide from chain-of-thought reasoning to language agents",
            "Judging llm-as-a-judge with mt-bench and chatbot arena"
        ]
    },
    {
        "index": 22,
        "title": "Sleeper Agents: Training Deceptive LLMs That Persist Through Safety Training",
        "publication_date": "2024-01-17",
        "references": [
            "Model Organisms",
            "A general language assistant as a laboratory for alignment",
            "T-Miner: A generative approach to defend against trojan attacks on {DNN-based }text classification",
            "Blind backdoors in deep learning models",
            "Training a helpful and harmless assistant with reinforcement learning from human feedback",
            "Universal litmus patterns: Revealing backdoor attacks in cnns",
            "ImageNet-trained CNNs are biased towards texture; Increasing shape bias improves accuracy and robustness",
            "Measuring massive multitask language understanding",
            "The alignment problem from a deep learning perspective",
            "In-context learning and induction heads",
            "GPT-4 technical report",
            "A survey on large language model based autonomous agents",
            "AI deception: A survey of examples, risks, and potential solutions",
            "Improving language understanding by generative pre-training",
            "Jailbroken: How does llm safety training fail?",
            "Finetuned language models are zero-shot learners",
            "Chain of thought prompting elicits reasoning in large language models",
            "Adversarial unlearning of backdoors via implicit hypergradient",
            "ONION: A simple and effective defense against textual backdoor attacks",
            "Hidden killer: Invisible textual backdoor attacks with syntactic trigger",
            "Learning by distilling context",
            "Proximal policy optimization algorithms",
            "A survey of examples, risks, and potential solutions",
            "Backdoor chain-of-thought prompting for large language models",
            "Instructions as backdoors: Backdoor vulnerabilities of instruction tuning for large language models",
            "Defending against backdoor attack on deep neural networks",
            "Detecting AI trojans using meta neural analysis",
            "A comprehensive overview of backdoor attacks in large language models within communication networks",
            "Uncovering mesa-optimization algorithms in transformers",
            "You autocomplete me: Poisoning vulnerabilities in neural code completion",
            "Constituting AI: Harmlessness from AI feedback",
            "Backdoor attacks and countermeasures in natural language processing models: A comprehensive security review",
            "The trojan detection challenge",
            "How hard is trojan detection in DNNs? Fooling detectors with evasive trojans",
            "The trojan detection challenge 2023 (llm edition)",
            "Towards a situational awareness benchmark for LLMs",
            "Role play with large language models",
            "Underspecification presents challenges for credibility in modern machine learning",
            "Learning by distilling context",
            "Measuring faithfulness in chain-of-thought reasoning",
            "A survey on large language model based autonomous agents",
            "Defending against backdoor attack on deep neural networks",
            "On the exploitability of instruction tuning",
            "Neural trojans",
            "Discovering language model behaviors with model-written evaluations",
            "Neural attention distillation: Erasing backdoor triggers from deep neural networks",
            "Backdoor learning: A survey",
            "Backdoor attacks against learning systems",
            "Backdoor attacks against NLP models",
            "Adversarial neuron pruning purifies backdoored deep models",
            "Universal jailbreak backdoors from poisoned human feedback",
            "Technical report: Large language models can strategically deceive their users when put under pressure",
            "Training language models to follow instructions with human feedback",
            "Fine-pruning: Defending against backdooring attacks on deep neural networks",
            "ABS: Scanning neural networks for back-doors by artificial brain stimulation",
            "Piccolo: Exposing complex backdoors in NLP transformer models",
            "Scheming AIs: Will AIs fake alignment during training in order to get power?",
            "Are aligned neural networks adversarially aligned?",
            "Worst-case guarantees",
            "Backdoor attacks against learning systems",
            "Constituting AI: Harmlessness from AI feedback",
            "The Volkswagen scandal",
            "Unterspecification presents challenges for credibility in modern machine learning",
            "ImageNet-trained CNNs are biased towards texture; Increasing shape bias improves accuracy and robustness",
            "Measuring massive multitask language understanding",
            "Jailbreaking black box large language models in twenty queries",
            "Poisoning web-scale training datasets is practical",
            "Neural cleanse: Identifying and mitigating backdoor attacks in neural networks",
            "A survey on large language model based autonomous agents",
            "Defending against backdoor attack on deep neural networks",
            "RAB: Provable robustness against backdoor attacks",
            "Uncovering mesa-optimization algorithms in transformers",
            "Jailbroken: How does llm safety training fail?",
            "Finetuned language models are zero-shot learners",
            "Chain of thought prompting elicits reasoning in large language models",
            "Chain-of-thought prompting elicits reasoning in large language models",
            "Adversarial neuron pruning purifies backdoored deep models",
            "Backdoor chain-of-thought prompting for large language models",
            "Instructions as backdoors: Backdoor vulnerabilities of instruction tuning for large language models",
            "Defending against backdoor attack on deep neural networks",
            "Detecting AI trojans using meta neural analysis",
            "ABS: Scanning neural networks for back-doors by artificial brain stimulation",
            "Piccolo: Exposing complex backdoors in NLP transformer models",
            "A survey on large language model based autonomous agents",
            "Defending against backdoor attack on deep neural networks",
            "RAB: Provable robustness against backdoor attacks",
            "Neural trojans",
            "A survey on large language model based autonomous agents",
            "Defending against backdoor attack on deep neural networks",
            "Neural attention distillation: Erasing backdoor triggers from deep neural networks",
            "ONION: A simple and effective defense against textual backdoor attacks",
            "Hidden killer: Invisible textual backdoor attacks with syntactic trigger",
            "Learning by distilling context",
            "Proximal policy optimization algorithms",
            "A survey on large language model based autonomous agents",
            "Defending against backdoor attack on deep neural networks",
            "Detecting AI trojans using meta neural analysis",
            "ABS: Scanning neural networks for back-doors by artificial brain stimulation",
            "Piccolo: Exposing complex backdoors in NLP transformer models",
            "A survey on large language model based autonomous agents",
            "Defending against backdoor attack on deep neural networks",
            "Neural trojans",
            "A survey on large language model based autonomous agents",
            "Defending against backdoor attack on deep neural networks",
            "Neural attention distillation: Erasing backdoor triggers from deep neural networks",
            "ONION: A simple and effective defense against textual backdoor attacks",
            "Hidden killer: Invisible textual backdoor attacks with syntactic trigger",
            "Learning by distilling context",
            "Proximal policy optimization algorithms",
            "A survey on large language model based autonomous agents",
            "Defending against backdoor attack on deep neural networks",
            "Neural trojans",
            "A survey on large language model based autonomous agents",
            "Defending against backdoor attack on deep neural networks",
            "Neural attention distillation: Erasing backdoor triggers from deep neural networks",
            "ONION: A simple and effective defense against textual backdoor attacks",
            "Hidden killer: Invisible textual backdoor attacks with syntactic trigger",
            "Learning by distilling context",
            "Proximal policy optimization algorithms",
            "A survey on large language model based autonomous agents",
            "Defending against backdoor attack on deep neural networks",
            "Neural trojans",
            "A survey on large language model based autonomous agents",
            "Defending against backdoor attack on deep neural networks",
            "Neural attention distillation: Erasing backdoor triggers from deep neural networks",
            "ONION: A simple and effective defense against textual backdoor attacks",
            "Hidden killer: Invisible textual backdoor attacks with syntactic trigger",
            "Learning by distilling context",
            "Proximal policy optimization algorithms",
            "A survey on large language model based autonomous agents",
            "Defending against backdoor attack on deep neural networks",
            "Neural trojans",
            "A survey on large language model based autonomous agents",
            "Defending against backdoor attack on deep neural networks",
            "Neural attention distillation: Erasing backdoor triggers from deep neural networks",
            "ONION: A simple and effective defense against textual backdoor attacks",
            "Hidden killer: Invisible textual backdoor attacks with syntactic trigger",
            "Learning by distilling context",
            "Proximal policy optimization algorithms"
        ]
    },
    {
        "index": 23,
        "title": "Tiny Refinements Elicit Resilience: Toward Efficient Prefix-Model Against LLM Red-Teaming",
        "publication_date": "2024-06-17",
        "references": [
            "Constitutional ai: Harmlessness from ai feedback.",
            "Red-teaming large language models using chain of utterances for safety-alignment.",
            "Learning stackelberg equilibria and applications to economic design games.",
            "Discovering latent knowledge in language models without supervision.",
            "The secret sharer: Evaluating and testing unintended memorization in neural networks.",
            "Explore, establish, exploit: Red teaming language models from scratch.",
            "trlX: A scalable framework for RLHF.",
            "Learning to generate better than your llm.",
            "Black-box prompt optimization: Aligning large language models without model training.",
            "Deep reinforcement learning from human preferences.",
            "Building guardrails for large language models.",
            "Counterfactuals of counterfactuals: a back-translation-inspired approach to analyse counterfactual editors.",
            "RealToxicityPrompts: Evaluating neural toxic degeneration in language models.",
            "Not what you’ve signed up for: Compromising real-world llm-integrated applications with indirect prompt injection.",
            "Toxigen: A large-scale machine-generated dataset for adversarial and implicit hate speech detection.",
            "Curiosity-driven red-teaming for large language models.",
            "Llama guard: Llm-based input-output safeguard for human-ai conversations.",
            "Exploiting programmatic behavior of llms: Dual-use through standard security attacks.",
            "Propile: Probing privacy leakage in large language models.",
            "Semantic uncertainty: Linguistic invariances for uncertainty estimation in natural language generation.",
            "Rlaif: Scaling reinforcement learning from human feedback with ai feedback.",
            "HaluEval: A large-scale hallucination evaluation benchmark for large language models.",
            "A review of dynamic stackelberg game models.",
            "Teaching models to express their uncertainty in words.",
            "Truthfulqa: Measuring how models mimic human falsehoods.",
            "Gpteval: Nlg evaluation using gpt-4 with better human alignment.",
            "Learning word vectors for sentiment analysis.",
            "Flirt: Feedback loop in-context red teaming.",
            "Mitigating harm in language models with conditional-likelihood filtration.",
            "Chatgpt.",
            "GPT-4 Technical Report.",
            "Probing toxic content in large pre-trained language models.",
            "Stanford alpaca: An instruction-following llama model.",
            "Fever: a large-scale dataset for fact extraction and verification.",
            "Llama: Open and efficient foundation language models.",
            "Learning from the worst: Dynamically generated datasets to improve online hate detection.",
            "Trl: Transformer reinforcement learning.",
            "Trick me if you can: Human-in-the-loop generation of adversarial examples for question answering.",
            "Leashing the inner demons: Self-detoxification for language models.",
            "Bot-adversarial dialogue for safe conversational agents."
        ]
    },
    {
        "index": 25,
        "title": "Watch Out for Your Agents! Investigating Backdoor Threats to LLM-Based Agents",
        "publication_date": "2024-02-17",
        "references": [
            "Superintelligence: Paths, Dangers, Strategies",
            "Language models are few-shot learners",
            "Stealthy and persistent unalignment on large language models via backdoor injections",
            "Badnl: Back-door attacks against nlp models",
            "Mind2web: Towards a generalist agent for the web",
            "Unleashing cheapfakes through trojan plugins of large language models",
            "Challenges of real-world reinforcement learning: definitions, benchmarks and analysis",
            "Learning to communicate with deep multi-agent reinforcement learning",
            "Pal: Program-aided language models",
            "Badnets: Identifying vulnerabilities in the machine learning model supply chain",
            "A real-world webagent with planning, long context understanding, and program synthesis",
            "Language models as zero-shot planners: Extracting actionable knowledge for embodied agents",
            "Sleeper agents: Training deceptive llms that persist through safety training",
            "Adam: A method for stochastic optimization",
            "Large language models are zero-shot reasoners",
            "Weight poisoning attacks on pretrained models",
            "Coderl: Mastering code generation through pretrained models and deep reinforcement learning",
            "Backdoor attacks on pre-trained models by layerwise weight poisoning",
            "Competition-level code generation with alphacode",
            "Llm+ p: Empowering large language models with optimal planning proficiency",
            "Agent-bench: Evaluating llms as agents",
            "Agents that reduce work and information overload",
            "Learning to adapt in dynamic, real-world environments through meta-reinforcement learning",
            "Babyagi",
            "Webgpt: Browser-assisted question-answering with human feedback",
            "ChatGPT: Optimizing Language Models for Dialogue",
            "Chatgpt plugins",
            "Gpt-4 technical report",
            "Training language models to follow instructions with human feedback",
            "Gorilla: Large language model connected with massive apis",
            "Are you copying my model? protecting the copyright of large language models for EaaS via backdoor watermark",
            "Hidden killer: Invisible textual backdoor attacks with syntactic trigger",
            "Tool learning with foundation models",
            "Toolllm: Facilitating large language models to master 16000+ real-world apis",
            "Auto-gpt: Autonomous artificial intelligence software agent",
            "Artificial intelligence a modern approach",
            "Toolformer: Language models can teach themselves to use tools",
            "Backdoor pre-trained models can transfer to all",
            "Reflexion: an autonomous agent with dynamic memory and self-reflection",
            "Alfworld: Aligning text and embodied environments for interactive learning",
            "Evil geniuses: Delving into the safety of llm-based agents",
            "Llama: Open and efficient foundation language models",
            "Llama 2: Open foundation and fine-tuned chat models",
            "Poisoning language models during instruction tuning",
            "Backdoor activation attack: Attack large language models using activation steering for safety-alignment",
            "When large language model based agent meets user behavior analysis: A novel user simulation paradigm",
            "Chain-of-thought prompting elicits reasoning in large language models",
            "Intelligent agents: Theory and practice",
            "Badchain: Backdoor chain-of-thought prompting for large language models",
            "Instructions as backdoors: Backdoor vulnerabilities of instruction tuning for large language models",
            "Backdooring instruction-tuned large language models with virtual prompt injection",
            "Be careful about poisoned word embeddings: Exploring the vulnerability of the embedding layers in NLP models",
            "Rethinking stealthiness of backdoor attack against NLP models",
            "Webshop: Towards scalable real-world web interaction with grounded language agents",
            "Tree of thoughts: Deliberate problem solving with large language models",
            "React: Synergizing reasoning and acting in language models",
            "Agenttuning: Enabling generalized agent abilities for llms"
        ]
    }
]