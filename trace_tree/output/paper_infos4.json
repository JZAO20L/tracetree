[
    {
        "index": 1,
        "title": "SafeAgentBench: A Benchmark for Safe Task Planning of Embodied LLM Agents",
        "publication_date": "2025-03-10",
        "references": [
            "Gpt-4 technical report.",
            "Managing extreme ai risks amid rapid progress.",
            "Multi-agent planning using visual language models.",
            "Object goal navigation using goal-oriented semantic exploration.",
            "Towards end-to-end embodied decision making via multi-modal large language model: Explorations with gpt4-vision and beyond.",
            "Lota-bench: Benchmarking language-oriented task planners for embodied agents.",
            "The llama 3 herd of models.",
            "The threedworld transport challenge: A visually guided task-and-motion planning benchmark for physically realistic embodied ai.",
            "Safetext: A benchmark for exploring physical safety in language models.",
            "Behavior-1k: A benchmark for embodied ai with 1,000 everyday activities and realistic simulation.",
            "Decodingtrust: A comprehensive assessment of trustworthiness in gpt models.",
            "Mldt: Multi-level decomposition for complex long-horizon robotic task planning with open-source large language model.",
            "Qwen2 technical report.",
            "React: Synergizing reasoning and acting in language models.",
            "On the vulnerability of safety alignment in open-access llms.",
            "R-judge: Benchmarking safety risk awareness for llm agents.",
            "Building cooperative embodied agents modularly with large language models.",
            "Badrobot: Jailbreaking llm-based embodied ai in the physical world.",
            "Hazard challenge: Embodied decision making in dynamically changing environments.",
            "Earbench: Towards evaluating physical risk awareness for task planning of foundation model-based embodied ai agents.",
            "Riskawarebench: Towards evaluating physical risk awareness for high-level planning of llm-based embodied agents."
        ]
    },
    {
        "index": 2,
        "title": "A Causal Explainable Guardrails for Large Language Models",
        "publication_date": "2024-10-18",
        "references": [
            "Understanding intermediate layers using linear classifier probes",
            "The internal state of an llm knows when its lying",
            "Defending pre-trained language models from adversarial word substitutions without performance sacrifice",
            "Probing classifiers: Promises, shortcomings, and advances",
            "Man is to computer programmer as woman is to homemaker? debiasing word embeddings",
            "Language models are few-shot learners",
            "Discovering latent knowledge in language models without supervision",
            "Can prompt probe pretrained language models? understanding the invisible risks from a causal view",
            "Jailbreaking Black Box Large Language Models in Twenty Queries",
            "INSIDE: LLMs’ Internal States Retain the Power of Hallucination Detection",
            "Learning a structural causal model for intuition reasoning in conversation",
            "Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality",
            "Causal Interventional Prediction System for Robust and Explainable Effect Forecasting",
            "Data-centric financial large language models",
            "Causal Effect Estimation: Recent Progress, Challenges, and Opportunities",
            "Graph infomax adversarial learning for treatment effect estimation with networked observational data",
            "Multi-task adversarial learning for treatment effect estimation in basket trials",
            "Llm-guided multi-view hypergraph learning for human-centric explainable recommendation",
            "Professional Agents–Evolving Large Language Models into Autonomous Experts with Human-Level Competencies",
            "Sora Detector: A Unified Hallucination Detection for Large Text-to-Video Models",
            "Causal inference meets machine learning",
            "Plug and play language models: A simple approach to controlled text generation",
            "Bold: Dataset and metrics for measuring biases in open-ended language generation",
            "Word embeddings via causal inference: Gender bias reducing and semantic information preserving",
            "Unsupervised domain adaptation by backpropagation",
            "Is chatgpt a good causal reasoner? a comprehensive evaluation",
            "Intelligent Agents with LLM-based Process Automation",
            "Language models represent space and time",
            "Toxigen: A large-scale machine-generated dataset for adversarial and implicit hate speech detection",
            "Inspecting and editing knowledge representations in language models",
            "Explainability in Deep Reinforcement Learning: A Review into Current Methods and Applications",
            "An Empirical Study of Metrics to Measure Representational Harms in Pre-Trained Language Models",
            "A survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions",
            "VADER-Sentiment-Analysis",
            "Improving activation steering in language models with mean-centring",
            "Large language models are temporal and causal reasoners for video question answering",
            "RLAIF: Scaling Reinforcement Learning from Human Feedback with AI Feedback",
            "Inference-time intervention: Eliciting truthful answers from a language model",
            "DeepInception: Hypnotize Large Language Model to Be Jailbreaker",
            "Ai transparency in the age of llms: A human-centered research roadmap",
            "Truthfulqa: Measuring how models mimic human falsehoods",
            "Hallusionbench: You see what you think? or you think what you see? an image-context reasoning benchmark challenging for gpt-4v (ision), llava-1.5, and other multi-modality models",
            "Aligning large multi-modal model with robust instruction tuning",
            "Incorporating Causal Analysis into Diversified and Logical Response Generation",
            "A Survey on Medical Large Language Models: Technology, Application, Trustworthiness, and Future Directions",
            "AutoDAN: Generating Stealthy Jailbreak Prompts on Aligned Large Language Models",
            "An empirical survey of the effectiveness of debiasing techniques for pre-trained language models",
            "Locating and editing factual associations in GPT",
            "Linguistic regularities in continuous space word representations",
            "Training language models to follow instructions with human feedback",
            "Answering causal questions with augmented llms",
            "Causality: models, reasoning and inference",
            "The Book of Why",
            "Fine-tuning aligned language models compromises safety, even when users do not intend to!",
            "Direct Preference Optimization: Your Language Model is Secretly a Reward Model",
            "Causal inference using potential outcomes: Design, modeling, decisions",
            "Evaluating gender bias in machine translation",
            "Extracting latent steering vectors from pretrained language models",
            "Large Language Models for Data Annotation: A Survey",
            "Deception and Manipulation in Generative AI",
            "BERT rediscovers the classical NLP pipeline",
            "Llama 2: Open foundation and fine-tuned chat models",
            "Activation Addition: Steering Language Models Without Optimization",
            "Bridging Causal Discovery and Large Language Models: A Comprehensive Survey of Integrative Approaches and Future Directions",
            "Backdoor activation attack: Attack large language models using activation steering for safety-alignment",
            "LLMRG: Improving Recommendations through Large Language Model Reasoning Graphs",
            "A Comprehensive Survey of LLM Alignment Techniques: RLHF, RLAIF, PPO, DPO and More",
            "Jailbreak and Guard Aligned Language Models with Only Few In-Context Demonstrations",
            "Unveiling the implicit toxicity in large language models",
            "A survey on causal inference",
            "Rock: Causal inference principles for reasoning about commonsense causality",
            "Explainability for large language models: A survey",
            "Mquake: Assessing knowledge editing in language models via multi-hop questions",
            "Representation engineering: A top-down approach to ai transparency",
            "Universal and transferable adversarial attacks on aligned language models"
        ]
    },
    {
        "index": 3,
        "title": "A Jailbroken GenAI Model Can Cause Substantial Harm: GenAI-powered Applications are Vulnerable to PromptWares",
        "publication_date": "2024-08-09",
        "references": [
            "Not what you've signed up for: Compromising real-world llm-integrated applications with indirect prompt injection",
            "(ab) using images and sounds for indirect instruction injection in multi-modal llms",
            "Are aligned neural networks adversarially aligned?",
            "Stealing part of a production language model",
            "Jailbreaking black box large language models in twenty queries",
            "Here comes the ai worm: Unleashing zero-click worms that target genai-powered applications",
            "Jailbreaker: Automated jailbreak across multiple large language model chatbots",
            "Llm agents can autonomously exploit one-day vulnerabilities",
            "Llm agents can autonomously hack websites",
            "Teams of llm agents can exploit zero-day vulnerabilities",
            "Agent smith: A single image can jailbreak one million multimodal llm agents exponentially fast",
            "Getting pwn’d by ai: Penetration testing with large language models",
            "Generative ai for pentesting: the good, the bad, the ugly",
            "An llm compiler for parallel function calling",
            "Demystifying rce vulnerabilities in llm-integrated apps",
            "Prompt injection attacks and defenses in llm-integrated applications",
            "Scalable extraction of training data from (production) language models",
            "Ignore previous prompt: Attack techniques for language models",
            "Prompt stealing attacks against large language models",
            "Machine against the rag: Jamming retrieval-augmented generation with blocker documents",
            "Plan-and-solve prompting: Improving zero-shot chain-of-thought reasoning by large language models",
            "What was your prompt? a remote keylogging attack on ai assistants",
            "A new era in llm security: Exploring security concerns in real-world llm-based systems",
            "Rewoo: Decoupling reasoning from observations for efficient augmented language models",
            "Badrag: Identifying vulnerabilities in retrieval augmented generation of large language models",
            "Prsa: Prompt reverse stealing attacks against large language models",
            "Effective prompt extraction from language models",
            "Universal and transferable adversarial attacks on aligned language models",
            "Poisonedrag: Knowledge poisoning attacks to retrieval-augmented generation of large language models"
        ]
    },
    {
        "index": 4,
        "title": "A Quality-Centric Framework for Generic Deepfake Detection",
        "publication_date": "2024-11-26",
        "references": [
            "MesoNet: A Compact Facial Video Forgery Detection Network",
            "FaceForensics++: Learning to Detect Manipulated Facial Images",
            "Metric Learning for Anti-Compression Facial Forgery Detection",
            "Spatiotemporal Inconsistency Learning for Deepfake Video Detection",
            "Video Transformer for Deepfake Detection with Incremental Learning",
            "CORED: Generalizing Fake Media Detection with Continual Representation Using Distillation",
            "Face X-Ray for More General Face Forgery Detection",
            "Shortcut Learning in Deep Neural Networks",
            "Transcending Forgery Specificity with Latent Space Augmentation for Generalizable Deepfake Detection",
            "Detecting Deepfakes with Self-Blended Images",
            "An Examination of Fairness of AI Models for Deepfake Detection",
            "Xception: Deep Learning With Depthwise Separable Convolutions",
            "WildDeepfake: A Challenging Real-World Dataset for Deepfake Detection",
            "The Deepfake Detection Challenge (DFDC) Preview Dataset",
            "ArcFace: Additive Angular Margin Loss for Deep Face Recognition",
            "Towards Generic Deepfake Detection with Dynamic Curriculum",
            "EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks",
            "Swin Transformer V2: Scaling Up Capacity and Resolution",
            "Celeb-DF: A Large-Scale Challenging Dataset for DeepFake Forensics",
            "The Deepfake Detection Challenge (DFDC) Dataset",
            "In Ictu Oculi: Exposing AI Created Fake Videos by Detecting Eye Blinking",
            "Exposing Deep Fakes Using Inconsistent Head Poses",
            "Exposing DeepFake Videos By Detecting Face Warping Artifacts",
            "Multi-Attentional Deepfake Detection",
            "Generalizing Face Forgery Detection with High-Frequency Features",
            "Deepfakebench: A Comprehensive Benchmark of Deepfake Detection",
            "DF40: Toward Next-Generation Deepfake Detection",
            "UCF: Uncovering Common Features for Generalizable Deepfake Detection",
            "Exploring Disentangled Content Information for Face Forgery Detection",
            "End-to-End Reconstruction-Classification Learning for Face Forgery Detection",
            "Representative Forgery Mining for Fake Face Detection",
            "UIA-ViT: Unsupervised Inconsistency-Aware Method Based on Vision Transformer for Face Forgery Detection",
            "Noise Based Deepfake Detection via Multi-Head Relative-Interaction",
            "Learning to Discover Forgery Cues for Face Forgery Detection",
            "Dynamic Difference Learning with Spatio-Temporal Correlation for Deepfake Video Detection",
            "MSVT: Multiple Spatiotemporal Views Transformer for DeepFake Video Detection",
            "Exploiting Complementary Dynamic Incoherence for DeepFake Video Detection",
            "Dual Contrastive Learning for General Face Forgery Detection",
            "Beyond the Prior Forgery Knowledge: Mining Critical Clues for General Face Forgery Detection",
            "SeeABLE: Soft Discrepancies and Bounded Contrastive Learning for Exposing Deepfakes",
            "Learning Self-Consistency for Deepfake Detection",
            "Self-Supervised Learning of Adversarial Example: Towards Good Generalizations for Deepfake Detection",
            "AUNet: Learning Relations Between Action Units for Face Forgery Detection",
            "AltFreezing for More General Video Face Forgery Detection",
            "Generalizing Deepfake Video Detection with Plug-and-Play: Video-Level Blending and Spatiotemporal Adapter Tuning",
            "Quality-Agnostic Deepfake Detection with Intra-Model Collaborative Learning",
            "On the Correlation between Deepfake Detection Performance and Image Quality Metrics",
            "BZNet: Unsupervised Multi-Scale Branch Zooming Network for Detecting Low-Quality Deepfake Videos",
            "Visualizing and Understanding Curriculum Learning for Long Short-Term Memory Networks",
            "Baby Steps: How “Less is More” in Unsupervised Dependency Parsing",
            "Dynamic Curriculum Learning for Imbalanced Data Classification",
            "Self-Paced Learning for Latent Variable Models",
            "Adaptive Curriculum Learning",
            "Curriculum Learning",
            "Delving into the Local: Dynamic Inconsistency Learning for Deepfake Video Detection",
            "FAMM: Facial Muscle Motions for Detecting Compressed Deepfake Videos over Social Networks",
            "Improving Deepfake Detection Generalization by Invariant Risk Minimization",
            "Improving Generalization of Deepfake Detectors by Imposing Gradient Regularization",
            "Temporal Diversified Self-Contrastive Learning for Generalized Face Forgery Detection",
            "Can We Leave Deepfake Data Behind in Training Deepfake Detector?"
        ]
    },
    {
        "index": 5,
        "title": "AdaPPA: Adaptive Position Pre-Fill Jailbreak Attack Approach Targeting LLMs",
        "publication_date": "2024-09-11",
        "references": [
            "GPT-4 Technical Report",
            "Llama 2: Open Foundation and Fine-Tuned Chat Models",
            "Toxicity in ChatGPT: Analyzing Persona-assigned Language Models",
            "Universal and Transferable Adversarial Attacks on Aligned Language Models",
            "AmpleGCG: Learning a Universal and Transferable Generative Model of Adversarial Suffixes for Jailbreaking Both Open and Closed LLMs",
            "MASTERKEY: Automated Jailbreaking of Large Language Model Chatbots",
            "AutoDAN: Generating Stealthy Jailbreak Prompts on Aligned LLMs",
            "Tree of Attacks: Jailbreaking Black-Box LLMs Automatically",
            "How Johnny Can Persuade LLMs to Jailbreak Them: Rethinking Persuasion to Challenge AI Safety by Humanizing LLMs",
            "Safety Alignment Should Be Made More Than Just a Few Tokens Deep",
            "ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools",
            "Term-weighting approaches in automatic text retrieval",
            "Many-shot jailbreaking",
            "Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena",
            "LMFlow: An Extensible Toolkit for Fine-tuning and Inference of Large Foundation Models",
            "Meta Llama Guard 2",
            "Llama 3 Model Card",
            "OpenAI's Content Moderation",
            "Perspective API",
            "Beaver-Tails: Towards Improved Safety Alignment of LLM via a Human-Preference Dataset",
            "Automatically Auditing Large Language Models via Discrete Optimization",
            "Open Sesame! Universal Black Box Jailbreaking of Large Language Models",
            "Jailbreaking Black Box Large Language Models in Twenty Queries",
            "GPT-4 Is Too Smart To Be Safe: Stealthy Chat with LLMs via Cipher",
            "Multilingual Jailbreak Challenges in Large Language Models",
            "Low-Resource Languages Jailbreak GPT-4",
            "GPTFUZZER: Red Teaming Large Language Models with Auto-Generated Jailbreak Prompts",
            "Jailbreak and Guard Aligned Language Models with Only Few In-Context Demonstrations",
            "Adversarial Demonstration Attacks on Large Language Models"
        ]
    },
    {
        "index": 6,
        "title": "Advancing Adversarial Suffix Transfer Learning on Aligned Large Language Models",
        "publication_date": "2024-10-05",
        "references": [
            "Jailbreak chat.",
            "Training a helpful and harmless assistant with reinforcement learning from human feedback.",
            "Deep reinforcement learning from human preferences.",
            "Scaling instruction-finetuned language models.",
            "Multilingual jailbreak challenges in large language models.",
            "Cold-attack: Jailbreaking llms with stealthiness and controllability.",
            "Large language models can be used to effectively scale spear phishing campaigns.",
            "Catastrophic jailbreak of open-source llms via exploiting generation.",
            "Mistral 7b.",
            "Exploiting programmatic behavior of llms: Dual-use through standard security attacks.",
            "Pretraining language models with human preferences.",
            "Harmbench: A standardized evaluation framework for automated red teaming and robust refusal.",
            "Universal adversarial triggers are not universal.",
            "Gpt-4 technical report.",
            "Training language models to follow instructions with human feedback.",
            "An important next step on our ai journey.",
            "Hijacking large language models via adversarial in-context learning.",
            "Autoprompt: Eliciting knowledge from language models with automatically generated prompts.",
            "Llama 2: Open foundation and fine-tuned chat models.",
            "Openchat: Advancing open-source language models with mixed-quality data.",
            "Jailbroken: How does llm safety training fail?",
            "Finetuned language models are zero-shot learners.",
            "Gpt-4 is too smart to be safe: Stealthy chat with llms via cipher.",
            "Accelerating greedy coordinate gradient via probe sampling.",
            "Autodan: Automatic and interpretable adversarial attacks on large language models.",
            "Universal and transferable adversarial attacks on aligned language models."
        ]
    },
    {
        "index": 7,
        "title": "Adversarial Attacks on Large Language Models Using Regularized Relaxation",
        "publication_date": "2024-10-24",
        "references": [
            "Unified pre-training for program understanding and generation",
            "The falcon series of open language models",
            "Geometric analysis and metric learning of instruction embeddings",
            "Language models are few-shot learners",
            "Extracting training data from large language models",
            "Towards evaluating the robustness of neural networks",
            "Jailbreaking black box large language models in twenty queries",
            "Safe rlhf: Safe reinforcement learning from human feedback",
            "Jailbreaker: Automated jailbreak across multiple large language model chatbots",
            "The llama 3 herd of models",
            "Efficient projections onto the l1-ball for learning in high dimensions",
            "Hotflip: White-box adversarial examples for text classification",
            "Attacking large language models with projected gradient descent",
            "Improving alignment of dialogue agents via targeted human judgements",
            "Deep Learning",
            "Explaining and harnessing adversarial examples",
            "Gradient-based adversarial attacks against text transformers",
            "Catastrophic jailbreak of open-source llms via exploiting generation",
            "Malicious path manipulations via exploitation of representation vulnerabilities of vision-language navigation systems",
            "Baseline defenses for adversarial attacks against aligned language models",
            "Mistral 7b",
            "Exploiting programmatic behavior of llms: Dual-use through standard security attacks",
            "Pretraining language models with human preferences",
            "Certifying llm safety against adversarial prompting",
            "Crafting adversarial input sequences for recurrent neural networks",
            "Ignore previous prompt: Attack techniques for language models",
            "Modern language models refute chomsky’s approach to language",
            "Effects of architecture and training on embedding geometry and feature discriminability in bert",
            "Fine-tuning aligned language models compromises safety, even when users do not intend to!",
            "Fast adversarial attacks on language models in one gpu minute",
            "Adversarial attacks and defenses in large language models: Old and new threats",
            "Soft prompt threats: Attacking safety alignment and unlearning in open-source llms through the embedding space",
            "Scalable and transferable black-box jailbreaks for language models via persona modulation",
            "Autoprompt: Eliciting knowledge from language models with automatically generated prompts",
            "Intriguing properties of neural networks",
            "Introducing mpt-7b: A new standard for open-source, commercially usable llms",
            "Llama 2: Open foundation and fine-tuned chat models",
            "Attention is all you need",
            "Deep learning for computer vision: A brief review",
            "Universal adversarial triggers for attacking and analyzing nlp",
            "Jailbroken: How does llm safety training fail?",
            "Ethical and social risks of harm from language models",
            "Assessing adversarial robustness of large language models: An empirical study",
            "Gptfuzzer: Red teaming large language models with auto-generated jailbreak prompts",
            "Gpt-4 is too smart to be safe: Stealthy chat with llms via cipher",
            "On large language models’ resilience to coercive interrogation",
            "Judging llm-as-a-judge with mt-bench and chatbot arena",
            "Don’t say no: Jailbreaking llm by suppressing refusal",
            "Universal and transferable adversarial attacks on aligned language models"
        ]
    },
    {
        "index": 8,
        "title": "Adversarial Search Engine Optimization for Large Language Models",
        "publication_date": "2024-07-02",
        "references": [
            "Introducing the next generation of Claude",
            "Attributed Question Answering: Evaluation and Modeling for Attributed Large Language Models",
            "Are aligned neural networks adversarially aligned?",
            "Phantom: General Trigger Attacks on Retrieval Augmented Language Generation",
            "StruQ: Defending Against Prompt Injection with Structured Queries",
            "Search Engine Optimization (SEO) Starter Guide",
            "Spam policies for Google web search",
            "Google Project Zero Vulnerability Disclosure FAQ",
            "Not What You’ve Signed Up For: Compromising Real-World LLM-Integrated Applications with Indirect Prompt Injection",
            "Defending Against Indirect Prompt Injection Attacks With Spotlighting",
            "Manipulating Large Language Models to Increase Product Visibility",
            "A Survey on Search Engine Optimization Techniques",
            "The influence of search engine optimization on Google’s results: A multi-dimensional approach for detecting SEO",
            "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks",
            "Prompt Injection attack against LLM-integrated Applications",
            "Microsoft Copilot is now generally available",
            "ChatGPT Plugins",
            "GPT-4 Technical Report",
            "Perplexity AI",
            "An important next step on our AI journey",
            "Toolformer: Language Models Can Teach Themselves to Use Tools",
            "Machine Against the RAG: Jamming Retrieval-Augmented Generation with Blocker Documents",
            "A Brief Review on Search Engine Optimization",
            "An Investigation of N-person Prisoners’ Dilemmas",
            "Sydney.py",
            "The Instruction Hierarchy: Training LLMs to Prioritize Privileged Instructions",
            "What Evidence Do Language Models Find Convincing?",
            "An empirical study on the search engine optimization technique and its outcomes",
            "Prompt injection explained, with video, slides, and a transcript",
            "Prompt injection: What’s the worst that can happen?",
            "Unifying corroborative and contributive attributions in large language models",
            "Certifiably Robust RAG against Retrieval Corruption",
            "BadRAG: Identifying Vulnerabilities in Retrieval Augmented Generation of Large Language Models",
            "PoisonedRAG: Knowledge Poisoning Attacks to Retrieval-Augmented Generation of Large Language Models"
        ]
    },
    {
        "index": 9,
        "title": "Adversarial Tuning: Defending Against Jailbreak Attacks for LLMs",
        "publication_date": "2024-06-07",
        "references": [
            "GPTFUZZER: red teaming large language models with auto-generated jailbreak prompts.",
            "Universal and transferable adversarial attacks on aligned language models.",
            "Weak-to-strong extrapolation expedites alignment.",
            "Jailbreaklens: Visual analysis of jailbreak attacks against large language models.",
            "Great, now write an article about that: The crescendo multi-turn llm jailbreak attack.",
            "Smoothllm: Defending large language models against jailbreaking attacks.",
            "Defending chatgpt against jailbreak attack via self-reminders.",
            "Gradsafe: Detecting unsafe prompts for llms via safety-critical gradient analysis.",
            "Towards deep learning models resistant to adversarial attacks.",
            "On prompt-driven safeguarding for large language models.",
            "Llama 2: Open foundation and fine-tuned chat models.",
            "Understanding hidden context in preference learning: Consequences for rlhf.",
            "Autodan: Generating stealthy jailbreak prompts on aligned large language models.",
            "Jailbreaking leading safety-aligned llms with simple adaptive attacks.",
            "“Do Anything Now”: Characterizing and Evaluating In-The-Wild Jailbreak Prompts on Large Language Models.",
            "Analyzing the inherent response tendency of llms: Real-world instructions-driven jailbreak.",
            "Jailbreaking black box large language models in twenty queries.",
            "Comprehensive assessment of jailbreak attacks against llms.",
            "Catastrophic jailbreak of open-source llms via exploiting generation.",
            "Tree of attacks: Jailbreaking black-box llms automatically.",
            "Robust prompt optimization for defending language models against jailbreaking attacks.",
            "Large language model unlearning.",
            "Prp: Propagating universal perturbations to attack large language model guard-rails.",
            "Harmbench: A standardized evaluation framework for automated red teaming and robust refusal.",
            "Advprompter: Fast adaptive adversarial prompting for llms.",
            "Jailbreakbench: An open robustness benchmark for jailbreaking large language models.",
            "Making them ask and answer: Jailbreaking large language models in few queries via disguise and reconstruction.",
            "Gpt-4 is too smart to be safe: Stealthy chat with llms via cipher.",
            "Improved few-shot jailbreaking can circumvent aligned language models and their defenses.",
            "Attacking large language models with projected gradient descent.",
            "Improved techniques for optimization-based jailbreaking on large language models.",
            "C-rag: Certified generation risks for retrieval-augmented language models.",
            "Multilingual jailbreak challenges in large language models.",
            "Jailbreak in pieces: Compositional adversarial attacks on multi-modal language models.",
            "The instruction hierarchy: Training llms to prioritize privileged instructions.",
            "Defending llms against jailbreaking attacks via backtranslation.",
            "Protecting your llms with information bottleneck.",
            "Exploring safety generalization challenges of large language models via code.",
            "Bells: A framework towards future proof benchmarks for the evaluation of llm safeguards.",
            "Autodefense: Multi-agent llm defense against jailbreak attacks.",
            "Gradient cuff: Detecting jailbreak attacks on large language models by exploring refusal loss landscapes.",
            "Defending large language models against jailbreak attacks via semantic smoothing.",
            "Is the system message really important to jailbreaks in large language models?",
            "Rain: Your language models can align themselves without finetuning.",
            "Mitigating fine-tuning jailbreak attack with backdoor enhanced alignment.",
            "Prompt-driven llm safeguarding via directed representation optimization.",
            "Pruning for protection: Increasing jailbreak resistance in aligned llms without fine-tuning.",
            "Efficient adversarial training in llms with continuous attacks.",
            "Cross-task defense: Instruction-tuning llms for content safety.",
            "Universal adversarial perturbations.",
            "Upper and lower probabilities induced by a multivalued mapping.",
            "Combining belief functions based on distance of evidence.",
            "Deng entropy.",
            "How far can camels go? exploring the state of instruction tuning on open resources."
        ]
    },
    {
        "index": 10,
        "title": "Adversaries Can Misuse Combinations of Safe Models",
        "publication_date": "2024-07-01",
        "references": [
            "Many-shot jailbreaking",
            "Anthropic’s responsible scaling policy (rsp)",
            "The claude 3 model family: Opus, sonnet, haiku",
            "Foundational challenges in assuring alignment and safety of large language models",
            "Identifying and mitigating the security risks of generative AI",
            "Improving image generation with better captions",
            "On the opportunities and risks of foundation models",
            "Picking on the same person: Does algorithmic monoculture lead to outcome homogenization?",
            "Ecosystem graphs: The social footprint of foundation models",
            "Instructpix2pix learning to follow image editing instructions",
            "Characterizing manipulation from AI systems",
            "Improving factuality and reasoning in language models through multiagent debate",
            "LLM agents can autonomously hack websites",
            "LLM censorship: A machine learning challenge or a computer security problem?",
            "Backdoor found in widely used linux utility targets encrypted SSH connections",
            "Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context",
            "AI control: Improving safety despite intentional subversion",
            "Defending against adversarial samples without security through obscurity",
            "An overview of catastrophic AI risks",
            "Mistral 7B",
            "Automatically auditing large language models via discrete optimization",
            "Small language models fine-tuned to coordinate larger language models improve complex reasoning",
            "On the societal impact of open foundation models",
            "Debating with more persuasive llms leads to more truthful answers",
            "Efficient memory management for large language model serving with pagedattention",
            "LoRA fine-tuning efficiently undoes safety training in llama 2-chat 70B",
            "The WMDP benchmark: Measuring and reducing malicious use with unlearning",
            "Composing ensembles of pre-trained models via iterative consensus",
            "AI safety is not a model property",
            "GPT-4 technical report",
            "Preparedness framework (beta)",
            "Do the rewards justify the means? measuring trade-offs between rewards and ethical behavior in the MACHIAVELLI benchmark",
            "Feedback loops with language models drive in-context reward hacking",
            "Generative agents: Interactive simulacra of human behavior",
            "AI deception: A survey of examples, risks, and potential solutions",
            "Can large language models democratize access to dual-use biotechnology?",
            "Release strategies and the social impacts of language models",
            "High-resolution image synthesis with latent diffusion models",
            "Learning to reinforcement learn",
            "A survey on large language model based autonomous agents",
            "The rise and potential of large language model based agents: A survey",
            "Socratic models: Composing zero-shot multimodal reasoning with language",
            "Universal and transferable adversarial attacks on aligned language models",
            "Mass-producing failures of multimodal systems with language models"
        ]
    },
    {
        "index": 11,
        "title": "AdvPrompter: Fast Adaptive Adversarial Prompting for LLMs",
        "publication_date": "2024-04-29",
        "references": [
            "Back to basics: Revisiting reinforce style optimization for learning from human feedback in llms",
            "Do as i can and not as i say: Grounding language in robotic affordances",
            "Tutorial on amortized optimization",
            "Training a helpful and harmless assistant with reinforcement learning from human feedback",
            "Pythia: A suite for analyzing large language models across training and scaling",
            "Rt-2: Vision-language-action models transfer web knowledge to robotic control",
            "Jailbreaking black box large language models in twenty queries",
            "Instructzero: Efficient instruction optimization for black-box large language models",
            "Learning to optimize: A primer and a benchmark",
            "Black-box prompt learning for pre-trained language models",
            "Gradient-based adversarial attacks against text transformers",
            "Connecting large language models with evolutionary algorithms yields powerful prompt optimizers",
            "Measuring massive multitask language understanding",
            "LoRA: Low-rank adaptation of large language models",
            "Catastrophic jailbreak of open-source llms via exploiting generation",
            "Baseline defenses for adversarial attacks against aligned language models",
            "Mistral 7b",
            "Automatically auditing large language models via discrete optimization",
            "Exploiting programmatic behavior of llms: Dual-use through standard security attacks",
            "Llm+p: Empowering large language models with optimal planning proficiency",
            "Black box adversarial prompting for foundation models",
            "Ignore previous prompt: Attack techniques for language models",
            "Automatic prompt optimization with \"gradient descent\" and beam search",
            "Fine-tuning aligned language models compromises safety, even when users do not intend to!",
            "Direct preference optimization: Your language model is secretly a reward model",
            "Mathematical discoveries from program search with large language models",
            "Rainbow teaming: Open-ended generation of diverse adversarial prompts",
            "Proximal policy optimization algorithms",
            "Autoplan: Automatic planning of interactive decision-making tasks with large language models",
            "The RefinedWeb dataset for Falcon LLM: outperforming curated corpora with web data, and web data only",
            "Red teaming language models with language models",
            "Universal adversarial triggers for attacking and analyzing NLP",
            "Llama 2: Open foundation and fine-tuned chat models",
            "Solving olympiad geometry without human demonstrations",
            "Trl: Transformer reinforcement learning",
            "Universal adversarial triggers for attacking and analyzing NLP",
            "AutoPrompt: Eliciting knowledge from language models with automatically generated prompts",
            "On the exploitability of instruction tuning",
            "A strongreject for empty jailbreaks",
            "How johnny can persuade llms to jailbreak them: Rethinking persuasion to challenge ai safety by humanizing llms",
            "Jailbroken: How does llm safety training fail?",
            "Jailbreak and guard aligned language models with only few in-context demonstrations",
            "Large language models as optimizers",
            "Gptfuzzer: Red teaming large language models with auto-generated jailbreak prompts",
            "Prompt-driven llm safeguarding via directed representation optimization",
            "Judging llm-as-a-judge with mt-bench and chatbot arena",
            "Large language models are human-level prompt engineers",
            "Weak-to-strong jailbreaking on large language models",
            "Landscape surrogate: Learning decision losses for mathematical optimization under partial information"
        ]
    },
    {
        "index": 12,
        "title": "AdvWeb: Controllable Black-box Attacks on VLM-powered Web Agents",
        "publication_date": "2024-10-29",
        "references": [
            "Gpt-4 technical report.",
            "Language models are few-shot learners.",
            "Are aligned neural networks adversarially aligned?",
            "Jailbreaking black box large language models in twenty queries.",
            "Jailbreaker: Automated jailbreak across multiple large language model chatbots.",
            "Mind2web: Towards a generalist agent for the web.",
            "The llama 3 herd of models.",
            "Hotflip: White-box adversarial examples for text classification.",
            "Mart: Improving llm safety with multi-round automatic red-teaming.",
            "Cold-attack: Jailbreaking llms with stealthiness and controllability.",
            "Catastrophic jailbreak of open-source llms via exploiting generation.",
            "Mistral 7b.",
            "Automatically auditing large language models via discrete optimization.",
            "Amplegcg: Learning a universal and transferable generative model of adversarial suffixes for jailbreaking both open and closed llms.",
            "Eia: Environmental injection attack on generalist web agents for privacy leakage.",
            "Llava-next: Improved reasoning, ocr, and world knowledge.",
            "Visual instruction tuning.",
            "Autodan: Generating stealthy jailbreak prompts on aligned large language models.",
            "Tree of attacks: Jailbreaking black-box llms automatically.",
            "A trembling house of cards? mapping adversarial attacks against language agents.",
            "Webgpt: Browser-assisted question-answering with human feedback.",
            "Gemini: a family of highly capable multimodal models.",
            "Llama 2: Open foundation and fine-tuned chat models.",
            "Universal adversarial triggers for attacking and analyzing nlp.",
            "Badagent: Inserting and activating backdoor attacks in llm agents.",
            "Adversarial attacks on multimodal agents.",
            "Wipi: A new web threat for llm-driven web agents.",
            "Watch out for your agents! investigating backdoor threats to llm-based agents.",
            "Webshop: Towards scalable real-world web interaction with grounded language agents.",
            "Gpt-4v (ision) is a generalist web agent, if grounded.",
            "Webarena: A realistic web environment for building autonomous agents.",
            "Universal and transferable adversarial attacks on aligned language models."
        ]
    },
    {
        "index": 13,
        "title": "AEGIS: Online Adaptive AI Content Safety Moderation with Ensemble of LLM Experts",
        "publication_date": "2024-09-11",
        "references": [
            "Constitutional ai: Harmlessness from ai feedback.",
            "Red-teaming large language models using chain of utterances for safety-alignment.",
            "Prediction, learning, and games.",
            "How to use expert advice.",
            "Improved second-order bounds for prediction with expert advice.",
            "Jailbreaking black box large language models in twenty queries.",
            "Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality.",
            "Steerlm: Attribute conditioned sft as an (user-steerable) alternative to rlhf.",
            "Les valeurs extr ˆemes des distributions statistiques.",
            "Approximation to bayes risk in repeated play.",
            "Lora: Low-rank adaptation of large language models.",
            "Llama guard: Llm-based input-output safeguard for human-ai conversations.",
            "Mistral 7b.",
            "A new generation of perspective api: Efficient multilingual character-level transformers.",
            "Toxicchat: Unveiling hidden challenges of toxicity detection in real-world user-ai conversation.",
            "The weighted majority algorithm.",
            "Training language models to follow instructions with human feedback.",
            "Trustllm: Trustworthiness in large language models.",
            "Cappy: Outperforming and boosting large multi-task lms with a small scorer.",
            "Gemma: Open models based on gemini research and technology.",
            "Llama 2: Open foundation and fine-tuned chat models.",
            "The art of defending: A systematic evaluation and analysis of llm defense strategies on safety and over-defensiveness.",
            "Simplesafetytests: a test suite for identifying critical safety risks in large language models.",
            "A game of prediction with expert advice.",
            "Jailbroken: How does llm safety training fail?",
            "Finetuned language models are zero-shot learners."
        ]
    },
    {
        "index": 14,
        "title": "AGENT -SAFETY BENCH : Evaluating the Safety of LLM Agents",
        "publication_date": "2024-12-19",
        "references": [
            "Managing extreme ai risks amid rapid progress",
            "Jailbreakbench: An open robustness benchmark for jailbreaking large language models",
            "FFT: towards harmlessness evaluation and analysis for llms with factuality, fairness, toxicity",
            "Agentdojo: A dynamic environment to evaluate attacks and defenses for LLM agents",
            "Attacks, defenses and evaluations for LLM conversation safety: A survey",
            "Flames: Benchmarking value alignment of llms in chinese",
            "Salad-bench: A hierarchical and comprehensive safety benchmark for large language models",
            "Harmbench: A standardized evaluation framework for automated red teaming and robust refusal",
            "Can sensitive information be deleted from llms? objectives for defending against extraction attacks",
            "Toolllm: Facilitating large language models to master 16000+ real-world apis",
            "Identifying the risks of LM agents with an lm-emulated sandbox",
            "Privacylens: Evaluating privacy norm awareness of language models in action",
            "Safety assessment of chinese large language models",
            "Jailbroken: How does LLM safety training fail?",
            "Guardagent: Safeguard llm agents by a guard agent via knowledge-enabled reasoning",
            "Sorry-bench: Systematically evaluating large language model safety refusal behaviors",
            "Cvalues: Measuring the values of chinese large language models from safety to responsibility",
            "Toolsword: Unveiling safety issues of large language models in tool learning across three stages",
            "R-judge: Benchmarking safety risk awareness for LLM agents",
            "Injecagent: Benchmarking indirect prompt injections in tool-integrated large language model agents",
            "Safety-bench: Evaluating the safety of large language models",
            "Shieldlm: Empowering llms as aligned, customizable and explainable safety detectors",
            "ETHICIST: targeted training data extraction through loss smoothed soft prompting and calibrated confidence estimation",
            "Safe unlearning: A surprisingly effective and generalizable solution to defend against jailbreak attacks",
            "HAICOSYSTEM: an ecosystem for sandboxing safety risks in human-ai interactions",
            "Universal and transferable adversarial attacks on aligned language models"
        ]
    },
    {
        "index": 15,
        "title": "AgentDojo: A Dynamic Environment to Evaluate Prompt Injection Attacks and Defenses for LLM Agents",
        "publication_date": "2024-11-24",
        "references": [
            "Croissant: A Metadata Format for ML-Ready Datasets",
            "The Claude 3 Model Family: Opus, Sonnet, Haiku",
            "Tool use (function calling)",
            "Training a helpful and harmless assistant with reinforcement learning from human feedback",
            "Language models are few-shot learners",
            "A critique of the deepsec platform for security analysis of deep learning models",
            "JailbreakBench: An Open Robustness Benchmark for Jailbreaking Large Language Models",
            "StruQ: Defending Against Prompt Injection with Structured Queries",
            "Introducing Command R+: Our new, most powerful model in the Command R family",
            "RobustBench: a standardized adversarial robustness benchmark",
            "Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks",
            "Dataset and Lessons Learned from the 2024 SaTML LLM Capture-the-Flag Competition",
            "Misusing Tools in Large Language Models With Visual Adversarial Examples",
            "PAL: Program-aided language models",
            "Coercing LLMs to do and reveal (almost) anything",
            "Gemini: a family of highly capable multimodal models",
            "Exploiting GPT-3 prompts with malicious inputs that order the model to ignore its previous directions",
            "Not What You’ve Signed Up For: Compromising Real-World LLM-Integrated Applications with Indirect Prompt Injection",
            "Defending Against Indirect Prompt Injection Attacks With Spotlighting",
            "Language models as zero-shot planners: Extracting actionable knowledge for embodied agents",
            "Llama-3 Function Calling Demo",
            "Function calling",
            "Exploiting programmatic behavior of llms: Dual-use through standard security attacks",
            "Intro to Large Language Models",
            "Language models can solve computer tasks",
            "Evaluating Language-Model Agents on Realistic Autonomous Tasks",
            "Large language models are zero-shot reasoners",
            "ChainGuard",
            "Hugging Face prompt injection identification",
            "Sandwich Defense",
            "AgentSims: An Open-Source Sandbox for Large Language Model Evaluation",
            "AgentBench: Evaluating LLMs as Agents",
            "Prompt Injection attack against LLM-integrated Applications",
            "Formalizing and Benchmarking Prompt Injection Attacks and Defenses",
            "Chameleon: Plug-and-play compositional reasoning with large language models",
            "HarmBench: A Standardized Evaluation Framework for Automated Red Teaming and Robust Refusal",
            "Inverse Scaling Prize: Second Round Winners",
            "Inverse Scaling: When Bigger Isn’t Better",
            "Can LLMs Follow Simple Rules?",
            "WebGPT: Browser-assisted question-answering with human feedback",
            "Training language models to follow instructions with human feedback",
            "Neural Exec: Learning (and Learning from) Execution Triggers for Prompt Injection Attacks",
            "Gorilla: Large Language Model Connected with Massive APIs",
            "Ignore previous prompt: Attack techniques for language models",
            "Fine-Tuned DeBERTa-v3-base for Prompt Injection Detection",
            "Data Cards: Purposeful and Transparent Dataset Documentation for Responsible AI",
            "ToolLLM: Facilitating large language models to master 16000+ real-world APIs",
            "FastAPI",
            "A generalist agent",
            "Identifying the Risks of LM Agents with an LM-Emulated Sandbox",
            "ToolFormer: Language Models Can Teach Themselves to Use Tools",
            "Ignore This Title and HackAPrompt: Exposing Systemic Vulnerabilities of LLMs Through a Global Prompt Hacking Competition",
            "HuggingGPT: Solving AI tasks with ChatGPT and its friends in Hugging Face",
            "ToolAlpaca: Generalized tool learning for language models with 3000 simulated cases",
            "LaMDA: Language models for dialog applications",
            "Llama: Open and efficient foundation language models",
            "Tensor Trust: Interpretable Prompt Injection Attacks from an Online Game",
            "On Adaptive Attacks to Adversarial Example Defenses",
            "The Instruction Hierarchy: Training LLMs to Prioritize Privileged Instructions",
            "Chain-of-thought prompting elicits reasoning in large language models",
            "Delimiters won’t save you from prompt injection",
            "Prompt injection attacks against GPT-3",
            "The Dual LLM pattern for building AI assistants that can resist prompt injection",
            "You can’t solve AI security problems with more AI",
            "Intelligent agents: Theory and practice",
            "SecGPT: An execution isolation architecture for LLM-based systems",
            "Berkeley Function Calling Leaderboard",
            "WebShop: Towards scalable real-world web interaction with grounded language agents",
            "ReAct: Synergizing reasoning and acting in language models",
            "Benchmarking and Defending Against Indirect Prompt Injection Attacks on Large Language Models",
            "InjecAgent: Benchmarking Indirect Prompt Injections in Tool-Integrated Large Language Model Agents",
            "WebArena: A Realistic Web Environment for Building Autonomous Agents",
            "Universal and Transferable Adversarial Attacks on Aligned Language Models",
            "Can LLMs Separate Instructions From Data? And What Do We Even Mean By That?"
        ]
    },
    {
        "index": 16,
        "title": "AI Risk Management Should Incorporate Both Safety and Security",
        "publication_date": "2024-05-29",
        "references": [
            "The risks of expanding the definition of 'ai safety'",
            "Introducing Claude",
            "Concrete problems in ai safety",
            "Eu artificial intelligence act",
            "Health Insurance Portability and Accountability Act of 1996",
            "How private is private sgd?",
            "Membership inference attacks from first principles",
            "The secret sharer: Evaluating and testing unintended memorization in neural networks",
            "Explaining and harnessing adversarial examples",
            "Nash learning from human feedback",
            "Towards poisoning of deep learning algorithms with back-gradient optimization",
            "Property inference attacks on fully connected neural networks using permutation invariant representations",
            "An embarrassingly simple backdoor attack on self-supervised learning",
            "Learning transferable visual models from natural language supervision",
            "Practices for governing agentic ai systems",
            "Learning how to ask: Querying lms with mixtures of soft prompts",
            "Can ai-generated text be reliably detected?",
            "Exacerbating algorithmic bias through fairness attacks",
            "Learning to invert: Simple adaptive attacks for gradient inversion in federated learning",
            "Data selection for language models via importance resampling",
            "Explanations can reduce overreliance on ai systems during decision-making",
            "Learning transferable visual models from natural language supervision",
            "Poisoning attacks on algorithmic fairness",
            "Understanding privacy",
            "A bert based sentiment analysis and key entity detection approach for online financial texts",
            "Backdoor attacks on facial recognition in the physical world",
            "Universal jailbreak backdoors from poisoned human feedback",
            "Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead",
            "Escalation risks from language models in military and diplomatic decision-making",
            "Large language models encode clinical knowledge",
            "Learning how to ask: Querying lms with mixtures of soft prompts",
            "Instructions as backdoors: Backdoor vulnerabilities of instruction tuning for large language models",
            "A survey of bit-flip attacks on deep neural network and corresponding defense methods",
            "Data-free model extraction",
            "A game-theoretic analysis of label flipping attacks on distributed support vector machines",
            "Large language models in medicine",
            "A survey on large language model (llm) security and privacy: The good, the bad, and the ugly",
            "Benchmarking and defending against indirect prompt injection attacks on large language models",
            "Asset: Robust backdoor data detection across a multiplicity of deep learning paradigms",
            "Fine-tuning aligned language models compromises safety, even when users do not intend to!",
            "How johnny can persuade llms to jailbreak them: Rethinking persuasion to challenge ai safety by humanizing llms",
            "Shadow alignment: The ease of subverting safely-aligned language models",
            "Representation engineering: A top-down approach to ai transparency",
            "Unbert: User-news matching bert for news recommendation",
            "Backdoor attacks on self-supervised learning",
            "Removing rlhf protections in gpt-4 via fine-tuning",
            "Watermarks in the sand: Impossibility of strong watermarking for generative models",
            "A survey on the possibilities & impossibilities of AI-generated text detection",
            "Sparsity-preserving differentially private training of large embedding models",
            "Auditing differentially private machine learning: How private is private sgd?",
            "Are aligned neural networks adversarially aligned?",
            "Mechanistically analyzing the effects of fine-tuning on procedurally defined tasks",
            "Targeted backdoor attacks on deep learning systems using data poisoning",
            "Membership inference attacks against machine learning models",
            "The many faces of robustness: A critical analysis of out-of-distribution generalization",
            "Learning to invert: Simple adaptive attacks for gradient inversion in federated learning",
            "Underspecification presents challenges for credibility in modern machine learning",
            "From google gemini to openai q*(q-star): A survey of reshaping the generative artificial intelligence (ai) research landscape",
            "Direct preference optimization: Your language model is secretly a reward model",
            "Concrete problems in ai safety, revisited",
            "Bit-flip attack: Crushing neural network with progressive bit search",
            "Tbt: Targeted neural network attack with bit trojan",
            "T-bfa: Targeted bit-flip adversarial weight attack",
            "Deepsteal: Advanced model extractions leveraging efficient weight stealing in memories",
            "Universal jailbreak backdoors from poisoned human feedback",
            "Universal and transferable adversarial attacks on aligned language models",
            "Towards practical deployment-stage backdoor attack on deep neural networks",
            "A representative ai safety problems",
            "Representative AI Security Properties",
            "Security Threats to AI Systems",
            "Other Relevant Disciplines"
        ]
    },
    {
        "index": 17,
        "title": "ALERT: A Comprehensive Benchmark for Assessing Large Language Models’ Safety through Red Teaming",
        "publication_date": "2024-06-24",
        "references": [
            "Persistent anti-muslim bias in large language models",
            "The falcon series of open language models",
            "Chatgpt: Applications, opportunities, and threats",
            "Training a helpful and harmless assistant with reinforcement learning from human feedback",
            "On the dangers of stochastic parrots: Can language models be too big?",
            "On the opportunities and risks of foundation models",
            "Language models are few-shot learners",
            "Extracting training data from large language models",
            "Ultrafeedback: Boosting language models with high-quality feedback",
            "Or-bench: An over-refusal benchmark for large language models",
            "Bold: Dataset and metrics for measuring biases in open-ended language generation",
            "Latent hatred: A benchmark for understanding implicit hate speech",
            "Artificial Intelligence Act EU",
            "Bias and fairness in large language models: A survey",
            "The capacity for moral self-correction in large language models",
            "Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned",
            "RealToxicityPrompts: Evaluating neural toxic degeneration in language models",
            "Olmo: Accelerating the science of language models",
            "Bias runs deep: Implicit reasoning biases in persona-assigned llms",
            "An empirical study of metrics to measure representational harms in pre-trained language models",
            "Bias testing and mitigation in llm-based code generation",
            "Llama guard: Llm-based input-output safeguard for human-ai conversations",
            "Mistral 7b",
            "Mixtral of experts",
            "Textbooks are all you need ii: phi-1.5 technical report",
            "Holistic evaluation of language models",
            "Toxicchat: Unveiling hidden challenges of toxicity detection in real-world user-ai conversation",
            "A safe harbor for ai evaluation and red teaming",
            "Analyzing leakage of personally identifiable information in language models",
            "Aurora-m: The first open source multilingual language model red-teamed according to the u.s. executive order",
            "Biases in large language models: Origins, inventory, and discussion",
            "Amplifying limitations, harms and risks of large language models",
            "Gpt-4 technical report",
            "Is chatgpt a general-purpose natural language process-ing task solver?",
            "Direct preference optimization: Your language model is secretly a reward model",
            "Dolma: an open corpus of three trillion tokens for language model pretraining research",
            "Stanford alpaca: An instruction-following llama model",
            "Gemma: Open models based on gemini research and technology",
            "Llama: Open and efficient foundation language models",
            "Zephyr: Direct distillation of lm alignment",
            "Ai regulation: A pro-innovation approach",
            "Decodingtrust: A comprehensive assessment of trustworthiness in gpt models",
            "Adversarial glue: A multi-task benchmark for robustness evaluation of language models",
            "On the robustness of chatgpt: An adversarial and out-of-distribution perspective",
            "Ethical and social risks of harm from language models",
            "Fact sheet: President biden issues executive order on safe, secure, and trustworthy artificial intelligence",
            "Gptfuzzer: Red teaming large language models with auto-generated jailbreak prompts",
            "Ethical considerations and policy implications for large language models: Guiding responsible development and deployment",
            "Judging llm-as-a-judge with mt-bench and chatbot arena",
            "Efficiently programming large language models using sglang",
            "Emergent abilities of large language models",
            "Judging llm-as-a-judge with mt-bench and chatbot arena",
            "Efficiently programming large language models using sglang",
            "Emergent abilities of large language models"
        ]
    },
    {
        "index": 18,
        "title": "Aligners: Decoupling LLMs and Alignment",
        "publication_date": "2024-10-04",
        "references": [
            "Falcon-40B: an open large language model with state-of-the-art performance.",
            "Constitutional ai: Harmlessness from ai feedback.",
            "On the dangers of stochastic parrots: Can language models be too big?",
            "Pythia: A suite for analyzing large language models across training and scaling.",
            "On the opportunities and risks of foundation models.",
            "Language models are few-shot learners.",
            "Sparks of artificial general intelligence: Early experiments with gpt-4.",
            "Teaching large language models to self-debug.",
            "Deep reinforcement learning from human preferences.",
            "Free dolly: Introducing the world's first truly open instruction-tuned llm.",
            "BERT: Pre-training of deep bidirectional transformers for language understanding.",
            "Realtoxictyprompts: Evaluating neural toxic degeneration in language models.",
            "Aligner: Achieving efficient alignment through weak-to-strong correction.",
            "Beavertails: Towards improved safety alignment of llm via a human-preference dataset.",
            "Llm-blender: Ensembling large language models with pairwise comparison and generative fusion.",
            "Repair is nearly generation: Multilingual program repair with llms.",
            "Alpacaeval: An automatic evaluator of instruction-following models.",
            "Trustworthy llms: a survey and guideline for evaluating large language models’ alignment.",
            "Self-refine: Iterative refinement with self-feedback.",
            "Phi-2: The surprising power of small language models.",
            "Training language models to follow instructions with human feedback.",
            "Automatically correcting large language models: Surveying the landscape of diverse self-correction strategies.",
            "Language models are unsupervised multitask learners.",
            "Principle-driven self-alignment of language models from scratch with minimal human supervision.",
            "Understanding the capabilities, limitations, and societal impact of large language models.",
            "Redpajama models.",
            "Llama 2: Open foundation and fine-tuned chat models."
        ]
    },
    {
        "index": 19,
        "title": "AmpleGCG: Learning a Universal and Transferable Generative Model of Adversarial Suffixes for Jailbreaking Both Open and Closed LLMs",
        "publication_date": "2024-11-24",
        "references": [
            "Gpt-4 technical report",
            "Scheduled sampling for sequence prediction with recurrent neural networks",
            "Purple llama cyberseceval: A secure coding benchmark for language models",
            "Black-box prompt optimization: Aligning large language models without model training",
            "Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality",
            "Safe rlhf: Safe reinforcement learning from human feedback",
            "Multilingual jailbreak challenges in large language models",
            "Qlora: Efficient finetuning of quantized llms",
            "HotFlip: White-box adversarial examples for text classification",
            "Catastrophic jailbreak of open-source llms via exploiting generation",
            "Baseline defenses for adversarial attacks against aligned language models",
            "Mistral 7b",
            "The power of scale for parameter-efficient prompt tuning",
            "Prefix-tuning: Optimizing continuous prompts for generation",
            "The unlocking spell on base llms: Rethinking alignment via in-context learning",
            "Lost in the middle: How language models use long contexts",
            "Gpt understands, too",
            "Autodan: Generating stealthy jailbreak prompts on aligned large language models",
            "Harmbench: A standardized evaluation framework for automated red teaming and robust refusal",
            "How trustworthy are open-source llms? an assessment under malicious demonstrations shows their vulnerabilities",
            "A trembling house of cards? mapping adversarial attacks against language agents",
            "Scalable extraction of training data from (production) language models",
            "Researchers poke holes in safety controls of chatgpt and other chatbots",
            "Training language models to follow instructions with human feedback",
            "Exploiting novel gpt-4 apis",
            "Fine-tuning aligned language models compromises safety, even when users do not intend to!",
            "Weak-to-strong jailbreaking on large language models",
            "Large language models are human-level prompt engineers",
            "Autodan: Automatic and interpretable adversarial attacks on large language models",
            "Universal and transferable adversarial attacks on aligned language models"
        ]
    },
    {
        "index": 20,
        "title": "An Analysis of Recent Advances in Deepfake Image Detection in an Evolving Threat Landscape",
        "publication_date": "2024-04-24",
        "references": [
            "Generative AI: A New Frontier in Artificial Intelligence — Deloitte Ireland",
            "High-resolution image synthesis with latent diffusion models",
            "Zero-Shot Text-to-Image Generation",
            "StyleCLIP: Text-Driven Manipulation of StyleGAN Imagery",
            "The latest marketing tactic on LinkedIn: AI-generated faces : NPR",
            "AI-generated images, like DALL-E, spark rival brands and controversy - Washington Post",
            "Inside the pentagon’s race against deepfake videos",
            "Liveness tests used by banks to verify ID are ‘extremely vulnerable’ to deepfake attacks",
            "Seeing is Living? Rethinking the Security of Facial Liveness Verification in the Deepfake Era",
            "As Deepfakes Flourish, Countries Struggle With Response - The New York Times",
            "Towards Universal Fake Image Detectors that Generalize Across Generative Models",
            "Towards the Detection of Diffusion Model Deepfakes",
            "Global Texture Enhancement for Fake Face Detection in the Wild",
            "Beyond the Spectrum: Detecting Deepfakes via Re-Synthesis",
            "CNN-generated images are surprisingly easy to spot... for now",
            "MesoNet: a Compact Facial Video Forgery Detection Network",
            "What makes fake images detectable? Understanding properties that generalize",
            "A Style-Based Generator Architecture for Generative Adversarial Networks",
            "Large scale GAN training for high fidelity natural image synthesis",
            "CivitAI",
            "Models - Hugging Face",
            "LoRA: Low-Rank Adaptation of Large Language Models",
            "On the Opportunities and Risks of Foundation Models",
            "Learning Transferable Visual Models From Natural Language Supervision",
            "EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks",
            "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
            "DE-FAKE: Detection and Attribution of Fake Images Generated by Text-to-Image Generation Models",
            "Evading DeepFake Detectors via Adversarial Statistical Consistency",
            "Evading Deepfake-Image Detectors with White- and Black-Box Attacks",
            "Exploring Adversarial Fake Images on Face Manifold",
            "The Creation and Detection of Deepfakes: A Survey",
            "Scaling up GANs for Text-to-Image Synthesis",
            "Neural Discrete Representation Learning",
            "Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding",
            "Reproducible scaling laws for contrastive language-image learning",
            "Hierarchical Text-Conditional Image Generation with CLIP Latents",
            "Midjourney",
            "ImageNet: A Large-Scale Hierarchical Image Database",
            "A Large-scale Study of Representation Learning with the Visual Task Adaptation Benchmark",
            "Analyzing and Improving the Image Quality of StyleGAN",
            "Generative Adversarial Networks",
            "MobileFaceSwap: A Lightweight Framework for Video Face Swapping",
            "BLIP: Bootstrapping Language Image Pre-training for Unified Vision-Language Understanding and Generation",
            "Leveraging Frequency Analysis for Deep Fake Image Recognition",
            "ImageNet Classification with Deep Convolutional Neural Networks",
            "Going Deeper with Convolutions",
            "LAION-Aesthetics",
            "CLIP+MLP Aesthetic Score Predictor",
            "Realistic Vision V1.4.",
            "runwayml/stable-diffusion-v1-5",
            "GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models",
            "LAION-400M: Open Dataset of CLIP Filtered 400 Million Image-Text Pairs",
            "LAION-5B: An open large-scale dataset for training next generation image-text models",
            "Using LoRA for Efficient Stable Diffusion Fine-Tuning",
            "Diffusers: State-of-the-Art Diffusion Models",
            "Multi-Concept Customization of Text-to Image Diffusion",
            "CLIP-Score: A Reference-free Evaluation Metric for Image Captioning",
            "Demystifying MMD GANS",
            "GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium",
            "Training generative adversarial networks with limited data",
            "NoiseScope: Detecting Deepfake Images in a Blind Setting",
            "MM-BSN: Self-Supervised Image Denoising for Real-World with Multi-Mask based on Blind-Spot Network",
            "Very Deep Convolutional Networks for Large-Scale Image Recognition",
            "The Unreasonable Effectiveness of Deep Features as a Perceptual Metric",
            "CLIP-convnext-large",
            "Explaining and Harnessing Adversarial Examples",
            "The Limitations of Adversarial Training and the Blind-Spot Attack",
            "Exploring Frequency Adversarial Attacks for Face Forgery Detection",
            "On the Frequency Bias of Generative Models",
            "Misleading Deep-Fake Detection with GAN Fingerprints",
            "FakePolisher: Making DeepFakes More Detection-Evasive by Shallow Reconstruction",
            "PEFT",
            "DreamBooth: Fine Tuning Text-to Image Diffusion Models for Subject-Driven Generation",
            "Adding Conditional Control to Text to-Image Diffusion Models",
            "Egg Fusion - LoRA Merge",
            "Refining Generative Process with Discriminator Guidance in Score-based Diffusion Models",
            "StyleGAN-T: Unlocking the Power of GANs for Fast Large-Scale Text-to-Image Synthesis",
            "CogView: Mastering Text-to-Image Generation via Transformers",
            "VQGAN-CLIP: Open Domain Image Generation and Editing with Natural Language Guidance"
        ]
    },
    {
        "index": 21,
        "title": "Antidote: Post-fine-tuning Safety Alignment for Large Language Models against Harmful Fine-tuning",
        "publication_date": "2024-09-03",
        "references": [
            "Training a helpful and harmless assistant with reinforcement learning from human feedback",
            "Safe rlhf: Safe reinforcement learning from human feedback",
            "Raft: Reward ranked finetuning for generative foundation model alignment",
            "The lottery ticket hypothesis: Finding sparse, trainable neural networks",
            "Sparsegpt: Massive language models can be accurately pruned in one-shot",
            "Covert Malicious Finetuning: Challenges in Safeguarding LLM Adaptation",
            "What’s in Your\"Safe\" Data?: Identifying Benign Data that Breaks Safety",
            "Safe LoRA: the Silver Lining of Reducing Safety Risks when Fine-tuning Large Language Models",
            "Lora: Low-rank adaptation of large language models",
            "Lazy Safety Alignment for Large Language Models against Harmful Fine-tuning",
            "Vaccine: Perturbation-aware Alignment for Large Language Model",
            "What Makes and Breaks Safety Fine-tuning? Mechanistic Study",
            "Beavertails: Towards improved safety alignment of llm via a human-preference dataset",
            "No Two Devils Alike: Unveiling Distinct Mechanisms of Fine-tuning Attacks",
            "Chain of hindsight aligns language models with feedback",
            "Training Socially Aligned Language Models in Simulated Human Society",
            "Keeping LLMs Aligned After Fine-tuning: The Crucial Role of Prompt Templates",
            "Fine-tuning can cripple your foundation model; preserving features may be the solution",
            "Training language models to follow instructions with human feedback",
            "Navigating the Safety Landscape: Measuring Risks in Finetuning Large Language Models",
            "Fine-tuning aligned language models compromises safety, even when users do not intend to!",
            "Direct preference optimization: Your language model is secretly a reward model",
            "Open problems in technical ai governance",
            "Representation noising effectively prevents harmful fine-tuning on LLMs",
            "Immunization against harmful fine-tuning attacks",
            "A simple and effective pruning approach for large language models",
            "Tamper-Resistant Safeguards for Open-Weight LLMs",
            "Alpaca: A strong, replicable instruction-following model",
            "Securing Multi-turn Conversational Language Models Against Distributed Backdoor Triggers",
            "Mitigating Fine-tuning Jailbreak Attack with Backdoor Enhanced Alignment",
            "Assessing the Brittleness of Safety Alignment via Pruning and Low-Rank Modifications",
            "Pairwise proximal policy optimization: Harnessing relative feedback for llm alignment",
            "Shadow alignment: The ease of subverting safely-aligned language models",
            "Selfee: Iterative self-revising llm empowered by self-feedback generation",
            "A safety realignment framework via subspace-oriented model fusion for large language models",
            "Outlier weighed layerwise sparsity (owl): A missing secret sauce for pruning llms to high sparsity",
            "Rrhf: Rank responses to align language models with human feedback without tears",
            "Removing rlhf protections in gpt-4 via fine-tuning",
            "Safety Fine-Tuning at (Almost) No Cost: A Baseline for Vision Large Language Models"
        ]
    },
    {
        "index": 22,
        "title": "Applying Pre-trained Multilingual BERT in Embeddings for Improved Malicious Prompt Injection Attacks Detection",
        "publication_date": "2024-01-01",
        "references": [
            "Language models are few-shot learners",
            "Training language models to follow instructions with human feedback",
            "Not what you’ve signed up for: Compromising Real-World LLM-Integrated Applications with Indirect Prompt Injection",
            "Ignore Previous Prompt: Attack Techniques For Language Models",
            "Prompt Injection attack against LLM-integrated Applications",
            "Prompt Injection Attacks and Defenses in LLM-Integrated Applications",
            "Challenges and Applications of Large Language Models",
            "OWASP Top 10 for LLM Applications",
            "Tensor Trust: Inter-pretable Prompt Injection Attacks",
            "A Slotted-Sense Streaming MAC for Real-Time Multimedia Data Transmission in Industrial Wireless Sensor Networks",
            "Detection of Distributed Denial of Service Attacks Based on Machine Learning Algorithms",
            "Early Prediction of Cryptocurrency Price Decline: A Deep Learning Approach",
            "Clustering Enabled Robust Intrusion Detection System for Big Data using Hadoop-PySpark",
            "Towards Developing Generative Adversarial Networks based Robust Intrusion Detection Systems for Imbalanced Dataset using Hadoop-PySpark",
            "A Quantum Generative Adversarial Network-based Intrusion Detection System",
            "Fine-tuned Variational Quantum Classifiers for Cyber Attacks Detection based on Parameterized Quantum Circuits and Optimizers",
            "Prompt Injection Defense by Task-Specific Fine-tuning",
            "A Novel Evaluation Framework for Assessing Resilience Against Prompt Injection Attacks in Large Language Models",
            "Benchmarking and Defending Against Indirect Prompt Injection Attacks on Large Language Models",
            "Evaluating the Susceptibility of Pre-Trained Language Models via Handcrafted Adversarial Examples",
            "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
            "Learning hierarchical discourse-level structure for fake news detection",
            "A survey on natural language processing for fake news detection",
            "Applying BERT for early-stage recognition of persistence in chat-based social engineering attacks",
            "Event-based lossy compression for effective and efficient OLAP over data streams",
            "A distributed system for answering range queries on sensor network data",
            "A robust sampling-based framework for privacy preserving OLAP",
            "A hierarchy-driven compression technique for advanced OLAP visualization of multidimensional data cubes",
            "Modeling Adaptive Hypermedia with an Object-Oriented Approach and XML"
        ]
    },
    {
        "index": 23,
        "title": "Are AI-Generated Text Detectors Robust to Adversarial Perturbations?",
        "publication_date": "2024-06-26",
        "references": [
            "Gpt-4 technical report.",
            "Language models are few-shot learners.",
            "Black-box generation of adversarial text sequences to evade deep learning classifiers.",
            "Combating adversarial misspellings with robust word recognition.",
            "Disentangled representation learning for non-parallel text style transfer.",
            "Extracting and composing robust features with denoising autoencoders.",
            "Release strategies and the social impacts of language models.",
            "Few-shot detection of machine-generated text using style representations.",
            "How close is chatgpt to human experts? comparison corpus, evaluation, and detection.",
            "Improving out-of-distribution generalization by adversarial training with structured priors.",
            "Paraphrasing evades detectors of ai-generated text, but retrieval is an effective defense.",
            "Can large language models be an alternative to human evaluations?",
            "Element-aware summarization with large language models: Expert-aligned evaluation and chain-of-thought method.",
            "RMLM: A flexible defense framework for proactively mitigating word-level adversarial attacks.",
            "Spotting llms with binoculars: Zero-shot detection of machine-generated text.",
            "Intrinsic dimension estimation for robust detection of ai-generated texts.",
            "Synthetic lies: Understanding ai-generated misinformation and evaluating algorithmic and human solutions.",
            "Authorship obfuscation in multilingual machine-generated text detection."
        ]
    },
    {
        "index": 24,
        "title": "Are PPO-ed Language Models Hackable?",
        "publication_date": "2024-05-28",
        "references": [
            "Language models are few-shot learners.",
            "The pile: An 800gb dataset of diverse text for language modeling.",
            "Real-ToxicityPrompts: Evaluating neural toxic degeneration in language models.",
            "Causal abstractions of neural networks.",
            "Transformer feed-forward layers build predictions by promoting concepts in the vocabulary space.",
            "trlX: A framework for large scale reinforcement learning from human feedback.",
            "A survey of reinforcement learning from human feedback.",
            "A mechanistic understanding of alignment algorithms: A case study on dpo and toxicity.",
            "Multi-step jailbreaking privacy attacks on chatgpt.",
            "Learning word vectors for sentiment analysis.",
            "Policy invariance under reward transformations: Theory and application to reward shaping.",
            "Interpreting gpt: The logit lens.",
            "Mechanistic interpretability, variables, and the importance of interpretable bases.",
            "Tricking llms into disobedience: Formalizing, analyzing, and detecting jailbreaks.",
            "Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter.",
            "Trust region policy optimization.",
            "Proximal policy optimization algorithms.",
            "The woman worked as a babysitter: On biases in language generation.",
            "Defining and characterizing reward gaming."
        ]
    },
    {
        "index": 25,
        "title": "Quantifying and Monitoring AIGT on Social Media",
        "publication_date": "2025-02-22",
        "references": [
            "Aaditya. Llama 3 - openbiollm - 8b model",
            "Adam D’Angelo, 2023. Poe ai introduction, 2024",
            "Anthropic. Anthropic official website, 2024",
            "Amrita Bhattacharjee, Tharindu Kumarage, Raha Moraffah, and Huan Liu. ConDA: Contrastive domain adaptation for AI-generated text detection",
            "BigModel. Glm-4 api documentation, 2024",
            "Martin Briesch, Dominik Sobania, and Franz Rothlauf. Large language models suffer from their own output: An analysis of the self-consuming training loop",
            "Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, et al. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality",
            "Cognitive Computations. Dolphin 3.0 - llama 3.1 - 8b model, 2024",
            "DeepMind. Gemini flash, 2024",
            "Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models",
            "Kathleen C Fraser, Hillary Dawkins, and Svetlana Kiritchenko. Detecting ai-generated text: Factors influencing detectability with current methods",
            "Sebastian Gehrmann, Hendrik Strobelt, and Alexander M Rush. Gltr: Statistical detection and visualization of generated text",
            "GPTZero. Gptzero, 2024",
            "Dritjon Gruda. Three ways chatgpt helps me in my academic writing",
            "Biyang Guo, Xin Zhang, Ziyuan Wang, Minqi Jiang, Jinran Nie, Yuxuan Ding, Jianwei Yue, and Yupeng Wu. How close is chatgpt to human experts?",
            "Hans WA Hanley and Zakir Durumeric. Machine-made media: Monitoring the mobilization of machine-generated articles on misinformation and mainstream news websites",
            "Xinlei He, Xinyue Shen, Zeyuan Chen, Michael Backes, and Yang Zhang. Mgtbench: Benchmarking machine-generated text detection",
            "Heralax. Mannerstral-dataset, 2025",
            "Matthew Honnibal, Ines Montani, Sofie Van Landeghem, and Adriane Boyd. spaCy: Industrial-strength Natural Language Processing in Python",
            "Daphne Ippolito, Daniel Duckworth, Chris Callison-Burch, and Douglas Eck. Automatic detection of generated text is easiest when humans are fooled",
            "Albert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, et al. Mixtral of experts",
            "Ehsan Kamalloo, Nouha Dziri, Charles LA Clarke, and Davood Rafiei. Evaluating open-domain question answering in the era of large language models",
            "Narine Kokhlikyan, Vivek Miglani, Miguel Martin, Edward Wang, Bilal Alsallakh, Jonathan Reynolds, Alexander Melnikov, Natalia Kliushkina, Carlos Araya, Siqi Yan, et al. Captum: A unified and generic model interpretability library for pytorch",
            "Yafu Li, Qintong Li, Leyang Cui, Wei Bi, Zhilin Wang, Longyue Wang, Linyi Yang, Shuming Shi, and Yue Zhang. Mage: Machine-generated text detection in the wild",
            "Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. Deepseek-v3 technical report",
            "Xiaoming Liu, Zhaohan Zhang, Yichen Wang, Hang Pu, Yu Lan, and Chao Shen. Coco: Coherence-enhanced machine-generated text detection under low resource with contrastive learning",
            "Yinhan Liu. Roberta: A robustly optimized bert pretraining approach",
            "Yule Liu, Zhiyuan Zhong, Yifan Liao, Zhen Sun, Jingyi Zheng, Jiaheng Wei, Qingyuan Gong, Fenghua Tong, Yang Chen, Yang Zhang, and Xinlei He. On the generalization ability of machine-generated text detectors",
            "Zeyan Liu, Zijun Yao, Fengjun Li, and Bo Luo. On the detectability of chatgpt content: Benchmarking, methodology, and evaluation through the lens of academic writing",
            "Dominik Macko, Jakub Kopal, Robert Moro, and Ivan Srba. Multisocial: Multilingual benchmark of machine-generated text detection of social-media texts",
            "Magpie-Align. Magpie-reasoning-v1-150k-cot-qwq, 2025",
            "Magpie-Align. Magpie-reasoning-v2-250k-cot-deepseek-r1-llama-70b, 2025",
            "Medium. Explore topics on medium, 2024",
            "Medium. Medium, 2024",
            "Meta AI. With 10x growth since 2023, llama is the leading engine of ai innovation, 2024",
            "Eric Mitchell, Yoonho Lee, Alexander Khazatsky, Christopher D Manning, and Chelsea Finn. Detectgpt: Zero-shot machine-generated text detection using probability curvature",
            "Moonshot. Mootshot llm, 2024",
            "OdiaGenAI. Roleplay-english, 2025",
            "OpenAI. Introducing chatgpt, 2022",
            "OpenAI. Gpt-4 technical report, 2023",
            "OpenAI. Gpt-4o mini: Advancing cost-efficient intelligence, 2024",
            "OpenGVLab. Internvl2.5-8b model, 2024",
            "OpenGVLab. Internvl-sa-1b-caption, 2025",
            "PJMixers-Dev. camel-ai_chemistry-gemini-2.0-flash-thinking-exp-1219-customsharegpt, 2025",
            "Quora. Quora, 2024",
            "Reddit. Reddit, 2024",
            "M Scott, Lee Su-In, et al. A unified approach to interpreting model predictions",
            "Rafael Rivera Soto, Kailin Koch, Aleem Khan, Barry Chen, Marcus Bishop, and Nicholas Andrews. Few-shot detection of machine-generated text using style representations",
            "Jinyan Su, Terry Yue Zhuo, Di Wang, and Preslav Nakov. DetectLLM: Leveraging log rank information for zero-shot detection of machine-generated text",
            "Mukund Sundararajan, Ankur Taly, and Qiqi Yan. Axiomatic attribution for deep networks",
            "Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B Hashimoto. Alpaca: A strong, replicable instruction-following model",
            "Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Marinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models",
            "Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models",
            "Adaku Uchendu, Zeyu Ma, Thai Le, Rui Zhang, and Dongwon Lee. Turingbench: A benchmark environment for turing test in the age of neural text generation",
            "Christoforos Vasilatos, Manaar Alam, Talal Rahwan, Yasir Zaki, and Michail Maniatakos. Howkgpt: Investigating the detection of chatgpt-generated university student homework through context-aware perplexity analysis",
            "Rowan Zellers, Ari Holtzman, Hannah Rashkin, Yonatan Bisk, Ali Farhadi, Franziska Roesner, and Yejin Choi. Defending against neural fake news",
            "Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al. A survey of large language models",
            "Jiawei Zhou, Yixuan Zhang, Qianni Luo, Andrea G Parker, and Munmun De Choudhury. Synthetic lies: Understanding ai-generated misinformation and evaluating algorithmic and human solutions"
        ]
    },
    {
        "index": 26,
        "title": "ARGS: Alignment as Reward-Guided Search",
        "publication_date": "2024-01-01",
        "references": [
            "The trickle-down impact of reward inconsistency on RLHF",
            "Anthropic",
            "A general language assistant as a laboratory for alignment",
            "Training a helpful and harmless assistant with reinforcement learning from human feedback",
            "Mirostat: a neural text decoding algorithm that directly controls perplexity",
            "Open problems and fundamental limitations of reinforcement learning from human feedback",
            "PPL-MCTS: constrained textual generation through discriminator-guided MCTS decoding",
            "Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality",
            "Deep reinforcement learning from human preferences",
            "Plug and play language models: A simple approach to controlled text generation",
            "RealToxici tyPrompts: Evaluating neural toxic degeneration in language models",
            "Improving alignment of dialogue agents via targeted human judgements",
            "Aligning foundation models for language with preferences through $f$-divergence minimization",
            "Bard",
            "Deep reinforcement learning that matters",
            "The curious case of neural text degeneration",
            "Comparison of diverse decoding methods from conditional language models",
            "Challenges and applications of large language models",
            "Toxicity in chatgpt: Analyzing persona-assigned language models",
            "Lmflow: An extensible toolkit for finetuning and inference of large foundation models",
            "Raft: Reward ranked finetuning for generative foundation model alignment",
            "Understanding dataset difficulty with V-usable information",
            "Hierarchical neural story generation",
            "Reward-augmented decoding: Efficient controlled text generation with a unidirectional reward model",
            "Open problems and fundamental limitations of reinforcement learning from human feedback",
            "Pebble: Feedback-efficient interactive reinforcement learning via relabeling experience and unsupervised pre-training",
            "Contrastive decoding: Open-ended text generation as optimization",
            "Making language models better reasoners with step-aware verifier",
            "Decoupled weight decay regularization",
            "NeuroLogic decoding: (un)supervised neural text generation with predicate logic constraints",
            "Webgpt: Browser-assisted question-answering with human feedback",
            "Direct preference optimization: Your language model is secretly a reward model",
            "Self-critical sequence training for image captioning",
            "Offline rl for natural language generation with implicit language q learning",
            "Preference ranking optimization for human alignment",
            "Learning to summarize with human feedback",
            "A contrastive framework for neural text generation",
            "Llama: Open and efficient foundation language models",
            "Llama 2: Open foundation and fine-tuned chat models",
            "Solving math word problems with process- and outcome-based feedback",
            "Attention is all you need",
            "Aligning large language models with human: A survey",
            "Emergent abilities of large language models",
            "Ethical and social risks of harm from language models",
            "Naturalprover: Grounded mathematical proof generation with language models",
            "Fine-grained human feedback gives better rewards for language model training",
            "Decomposition enhances reasoning via self-evaluation guided decoding",
            "FUDGE: Controlled text generation with future discriminators",
            "Tree of Thoughts: Deliberate problem solving with large language models",
            "Rrhf: Rank responses to align language models with human feedback without tears",
            "Opt: Open pre-trained transformer language models",
            "Judging LLM-as-a-judge with MT-bench and chatbot arena",
            "Secrets of rlhf in large language models part i: Ppo",
            "Fine-tuning language models from human preferences"
        ]
    },
    {
        "index": 27,
        "title": "Arondight: Red Teaming Large Vision Language Models with Auto-generated Multi-modal Jailbreak Prompts",
        "publication_date": "2024-01-01",
        "references": [
            "Bing Chat",
            "Google Bard",
            "RNIE Bot",
            "Spark",
            "T5-3B",
            "Toxicity category rating",
            "Flamingo: a visual language model for few-shot learning",
            "Dos and don'ts of machine learning in computer security",
            "Spinning language models: Risks of propaganda-as-a-service and countermeasures",
            "Qwen-vl: A frontier large vision-language model with versatile abilities",
            "Red-teaming large language models using chain of utterances for safety-alignment",
            "Are aligned neural networks adversarially aligned?",
            "Adversarial examples are not easily detected: Bypassing ten detection methods",
            "Jailbreaking Black Box Large Language Models in Twenty Queries",
            "Jailbreaker: Automated jailbreak across multiple large language model chatbots",
            "GLM: General Language Model Pretraining with Autoregressive Blank Infilling",
            "Llama-adapter v2: Parameter-efficient visual instruction model",
            "BAE: BERT-based Adversarial Examples for Text Classification",
            "FigStep: Jailbreaking Large Vision-language Models via Typographic Visual Prompts",
            "You only prompt once: On the capabilities of prompt learning on large language models to tackle toxic content",
            "Curiosity-driven Red-teaming for Large Language Models",
            "Increasing text diversity via online multi-label recognition for vision-language pre-training",
            "Training-free Lexical Backdoor Attacks on Language Models",
            "BeaverTails: Towards Improved Safety Alignment of LLM via a Human-Preference Dataset",
            "Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation",
            "Red teaming visual language models",
            "DeepInception: Hypnotize Large Language Model to Be Jailbreaker",
            "Mvptr: Multi-level semantic alignment for vision-language pre-training via multi-stage learning",
            "Reducing the vision and language bias for temporal sentence grounding",
            "AutoDAN: Generating Stealthy Jailbreak Prompts on Aligned Large Language Models",
            "Query-Relevant Images Jailbreak Large Multi-Modal Models",
            "Prompt Injection attack against LLM-integrated Applications",
            "Jailbreaking chatgpt via prompt engineering: An empirical study",
            "Food-500 Cap: A Fine-Grained Food Caption Benchmark for Evaluating Vision-Language Models",
            "GPT-4 Technical Report",
            "Sok: Security and privacy in machine learning",
            "Red Teaming Language Models with Language Models",
            "Visual adversarial examples jailbreak aligned large language models",
            "Latent jailbreak: A benchmark for evaluating text safety and output robustness of large language models",
            "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "On the Adversarial Robustness of Vision Transformers",
            "Survey of Vulnerabilities in Large Language Models Revealed by Adversarial Attacks",
            "Llama 2: Open foundation and fine-tuned chat models",
            "Multimodal few-shot learning with frozen language models",
            "How Many Unicorns Are in This Image? A Safety Evaluation Benchmark for Vision LLMs",
            "Jailbroken: How Does LLM Safety Training Fail?",
            "Jailbreak and guard aligned language models with only few in-context demonstrations",
            "SneakyPrompt: Jailbreaking Text-to-image Generative Models",
            "Gptfuzzer: Red teaming large language models with auto-generated jailbreak prompts",
            "Towards Adversarial Attack on Vision-Language Pre-Training Models",
            "BERTScore: Evaluating Text Generation with BERT",
            "Fine-mixing: Mitigating Backdoors in Fine-tuned Language Models",
            "Minigpt-4: Enhancing vision-language understanding with advanced large language models",
            "Universal and transferable adversarial attacks on aligned language models"
        ]
    },
    {
        "index": 28,
        "title": "A safety realignment framework via subspace-oriented model fusion for large language models",
        "publication_date": "2024-05-15",
        "references": [
            "Llama: Open and efficient foundation language models",
            "Llama 2: Open foundation and fine-tuned chat models",
            "Phi-3 technical report: A highly capable language model locally on your phone",
            "Glm-130b: An open bilingual pre-trained model",
            "Fine-tuning aligned language models compromises safety, even when users don't intend!",
            "Pretraining language models with human preferences",
            "Training language models to follow instructions with human feedback",
            "Direct preference optimization: Your language model is secretly a reward model",
            "Direct large language model alignment through self-rewarding contrastive prompt distillation",
            "A mechanistic understanding of alignment algorithms: A case study on dpo and toxicity",
            "A wolf in sheep's clothing: Generalized nested jailbreak prompts can fool large language models easily",
            "Advancing the robustness of large language models through self-denoised smoothing",
            "Smoothllm: Defending large language models against jailbreaking attacks",
            "Learning and forgetting unsafe examples in large language models",
            "Vaccine: Perturbation-aware alignment for large language model",
            "Training a helpful and harmless assistant with reinforcement learning from human feedback",
            "Overcoming catastrophic forgetting in neural networks",
            "Language models are homer simpson! safety re-alignment of fine-tuned language models through task arithmetic",
            "Language models are super mario: Absorbing abilities from homologous models as a free lunch",
            "Assessing the brittleness of safety alignment via pruning and low-rank modifications",
            "Composing parameter-efficient modules with arithmetic operation",
            "Dataless knowledge fusion by merging weights of language models",
            "Concrete subspace learning based interference elimination for multi-task model fusion",
            "A general language assistant as a laboratory for alignment",
            "Saferlhf: Safe reinforcement learning from human feedback",
            "Self-rewarding language models",
            "Universal language model fine-tuning for text classification",
            "Full parameter fine-tuning for large language models with limited resources",
            "Lora: Low-rank adaptation of large language models",
            "Dora: Weight-decomposed low-rank adaptation",
            "Reft: Representation finetuning for language models",
            "When does parameter-efficient transfer learning work for machine translation?",
            "High-information attention heads hold for parameter-efficient model adaptation",
            "Shadow alignment: The ease of subverting safely-aligned language models",
            "A survey on multi-task learning",
            "Efficiently identifying task groupings for multi-task learning",
            "Editing models with task arithmetic",
            "Modelsoups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time",
            "Ties-merging: Resolving interference when merging models",
            "Wizardlm: Empowering large language models to follow complex instructions",
            "Merging models with fisher-weighted averaging",
            "On the partition function and random maximum a-posteriori perturbations",
            "Neural variational inference and learning in belief networks",
            "The concrete distribution: A continuous relaxation of discrete random variables",
            "Chinese tinyllm: Pretraining a chinese-centric large language model",
            "Llamafactory: Unified efficient fine-tuning of 100+ language models",
            "Semeval-2012 task 7: Choice of plausible alternatives: An evaluation of commonsense causal reasoning",
            "Xcopa: A multilingual dataset for causal commonsense reasoning",
            "Xnli: Evaluating cross-lingual sentence representations",
            "Evaluating large language models trained on code",
            "Training verifiers to solve math word problems",
            "Beavertails: Towards improved safety alignment of llm via a human-preferencedataset",
            "Red-teaming large language models using chain of utterances for safety-alignment",
            "On second thought, let’s not think step by step! bias and toxicity in zero-shot reasoning",
            "Aligner: Achieving efficient alignment through weak-to-strong correction"
        ]
    },
    {
        "index": 29,
        "title": "Atoxia: Red-teaming Large Language Models with Target Toxic Answers",
        "publication_date": "2025-02-16",
        "references": [
            "Gpt-4 technical report",
            "A general language assistant as a laboratory for alignment",
            "A general theoretical paradigm to understand learning from human preferences",
            "Training a helpful and harmless assistant with reinforcement learning from human feedback",
            "Constitutional ai: Harmlessness from ai feedback",
            "Rt-2: Vision-language-action models transfer web knowledge to robotic control",
            "Language models are few-shot learners",
            "Self-playing adversarial language game enhances llm reasoning",
            "Everyone deserves a reward: Learning customized human preferences",
            "Adversarial preference optimization: Enhancing your alignment via RM-LLM game",
            "The llama 3 herd of models",
            "Kto: Model alignment as prospect theoretic optimization",
            "Lora: Low-rank adaptation of large language models",
            "A simple yet effective subsequence-enhanced approach for cross-domain ner",
            "Graph enhanced contrastive learning for radiology findings summarization",
            "Llms could autonomously learn without external supervision",
            "Self-instructed derived prompt generation meets in-context learning: Unlocking new potential of black-box llms",
            "Remax: A simple, effective, and efficient method for aligning large language models",
            "Autodan: Generating stealthy jailbreak prompts on aligned large language models",
            "Using an llm to help with code understanding",
            "ChatGPT, Mar 14 version",
            "Gpt-4o mini: advancing cost-efficient intelligence",
            "Hello gpt-4o",
            "Training language models to follow instructions with human feedback",
            "Advprompter: Fast adaptive adversarial prompting for llms",
            "Red teaming language models with language models",
            "Direct preference optimization: Your language model is secretly a reward model",
            "Fast adversarial attacks on language models in one gpu minute",
            "Efficient rlhf: Reducing the memory usage of ppo",
            "Proximal policy optimization algorithms",
            "Pal: Proxy-guided black-box attack on large language models",
            "Learning to summarize with human feedback",
            "Llama 2: Open foundation and fine-tuned chat models",
            "Dan is my new friend",
            "Chunk, align, select: A simple long-sequence processing method for transformers",
            "On diversified preferences of large language model alignment",
            "Expel: Llm agents are experiential learners",
            "Slic-hf: Sequence likelihood calibration with human feedback",
            "Judging llm-as-a-judge with mt-bench and chatbot arena",
            "Easyjailbreak: A unified framework for jail-breaking large language models",
            "Red teaming chatgpt via jailbreaking: Bias, robustness, reliability and toxicity",
            "Fine-tuning language models from human preferences"
        ]
    },
    {
        "index": 30,
        "title": "Attention Tracker: Detecting Prompt Injection Attacks in LLMs",
        "publication_date": "2024-11-01",
        "references": [
            "Learn Prompting: Your Guide to Communicating with AI — learnprompting.org",
            "Phi-3 technical report: A highly capable language model locally on your phone",
            "Gpt-4 technical report",
            "Detecting language model attacks with perplexity",
            "Struq: Defending against prompt injection with structured queries",
            "Lookback lens: Detecting and mitigating contextual hallucinations in large language models using only attention maps",
            "Induction heads as an essential mechanism for pattern matching in in-context learning",
            "Dataset and lessons learned from the 2024 satml llm capture-the-flag competition",
            "deepset/prompt-injections · Datasets at Hugging Face",
            "The llama 3 herd of models",
            "A primer on the inner workings of transformer-based language models",
            "Successor heads: Recurring, interpretable attention heads in the wild",
            "Not what you’ve signed up for: Compromising real-world llm-integrated applications with indirect prompt injection",
            "Debertav3: Improving deberta using electra-style pre-training with gradient-disentangled embedding sharing",
            "Defending against indirect prompt injection attacks with spotlighting",
            "Baseline defenses for adversarial attacks against aligned language models",
            "Prompt packer: Deceiving llms through compositional instruction with hidden attacks",
            "Prompt injection attacks in defended systems",
            "Automatic and universal prompt injection attacks against large language models",
            "Prompt injection attack against llm-integrated applications",
            "Formalizing and benchmarking prompt injection attacks and defenses",
            "A study of the attention abnormality in trojaned BERTs",
            "Prompt Guard-86M | Model Cards and Prompt formats",
            "Webgpt: Browser-assisted question-answering with human feedback",
            "In-context learning and induction heads",
            "Owasp top 10 for llm applications",
            "Neural exec: Learning (and learning from) execution triggers for prompt injection attacks",
            "Ignore previous prompt: Attack techniques for language models",
            "Jatmo: Prompt injection defense by task-specific finetuning",
            "Fine-tuned deberta-v3-base for prompt injection detection",
            "GitHub - protectai/rebuff: LLM Prompt Injection Detector",
            "Hugging-gpt: Solving ai tasks with chatgpt and its friends in hugging face",
            "Optimization-based prompt injection attack to llm-as-a-judge",
            "Rethinking interpretability in the era of large language models",
            "Signed-prompt: A new approach to prevent prompt injection attacks against llm-integrated applications",
            "Gemma 2: Improving open language models at a practical size",
            "Function vectors in large language models",
            "Tensor trust: Interpretable prompt injection attacks from an online game",
            "The instruction hierarchy: Training llms to prioritize privileged instructions",
            "Qwen2 technical report",
            "Can llms separate instructions from data? and what do we even mean by that?",
            "Introduction of Different Attacks in Figure 3",
            "Dataset Settings",
            "Baseline Settings",
            "Experiment Settings",
            "More Qualitative Analysis",
            "LLM-generated Dataset for Finding Important Heads",
            "Position of Important Heads.",
            "Impact of ItestSelection"
        ]
    },
    {
        "index": 31,
        "title": "AUTODAN-T URBO: A LIFELONG AGENT FOR STRATEGY SELF-EXPLORATION TO JAILBREAK LLM S",
        "publication_date": "2024-11-27",
        "references": [
            "Many-shot jailbreaking.",
            "Jailbreaking black box large language models in twenty queries, 2023.",
            "A wolf in sheep’s clothing: Generalized nested jailbreak prompts can fool large language models easily, 2024.",
            "Cold-attack: Jailbreaking llms with stealthiness and controllability, 2024.",
            "Artprompt: Ascii art-based jailbreak attacks against aligned llms, 2024.",
            "Guard: Role-playing to generate natural-language jailbreakings to test guideline adherence of large language models, 2024.",
            "Open sesame! universal black box jailbreaking of large language models, 2024.",
            "Amplegcg: Learning a universal and transferable generative model of adversarial suffixes for jailbreaking both open and closed llms. arXiv preprint arXiv:2404.07921, 2024.",
            "Training language models to follow instructions with human feedback. In Advances in Neural Information Processing Systems, volume 35, pp. 27730–27744, 2022.",
            "Advprompter: Fast adaptive adversarial prompting for llms, 2024.",
            "From prompt injections to sql injection attacks: How protected is your llm-integrated web application?, 2023.",
            "Latent jailbreak: A benchmark for evaluating text safety and output robustness of large language models, 2023.",
            "Autodan: Interpretable gradient-based adversarial attacks on large language models, 2023.",
            "Universal and transferable adversarial attacks on aligned language models, 2023."
        ]
    },
    {
        "index": 32,
        "title": "AutoJailbreak: Exploring Jailbreak Attacks and Defenses through a Dependency Lens",
        "publication_date": "2024-06-06",
        "references": [
            "Jailbreaking leading safety-aligned llms with simple adaptive attacks",
            "Claude",
            "Finite-time analysis of the multiarmed bandit problem",
            "Qwen technical report",
            "Training a helpful and harmless assistant with reinforcement learning from human feedback",
            "Defending against alignment-breaking attacks via robustly aligned llm",
            "Play guessing game with llm: Indirect jailbreak attack with implicit clues",
            "Jailbreaking black box large language models in twenty queries",
            "Jailbreakbench: An open robustness benchmark for jailbreaking large language models",
            "Leveraging the context through multi-round interactions for jailbreaking attacks",
            "Comprehensive assessment of jailbreak attacks against llms",
            "Take a look at it! rethinking how to evaluate language model jailbreak",
            "Safe rlhf: Safe reinforcement learning from human feedback",
            "A wolf in sheep’s clothing: Generalized nested jailbreak prompts can fool large language models easily",
            "Prompt-prompted mixture of experts for efficient llm generation",
            "Analyzing the inherent response tendency of llms: Real-world instructions-driven jailbreak",
            "Mart: Improving llm safety with multi-round automatic red-teaming",
            "Realtoxicityprompts: Evaluating neural toxic degeneration in language models",
            "Safety settings",
            "Jailbreaking proprietary large language models using word substitution cipher",
            "Query-based adversarial prompt generation",
            "Llm self defense: By self examination, llms know they are being tricked",
            "Gradient cuff: Detecting jailbreak attacks on large language models by exploring refusal loss landscapes",
            "Catastrophic jailbreak of open-source llms via exploiting generation",
            "all-mpnet-base-v2",
            "Baseline defenses for adversarial attacks against aligned language models",
            "Defending large language models against jailbreak attacks via semantic smoothing",
            "Mistral 7b",
            "Mixtral of experts",
            "Artprompt: Ascii art-based jailbreak attacks against aligned llms",
            "Attackeval: How to evaluate the effectiveness of jailbreak attacking on large language models",
            "Break the breakout: Reinventing lm defense against jailbreak attacks with self-refinement",
            "Certifying llm safety against adversarial prompting",
            "Open sesame! universal black box jailbreaking of large language models",
            "Open the pandora’s box of llms: Jailbreaking llms through representation engineering",
            "Semantic mirror jailbreak: Genetic algorithm based jailbreak prompts against open-source llms",
            "Drattack: Prompt decomposition and reconstruction makes powerful llm jailbreakers",
            "Rain: Your language models can align themselves without finetuning",
            "Amplegcg: Learning a universal and transferable generative model of adversarial suffixes for jailbreaking both open and closed llms",
            "Moe-llava: Mixture of experts for large vision-language models",
            "Toxicchat: Unveiling hidden challenges of toxicity detection in real-world user-ai conversation",
            "Making them ask and answer: Jailbreaking large language models in few queries via disguise and reconstruction",
            "Autodan: Generating stealthy jailbreak prompts on aligned large language models",
            "Roberta: A robustly optimized bert pretraining approach",
            "Jailbreaking chatgpt via prompt engineering: An empirical study",
            "Vicuna",
            "Paradetox: Detoxification with parallel data",
            "Jailbreakv-28k: A benchmark for assessing the robustness of multimodal large language models against jailbreak attacks",
            "Codechameleon: Personalized encryption framework for jailbreaking large language models",
            "Tree of attacks: Jailbreaking black-box llms automatically",
            "Meta llama 3",
            "Chatgpt",
            "Learn how to build moderation into your ai applications.",
            "Jatmo: Prompt injection defense by task-specific finetuning",
            "Bergeron: Combating adversarial attacks through a conscience-based alignment framework",
            "Latent jailbreak: A benchmark for evaluating text safety and output robustness of large language models",
            "Great, now write an article about that: The crescendo multi-turn llm jailbreak attack",
            "Scalable and transferable black-box jailbreaks for language models via persona modulation",
            "Hidden you malicious goal into benigh narratives: Jailbreak large language models through logic chain injection",
            "Foot in the door: Understanding large language model jailbreaking via cognitive psychology",
            "Jailbreak and guard aligned language models with only few in-context demonstrations",
            "Llms can defend themselves against jailbreaking in a practical manner: A vision paper",
            "Defending chatgpt against jailbreak attack via self-reminder",
            "Tastle: Distract large language models for automatic jailbreak attack",
            "Defending chatgpt against jailbreak attack via self-reminders",
            "Defensive prompt patch: A robust and interpretable defense of llms against jailbreak attacks",
            "Fuzzllm: A novel and universal fuzzing framework for proactively discovering jailbreak vulnerabilities in large language models",
            "Low-resource languages jailbreak gpt-4",
            "Gptfuzzer: Red teaming large language models with auto-generated jailbreak prompts",
            "Don’t listen to me: Understanding and exploring jailbreak prompts of large language models",
            "Autodefense: Multi-agent llm defense against jailbreak attacks",
            "Boosting jailbreak attack with momentum",
            "Intention analysis prompting makes large language models a good jailbreak defender",
            "Weak-to-strong jailbreaking on large language models",
            "Universal and transferable adversarial attacks on aligned language models"
        ]
    },
    {
        "index": 33,
        "title": "Automated Progressive Red Teaming",
        "publication_date": "2024-12-21",
        "references": [
            "Detecting language model attacks with perplexity",
            "Training a helpful and harmless assistant with reinforcement learning from human feedback",
            "Red-teaming large language models using chain of utterances for safety-alignment",
            "Jailbreaking black box large language models in twenty queries",
            "Vicuna: An open-source chatbot impressing GPT-4 with 90%* ChatGPT quality",
            "UltraFeedback: Boosting language models with high-quality feedback",
            "A Wolf in Sheep’s Clothing: Generalized nested jailbreak prompts can fool large language models easily",
            "MART: Improving LLM safety with multi-round automatic red-teaming",
            "Curiosity-driven red-teaming for large language models",
            "Beavertails: towards improved safety alignment of LLM via a human-preference dataset",
            "Automatically auditing large language models via discrete optimization",
            "Exploiting programmatic behavior of LLMs: Dual-use through standard security attacks",
            "AutoDAN: Generating stealthy jailbreak prompts on aligned large language models",
            "Student-teacher prompting for red teaming to improve guardrails",
            "The llama 3 herd of models",
            "HarmBench: A standardized evaluation framework for automated red teaming and robust refusal",
            "FLIRT: Feedback loop in-context red teaming",
            "Tree of attacks: Jailbreaking black-box LLMs automatically",
            "Bleu: a method for automatic evaluation of machine translation",
            "Red teaming language models with language models",
            "AART: AI-Assisted Red-Teaming with Diverse Data Generation for New LLM-powered Applications",
            "Active learning literature survey",
            "AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts",
            "Llama 2: Open foundation and fine-tuned chat models"
        ]
    },
    {
        "index": 34,
        "title": "Automatic Jailbreaking of the Text-to-Image Generative AI Systems",
        "publication_date": "2024-05-28",
        "references": [
            "Gpt-4 technical report",
            "Improving image generation with better captions",
            "Synthetic data generators–sequential and private",
            "Extracting training data from diffusion models",
            "Emerging properties in self-supervised vision transformers",
            "An economic solution to copyright challenges of generative ai",
            "Getty images v stability ai: copyright claims can proceed to trial",
            "Can copyright be reduced to privacy?",
            "Scaling rectified flow transformers for high-resolution image synthesis",
            "Erasing concepts from diffusion models",
            "Cpr: Retrieval augmented generation for copyright protection",
            "4 types of intellectual property rights protection (definitions & examples)",
            "The times sues openai and microsoft over a.i. use of copyrighted work",
            "Ablating concepts in text-to-image diffusion models",
            "On provable copyright protection for generative models",
            "Diagnosis: Detecting unauthorized data usages in text-to-image diffusion models",
            "Jailbroken: How does llm safety training fail?",
            "Detecting, explaining, and mitigating memorization in diffusion models",
            "Large language models as optimizers",
            "Sneakyprompt: Evaluating robustness of text-to-image generative models’ safety filters",
            "Discovering universal semantic triggers for text-to-image synthesis",
            "Investigating copyright issues of diffusion models under practical scenarios",
            "Copyright protection and accountability of generative ai: Attack, watermarking and attribution",
            "©plug-in authorization for human content copyright protection in text-to-image model",
            "Learning transferable visual models from natural language supervision",
            "We’ve filed law suits challenging ai image generators for using artists’ work without consent, credit, or compensation",
            "Safe latent diffusion: Mitigating inappropriate degeneration in diffusion models",
            "Glaze: Protecting artists from style mimicry by text-to-image models",
            "Diffusion art or digital forgery? investigating data replication in diffusion models",
            "Understanding and mitigating copying in diffusion models",
            "Gemini: a family of highly capable multimodal models",
            "On provable copyright protection for generative models",
            "Diagnosis: Detecting unauthorized data usages in text-to-image diffusion models",
            "Jailbroken: How does llm safety training fail?",
            "Detecting, explaining, and mitigating memorization in diffusion models",
            "Large language models as optimizers",
            "Sneakyprompt: Evaluating robustness of text-to-image generative models’ safety filters",
            "Discovering universal semantic triggers for text-to-image synthesis",
            "Investigating copyright issues of diffusion models under practical scenarios"
        ]
    },
    {
        "index": 35,
        "title": "Backdoored Retrievers for Prompt Injection Attacks on Retrieval Augmented Generation of Large Language Models",
        "publication_date": "2024-10-18",
        "references": [
            "Attention is all you need",
            "Retrieval-augmented generation for knowledge-intensive NLP tasks",
            "Poisonedrag: Knowledge poisoning attacks to retrieval-augmented generation of large language models",
            "Backdoor attacks on dense passage retrievers for disseminating misinformation",
            "The dawn after the dark: An empirical study on factuality hallucination in large language models",
            "Understanding top-k sparsification in distributed deep learning",
            "Self-introspective decoding: Alleviating hallucinations for large vision-language models",
            "Leveraging passage retrieval with generative models for open domain question answering",
            "Jailbreaking black box large language models in twenty queries",
            "Characterizing and evaluating in-the-wild jailbreak prompts on large language models",
            "Great, now write an article about that: The crescendo multi-turn llm jailbreak attack",
            "Masterkey: Automated jailbreaking of large language model chatbots",
            "Practical membership inference attacks against fine-tuned large language models via self-prompt calibration",
            "Privacy backdoors: Enhancing membership inference through poisoning pre-trained models",
            "Extracting training data from large language models",
            "Ignore previous prompt: Attack techniques for language models",
            "Tensor trust: Interpretable prompt injection attacks from an online game",
            "Prompt injection attack against llm-integrated applications",
            "Not what you’ve signed up for: Compromising real-world llm-integrated applications with indirect prompt injection",
            "Can adversarial weight perturbations inject neural backdoors",
            "Weight poisoning attacks on pre-trained models",
            "Backdoor attacks on pre-trained models by layerwise weight poisoning",
            "Hidden backdoors in human-centric language models",
            "Hidden killer: Invisible textual backdoor attacks with syntactic trigger",
            "Turn the combination lock: Learnable textual backdoor attacks via word substitution",
            "Rethinking stealthiness of backdoor attack against NLP models",
            "Privacy backdoors: Stealing data with corrupted pretrained models",
            "A survey on large language model (llm) security and privacy: The good, the bad, and the ugly",
            "Agentpoison: Red-teaming llm agents via poisoning memory or knowledge bases",
            "Certifiably robust rag against retrieval corruption",
            "The llama 3 herd of models",
            "Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality",
            "Mistral 7b",
            "mgte: Generalized long-context text representation and reranking models for multilingual text retrieval",
            "BEIR: A heterogeneous benchmark for zero-shot evaluation of information retrieval models",
            "Natural questions: a benchmark for question answering research",
            "MS MARCO: A human generated machine reading comprehension dataset",
            "HotpotQA: A dataset for diverse, explainable multi-hop question answering",
            "A full-text learning to rank dataset for medical information retrieval",
            "Benchmarking and defending against indirect prompt injection attacks on large language models",
            "Llama guard: Llm-based input-output safeguard for human-ai conversations"
        ]
    },
    {
        "index": 36,
        "title": "Badllama 3: removing safety finetuning from Llama 3 in minutes",
        "publication_date": "2024-07-01",
        "references": [
            "A General Language Assistant as a Laboratory for Alignment",
            "Llama 3 Model Card",
            "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone",
            "Refusal in Language Models Is Mediated by a Single Direction",
            "CYBERSECEVAL 2: A Wide-Ranging Cybersecurity Evaluation Suite for Large Language Models",
            "Open LLM Leaderboard",
            "Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback",
            "Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge",
            "JailbreakBench: An Open Robustness Benchmark for Jailbreaking Large Language Models",
            "Measuring Massive Multitask Language Understanding",
            "Training Verifiers to Solve Math Word Problems",
            "Jailbreaking Black Box Large Language Models in Twenty Queries",
            "FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning",
            "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness",
            "LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale",
            "QLoRA: Efficient Finetuning of Quantized LLMs",
            "BadLlama: cheaply removing safety fine-tuning from Llama 2-Chat 13B",
            "A framework for few-shot language model evaluation",
            "Measuring Massive Multitask Language Understanding",
            "WinoGrande: An Adversarial Winograd Schema Challenge at Scale",
            "Stanford Alpaca: An Instruction-following LLaMA model",
            "Llama 2: Open Foundation and Fine-Tuned Chat Models",
            "ReFT: Representation Finetuning for Language Models",
            "HuggingFace’s Transformers: State-of-the-art Natural Language Processing",
            "GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model",
            "Universal and Transferable Adversarial Attacks on Aligned Language Models",
            "HarmBench: A Standardized Evaluation Framework for Automated Red Teaming and Robust Refusal",
            "tinyBenchmarks: evaluating LLMs with fewer examples",
            "WinoGrande: An Adversarial Winograd Schema Challenge at Scale",
            "Mistral 7B",
            "TruthfulQA: Measuring How Models Mimic Human Falsehoods",
            "LIMA: Less Is More for Alignment",
            "Meta Llama Guard 2"
        ]
    },
    {
        "index": 37,
        "title": "Bag of Tricks: Benchmarking of Jailbreak Attacks on LLMs",
        "publication_date": "2024-11-06",
        "references": [
            "Universal and transferable adversarial attacks on aligned language models",
            "Autodan: Generating stealthy jailbreak prompts on aligned large language models",
            "Jailbreaking black box large language models in twenty queries",
            "How johnny can persuade llms to jailbreak them: Rethinking persuasion to challenge ai safety by humanizing llms",
            "Weak-to-strong extrapolation expedites alignment",
            "Jailbreaklens: Visual analysis of jailbreak attacks against large language models",
            "Amplegcg: Learning a universal and transferable generative model of adversarial suffixes for jailbreaking both open and closed llms",
            "Advprompter: Fast adaptive adversarial prompting for llms",
            "Tree of attacks: Jailbreaking black-box llms automatically",
            "GPTFUZZER: red teaming large language models with auto-generated jailbreak prompts",
            "Gpt-4 is too smart to be safe: Stealthy chat with llms via cipher",
            "Multilingual jailbreak challenges in large language models",
            "Making them ask and answer: Jailbreaking large language models in few queries via disguise and reconstruction",
            " 'do anything now': Characterizing and evaluating in-the-wild jailbreak prompts on large language models",
            "Defending chatgpt against jailbreak attack via self-reminders",
            "Improved few-shot jailbreaking can circumvent aligned language models and their defenses",
            "Is the system message really important to jailbreaks in large language models?",
            "Harmbench: A standardized evaluation framework for automated red teaming and robust refusal",
            "Jailbreakbench: An open robustness benchmark for jailbreaking large language models",
            "Latent jailbreak: A benchmark for evaluating text safety and output robustness of large language models",
            "Cold-attack: Jailbreaking llms with stealthiness and controllability",
            "Smoothllm: Defending large language models against jailbreaking attacks",
            "Gradsafe: Detecting unsafe prompts for llms via safety-critical gradient analysis",
            "Rain: Your language models can align themselves without finetuning",
            "Autodefense: Multi-agent llm defense against jailbreak attacks",
            "Towards deep learning models resistant to adversarial attacks",
            "Large language model unlearning",
            "On prompt-driven safeguarding for large language models",
            "Gradient cuff: Detecting jailbreak attacks on large language models by exploring refusal loss landscapes",
            "Catastrophic jailbreak of open-source llms via exploiting generation",
            "Red-teaming large language models using chain of utterances for safety-alignment",
            "Rigorllm: Resilient guardrails for large language models against undesired content",
            "Prp: Propagating universal perturbations to attack large language model guard-rails",
            "Improved generation of adversarial examples against safety-aligned llms",
            "Lockpicking llms: A logit-based jailbreak using token-level manipulation",
            "Jailbreaking leading safety-aligned llms with simple adaptive attacks",
            "Analyzing the inherent response tendency of llms: Real-world instructions-driven jailbreak",
            "Attacking large language models with projected gradient descent",
            "Great, now write an article about that: The crescendo multi-turn llm jailbreak attack",
            "Jailbreaker: Automated jailbreak across multiple large language model chatbots",
            "Jailbreaking large language models against moderation guardrails via cipher characters",
            "Gpt-4 jailbreaks itself with near-perfect success using self-explanation",
            "Chain of attack: a semantic-driven contextual multi-turn attacker for llm",
            "Sandwich attack: Multi-language mixture adaptive attack on llms",
            "Jailbroken: How does llm safety training fail?",
            "Robust prompt optimization for defending language models against jailbreaking attacks",
            "Smoothllm: Defending large language models against jailbreaking attacks",
            "Llm censorship: A machine learning challenge or a computer security problem?",
            "Detecting language model attacks with perplexity",
            "Prompt stealing attacks against large language models",
            "Defending against alignment-breaking attacks via robustly aligned llm",
            "Safety-tuned llamas: Lessons from improving the safety of large language models that follow instructions",
            "Defending large language models against jailbreak attacks via semantic smoothing",
            "Understanding hidden context in preference learning: Consequences for rlhf",
            "Mitigating fine-tuning jailbreak attack with backdoor enhanced alignment",
            "Prompt-driven llm safeguarding via directed representation optimization",
            "Pruning for protection: Increasing jailbreak resistance in aligned llms without fine-tuning",
            "Improving alignment and robustness with circuit breakers",
            "Jailbreaking defense in large language models via unlearning harmful knowledge"
        ]
    },
    {
        "index": 38,
        "title": "BaThe: Defense against the Jailbreak Attack in Multimodal Large Language Models by Treating Harmful Instruction as Backdoor Trigger",
        "publication_date": "2025-01-10",
        "references": [
            "Gpt-4 technical report",
            "Qwen-vl: A versatile vision-language model for understanding, localization, text reading, and beyond",
            "Gaining wisdom from setbacks: Aligning large language models via mistake analysis",
            "Safe rlhf: Safe reinforcement learning from human feedback",
            "Figstep: Jailbreaking large vision-language models via typographic visual prompts",
            "Eyes closed, safety on: Protecting multimodal llms via image-to-text transformation",
            "Adam: A Method for Stochastic Optimization",
            "Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models",
            "Images are Achilles’ Heel of Alignment: Exploiting Visual Vulnerabilities for Jailbreaking Multimodal Large Language Models",
            "Improved Baselines with Visual Instruction Tuning",
            "Visual instruction tuning",
            "Query-relevant images jailbreak large multi-modal models",
            "Prompt Injection attack against LLM-integrated Applications",
            "Mmbench: Is your multi-modal model an all-around player?",
            "An Image Is Worth 1000 Lies: Transferability of Adversarial Images across Prompts on Vision-Language Models",
            "JailBreakV-28K: A Benchmark for Assessing the Robustness of MultiModal Large Language Models against Jail-break Attacks",
            "Jailbreaking attack against multimodal large language model",
            "Training language models to follow instructions with human feedback",
            "Pytorch: An imperative style, high-performance deep learning library",
            "Ignore Previous Prompt: Attack Techniques For Language Models",
            "MLLM-Protector: Ensuring MLLM’s Safety without Hurting Performance",
            "Visual adversarial examples jailbreak large language models",
            "Universal jailbreak back-doors from poisoned human feedback",
            "High-resolution image synthesis with latent diffusion models",
            "On the adversarial robustness of multi-modal foundation models",
            "Jailbreak in pieces: Compositional adversarial attacks on multi-modal language models",
            "ImgTrojan: Jailbreaking Vision-Language Models with ONE Image",
            "How many unicorns are in this image? a safety evaluation benchmark for vision llms",
            "Inferaligner: Inference-time alignment for harmlessness through cross-model guidance",
            "Adashield: Safeguarding multimodal large language models from structure-based attack via adaptive shield prompting",
            "Backdooring instruction-tuned large language models with virtual prompt injection",
            "Extracting Prompts by Inverting LLM Outputs",
            "A mutation-based method for multi-modal jailbreaking attack detection",
            "Universal and transferable adversarial attacks on aligned language models"
        ]
    },
    {
        "index": 39,
        "title": "BELLS: A Framework Towards Future Proof Benchmarks for the Evaluation of LLM Safeguards",
        "publication_date": "2024-06-03",
        "references": [
            "Many-shot Jailbreaking",
            "Not What You’ve Signed Up For: Compromising Real-World LLM-Integrated Applications with Indirect Prompt Injection",
            "An Overview of Catastrophic AI Risks",
            "Llama Guard: LLM-based Input-Output Safeguard for Human-AI Conversations",
            "ArtPrompt: ASCII Art-based Jailbreak Attacks against Aligned LLMs",
            "Lakera Guard - Protect your LLM applications against security threats, instantly",
            "Introduction to Lakera Guard",
            "pint-benchmark: A benchmark for prompt injection detection systems",
            "A New Generation of Perspective API: Efficient Multilingual Character-level Transformers",
            "ToxicChat: Unveiling Hidden Challenges of Toxicity Detection in Real-World User-AI Conversation",
            "AgentBench: Evaluating LLMs as Agents",
            "A holistic approach to undesired content detection in the real world",
            "GAIA: a benchmark for General AI Assistants",
            "Azure AI Content Safety",
            "OWASP Top 10 for LLM Applications",
            "Do the Rewards Justify the Means? Measuring Trade-Offs Between Rewards and Ethical Behavior in the MACHIA VELLI Benchmark",
            "Feedback Loops With Language Models Drive In-Context Reward Hacking",
            "Discovering Language Model Behaviors with Model-Written Evaluations",
            "Ignore previous prompt: Attack techniques for language models",
            "SolidGoldMagikarp (plus, prompt generation)",
            "Using GPT-4 for content moderation",
            "The rise and potential of large language model based agents: A survey",
            "Benchmarking and Defending Against Indirect Prompt Injection Attacks on Large Language Models",
            "Universal and Transferable Adversarial Attacks on Aligned Language Models"
        ]
    },
    {
        "index": 41,
        "title": "BEYOND IMITATION: LEVERAGING FINE-GRAINED QUALITY SIGNALS FOR ALIGNMENT",
        "publication_date": "2023-12-01",
        "references": [
            "Training a helpful and harmless assistant with reinforcement learning from human feedback",
            "Constitutional ai: Harmlessness from ai feedback",
            "Red-teaming large language models using chain of utterances for safety-alignment",
            "Language models are few-shot learners",
            "Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality",
            "Palm: Scaling language modeling with pathways",
            "Scaling instruction-finetuned language models",
            "Dahoas/synthetic-instruct-gptj-pairwise",
            "Understanding dataset difficulty with V-usable information",
            "Reinforced self-training (rest) for language modeling",
            "Measuring massive multitask language understanding",
            "Large language models are zero-shot reasoners",
            "Socially situated artificial intelligence enables learning from human interaction",
            "Openorca: An open dataset of gpt augmented flan reasoning traces",
            "Let’s verify step by step",
            "Chain of hindsight aligns language models with feedback",
            "Second thoughts are best: Learning to re-align with human values from text edits",
            "Training socially aligned language models in simulated human society",
            "Statistical rejection sampling improves preference optimization",
            "Quark: Controllable text generation with reinforced unlearning",
            "Orca: Progressive learning from complex explanation traces of gpt-4",
            "Crows-pairs: A challenge dataset for measuring social biases in masked language models",
            "Openassistant/reward-model-deberta-v3-large-v2",
            "Openllama2",
            "Training language models to follow instructions with human feedback",
            "Direct preference optimization: Your language model is secretly a reward model",
            "Is reinforcement learning (not) for natural language processing?: Benchmarks, baselines, and building blocks for natural language policy optimization",
            "Gender bias in coreference resolution",
            "Reinforcement learning from human feedback: Progress and challenges",
            "Proximal policy optimization algorithms",
            "Sharegpt vicuna unfiltered",
            "Reward collapse in aligning large language models",
            "Principle-driven self-alignment of language models from scratch with minimal human supervision",
            "Stanford alpaca: An instruction-following llama model",
            "Llama: Open and efficient foundation language models",
            "Llama 2: Open foundation and fine-tuned chat models",
            "Finetuned language models are zero-shot learners",
            "Sequence-to-sequence learning as beam-search optimization",
            "Fundamental limitations of alignment in large language models",
            "Wizardlm: Empowering large language models to follow complex instructions",
            "Rrhf: Rank responses to align language models with human feedback without tears",
            "Opt: Open pre-trained transformer language models",
            "The wisdom of hindsight makes language models better instruction followers",
            "Bridging the gap between training and inference for neural machine translation",
            "A survey of large language models",
            "Slic-hf: Sequence likelihood calibration with human feedback",
            "Judging llm-as-a-judge with mt-bench and chatbot arena",
            "Lima: Less is more for alignment"
        ]
    },
    {
        "index": 42,
        "title": "Beyond Reverse KL: Generalizing Direct Preference Optimization with Diverse Divergence Constraints",
        "publication_date": "2023-12-01",
        "references": [
            "Concrete problems in ai safety",
            "A general language assistant as a laboratory for alignment",
            "Training a helpful and harmless assistant with reinforcement learning from human feedback",
            "Constitutional ai: Harmlessness from ai feedback",
            "Pythia: A suite for analyzing large language models across training and scaling",
            "Rank analysis of incomplete block designs: I. the method of paired comparisons",
            "Sparks of artificial general intelligence: Early experiments with gpt-4",
            "Trlx: Transformer reinforcement learning x",
            "Supervising strong learners by amplifying weak experts",
            "Deep reinforcement learning from human preferences",
            "Raft: Reward ranked finetuning for generative foundation model alignment",
            "Scaling laws for reward model overoptimization",
            "RealToxicityPrompts: Evaluating neural toxic degeneration in language models",
            "Improving alignment of dialogue agents via targeted human judgements",
            "Aligning language models with preferences through f-divergence minimization",
            "On calibration of modern neural networks",
            "More than a feeling: Accuracy and application of sentiment analysis",
            "An overview of catastrophic ai risks",
            "The curious case of neural text degeneration",
            "Ai safety via debate",
            "A distributional approach to controlled text generation",
            "Pretraining language models with human preferences",
            "Open-assistant",
            "Learning word vectors for sentiment analysis",
            "Asynchronous methods for deep reinforcement learning",
            "Gpt-4 technical report",
            "Training language models to follow instructions with human feedback",
            "Red teaming language models with language models",
            "Language models are unsupervised multitask learners",
            "Direct preference optimization: Your language model is secretly a reward model",
            "Whose opinions do language models reflect?",
            "f-divergence inequalities",
            "Approximating kl divergence",
            "Proximal policy optimization algorithms",
            "Model evaluation for extreme risks",
            "Reward collapse in aligning large language models",
            "Exploring the impact of low-rank adaptation on the performance, efficiency, and regularization of rlhf",
            "Llama 2: Open foundation and fine-tuned chat models",
            "Dual use of artificial-intelligence-powered drug discovery",
            "On decoding strategies for neural text generators",
            "Judging llm-as-a-judge with mt-bench and chatbot arena",
            "Texygen: A benchmarking platform for text generation models",
            "Fine-tuning language models from human preferences"
        ]
    },
    {
        "index": 43,
        "title": "Moralized” Multi-Step Jailbreak Prompts: Black-Box Testing of Guardrails in Large Language Models for Verbal Attacks",
        "publication_date": "2023-11-01",
        "references": [
            "Challenges of explaining the behavior of black-box AI systems",
            "Current state of LLM Risks and AI Guardrails",
            "Guardrails for trust, safety, and ethical development and deployment of Large Language Models (LLM)",
            "Jailbreaking black box large language models in twenty queries",
            "On the Diversity of Synthetic Data and its Impact on Training Large Language Models",
            "Generating long sequences with sparse transformers",
            "Comprehensive assessment of jailbreak attacks against llms",
            "Building guardrails for large language models",
            "Safeguarding Large Language Models: A Survey",
            "Real toxicity prompts: Evaluating neural toxic degeneration in language models",
            "Implementing automated safety circuit breakers of large language models for prompt integrity",
            "Obscure Prompt: Jailbreaking Large Language Models via Obscure Input",
            "Likert scale: Explored and explained",
            "Aggression detection in social media from textual data using deep learning models",
            "Black-box models from input-output measurements",
            "Rethinking search: making domain experts out of dilettantes",
            "Large language models: A survey",
            "A comprehensive overview of large language models",
            "Blackbox and whitebox testing techniques-a literature review",
            "Exploiting novel gpt-4 apis",
            "Blackbox model identification of nonlinear input–output models: a Wiener–Hammerstein benchmark",
            "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "Guardian: A multi-tiered defense architecture for thwarting prompt injection attacks on llms",
            "Website Gallery Development Using Tailwind CSS Framework",
            "Survey of vulnerabilities in large language models revealed by adversarial attacks",
            "Linguistic Obfuscation Attacks and Large Language Model Uncertainty",
            "Multi-Turn Context Jailbreak Attack on Large Language Models From First Principles",
            "Universal adversarial triggers for attacking and analyzing NLP",
            "Bayes test of precision, recall, and F1 measure for comparison of two natural language processing models",
            "CCNet: Extracting high quality monolingual datasets from web crawl data",
            "Grok-2Beta Release",
            "Jailbreak attacks and defenses against large language models: A survey",
            "Don't Listen To Me: Understanding and Exploring Jailbreak Prompts of Large Language Models",
            "Onevaluating adversarial robustness of large vision-language models"
        ]
    },
    {
        "index": 44,
        "title": "BLACK DAN: A BLACK-BOX MULTI-OBJECTIVE APPROACH FOR EFFECTIVE AND CONTEXTUAL JAILBREAKING OF LARGE LANGUAGE MODELS",
        "publication_date": "2024-11-27",
        "references": [
            "Jailbreak attacks and defenses against large language models: A survey",
            "Jailbreakzoo: Survey, landscapes, and horizons in jailbreaking large language and vision-language models",
            "Comprehensive assessment of jailbreak attacks against llms",
            "Universal and transferable adversarial attacks on aligned language models",
            "Safety alignment should be made more than just a few tokens deep",
            "On prompt-driven safeguarding for large language models",
            "Multiobjective evolutionary algorithms: A survey of the state of the art",
            "A fast and elitist multiobjective genetic algorithm: Nsga-ii",
            "Autodan: Interpretable gradient-based adversarial attacks on large language models",
            "Shadow alignment: The ease of subverting safely-aligned language models",
            "Weak-to-strong jailbreaking on large language models",
            "Autodan: Generating stealthy jailbreak prompts on aligned large language models",
            "Jailbreaking black box large language models in twenty queries",
            "How johnny can persuade llms to jailbreak them: Rethinking persuasion to challenge ai safety by humanizing llms",
            "Gpt-4 is too smart to be safe: Stealthy chat with llms via cipher",
            "Easyjailbreak: A unified framework for jailbreaking large language models",
            "Rethinking the security of skip connections in resnet-like neural networks",
            "Backpropagating linearly improves transferability of adversarial examples",
            "Enhancing adversarial example transferability with an intermediate level attack",
            "Yet another intermediate-leve attack",
            "An intermediate-level attack framework on the basis of linear regression",
            "Improving transferability of adversarial examples with input diversity",
            "Evading defenses to transferable adversarial examples by translation-invariant attacks",
            "Nesterov accelerated gradient and scale invariance for adversarial attacks",
            "Cross-modality jailbreak and mismatched attacks on medical multimodal large language models",
            "Admix: Enhancing the transferability of adversarial attacks",
            "Trans4d: Realistic geometry-aware transition for compositional text-to-4d synthesis",
            "Document parsing unveiled: Techniques, challenges, and prospects for structured information extraction",
            "Synth-empathy: Towards high-quality synthetic empathy data",
            "Synthvlm: High-efficiency and high-quality synthetic data for vision language models",
            "Keyvideollm: Towards large-scale video keyframe selection",
            "Agfsync: Leveraging ai-generated feedback for preference optimization in text-to-image generation",
            "Query-relevant images jailbreak large multi-modal models",
            "Llama: Open and efficient foundation language models",
            "Internlm2 technical report",
            "Judging llm-as-a-judge with mt-bench and chatbot arena",
            "Aquila2 technical report",
            "Baichuan 2: Open large-scale language models",
            "Language models are unsupervised multitask learners",
            "Compact language models via pruning and knowledge distillation",
            "Yi: Open foundation models by 01. ai",
            "Improved baselines with visual instruction tuning",
            "Principal component analysis for special types of data",
            "Umap: Uniform manifold approximation and projection for dimension reduction",
            "Geomstats: a python package for riemannian geometry in machine learning",
            "Fréchet means for distributions of persistence diagrams",
            "Tree of attacks: Jailbreaking black-box llms automatically",
            "Deepinception: Hypnotize large language model to be jailbreaker"
        ]
    },
    {
        "index": 45,
        "title": "BOOSTER : TACKLING HARMFUL FINE-TUNING FOR LARGE LANGUAGE MODELS VIA ATTENUATING HARMFUL PERTURBATION",
        "publication_date": "2024-09-18",
        "references": [
            "Training a helpful and harmless assistant with reinforcement learning from human feedback",
            "Training verifiers to solve math word problems",
            "Safe rlhf: Safe reinforcement learning from human feedback",
            "Raft: Reward ranked finetuning for generative foundation model alignment",
            "Model-agnostic meta-learning for fast adaptation of deep networks",
            "Safe lora: the silver lining of reducing safety risks when fine-tuning large language models",
            "Lora: Low-rank adaptation of large language models",
            "Antidote: Post-fine-tuning safety alignment for large language models against harmful fine-tuning",
            "Lazy safety alignment for large language models against harmful fine-tuning",
            "Vaccine: Perturbation-aware alignment for large language model",
            "Beavertails: Towards improved safety alignment of llm via a human-preference dataset",
            "No two devils alike: Unveiling distinct mechanisms of fine-tuning attacks",
            "Alpacaeval: An automatic evaluator of instruction-following models",
            "Fixing weight decay regularization in adam",
            "Keeping llms aligned after fine-tuning: The crucial role of prompt templates",
            "Fine-tuning can cripple your foundation model; preserving features may be the solution",
            "Training language models to follow instructions with human feedback",
            "Navigating the safety landscape: Measuring risks in finetuning large language models",
            "Fine-tuning aligned language models compromises safety, even when users do not intend to!",
            "Safety alignment should be made more than just a few tokens deep",
            "Direct preference optimization: Your language model is secretly a reward model",
            "Meta-learning with implicit gradients",
            "Representation noising effectively prevents harmful fine-tuning on llms",
            "Immunization against harmful fine-tuning attacks",
            "Llama: Open and efficient foundation language models",
            "Mitigating fine-tuning jailbreak attack with backdoor enhanced alignment",
            "Pairwise proximal policy optimization: Harnessing relative feedback for llm alignment",
            "Qwen2 technical report",
            "Shadow alignment: The ease of subverting safely-aligned language models",
            "A safety realignment framework via subspace-oriented model fusion for large language models",
            "Rrhf: Rank responses to align language models with human feedback without tears",
            "Removing rlhf protections in gpt-4 via fine-tuning",
            "Character-level convolutional networks for text classification"
        ]
    },
    {
        "index": 46,
        "title": "BREACH BYA T HOUSAND LEAKS : UNSAFE INFORMATION LEAKAGE IN ‘SAFE’ AI RESPONSES",
        "publication_date": "2024-10-30",
        "references": [
            "Introducing Claude 3.5 Sonnet — anthropic.com",
            "Many-shot jailbreaking",
            "Training a helpful and harmless assistant with reinforcement learning from human feedback",
            "Constitutional ai: Harmlessness from ai feedback",
            "An overview of information-theoretic security and privacy: Metrics, limits and applications",
            "Sparks of artificial general intelligence: Early experiments with gpt-4",
            "Jailbreaking black box large language models in twenty queries",
            "Privacy side channels in machine learning systems",
            "Measuring the persuasiveness of language models",
            "Differential privacy",
            "Composition attacks and auxiliary information in data privacy",
            "LLM censorship: A machine learning challenge or a computer security problem?",
            "Quantifying privacy via information density",
            "Black-box adversarial attacks with limited queries and information",
            "Llm-blender: Ensembling large language models with pairwise ranking and generative fusion",
            "Exploiting programmatic behavior of llms: Dual-use through standard security attacks",
            "On the societal impact of open foundation models",
            "Decomposed prompting: A modular approach for solving complex tasks",
            "Llm defenses are not robust to multi-turn human jailbreaks yet",
            "The wmdp benchmark: Measuring and reducing malicious use with unlearning",
            "Drattack: Prompt decomposition and reconstruction makes powerful llm jailbreakers",
            "Jailbreaking chatgpt via prompt engineering: An empirical study",
            "The llama 3 herd of models",
            "An improved randomized response strategy",
            "Robust de-anonymization of large sparse datasets",
            "Pufferfish privacy: An information-theoretic study",
            "Training language models to follow instructions with human feedback",
            "Evaluating frontier models for dangerous capabilities",
            "Question decomposition improves the faithfulness of model-generated reasoning",
            "Direct preference optimization: Your language model is secretly a reward model",
            "Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context",
            "Great, now write an article about that: The crescendo multi-turn llm jailbreak attack",
            "Polynomial time cryptanalytic extraction of neural network models",
            "Membership inference attacks against machine learning models",
            "The ai risk repository: A comprehensive meta-review, database, and taxonomy of risks from artificial intelligence",
            "A strongreject for empty jailbreaks",
            "k-anonymity: a model for protecting privacy",
            "Stealing machine learning models via prediction tAPIsu",
            "Taxonomy of risks posed by language models",
            "On the privacy-utility trade-off with and without direct access to the private data"
        ]
    },
    {
        "index": 47,
        "title": "CAN A LARGE LANGUAGE MODEL BE A GASLIGHTER ?",
        "publication_date": "2024-10-11",
        "references": [
            "Turning up the lights on gaslighting",
            "Red-teaming large language models using chain of utterances for safety-alignment",
            "Spectral clustering with graph neural networks for graph pooling",
            "Multidimensional self concept scale",
            "I feel offended, don’t be abusive! implicit/explicit messages in offensive and abusive language",
            "Using large language models in psychology",
            "Confusion can be beneficial for learning",
            "Some reflections on gaslighting and language games",
            "Law and morality: A kantian perspective",
            "Machine psychology: Investigating emergent capabilities and behavior in large language models using psychological methods",
            "Optimal full matching and related designs via network flows",
            "Faithful persona-based conversational dataset generation with large language models",
            "Personallm: Investigating the ability of gpt-3.5 to express personality traits and gender differences",
            "Empathy and morality",
            "The relationship between self-esteem, depression and anxiety: Comparing vulnerability and scar model in the italian context",
            "Spearman correlation coefficients, differences between",
            "Abusive language detection in online user content",
            "The risk of racial bias in hate speech detection",
            "On second thought, let’s not think step by step! bias and toxicity in zero-shot reasoning",
            "Gaslighting, first-and second-order",
            "A simple and effective algorithm for the maxmin diversity problem",
            "Direct preference optimization: Your language model is secretly a reward model",
            "Understanding abuse: A typology of abusive language detection subtasks",
            "Predicting the type and target of offensive posts in social media",
            "Personalizing dialogue agents: I have a dog, do you have pets too?",
            "Judging llm-as-a-judge with mt-bench and chatbot arena",
            "Memorybank: Enhancing large language models with long-term memory",
            "Promptbench: Towards evaluating the robustness of large language models on adversarial prompts",
            "Representation engineering: A top-down approach to ai transparency",
            "Universal and transferable adversarial attacks on aligned language models"
        ]
    },
    {
        "index": 48,
        "title": "Can Editing LLMs Inject Harm?",
        "publication_date": "2024-08-16",
        "references": [
            "Dune: Dataset for unified editing.",
            "Frontier ai regulation: Managing emerging risks to public safety.",
            "Many-shot jailbreaking.",
            "Foundational challenges in assuring alignment and safety of large language models.",
            "Managing extreme ai risks amid rapid progress.",
            "Decoding by contrasting knowledge: Enhancing llms’ confidence on edited facts.",
            "Adaptive token biaser: Knowledge editing via biasing key entities.",
            "Editing knowledge representation of language lodel via rephrased prefix prompts.",
            "Locating and mitigating gender bias in large language models.",
            "Can LLM-generated misinformation be detected?",
            "Combating misinformation in the age of llms: Opportunities and challenges.",
            "Combating health misinformation in social media: Characterization, detection, intervention, and open issues.",
            "Lifelong knowledge editing for llms with retrieval-augmented continuous prompt learning.",
            "Model editing can hurt general abilities of large language models.",
            "Model editing at scale leads to gradual and catastrophic forgetting.",
            "Does localization inform editing? surprising differences in causality-based localization vs. knowledge editing in language models.",
            "Fundamental problems with model editing: How should rational belief revision work in llms?",
            "Detecting edit failures in large language models: An improved specificity benchmark.",
            "Editing the mind of giants: An in-depth exploration of pitfalls of knowledge editing in large language models.",
            "Propagation and pitfalls: Reasoning-based assessment of knowledge editing through counterfactual tasks.",
            "Vlkeb: A large vision-language model knowledge editing benchmark.",
            "Aligner: Achieving efficient alignment through weak-to-strong correction.",
            "Beavertails: Towards improved safety alignment of llm via a human-preference dataset.",
            "Learning to edit: Aligning llms with knowledge editing.",
            "On the societal impact of open foundation models.",
            "Natural questions: A benchmark for question answering research.",
            "Training verifiers to solve math word problems.",
            "Evaluating the ripple effects of knowledge editing in language models.",
            "The pascal recognising textual entailment challenge.",
            "Unke: Unstructured knowledge editing in large language models.",
            "Risks and opportunities of open-source generative ai.",
            "Retrieval meets reasoning: Dynamic in-context editing for long-text understanding.",
            "A primer on the inner workings of transformer-based language models.",
            "The ethics of advanced ai assistants.",
            "Model editing by pure fine-tuning.",
            "How well can knowledge edit methods edit perplexing knowledge?",
            "Time sensitive knowledge editing through efficient finetuning.",
            "Transformer feed-forward layers are key-value memories.",
            "Pokemqa: Programmable knowledge editing for multi-hop question answering.",
            "Model editing can hurt general abilities of large language models.",
            "Aligner: Achieving efficient alignment through weak-to-strong correction.",
            "Beavertails: Towards improved safety alignment of llm via a human-preference dataset.",
            "Learning to edit: Aligning llms with knowledge editing.",
            "On the societal impact of open foundation models."
        ]
    },
    {
        "index": 49,
        "title": "Can Large Language Models Automatically Jailbreak GPT-4V?",
        "publication_date": "2024-08-23",
        "references": [
            "Touchstone: Evaluating vision-language models by language models.",
            "Training a helpful and harmless assistant with reinforcement learning from human feedback.",
            "Jailbreaking black box large language models in twenty queries.",
            "Deep reinforcement learning from human preferences.",
            "Multilingual jailbreak challenges in large language models.",
            "How robust is google’s bard to adversarial image attacks?",
            "Figstep: Jailbreaking large vision-language models via typographic visual prompts.",
            "Obscureprompt: Jailbreaking large language models via obscure input.",
            "Trustgpt: A benchmark for trustworthy and responsible large language models.",
            "Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models.",
            "Deepinception: Hypnotize large language model to be jailbreaker.",
            "Evaluating object hallucination in large vision-language models.",
            "I think, therefore i am: Awareness in large language models.",
            "Goat-bench: Safety insights to large multimodal models through meme-based social abuse.",
            "Visual instruction tuning.",
            "Autodan: Generating stealthy jailbreak prompts on aligned large language models.",
            "Safety of multimodal large language models on images and text.",
            "Trustworthy llms: a survey and guideline for evaluating large language models’ alignment.",
            "Deep learning face attributes in the wild.",
            "Umap: Uniform manifold approximation and projection for dimension reduction.",
            "Gpt-4v(ision) system card.",
            "Openai embedding models.",
            "Openai moderation api.",
            "Training language models to follow instructions with human feedback.",
            "On estimation of a probability density function and mode.",
            "Early stopping-but when?",
            "Visual adversarial examples jailbreak aligned large language models.",
            "Fine-tuning aligned language models compromises safety, even when users do not intend to!",
            "Smoothllm: Defending large language models against jailbreaking attacks.",
            "Towards an exhaustive evaluation of vision-language foundation models.",
            "On second thought, let’s not think step by step! bias and toxicity in zero-shot reasoning.",
            "“do anything now”: Characterizing and evaluating in-the-wild jailbreak prompts on large language models.",
            "Why do universal adversarial attacks work on large language models?: Geometry might be the answer.",
            "Temporal insight enhancement: Mitigating temporal hallucination in multimodal large language models.",
            "Trustllm: Trustworthiness in large language models.",
            "Celebrity face image dataset.",
            "Decodingtrust: A comprehensive assessment of trustworthiness in gpt models.",
            "Backdoor activation attack: Attack large language models using activation steering for safety-alignment.",
            "Jailbroken: How does llm safety training fail?",
            "Jailbreak and guard aligned language models with only few in-context demonstrations.",
            "Defending chatgpt against jailbreak attack via self-reminder.",
            "Jailbreaking gpt-4v via self-adversarial attacks with system prompts.",
            "Lvlm-ehub: A comprehensive evaluation benchmark for large vision-language models.",
            "Large language models as optimizers.",
            "On early stopping in gradient descent learning."
        ]
    },
    {
        "index": 50,
        "title": "CANLLM-Generated Misinformation Be Detected?",
        "publication_date": "2023-12-31",
        "references": [
            "Demystifying neural fake news via linguistic feature-based interpretation",
            "Frontier ai regulation: Managing emerging risks to public safety",
            "The looming threat of fake and llm-generated linkedin profiles: Challenges and opportunities for detection and prevention",
            "A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity",
            "Assessing the effectiveness of gpt-3 in detecting false political statements: A case study on the liar dataset",
            "GAN-based unsupervised learning approach to generate and detect fake news",
            "How effectively can machines defend against machine-generated fake news? an empirical study",
            "Synthetic disinformation attacks on automated fact verification systems",
            "Fine-grained analysis of propaganda in news article",
            "Quantifying and attributing the hallucination of large language models via association analysis",
            "Causal intervention and counterfactual reasoning for multi-modal fake news detection",
            "Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality",
            "Coaid: Covid-19 healthcare misinformation dataset",
            "What label should be applied to content produced by generative ai?",
            "The battlefront of combating misinformation and coping with media bias",
            "Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned",
            "Combating misinformation in the age of llms: Opportunities and challenges",
            "Combating health misinformation in social media: Characterization, detection, intervention, and open issues",
            "Misleading online content: recognizing clickbait as\" false news\"",
            "Causal intervention and counterfactual reasoning for multi-modal fake news detection",
            "A survey on automated fact-checking",
            "Multimodal automated fact-checking: A survey",
            "Augmented language models: a survey",
            "Trustllm: Trustworthiness in large language models",
            "Defending against neural fake news",
            "Synthetic lies: Understanding ai-generated misinformation and evaluating algorithmic and human solutions",
            "An overview of online fake news: Characterization, detection, and discussion",
            "Siren’s song in the ai ocean: A survey on hallucination in large language models",
            "Safetybench: Evaluating the safety of large language models with multiple choice questions",
            "Defending large language models against jailbreaking attacks through goal prioritization",
            "Retrieval-augmented generation for ai-generated content: A survey",
            "A survey of large language models",
            "Synthetic lies: Understanding ai-generated misinformation and evaluating algorithmic and human solutions",
            "From instructions to intrinsic human values - a survey of alignment goals for big models",
            "Jailbroken: How does llm safety training fail?",
            "Latent jailbreak: A benchmark for evaluating text safety and output robustness of large language models",
            "Can llms express their uncertainty? an empirical evaluation of confidence elicitation in llms",
            "Zoom out and observe: News environment perception for fake news detection",
            "The limitations of stylometry for detecting machine-generated fake news",
            "\"do anything now\": Characterizing and evaluating in-the-wild jailbreak prompts on large language models",
            "Large language models are zero-shot reasoners",
            "Certifying llm safety against adversarial prompting",
            "A comprehensive survey of hallucination mitigation techniques in large language models",
            "Training language models to follow instructions with human feedback",
            "Threat scenarios and best practices to detect neural fake news",
            "Evaluating frontier models for dangerous capabilities",
            "Latent jailbreak: A benchmark for evaluating text safety and output robustness of large language models",
            "A survey on computational propaganda detection",
            "Interpretable unified language checking",
            "Safetybench: Evaluating the safety of large language models with multiple choice questions",
            "Defending large language models against jailbreaking attacks through goal prioritization",
            "Retrieval-augmented generation for ai-generated content: A survey",
            "A survey of large language models",
            "Synthetic lies: Understanding ai-generated misinformation and evaluating algorithmic and human solutions",
            "From instructions to intrinsic human values - a survey of alignment goals for big models",
            "Jailbroken: How does llm safety training fail?",
            "Latent jailbreak: A benchmark for evaluating text safety and output robustness of large language models",
            "Can llms express their uncertainty? an empirical evaluation of confidence elicitation in llms",
            "Zoom out and observe: News environment perception for fake news detection",
            "The limitations of stylometry for detecting machine-generated fake news",
            "\"do anything now\": Characterizing and evaluating in-the-wild jailbreak prompts on large language models",
            "Large language models are zero-shot reasoners",
            "Certifying llm safety against adversarial prompting"
        ]
    },
    {
        "index": 51,
        "title": "Can LLMs Deeply Detect Complex Malicious Queries? A Framework for Jailbreaking via Obfuscating Intent",
        "publication_date": "2024-05-07",
        "references": [
            "Language models are few-shot learners.",
            "What is the size of the training set for gpt-3",
            "Prompting4debugging: Red-teaming text-to-image diffusion models by finding problematic prompts.",
            "Ratgpt: Turning online llms into proxies for malware attacks.",
            "Large language models can be used to effectively scale spear phishing campaigns.",
            "A first look at toxicity injection attacks on open-domain chatbots.",
            "Understanding multi-turn toxic behaviors in open-domain chatbots.",
            "Google’s secure ai framework (saif).",
            "Content policy.",
            "Cloud Natural Language.",
            "Characterizing and evaluating in-the-wild jailbreak prompts on large language models.",
            "Role play with large language models.",
            "Jailbreaking chatgpt via prompt engineering: An empirical study.",
            "Multi-step jailbreaking privacy attacks on chatgpt.",
            "Universal and transferable adversarial attacks on aligned language models.",
            "jailbreakchat.",
            "Adapting large language models for content moderation: Pitfalls in data engineering and supervised fine-tuning.",
            "Training a helpful and harmless assistant with reinforcement learning from human feedback.",
            "Jailbroken: How does llm safety training fail?",
            "Training language models to follow instructions with human feedback.",
            "Pretraining language models with human preferences.",
            "Improving alignment of dialogue agents via targeted human judgements.",
            "Red-teaming the stable diffusion safety filter.",
            "Red teaming language models with language models.",
            "Fuzzllm: A novel and universal fuzzing framework for proactively discovering jailbreak vulnerabilities in large language models.",
            "Attack prompt generation for red teaming and defending large language models.",
            "From chatgpt to threatgpt: Impact of generative ai in cybersecurity and privacy.",
            "Gptfuzzer: Red teaming large language models with auto-generated jailbreak prompts.",
            "Jailbreaker: Automated jailbreak across multiple large language model chatbots.",
            "Autodan: Generating stealthy jailbreak prompts on aligned large language models.",
            "Generating natural language adversarial examples.",
            "Generating natural language adversarial examples through probability weighted word saliency.",
            "Adversarial glue: A multi-task benchmark for robustness evaluation of language models.",
            "Open sesame! universal black box jailbreaking of large language models.",
            "Assert: Automated safety scenario red teaming for evaluating the robustness of large language models.",
            "Flirt: Feedback loop in-context red teaming.",
            "Are aligned neural networks adversarially aligned?",
            "Smoothllm: Defending large language models against jailbreaking attacks.",
            "Query-efficient black-box red teaming via bayesian optimization.",
            "Multilingual jailbreak challenges in large language models.",
            "Red-teaming large language models using chain of utterances for safety-alignment.",
            "Prompt packer: Deceiving llms through compositional instruction with hidden attacks.",
            "Drattack: Prompt decomposition and reconstruction makes powerful llm jailbreakers.",
            "GPT-3.5 Turbo.",
            "GPT-4 and GPT-4 Turbo.",
            "Qwen technical report.",
            "Baichuan 2: Open large-scale language models.",
            "Jade: A linguistics-based safety evaluation platform for llm."
        ]
    },
    {
        "index": 52,
        "title": "CAS: A Probability-Based Approach for Universal Condition Alignment Score",
        "publication_date": "2024-01-01",
        "references": [
            "Dreamlike Art. Dreamlike-photoreal-2.0, 2023.",
            "Tim Brooks, Aleksander Holynski, and Alexei A Efros. Instructpix2pix: Learning to follow image editing instructions.",
            "Ricky TQ Chen, Yulia Rubanova, Jesse Bettencourt, and David K Duvenaud. Neural ordinary differential equations.",
            "Laurent Dinh, David Krueger, and Yoshua Bengio. NICE: non-linear independent components estimation.",
            "Berthy T Feng, Jamie Smith, Michael Rubinstein, Huiwen Chang, Katherine L Bouman, and William T Freeman. Score-based diffusion models as principled priors for inverse imaging.",
            "Rinon Gal, Or Patashnik, Haggai Maron, Amit Bermano, Gal Chechik, and Daniel Cohen-Or. Stylegan-nada: Clip-guided domain adaptation of image generators.",
            "Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. CLIPScore: a reference-free evaluation metric for image captioning.",
            "Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance.",
            "Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models.",
            "Michael F Hutchinson. A stochastic estimator of the trace of the influence matrix for laplacian smoothing splines.",
            "Durk P Kingma and Prafulla Dhariwal. Glow: Generative flow with invertible 1x1 convolutions.",
            "Polina Kirichenko, Pavel Izmailov, and Andrew G Wilson. Why normalizing flows fail to detect out-of-distribution data.",
            "Yuval Kirstain, Adam Polyak, Uriel Singer, Shahbuland Matiana, Joe Penna, and Omer Levy. Pick-a-pic: An open dataset of user preferences for text-to-image generation.",
            "Haohe Liu, Zehua Chen, Yi Yuan, Xinhao Mei, Xubo Liu, Danilo Mandic, Wenwu Wang, and Mark D Plumbley. Audioldm: Text-to-audio generation with latent diffusion models.",
            "Luping Liu, Yi Ren, Zhijie Lin, and Zhou Zhao. Pseudo numerical methods for diffusion models on manifolds.",
            "Dallin Mackay. Van-gogh-diffusion, 2023.",
            "Daiki Miyake, Akihiro Iohara, Yu Saito, and Toshiyuki Tanaka. Negative-prompt inversion: Fast image inversion for editing with text-guided diffusion models.",
            "Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Null-text inversion for editing real images using guided diffusion models.",
            "Utkarsh Ojha, Yuheng Li, and Yong Jae Lee. Towards universal fake image detectors that generalize across generative models.",
            "Prompthero. Openjourney, 2023.",
            "Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision.",
            "Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents.",
            "Danilo Rezende and Shakir Mohamed. Variational inference with normalizing flows.",
            "Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models.",
            "Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding.",
            "Vikash Sehwag, Caner Hazirbas, Albert Gordo, Firat Ozgenel, and Cristian Canton. Generating high fidelity data from low-density regions using diffusion models.",
            "SG1612222023. Realistic vision v1.4, 2023.",
            "John Skilling. The eigenvalues of mega-dimensional matrices.",
            "John Slegers. Epic-diffusion, 2023.",
            "Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models.",
            "Yang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations.",
            "Patrick von Platen, Suraj Patil, Anton Lozhkov, Pedro Cuenca, Nathan Lambert, Kashif Rasul, Mishig Davaadorj, and Thomas Wolf. Diffusers: State-of-the-art diffusion models.",
            "Bram Wallace, Akash Gokul, and Nikhil Naik. Edict: Exact diffusion inversion via coupled transformations.",
            "Xiaoshi Wu, Keqiang Sun, Feng Zhu, Rui Zhao, and Hongsheng Li. Better aligning text-to-image models with human preference.",
            "Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai Li, Ming Ding, Jie Tang, and Yuxiao Dong. Imagereward: Learning and evaluating human preferences for text-to-image generation.",
            "Binxin Yang, Shuyang Gu, Bo Zhang, Ting Zhang, Xuejin Chen, Xiaoyan Sun, Dong Chen, and Fang Wen. Paint by example: Exemplar-based image editing with diffusion models.",
            "Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models.",
            "Roland S Zimmermann, Lukas Schott, Yang Song, Benjamin A Dunn, and David A Klindt. Score-based generative classifiers."
        ]
    },
    {
        "index": 53,
        "title": "Chain of Attack: a Semantic-Driven Contextual Multi-Turn attacker for LLM",
        "publication_date": "2024-05-09",
        "references": [
            "Training a helpful and harmless assistant with reinforcement learning from human feedback",
            "Red-teaming large language models using chain of utterances for safety-alignment",
            "Chatgpt evaluation on sentence level relations: A focus on temporal, causal, and discourse relations",
            "Jailbreaking black box large language models in twenty queries",
            "Understanding multi-turn toxic behaviors in open-domain chatbots",
            "Learning to teach large language models logical reasoning",
            "Jailbreaker: Automated jailbreak across multiple large language model chatbots",
            "Simcse: Simple contrastive learning of sentence embeddings",
            "Toxigen: A large-scale machine-generated dataset for adversarial and implicit hate speech detection",
            "Large language models can be used to effectively scale spear phishing campaigns",
            "Catastrophic jailbreak of open-source llms via exploiting generation",
            "Exploiting programmatic behavior of llms: Dual-use through standard security attacks",
            "Pretraining language models with human preferences",
            "Open sesame! universal black box jailbreaking of large language models",
            "Rlaif: Scaling reinforcement learning from human feedback with ai feedback",
            "A new generation of perspective api: Efficient multi-lingual character-level transformers",
            "Multi-step jailbreaking privacy attacks on chatgpt",
            "Toxicchat: Unveiling hidden challenges of toxicity detection in real-world user-ai conversation",
            "Autodan: Generating stealthy jailbreak prompts on aligned large language models",
            "Tree of attacks: Jailbreaking black-box llms automatically",
            "A comprehensive overview of large language models",
            "Gpt-4 technical report",
            "Training language models to follow instructions with human feedback",
            "In-context impersonation reveals large language models’ strengths and biases",
            "LOFT: Local proxy fine-tuning for improving transferability of adversarial attacks against large language model",
            "Unveiling gender bias in terms of profession across llms: Analyzing and addressing sociological implications",
            "Llama 2: Open foundation and fine-tuned chat models",
            "The silence of the llms: Cross-lingual analysis of political bias and false information prevalence in chatgpt, google bard, and bing chat",
            "Using gpt-4 for content moderation",
            "Gpt-fuzzer: Red teaming large language models with auto-generated jailbreak prompts",
            "Judging llm-as-a-judge with mt-bench and chatbot arena",
            "Autodan: Automatic and interpretable adversarial attacks on large language models",
            "Universal and transferable adversarial attacks on aligned language models"
        ]
    },
    {
        "index": 54,
        "title": "CHAIN -OF-JAILBREAK ATTACK FOR IMAGE GENERATION MODELS VIA EDITING STEP BY STEP",
        "publication_date": "2024-10-04",
        "references": [
            "The catch-22 of ai chatbots",
            "Easily accessible text-to-image generation amplifies demographic stereotypes at large scale",
            "Automatic detection of pornographic and gambling websites based on visual and textual content using a decision mechanism",
            "Can large language models be an alternative to human evaluations?",
            "Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality",
            "Dall-eval: Probing the reasoning skills and social biases of text-to-image generation models",
            "Deep reinforcement learning from human preferences",
            "Number of midjourney users and statistics",
            "Divide-and-conquer attack: Harnessing the power of llm to bypass the censorship of text-to-image generation model",
            "Multilingual jailbreak challenges in large language models",
            "The llama 3 herd of models",
            "Generative adversarial networks",
            "Adversaries can misuse combinations of safe models",
            "Investigating subtler biases in llms: Ageism, beauty, institutional, and nationality bias in generative models",
            "The hateful memes challenge: Detecting hate speech in multimodal memes",
            "Automatic jailbreaking of the text-to-image generative ai systems",
            "I see dead people: Gray-box adversarial attack on image-to-text models",
            "Binary codes capable of correcting deletions, insertions, and reversals",
            "Latent guard: a safety framework for text-to-image generation",
            "Mm-safetybench: A benchmark for safety evaluation of multimodal large language models",
            "The art of deception: Black-box attack against text-to-image diffusion model",
            "Jailbreakv-28k: A benchmark for assessing the robustness of multimodal large language models against jailbreak attacks",
            "Google chief admits ‘biased’ ai tool’s photo diversity offended users",
            "The protection of children online: a brief scoping review to identify vulnerable groups",
            "Jailbreaking attack against multimodal large language model",
            "Training language models to follow instructions with human feedback",
            "High-resolution image synthesis with latent diffusion models",
            "Ignore this title and hackaprompt: Exposing systemic vulnerabilities of llms through a global prompt hacking competition",
            "Jailbreak in pieces: Compositional adversarial attacks on multi-modal language models",
            "Safety assessment of chinese large language models",
            "Neural discrete representation learning",
            "New job, new gender? measuring the social bias in image generation models",
            "Jailbroken: How does llm safety training fail?",
            "Recipes for safety in open-domain chatbots",
            "High-resolution image synthesis with latent diffusion models",
            "Shadow alignment: The ease of subverting safely-aligned language models",
            "Sneakyprompt: Jailbreaking text-to-image generative models",
            "Sneakyprompt: Jailbreaking text-to-image generative models",
            "Internet misconduct impact adolescent mental health in taiwan: The moderating roles of internet addiction",
            "How johnny can persuade llms to jailbreak them: Rethinking persuasion to challenge AI safety by humanizing llms",
            "Forget-me-not: Learning to forget in text-to-image diffusion models",
            "Safetybench: Evaluating the safety of large language models with multiple choice questions"
        ]
    },
    {
        "index": 55,
        "title": "ChatBug: A Common Vulnerability of Aligned LLMs Induced by Chat Templates",
        "publication_date": "2025-01-07",
        "references": [
            "GPT-4 technical report",
            "Jailbreaking leading safety-aligned llms with simple adaptive attacks",
            "Introducing Claude 2.1",
            "Introducing the next generation of Claude",
            "Training a helpful and harmless assistant with reinforcement learning from human feedback",
            "Fine-tuning language models to find agreement among humans with diverse preferences",
            "Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90%* ChatGPT Quality",
            "Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference",
            "Deep reinforcement learning from human preferences",
            "Multi-lingual jailbreak challenges in large language models",
            "Build it break it fix it for dialogue safety: Robustness from adversarial human attack",
            "MART: Improving LLM Safety with Multi-round Automatic Red-Teaming",
            "Demystifying Prompts in Language Models via Perplexity Estimation",
            "Build with the Gemini API",
            "LoRA: Low-Rank Adaptation of Large Language Models",
            "Catastrophic jailbreak of open-source LLMs via exploiting generation",
            "Templates for Chat Models",
            "Human-Machine Collaboration for Content Regulation: The Case of Reddit Automoderator",
            "Mistral 7B",
            "ChatBug: A Common Vulnerability of Aligned LLMs Induced by Chat Templates",
            "ArtPrompt: ASCII Art-based Jailbreak Attacks against Aligned LLMs",
            "Automatically Auditing Large Language Models via Discrete Optimization",
            "Adversarial machine learning at scale",
            "A new generation of perspective API: Efficient multilingual character-level transformers",
            "Alpacaeval: An automatic evaluator of instruction-following models",
            "Deepinception: Hypnotize large language model to be jail-breaker",
            "Single character perturbations break llm alignment",
            "Autodan: Generating stealthy jailbreak prompts on aligned large language models",
            "Jailbreaking ChatGPT via prompt engineering: An empirical study",
            "Meta Llama Guard 2",
            "Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity",
            "Harmbench: A standardized evaluation framework for automated red teaming and robust refusal",
            "Llama 3",
            "ChatML",
            "Models-OpenAI API",
            "Training language models to follow instructions with human feedback",
            "Advprompter: Fast adaptive adversarial prompting for LLMs",
            "Ignore previous prompt: Attack techniques for language models",
            "Fine-tuning Aligned Language Models Compromises Safety, Even When Users Do Not Intend To!",
            "Quantifying Language Models’ Sensitivity to Spurious Features in Prompt Design or: How I learned to start worrying about prompt formatting",
            "Llama 2: Open foundation and fine-tuned chat models",
            "Bypassing the Safety Training of Open-Source LLMs with Priming Attacks",
            "Jailbroken: How does LLM safety training fail?",
            "Finetuned Language Models are Zero-Shot Learners",
            "Google’s neural machine translation system: Bridging the gap between human and machine translation",
            "BadChain: Backdoor Chain-of-Thought Prompting for Large Language Models",
            "Defending ChatGPT against jailbreak attack via self-reminders",
            "SafeDecoding: Defending against Jailbreak Attacks via Safety-Aware Decoding",
            "Gptfuzzer: Red teaming large language models with auto-generated jailbreak prompts",
            "How Johnny Can Persuade LLMs to Jailbreak Them: Rethinking Persuasion to Challenge AI Safety by Humanizing LLMs",
            "Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena",
            "Autodan: Automatic and interpretable adversarial attacks on large language models"
        ]
    },
    {
        "index": 56,
        "title": "Competition Report: Finding Universal Jailbreak Backdoors in Aligned LLMs",
        "publication_date": "2024-06-06",
        "references": [
            "Jailbreak chat.",
            "Foundational challenges in assuring alignment and safety of large language models.",
            "Training a helpful and harmless assistant with reinforcement learning from human feedback.",
            "Poisoning attacks against support vector machines.",
            "Towards making systems forget with machine unlearning.",
            "The satml ’24 cnn interpretability competition: New innovations for concept-level interpretability.",
            "The trojan detection challenge 2023 (LLM edition) - the trojan detection challenge.",
            "Targeted backdoor attacks on deep learning systems using data poisoning.",
            "Deep reinforcement learning from human preferences.",
            "Weight poisoning attacks on pre-trained models.",
            "The unlocking spell on base llms: Rethinking alignment via in-context learning.",
            "Rethinking machine unlearning for large language models.",
            "Text and code embeddings by contrastive pre-training.",
            "Universal jailbreak backdoors from poisoned human feedback.",
            "Humpty dumpty: Controlling word meanings via corpus poisoning.",
            "Rapid optimization for jailbreaking llms via subconscious exploitation and echopraxia.",
            "Badgpt: Exploring security vulnerabilities of chatgpt via backdoor attacks to instructgpt.",
            "Be careful about poisoned word embeddings: Exploring the vulnerability of the embedding layers in nlp models.",
            "Universal and transferable adversarial attacks on aligned language models."
        ]
    },
    {
        "index": 57,
        "title": "Continuous Embedding Attacks via Clipped Inputs in Jailbreaking Large Language Models",
        "publication_date": "2024-07-16",
        "references": [
            "Masterkey: Automated jailbreaking of large language model chatbots.",
            "Pandora: Jailbreak gpts by retrieval augmented generation poisoning.",
            "Multilingual jailbreak challenges in large language models.",
            "Explaining and harnessing adversarial examples.",
            "Meta llama.",
            "Vicuna 7b v1.5.",
            "A cross-language investigation into jailbreak attacks in large language models.",
            "Open the pandora’s box of llms: Jailbreaking llms through representation engineering.",
            "Jailbreaking chatgpt via prompt engineering: An empirical study.",
            "Training language models to follow instructions with human feedback.",
            "Pytorch: An imperative style, high-performance deep learning library.",
            "Adversarial attacks and defenses in large language models: Old and new threats.",
            "Soft prompt threats: Attacking safety alignment and unlearning in open-source llms through the embedding space.",
            "Survey of vulnerabilities in large language models revealed by adversarial attacks.",
            "Huggingface’s transformers: State-of-the-art natural language processing.",
            "Cognitive overload: Jailbreaking large language models with overloaded logical thinking.",
            "Llm jailbreak attack versus defense techniques–a comprehensive study.",
            "Low-resource languages jailbreak gpt-4."
        ]
    },
    {
        "index": 58,
        "title": "Covert Malicious Finetuning: Challenges in Safeguarding LLM Adaptation",
        "publication_date": "2024-06-28",
        "references": [
            "GPT-4 technical report",
            "ShareGPT_Vicuna_unfiltered",
            "Introducing Claude 2.1",
            "Constitutional AI: Harmlessness from AI feedback",
            "Safety-tuned LLaMAs: Lessons from improving the safety of large language models that follow instructions",
            "Targeted backdoor attacks on deep learning systems using data poisoning",
            "Think you have solved question answering? Try ARC, the AI2 reasoning challenge",
            "BadLLaMA: cheaply removing safety fine-tuning from LLaMA 2-chat 13B",
            "Bard: A conversational AI tool by Google",
            "Overthinking the truth: Understanding how language models process false demonstrations",
            "Baseline defenses for adversarial attacks against aligned language models",
            "LoRA fine-tuning efficiently undoes safety training in LLaMA 2-Chat 70B",
            "Backdoor learning: A survey",
            "Let’s verify step by step",
            "Concealed data poisoning attacks on NLP models",
            "Poisoning language models during instruction tuning",
            "Jailbroken: How does LLM safety training fail?",
            "OpenAI debuts GPT-4 Turbo and fine-tuning program for GPT-4",
            "Shadow alignment: The ease of subverting safely-aligned language models",
            "Removing RLHF protections in GPT-4 via fine-tuning",
            "Learning and forgetting unsafe examples in large language models",
            "Robust prompt optimization for defending language models against jailbreaking attacks",
            "Making harmful behaviors unlearnable for large language models",
            "Universal and transferable adversarial attacks on aligned language models",
            "Improving alignment and robustness with circuit breakers",
            "Solving math word problems with process-and outcome-based feedback",
            "Concealed data poisoning attacks on NLP models",
            "Poisoning language models during instruction tuning",
            "Jailbroken: How does LLM safety training fail?",
            "Exploiting novel GPT-4 APIs",
            "Instruction tuning with GPT-4",
            "Red teaming language models with language models",
            "Fine-tuning aligned language models compromises safety, even when users do not intend to!",
            "On the exploitability of instruction tuning",
            "Stanford Alpaca: An instruction-following LLaMA model",
            "LLaMA 2: Open foundation and fine-tuned chat models",
            "Solving math word problems with process-and outcome-based feedback",
            "Concealed data poisoning attacks on NLP models",
            "Poisoning language models during instruction tuning",
            "Jailbroken: How does LLM safety training fail?"
        ]
    },
    {
        "index": 59,
        "title": "CPPO: Continual Learning for Reinforcement Learning with Human Feedback",
        "publication_date": "2024-01-01",
        "references": [
            "Memory retention – the synaptic stability versus plasticity dilemma",
            "Memory aware synapses: Learning what (not) to forget",
            "Task-free continual learning",
            "Training a helpful and harmless assistant with reinforcement learning from human feedback",
            "Constitutional ai: Harmlessness from ai feedback",
            "METEOR: An automatic metric for MT evaluation with improved correlation with human judgments",
            "Continual lifelong learning in natural language processing: A survey",
            "Tamer: Training an agent manually via evaluative reinforcement",
            "End-to-end incremental learning",
            "Riemannian walk for incremental learning: Understanding forgetting and intransigence",
            "Decision transformer: Reinforcement learning via sequence modeling",
            "Podnet: Pooled outputs distillation for small-tasks incremental learning",
            "Adversarial continual learning",
            "Understanding dataset difficulty with V-usable information",
            "Self-supervised training enhances online continual learning",
            "Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned",
            "Scaling laws for reward model overoptimization",
            "Teaching machines to read and comprehend",
            "Learning to write with cooperative discriminators",
            "Learning a unified classifier incrementally via rebalancing",
            "Continual learning for text classification with information disentanglement based regularization",
            "Human-centric dialog training via offline reinforcement learning",
            "Meta-learning representations for continual learning",
            "Policy consolidation for continual reinforcement learning",
            "Overcoming catastrophic forgetting in neural networks",
            "Can neural machine translation be improved with user feedback?",
            "Learning word vectors for sentiment analysis",
            "An empirical investigation of the role of pre-training in lifelong learning",
            "Distilbert, a distilled version of BERT: smaller, faster, cheaper and lighter",
            "Training language models with language feedback at scale",
            "Hypernetwork-PPO for continual reinforcement learning",
            "Proximal policy optimization algorithms",
            "Overcoming catastrophic forgetting with hard attention to the task",
            "Towards out-of-distribution generalization: A survey",
            "Learning to summarize from human feedback",
            "Language models are unsupervised multitask learners",
            "A comprehensive survey of continual learning: Theory, method and application",
            "TL;DR: Mining Reddit to learn automatic summarization",
            "GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model",
            "A technique for the measurement of attitudes",
            "ROUGE: A package for automatic evaluation of summaries",
            "Self-improving reactive agents based on reinforcement learning, planning and teaching",
            "Generative feature replay for class-incremental learning",
            "Gradient episodic memory for continual learning",
            "Learning without forgetting",
            "Experience replay for continual learning",
            "Progressive neural networks"
        ]
    },
    {
        "index": 60,
        "title": "Cross-Modal Safety Alignment: Is textual unlearning all you need?",
        "publication_date": "2024-05-27",
        "references": [
            "Gpt-4 technical report.",
            "Flamingo: a visual language model for few-shot learning.",
            "Openflamingo: An open-source framework for training large autoregressive vision-language models.",
            "Training a helpful and harmless assistant with reinforcement learning from human feedback.",
            "Machine unlearning.",
            "Sparks of artificial general intelligence: Early experiments with gpt-4.",
            "Towards making systems forget with machine unlearning.",
            "Towards evaluating the robustness of neural networks.",
            "The secret sharer: Evaluating and testing unintended memorization in neural networks.",
            "Unlearn what you want to forget: Efficient unlearning for llms.",
            "Deep reinforcement learning from human preferences.",
            "Instructblip: Towards general-purpose vision-language models with instruction tuning.",
            "Multilingual jailbreak challenges in large language models.",
            "Pengi: An audio language model for audio tasks.",
            "Qlora: Efficient finetuning of quantized llms.",
            "Measuring the carbon intensity of ai in cloud instances.",
            "Attacks, defenses and evaluations for llm conversation safety: A survey.",
            "Who’s harry potter? approximate unlearning in llms.",
            "Unbridled icarus: A survey of the potential perils of image inputs in multimodal large language model security.",
            "Erasing concepts from diffusion models.",
            "Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned.",
            "Llama-adapter v2: Parameter-efficient visual instruction model.",
            "Figstep: Jailbreaking large vision-language models via typographic visual prompts.",
            "Explaining and harnessing adversarial examples.",
            "Making the v in vqa matter: Elevating the role of image understanding in visual question answering.",
            "Certified data removal from machine learning models.",
            "Cold-attack: Jailbreaking llms with stealthiness and controllability.",
            "Onellm: One framework to align all modalities with language.",
            "Debertav3: Improving deberta using electra-style pre-training with gradient-disentangled embedding sharing.",
            "Selective amnesia: A continual learning approach to forgetting in deep generative models.",
            "Lora: Low-rank adaptation of large language models.",
            "Adversarial examples are not bugs, they are features.",
            "Llama guard: Llm-based input-output safeguard for human-ai conversations.",
            "Beaver-tails: Towards improved safety alignment of llm via a human-preference dataset.",
            "Mistral 7b.",
            "Segment anything.",
            "Pretraining language models with human preferences.",
            "Ablating concepts in text-to-image diffusion models.",
            "Jailbreakv-28k: A benchmark for assessing the robustness of multimodal large language models against jailbreak attacks.",
            "Jailbreakv-28k: A benchmark for assessing the robustness of multimodal large language models against jailbreak attacks.",
            "Inverse scaling: When bigger isn’t better.",
            "Locating and editing factual associations in gpt.",
            "Mass-editing memory in a transformer.",
            "Jailbreaking attack against multimodal large language model.",
            "Dinov2: Learning robust visual features without supervision.",
            "Training language models to follow instructions with human feedback.",
            "Can sensitive information be deleted from llms? objectives for defending against extraction attacks.",
            "In-context unlearning: Language models as few shot unlearners.",
            "Fine-tuning aligned language models compromises safety, even when users do not intend to!",
            "Visual adversarial examples jailbreak aligned large language models.",
            "Learning transferable visual models from natural language supervision.",
            "Zero-shot text-to-image generation.",
            "Finetuned language models are zero-shot learners.",
            "Simple statistical gradient-following algorithms for connectionist reinforcement learning.",
            "Fundamental limitations of alignment in large language models.",
            "Next-gpt: Any-to-any multimodal llm.",
            "Parameter-efficient fine-tuning methods for pretrained language models: A critical review and assessment.",
            "Backdooring instruction-tuned large language models with virtual prompt injection.",
            "Large language model unlearning.",
            "A survey on multimodal large language models.",
            "Unlearning bias in language models by partitioning gradients.",
            "Forget-me-not: Learning to forget in text-to-image diffusion models.",
            "Video-llama: An instruction-tuned audio-visual language model for video understanding.",
            "A survey of large language models.",
            "Judging llm-as-a-judge with mt-bench and chatbot arena.",
            "Minigpt-4: Enhancing vision-language understanding with advanced large language models.",
            "Safety fine-tuning at (almost) no cost: A baseline for vision large language models.",
            "Universal and transferable adversarial attacks on aligned language models."
        ]
    },
    {
        "index": 61,
        "title": "Cross-modality Information Check for Detecting Jailbreaking in Multimodal Large Language Models",
        "publication_date": "2024-10-17",
        "references": [
            "Gpt-4 technical report.",
            "Qwen-vl: A frontier large vision-language model with versatile abilities.",
            "Non-local means denoising.",
            "Are aligned neural networks adversarially aligned?",
            "Jailbreaking black box large language models in twenty queries.",
            "DRESS: Instructing large vision-language models to align and interact with humans via natural language feedback.",
            "Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality.",
            "Instructblip: Towards general-purpose vision-language models with instruction tuning.",
            "Masterkey: Automated jailbreaking of large language model chatbots.",
            "Imagenet: A large-scale hierarchical image database.",
            "An image is worth 16x16 words: Transformers for image recognition at scale.",
            "Figstep: Jailbreaking large vision-language models via typographic visual prompts.",
            "Explaining and harnessing adversarial examples.",
            "Deepinception: Hypnotize large language model to be jailbreaker.",
            "Images are achilles’ heel of alignment: Exploiting visual vulnerabilities for jail-breaking multimodal large language models.",
            "Visual instruction tuning.",
            "Query-relevant images jailbreak large multi-modal models.",
            "Jailbreakv-28k: A benchmark for assessing the robustness of multimodal large language models against jailbreak attacks.",
            "Harmbench: A standardized evaluation framework for automated red teaming and robust refusal.",
            "MM-Vet Evaluator - a Hugging Face Space by whyu — huggingface.co.",
            "Improved denoising diffusion probabilistic models.",
            "Jailbreaking attack against multimodal large language model.",
            "Summary generation using natural language processing techniques and cosine similarity.",
            "A methodology combining cosine similarity with classifier for text classification.",
            "Mllm-protector: Ensuring mllm’s safety without hurting performance.",
            "Visual adversarial examples jailbreak aligned large language models.",
            "Jailbreak in pieces: Compositional adversarial attacks on multi-modal language models.",
            "Llama 2: Open foundation and fine-tuned chat models.",
            "Inferaligner: Inference-time alignment for harmlessness through cross-model guidance.",
            "Adashield: Safeguarding multimodal large language models from structure-based attack via adaptive shield prompting.",
            "A survey on multimodal large language models.",
            "Mm-vet: Evaluating large multimodal models for integrated capabilities.",
            "A mutation-based method for multi-modal jailbreaking attack detection.",
            "On evaluating adversarial robustness of large vision-language models.",
            "Minigpt-4: Enhancing vision-language understanding with advanced large language models.",
            "Safety fine-tuning at (almost) no cost: A baseline for vision large language models.",
            "Universal and transferable adversarial attacks on aligned language models."
        ]
    },
    {
        "index": 62,
        "title": "Safe Inputs but Unsafe Output: Benchmarking Cross-modality Safety Alignment of Large Vision-Language Models",
        "publication_date": "2024-06-15",
        "references": [
            "GPT-4o.",
            "2024. Usage policies - openai. https://openai.com/policies/usage-policies . Accessed: 2024-01-12.",
            "Pranav Agarwal, Alejandro Betancourt, Vana Panagiottou, and Natalia Díaz-Rodríguez. 2020. Egoshots, an ego-vision life-logging dataset and semantic fidelity metric to evaluate diversity in image captioning models. In Machine Learning in Real Life (ML-IRL) Workshop at the International Conference on Learning Representations (ICLR) .",
            "Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. 2023. Qwen-vl: A frontier large vision-language model with versatile abilities. arXiv preprint arXiv:2308.12966 .",
            "Trishna Chakraborty, Erfan Shayegani, Zikui Cai, Nael Abu-Ghazaleh, M Salman Asif, Yue Dong, Amit K Roy-Chowdhury, and Chengyu Song. 2024. Cross-modal safety alignment: Is textual unlearning all you need? arXiv preprint arXiv:2406.02575 .",
            "Jun Chen, Deyao Zhu, Xiaoqian Shen, Xiang Li, Zechun Liu, Pengchuan Zhang, Raghuraman Krishnamoorthy, Vikas Chandra, Yunyang Xiong, and Mohamed Elhoseiny. 2023a. Minigpt-v2: large language model as a unified interface for vision-language multi-task learning. arXiv preprint arXiv:2310.09478 .",
            "Shuo Chen, Zhen Han, Bailan He, Zifeng Ding, Wenqian Yu, Philip Torr, Volker Tresp, and Jindong Gu. 2024. Red teaming gpt-4v: Are gpt-4v safe against uni/multi-modal jailbreak attacks?",
            "Yangyi Chen, Karan Sikka, Michael Cogswell, Heng Ji, and Ajay Divakaran. 2023b. Dress: Instructing large vision-language models to align and interact with humans via natural language feedback. ArXiv , abs/2311.10081.",
            "Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Albert Li, Pascale Fung, and Steven C. H. Hoi. 2023. Instructblip: Towards general-purpose vision-language models with instruction tuning. ArXiv , abs/2305.06500.",
            "Yichen Gong, Delong Ran, Jinyuan Liu, Conglei Wang, Tianshuo Cong, Anyu Wang, Sisi Duan, and Xiaoyun Wang. 2023. Figstep: Jailbreaking large vision-language models via typographic visual prompts. ArXiv , abs/2311.05608.",
            "Yuanfeng Ji, Chongjian Ge, Weikai Kong, Enze Xie, Zhengying Liu, Zhengguo Li, and Ping Luo. 2023. Large language models as automated aligners for benchmarking vision-language models. ArXiv , abs/2311.14580.",
            "Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Uijlings, Ivan Krasin, Jordi Pont-Tuset, Shahab Kamali, Stefan Popov, Matteo Malloci, Alexander Kolesnikov, Tom Duerig, and Vittorio Ferrari. 2020. The open images dataset v4: Unified image classification, object detection, and visual relationship detection at scale. IJCV .",
            "Mukai Li, Lei Li, Yuwei Yin, Masood Ahmed, Zhenzhu Liu, and Qi Liu. 2024a. Red teaming visual language models. ArXiv , abs/2401.12915.",
            "Yifan Li, Hangyu Guo, Kun Zhou, Wayne Xin Zhao, and Ji-Rong Wen. 2024b. Images are achilles’ heel of alignment: Exploiting visual vulnerabilities for jailbreaking multimodal large language models. ArXiv , abs/2403.09792.",
            "Hongzhan Lin, Ziyang Luo, Bo Wang, Ruichao Yang, and Jing Ma. 2024. Goat-bench: Safety insights to large multimodal models through meme-based social abuse. ArXiv , abs/2401.01523.",
            "Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2023a. Visual instruction tuning. arXiv preprint arXiv:2304.08485 .",
            "Xin Liu, Yichen Zhu, Jindong Gu, Yunshi Lan, Chao Yang, and Yu Qiao. 2023b. Mm-safetybench: A benchmark for safety evaluation of multimodal large language models.",
            "Xin Liu, Yichen Zhu, Yunshi Lan, Chao Yang, and Yu Qiao. 2024. Safety of multimodal large language models on images and text. ArXiv , abs/2402.00357.",
            "Weidi Luo, Siyuan Ma, Xiaogeng Liu, Xiaoyu Guo, and Chaowei Xiao. 2024. Jailbreakv-28k: A benchmark for assessing the robustness of multimodal large language models against jailbreak attacks.",
            "Meta AI. 2024. Llama 2 - acceptable use policy. https://ai.meta.com/llama/use-policy/ . Accessed: 2024-01-19.",
            "OpenAI. 2023. Gpt-4 technical report. ArXiv , abs/2303.08774.",
            "Georgios Pantazopoulos, Amit Parekh, Malvina Nikandrour, and Alessandro Suglia. 2024. Learning to see but forgetting to follow: Visual instruction tuning makes llms more prone to jailbreak attacks.",
            "Renjie Pi, Tianyang Han, Yueqi Xie, Rui Pan, Qing Lian, Hanze Dong, Jipeng Zhang, and Tong Zhang. 2024. Mllm-protector: Ensuring mllm’s safety without hurting performance. ArXiv , abs/2401.02906.",
            "Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastri, Amanda Askell, Pamela Mishkin, Jack Clark, et al. 2021. Learning transferable visual models from natural language supervision. In International conference on machine learning , pages 8748–8763. PMLR.",
            "Zhelun Shi, Zhipin Wang, Hongxing Fan, Zaibin Zhang, Lijun Li, Yongting Zhang, Zhenfei Yin, Lu Sheng, Yu Qiao, and Jing Shao. 2024. Assessment of multimodal large language models in alignment with human values. ArXiv , abs/2403.17830.",
            "Gemini Team. 2024. Gemini 1.5: Unlocking multi-modal understanding across millions of tokens of context. ArXiv , abs/2403.05530.",
            "Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. 2023. Gemini: a family of highly capable multimodal models. arXiv preprint arXiv:2312.11805 .",
            "Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Marinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971 .",
            "Haoqin Tu, Chenhang Cui, Zijun Wang, Yiyang Zhou, Bingchen Zhao, Junlin Han, Wangchunshu Zhou, Huaxiu Yao, and Cihang Xie. 2023. How many unicorns are in this image? a safety evaluation benchmark for vision llms. ArXiv , abs/2311.16101.",
            "Pengyu Wang, Dong Zhang, Linyang Li, Chenkun Tan, Xinghao Wang, Ke Ren, Botian Jiang, and Xipeng Qiu. 2024. Inferaligner: Inference-time alignment for harmlessness through cross-model guidance. ArXiv , abs/2401.11206.",
            "Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji Qi, Yan Wang, Junhui Ji, Zhuoyi Yang, Lei Zhao, Xixuan Song, Jiazheng Xu, Bin Xu, Juanzi Li, Yuxiao Dong, Ming Ding, and Jie Tang. 2023a. Cogvlm: Visual expert for pretrained language models. ArXiv , abs/2311.03079.",
            "Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji Qi, Yan Wang, Junhui Ji, Zhuoyi Yang, Lei Zhao, Xixuan Song, et al. 2023b. Cogvlm: Visual expert for pretrained language models. arXiv preprint arXiv:2311.03079 .",
            "Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, et al. 2023a. mplug-owl: Modularization empowers large language models with multimodality.",
            "Qinghao Ye, Haiyang Xu, Jiabo Ye, Mingshi Yan, Anwen Hu, Haowei Liu, Qi Qian, Ji Zhang, Fei Huang, and Jingren Zhou. 2023b. mplug-owl2: Revolutionizing multi-modal large language model with modality collaboration. ArXiv , abs/2311.04257.",
            "01.AI Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge Zhang, Guanwei Zhang, Heng Li, Jiangcheng Zhu, Jianqun Chen, Jing Chang, Kaidong Yu, Peng Liu, Qiang Liu, Shawn Yue, Senbin Yang, ShimingYang, Tao Yu, Wen Xie, Wenhao Huang, Xiaohui Hu, Xiaoyi Ren, Xinyao Niu, Pengcheng Nie, Yuchi Xu, Yudong Liu, Yue Wang, Yuxuan Cai, Zhenyu Gu, Zhiyuan Liu, and Zonghong Dai. 2024. Yi: Open foundation models by 01.ai. ArXiv , abs/2403.04652.",
            "Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, Cong Wei, Botao Yu, Ruibin Yuan, Renliang Sun, Ming Yin, Boyuan Zheng, Zhenzhu Yang, Yibo Liu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. 2023. Mmmu: A massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. ArXiv , abs/2311.16502.",
            "Hao Zhang, Wenqi Shao, Hong Liu, Yongqiang Ma, Ping Luo, Yu Qiao, and Kaipeng Zhang. 2024. Avibench: Towards evaluating the robustness of large vision-language model on adversarial visual-instructions. ArXiv , abs/2403.09346.",
            "Yunqing Zhao, Tianyu Pang, Chao Du, Xiao Yang, Chongxuan Li, Ngai-Man Cheung, and Min Lin. 2023. On evaluating adversarial robustness of large vision-language models. ArXiv , abs/2305.16934.",
            "Yongshuo Zong, Ondrej Bohdal, Tingyang Yu, Yongxin Yang, and Timothy M. Hospedales. 2024. Safety fine-tuning at (almost) no cost: A baseline for vision large language models. ArXiv , abs/2402.02207."
        ]
    },
    {
        "index": 63,
        "title": "Cross-Task Defense: Instruction-Tuning LLMs for Content Safety",
        "publication_date": "2024-05-24",
        "references": [
            "Detecting language model attacks with perplexity",
            "Identifying and mitigating the security risks of generative ai",
            "Safety-tuned llamas: Lessons from improving the safety of large language models that follow instructions",
            "Play guessing game with llm: Indirect jailbreak attack with implicit clues",
            "Palm: Scaling language modeling with pathways",
            "Safety alignment in nlp tasks: Weakly aligned summarization as an in-context attack",
            "Lora: Low-rank adaptation of large language models",
            "Gradient cuff: Detecting jailbreak attacks on large language models by exploring refusal loss landscapes",
            "Catastrophic jailbreak of open-source llms via exploiting generation",
            "Baseline defenses for adversarial attacks against aligned language models",
            "Beavertails: Towards improved safety alignment of llm via a human-preference dataset",
            "Mistral 7b",
            "Training language models to follow instructions with human feedback",
            "Smoothllm: Defending large language models against jailbreaking attacks",
            "Get to the point: Summarization with pointer-generator networks",
            "Stanford alpaca: An instruction-following llama model",
            "Llama: Open and efficient foundation language models",
            "Llama 2: Open foundation and fine-tuned chat models",
            "Finetuned language models are zero-shot learners",
            "Adversarial attacks on llms",
            "How johnny can persuade llms to jailbreak them: Rethinking persuasion to challenge ai safety by humanizing llms",
            "Autodan: Interpretable gradient-based adversarial attacks on large language models",
            "Universal and transferable adversarial attacks on aligned language models"
        ]
    },
    {
        "index": 64,
        "title": "Dataset and Lessons Learned from the 2024 SaTML LLM Capture-the-Flag Competition",
        "publication_date": "2024-06-12",
        "references": [
            "Are you still on track!? Catching LLM task drift with activations",
            "Croissant: A metadata format for ml-ready datasets",
            "Jailbreaking leading safety-aligned LLMs with simple adaptive attacks",
            "Many-shot jailbreaking",
            "Foundational challenges in assuring alignment and safety of large language models",
            "Evaluating the susceptibility of pre-trained language models via handcrafted adversarial examples",
            "Adversarial examples are not easily detected: Bypassing ten detection methods",
            "The trojan detection challenge 2023 (LLM edition)",
            "Privacy side channels in machine learning systems",
            "Whispers in the machine: Confidentiality in LLM-integrated systems",
            "Position paper: Rethinking LLM censorship as a security problem",
            "Not what you’ve signed up for: Compromising real-world LLM-integrated applications with indirect prompt injection",
            "Measuring massive multitask language understanding",
            "Gandalf ignore instructions",
            "Prompt injection attack against LLM-integrated applications",
            "Jailbreaking chatgpt via prompt engineering: An empirical study",
            "Formalizing and benchmarking prompt injection attacks and defenses",
            "Ignore previous prompt: Attack techniques for language models",
            "Data cards: Purposeful and transparent dataset documentation for responsible ai",
            "Competition report: Finding universal jailbreak backdoors in aligned LLMs",
            "Great, now write an article about that: The crescendo multi-turn LLM jailbreak attack",
            "ARB: Advanced reasoning benchmark for large language models",
            "Ignore this title and HackAPrompt: Exposing systemic vulnerabilities of LLMs through a global prompt hacking competition",
            "Tensor trust: Interpretable prompt injection attacks from an online game"
        ]
    },
    {
        "index": 65,
        "title": "DECIPHERING THE CHAOS : ENHANCING JAILBREAK ATTACKS VIA ADVERSARIAL PROMPT TRANSLATION",
        "publication_date": "2025-01-20",
        "references": [
            "Phi-3 technical report: A highly capable language model locally on your phone.",
            "Gpt-4 technical report.",
            "Llama 3 model card.",
            "Jailbreaking leading safety-aligned LLMs with simple adaptive attacks.",
            "Constitutional ai: Harmlessness from ai feedback.",
            "Are aligned neural networks adversarially aligned?",
            "Explore, establish, exploit: Red teaming language models from scratch.",
            "Jailbreaking black box large language models in twenty queries.",
            "Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality.",
            "Jailbreaker: Automated jailbreak across multiple large language model chatbots.",
            "The llama 3 herd of models.",
            "Improving alignment of dialogue agents via targeted human judgements.",
            "Gradient-based adversarial attacks against text transformers.",
            "Enhancing adversarial example transferability with an intermediate level attack.",
            "Baseline defenses for adversarial attacks against aligned language models.",
            "Mistral 7b.",
            "Automatically auditing large language models via discrete optimization.",
            "Pretraining language models with human preferences.",
            "Improved generation of adversarial examples against safety-aligned llms.",
            "Amplegcg: Learning a universal and transferable generative model of adversarial suffixes for jailbreaking both open and closed LLMs.",
            "Autodan: Generating stealthy jailbreak prompts on aligned large language models.",
            "Towards deep learning models resistant to adversarial attacks.",
            "Harmbench: A standardized evaluation framework for automated red teaming and robust refusal.",
            "Tree of attacks: Jailbreaking black-box LLMs automatically.",
            "Red teaming language models with language models.",
            "A strongreject for empty jailbreaks.",
            "Gemma 2: Improving open language models at a practical size.",
            "Qwen2.5: A party of foundation models.",
            "Llama: Open and efficient foundation language models.",
            "Universal adversarial triggers for attacking and analyzing nlp.",
            "Autoprompt: Eliciting knowledge from language models with automatically generated prompts.",
            "Pal: Proxy-guided black-box attack on large language models.",
            "A jailbroken: How does LLM safety training fail?",
            "Jailbreak and guard aligned language models with only few in-context demonstrations.",
            "Hard prompts made easy: Gradient-based discrete optimization for prompt tuning and discovery.",
            "Rethinking the security of skip connections in resnet-like neural networks.",
            "How johnny can persuade LLMs to jailbreak them: Rethinking persuasion to challenge ai safety by humanizing LLMs.",
            "Judging LLM-as-a-judge with mt-bench and chatbot arena.",
            "Autodan: Automatic and interpretable adversarial attacks on large language models.",
            "Universal and transferable adversarial attacks on aligned language models."
        ]
    },
    {
        "index": 66,
        "title": "Decoupled Alignment for Robust Plug-and-Play Adaptation",
        "publication_date": "2024-06-06",
        "references": [
            "Qwen technical report",
            "Training a helpful and harmless assistant with reinforcement learning from human feedback",
            "When automated assessment meets automated content generation: Examining text quality in the era of gpts",
            "Language models are few-shot learners",
            "Jailbreaking black box large language models in twenty queries",
            "Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality",
            "Safe RLHF: Safe reinforcement learning from human feedback",
            "Jailbreaker: Automated jailbreak across multiple large language model chatbots",
            "Qlora: Efficient fine-tuning of quantized llms",
            "Beyond the imitation game: Quantifying and extrapolating the capabilities of language models",
            "Measuring massive multitask language understanding",
            "Locating and editing factual associations in gpt",
            "Mass-editing memory in a transformer",
            "Pointer sentinel mixture models",
            "Memory-based model editing at scale",
            "Gpt-4 technical report",
            "Training language models to follow instructions with human feedback",
            "Chain-of-action: Faithful and multimodal question answering through large language models",
            "Conv-coa: Improving open-domain question answering in large language models via conversational chain-of-action",
            "Instruction tuning with gpt-4",
            "Direct preference optimization: Your language model is secretly a reward model",
            "Hopfield networks is all you need",
            "Glu variants improve transformer",
            "Gemma: Open models based on gemini research and technology",
            "Llama 2: Open foundation and fine-tuned chat models",
            "Attention is all you need",
            "Detoxifying large language models via knowledge editing",
            "Chain-of-thought prompting elicits reasoning in large language models",
            "Jailbreak and guard aligned language models with only few in-context demonstrations",
            "Big data and machine learning in defence",
            "Llms can defend themselves against jail-breaking in a practical manner: A vision paper",
            "Uniform memory retrieval with larger capacity for modern hopfield models",
            "STanhop: Sparse tandem hopfield model for memory-enhanced time series prediction",
            "Wizardlm: Empowering large language models to follow complex instructions",
            "A survey on knowledge distillation of large language models",
            "Editing large language models: Problems, methods, and opportunities",
            "Deepspeed-chat: Easy, fast and affordable rlhf training of chatgpt-like models at all scales",
            "Gptfuzzer: Red teaming large language models with auto-generated jailbreak prompts",
            "Assessing prompt injection risks in 200+ custom gpts",
            "Enhancing jailbreak attack against large language models through silent tokens",
            "Simplifying and isolating failure-inducing input"
        ]
    },
    {
        "index": 67,
        "title": "Defending Large Language Models Against Attacks With Residual Stream Activation Analysis",
        "publication_date": "2024-11-13",
        "references": [
            "ChatGPT_DAN",
            "Jailbreaking black box large language models in twenty queries",
            "Ignore previous prompt: Attack techniques for language models",
            "Explaining and harnessing adversarial examples",
            "Lightgbm: a highly efficient gradient boosting decision tree",
            "Bert rediscovers the classical nlp pipeline",
            "Representation engineering: A top-down approach to ai transparency",
            "Simple probes can catch sleeper agents",
            "Scaling monosemanticity: Extracting interpretable features from claude 3 sonnet",
            "Judging LLM-as-a-judge with MT-bench and chatbot arena",
            "Baseline defenses for adversarial attacks against aligned language models",
            "Bergeron: Combating adversarial attacks through a conscience-based alignment framework",
            "Jailbreakv-28k: A benchmark for assessing the robustness of multimodal large language models against jailbreak attacks",
            "Openorca: An open dataset of gpt augmented flan reasoning traces",
            "Sujet finance dataset",
            "Ignore this title and hackAPrompt: Exposing systemic vulnerabilities of LLMs through a global prompt hacking competition",
            "prompt_injection_cleaned_dataset-v2",
            "Wildteaming at scale: From in-the-wild jailbreaks to (adversarially) safer language models",
            "Llama 2: Open foundation and fine-tuned chat models",
            "Tinyllama: An open-source small language model",
            "Mistral 7b",
            "Wizardvicunalm",
            "LoRA: Low-rank adaptation of large language models",
            "Model-tuning via prompts makes NLP models adversarially robust",
            "Universal and transferable adversarial attacks on aligned language models"
        ]
    },
    {
        "index": 68,
        "title": "Defending Large Language Models Against Jailbreak Attacks via Layer-specific Editing",
        "publication_date": "2024-06-14",
        "references": [
            "Gpt-4 technical report",
            "Jailbreak chat",
            "Detecting language model attacks with perplexity",
            "Training a helpful and harmless assistant with reinforcement learning from human feedback",
            "Defending against alignment-breaking attacks via robustly aligned llm",
            "Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality",
            "Editing factual knowledge in language models",
            "Masterkey: Automated jailbreak across multiple large language model chatbots",
            "Not all layers of llms are necessary during inference",
            "Transformer feed-forward layers build predictions by promoting concepts in the vocabulary space",
            "Transformer feed-forward layers are key-value memories",
            "Improving alignment of dialogue agents via targeted human judgements",
            "The unreasonable ineffectiveness of the deeper layers",
            "Query-based adversarial prompt generation",
            "Llm self defense: By self examination, llms know they are being tricked",
            "Lora: Low-rank adaptation of large language models",
            "Catastrophic jailbreak of open-source LLMs via exploiting generation",
            "Baseline defenses for adversarial attacks against aligned language models",
            "Mistral 7b",
            "Plug-and-play adaptation for continuously-updated qa",
            "Deepinception: Hypnotize large language model to be jailbreaker",
            "Rain: Your language models can align themselves without finetuning",
            "The unlocking spell on base llms: Rethinking alignment via in-context learning",
            "Generating stealthy jailbreak prompts on aligned large language models",
            "Is it possible to edit large language models robustly?",
            "Shortgpt: Layers in large language models are more redundant than you expect",
            "Locating and editing factual associations in gpt",
            "Mass-editing memory in a transformer",
            "Fast model editing at scale",
            "Jailbreaking chat-gpt on release day",
            "Forgetting before learning: Utilizing parametric arithmetic for knowledge updating in large language models",
            "The trojan detection challenge 2023 (llm edition)",
            "Training language models to follow instructions with human feedback",
            "Future lens: Anticipating subsequent tokens from a single hidden state",
            "Smoothllm: Defending large language models against jailbreaking attacks",
            "Do anything now\": Characterizing and evaluating in-the-wild jailbreak prompts on large language models",
            "Llama 2: Open foundation and fine-tuned chat models",
            "Detoxifying large language models via knowledge editing",
            "Aligning large language models with human: A survey",
            "Jailbroken: How does llm safety training fail?",
            "Jailbreak and guard aligned language models with only few in-context demonstrations",
            "Depn: Detecting and editing privacy neurons in pretrained language models",
            "Defending chatgpt against jailbreak attack via self-reminders",
            "Safedecoding: Defending against jailbreak attacks via safety-aware decoding",
            "Gpt-fuzzer: Red teaming large language models with auto-generated jailbreak prompts",
            "How johnny can persuade llms to jailbreak them: Rethinking persuasion to challenge ai safety by humanizing llms",
            "Causality analysis for evaluating the security of large language models",
            "Judging llm-as-a-judge with mt-bench and chatbot arena",
            "Lima: Less is more for alignment",
            "Easyjailbreak: A unified framework for jailbreaking large language models"
        ]
    },
    {
        "index": 69,
        "title": "Defending LLMs against Jailbreaking Attacks via Backtranslation",
        "publication_date": "2024-08-11",
        "references": [
            "Gpt-4 technical report",
            "Detecting language model attacks with perplexity",
            "Training a helpful and harmless assistant with reinforcement learning from human feedback",
            "Jailbreaking black box large language models in twenty queries",
            "Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality",
            "Palm: Scaling language modeling with pathways",
            "Jailbreaking proprietary large language models using word substitution cipher",
            "Baseline defenses for adversarial attacks against aligned language models",
            "Defending large language models against jailbreak attacks via semantic smoothing",
            "Certifying llm safety against adversarial prompting",
            "Open sesame! universal black box jailbreaking of large language models",
            "Autodan: Generating stealthy jailbreak prompts on aligned large language models",
            "Jailbreaking chatgpt via prompt engineering: An empirical study",
            "Chatgpt",
            "Fine-tuning aligned language models compromises safety, even when users do not intend to!",
            "Smoothllm: Defending large language models against jailbreaking attacks",
            "Llama: Open and efficient foundation language models",
            "Llama 2: Open foundation and fine-tuned chat models",
            "Jailbroken: How does llm safety training fail?",
            "Jailbreak and guard aligned language models with only few in-context demonstrations",
            "Defending chatgpt against jailbreak attack via self-reminders",
            "An llm can fool itself: A prompt-based adversarial attack",
            "Gpt-fuzzer: Red teaming large language models with auto-generated jailbreak prompts",
            "Gpt-4 is too smart to be safe: Stealthy chat with llms via cipher",
            "How johnny can persuade llms to jailbreak them: Rethinking persuasion to challenge ai safety by humanizing llms",
            "Defending large language models against jail-breaking attacks through goal prioritization",
            "Prompt-driven llm safeguarding via directed representation optimization",
            "Judging llm-as-a-judge with mt-bench and chatbot arena",
            "Robust prompt optimization for defending language models against jailbreaking attacks",
            "Large language models are human-level prompt engineers",
            "Autodan: Automatic and interpretable adversarial attacks on large language models",
            "Universal and transferable adversarial attacks on aligned language models"
        ]
    },
    {
        "index": 70,
        "title": "Defense Against Prompt Injection Attack by Leveraging Attack Techniques",
        "publication_date": "2025-02-25",
        "references": [
            "Instruction defense",
            "Sandwich defense",
            "Llama 3 model card",
            "Don’t you (forget nlp): Prompt injection with control characters in chatgpt",
            "Evaluating large language models trained on code",
            "Struq: Defending against prompt injection with structured queries",
            "The llama 3 herd of models",
            "Alpacafarm: A simulation framework for methods that learn from human feedback",
            "Not what you’ve signed up for: Compromising real-world llm-integrated applications with indirect prompt injection",
            "Defending against indirect prompt injection attacks with spotlighting",
            "Semantic-guided prompt organization for universal goal hijacking against llms",
            "Gpt-4o system card",
            "Structgpt: A general framework for large language model to reason over structured data",
            "Large language models are zero-shot reasoners",
            "Evaluating the instruction-following robustness of large language models to prompt injection",
            "Automatic and universal prompt injection attacks against large language models",
            "Prompt injection attack against llm-integrated applications",
            "Formalizing and benchmarking prompt injection attacks and defenses",
            "Injecagent: Benchmarking indirect prompt injections in tool-integrated large language model agents",
            "Least-to-most prompting enables complex reasoning in large language models",
            "Autodan: Automatic and interpretable adversarial attacks on large language models",
            "Universal and transferable adversarial attacks on aligned language models",
            "Pytorch: An imperative style, high-performance deep learning library",
            "Ignore previous prompt: Attack techniques for language models",
            "Jatmo: Prompt injection defense by task-specific finetuning",
            "Machine against the rag: Jamming retrieval-augmented generation with blocker documents",
            "Optimization-based prompt injection attack to llm-as-a-judge",
            "Recursive deep models for semantic compositionality over a sentiment treebank",
            "Signed-prompt: A new approach to prevent prompt injection attacks against llm-integrated applications",
            "The instruction hierarchy: Training llms to prioritize privileged instructions",
            "Delimiters won’t save you from prompt injection",
            "Qwen2 technical report",
            "Benchmarking and defending against indirect prompt injection attacks on large language models",
            "OWASP Top 10 for LLM Applications, 2023",
            "Evaluating the instruction-following robustness of large language models to prompt injection",
            "Automatic and universal prompt injection attacks against large language models",
            "Prompt injection attack against llm-integrated applications",
            "Formalizing and benchmarking prompt injection attacks and defenses",
            "Injecagent: Benchmarking indirect prompt injections in tool-integrated large language model agents",
            "Least-to-most prompting enables complex reasoning in large language models",
            "Autodan: Automatic and interpretable adversarial attacks on large language models",
            "Universal and transferable adversarial attacks on aligned language models",
            "Pytorch: An imperative style, high-performance deep learning library",
            "Ignore previous prompt: Attack techniques for language models",
            "Jatmo: Prompt injection defense by task-specific finetuning",
            "Machine against the rag: Jamming retrieval-augmented generation with blocker documents",
            "Optimization-based prompt injection attack to llm-as-a-judge",
            "Recursive deep models for semantic compositionality over a sentiment treebank",
            "Signed-prompt: A new approach to prevent prompt injection attacks against llm-integrated applications",
            "The instruction hierarchy: Training llms to prioritize privileged instructions",
            "Delimiters won’t save you from prompt injection"
        ]
    },
    {
        "index": 71,
        "title": "Defensive Prompt Patch: A Robust and Interpretable Defense of LLMs against Jailbreak Attacks",
        "publication_date": "2024-05-30",
        "references": [
            "Universal and transferable adversarial attacks on aligned language models",
            "Llama: Open and efficient foundation language models",
            "Autodan: Generating stealthy jailbreak prompts on aligned large language models",
            "Tree of attacks: Jailbreaking black-box llms automatically",
            "Jailbreaking black box large language models in twenty queries",
            "Jailbroken: How does LLM safety training fail?",
            "Mistral 7b",
            "Jailbreak and guard aligned language models with only few in-context demonstrations",
            "Catastrophic jailbreak of open-source llms via exploiting generation",
            "Defending chatgpt against jailbreak attack via self-reminders",
            "Defending large language models against jailbreaking attacks through goal prioritization",
            "Robust prompt optimization for defending language models against jailbreaking attacks",
            "Llm self defense: By self examination, llms know they are being tricked",
            "Alpacaeval: An automatic evaluator of instruction-following models",
            "On prompt-driven safeguarding for large language models",
            "On adaptive attacks to adversarial example defenses",
            "Amplegcg: Learning a universal and transferable generative model of adversarial suffixes for jailbreaking both open and closed llms",
            "GPT-4 technical report",
            "Baseline defenses for adversarial attacks against aligned language models",
            "Smoothllm: Defending large language models against jailbreaking attacks",
            "Starling-7b: Improving llm helpfulness and harmlessness with rlaif",
            "A general language assistant as a laboratory for alignment",
            "Training a helpful and harmless assistant with reinforcement learning from human feedback",
            "Training language models to follow instructions with human feedback",
            "Attention is all you need",
            "Agieval: A human-centric benchmark for evaluating foundation models",
            "Summarization is (almost) dead",
            "Intention analysis makes llms a good jailbreak defender",
            "Low-resource languages jailbreak gpt-4",
            "Safetybench: Evaluating the safety of large language models with multiple choice questions",
            "Bert: Pre-training of deep bidirectional transformers for language understanding",
            "Language models show human-like content effects on reasoning tasks",
            "Jailbreakbench: An open robustness benchmark for jailbreaking large language models",
            "On prompt-driven safeguarding for large language models"
        ]
    },
    {
        "index": 72,
        "title": "Derail Yourself: MULTI-TURN LLM JAILBREAK ATTACK THROUGH SELF-DISCOVERED CLUES",
        "publication_date": "2024-10-14",
        "references": [
            "Detecting language model attacks with perplexity",
            "Many-shot jailbreaking",
            "Claude-3.5-sonnet",
            "Training a helpful and harmless assistant with reinforcement learning from human feedback",
            "On the opportunities and risks of foundation models",
            "Defending against alignment-breaking attacks via robustly aligned llm",
            "Explore, establish, exploit: Red teaming language models from scratch",
            "Jailbreaking black box large language models in twenty queries",
            "Jailbreaking black box large language models in twenty queries",
            "Evaluating large language models trained on code",
            "Training verifiers to solve math word problems",
            "Opencompass: A universal evaluation platform for foundation models",
            "Multilingual jailbreak challenges in large language models",
            "Enhancing chat language models by scaling high-quality instructional conversations",
            "The llama 3 herd of models",
            "Measuring massive multitask language understanding",
            "Curiosity-driven red-teaming for large language models",
            "Lora: Low-rank adaptation of large language models",
            "Gradient cuff: Detecting jailbreak attacks on large language models by exploring refusal loss landscapes",
            "Llama guard: Llm-based input-output safeguard for human-ai conversations",
            "Baseline defenses for adversarial attacks against aligned language models",
            "Beavertails: Towards improved safety alignment of llm via a human-preference dataset",
            "Science in Action: How to Follow Scientists and Engineers Through Society",
            "Learning diverse attacks on large language models for robust red-teaming and safety tuning",
            "Salad-bench: A hierarchical and comprehensive safety benchmark for large language models",
            "Llm defenses are not robust to multi-turn human jailbreaks yet",
            "The wmdp benchmark: Measuring and reducing malicious use with unlearning",
            "Deepseek-v2: A strong, economical, and efficient mixture-of-experts language model",
            "Improved baselines with visual instruction tuning",
            "Visual instruction tuning",
            "Adversarial attacks with hidden intentions towards aligned large language models",
            "Generating stealthy jailbreak prompts on aligned large language models",
            "Protecting your llms with information bottleneck",
            "Harmbench: A standardized evaluation framework for automated red teaming and robust refusal",
            "Tree of attacks: Jailbreaking black-box llms automatically",
            "Simple preference optimization with a reference-free reward",
            "Seallms–large language models for southeast asia",
            "Gpt-3.5 turbo",
            "Gpt-4o system card",
            "Openai o1 system card",
            "Training language models to follow instructions with human feedback",
            "Fast adaptive adversarial prompting for llms",
            "Red teaming language models with language models",
            "Llm self defense: By self examination, llms know they are being tricked",
            "Fine-tuning aligned language models compromises safety, even when users do not intend to!",
            "Direct preference optimization: Your language model is secretly a reward model",
            "Exploring safety generalization challenges of large language models via code",
            "Smoothllm: Defending large language models against jailbreaking attacks",
            "Great, now write an article about that: The crescendo multi-turn llm jailbreak attack",
            "Jais and jais-chat: Arabic-centric foundation and instruction-tuned open generative large language models",
            "All languages matter: On the multilingual safety of large language models",
            "Defending chatgpt against jailbreak attack via self-reminders",
            "Wizardlm: Empowering large language models to follow complex instructions",
            "Safedecoding: Defending against jailbreak attacks via safety-aware decoding",
            "Chain of attack: a semantic-driven contextual multi-turn attacker for llm",
            "On the vulnerability of safety alignment in open-access llms",
            "Low-resource languages jailbreak gpt-4",
            "Evaluating large language model safety in multi-turn dialogue coreference",
            "Gptfuzzer: Red teaming large language models with auto-generated jailbreak prompts",
            "Refuse whenever you feel unsafe: Improving safety in llms via decoupled refusal training",
            "GPT-4 is too smart to be safe: Stealthy chat with LLMs via cipher",
            "Rigor-llm: Resilient guardrails for large language models against undesired content",
            "How johnny can persuade llms to jailbreak them: Rethinking persuasion to challenge ai safety by humanizing llms",
            "Psysafe: A comprehensive framework for psychological-based attack, defense, and evaluation of multi-agent system safety",
            "Defending large language models against jailbreaking attacks through goal prioritization",
            "Safe unlearning: A surprisingly effective and generalizable solution to defend against jailbreak attacks",
            "Parden, can you repeat that? defending against jailbreaks via repetition",
            "Prompt-driven llm safeguarding via directed representation optimization",
            "Judging llm-as-a-judge with mt-bench and chatbot arena",
            "Speak out of turn: Safety vulnerability of large language models in multi-turn dialogue",
            "Autodan: Interpretable gradient-based adversarial attacks on large language models",
            "Representation engineering: A top-down approach to ai transparency",
            "Universal and transferable adversarial attacks on aligned language models",
            "Improving alignment and robustness with circuit breakers",
            "Improving alignment and robustness with circuit breakers"
        ]
    },
    {
        "index": 73,
        "title": "DiffZOO: A Purely Query-Based Black-Box Attack for Red-teaming Text-to-Image Generative Model via Zeroth Order Optimization",
        "publication_date": "2025-02-06",
        "references": [
            "Towards evaluating the robustness of neural networks",
            "Deepzero: Scaling up zeroth-order optimization for deep model training",
            "ZOO: zeroth order optimization based black-box attacks to deep neural networks without training substitute models",
            "Zoadamm: Zeroth-order adaptive momentum method for black-box optimization",
            "Prompting4debugging: Red-teaming text-to-image diffusion models by finding problematic prompts",
            "Forget unlearning: Towards true data-deletion in machine learning",
            "Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks",
            "BERT: pre-training of deep bidirectional transformers for language understanding",
            "Are diffusion models vulnerable to membership inference attacks?",
            "Erasing concepts from diffusion models",
            "Denoising diffusion probabilistic models",
            "Textgrad: Advancing robustness evaluation in NLP by gradient-driven optimization",
            "Neural program synthesis with query",
            "Copyrnerf: Protecting the copyright of neural radiance fields",
            "Jailbreaking prompt attack: A controllable adversarial attack against diffusion models",
            "Towards deep learning models resistant to adversarial attacks",
            "Learning transferable visual models from natural language supervision",
            "Language models are unsupervised multitask learners",
            "Red-teaming the stable diffusion safety filter",
            "Safe latent diffusion: Mitigating inappropriate degeneration in diffusion models",
            "Can machines help us answering question 16 in datasheets, and in turn reflecting on inappropriate content?",
            "Ring-a-bell! how reliable are concept removal methods for diffusion models?",
            "Imagereward: Learning and evaluating human preferences for text-to-image generation",
            "Mma-diffusion: Multi-modal attack on diffusion models",
            "Sneakyprompt: Jailbreaking text-to-image generative models",
            "Forget-me-not: Learning to forget in text-to-image diffusion models",
            "A pilot study of query-free adversarial attack against stable diffusion"
        ]
    },
    {
        "index": 74,
        "title": "Does Refusal Training in LLMs Generalize to the Past Tense?",
        "publication_date": "2024-10-03",
        "references": [
            "Phi-3 technical report: A highly capable language model locally on your phone.",
            "Llama 3 model card.",
            "Jailbreaking leading safety-aligned llms with simple adaptive attacks.",
            "Many-shot jailbreaking.",
            "Introducing claude 3.5 sonnet.",
            "Training a helpful and harmless assistant with reinforcement learning from human feedback.",
            "Managing ai risks in an era of rapid progress.",
            "The reversal curse: Llms trained on” a is b” fail to learn” b is a”.",
            "Wild patterns: ten years after the rise of adversarial machine learning.",
            "Evasion attacks against machine learning at test time.",
            "Jailbreaking black box large language models in twenty queries.",
            "Jailbreakbench: An open robustness benchmark for jailbreaking large language models.",
            "Scaling instruction-finetuned language models.",
            "Rlhf can speak many languages: Unlocking multilingual preference optimization for llms.",
            "Gemma-2 report.",
            "Open sesame! universal black box jailbreaking of large language models.",
            "Chatgpt doesn’t trust chargers fans: Guardrail sensitivity in context.",
            "Preference tuning for toxicity mitigation generalizes across languages.",
            "Autodan: Generating stealthy jailbreak prompts on aligned large language models.",
            "Towards deep learning models resistant to adversarial attacks.",
            "Tdc 2023 (llm edition): The trojan detection challenge.",
            "Harmbench: A standardized evaluation framework for automated red teaming and robust refusal.",
            "Jailbroken: How does llm safety training fail?",
            "Jailbreak and guard aligned language models with only few in-context demonstrations.",
            "Do llamas work in english? on the latent language of multilingual transformers.",
            "Llama 2: Open foundation and fine-tuned chat models.",
            "Zephyr: Direct distillation of lm alignment."
        ]
    },
    {
        "index": 75,
        "title": "Don’t Say No: Jailbreaking LLM by Suppressing Refusal",
        "publication_date": "2024-10-12",
        "references": [
            "jailbreakchat.com. http://jailbreakchat.com , 2023.",
            "Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.",
            "Llama 3 model card. 2024.",
            "Take a look at it! rethinking how to evaluate language model jailbreak. arXiv preprint arXiv:2404.06407, 2024.",
            "Are aligned neural networks adversarially aligned? arXiv preprint arXiv:2306.15447, 2023.",
            "Towards evaluating the robustness of neural networks. In 2017 ieee symposium on security and privacy (sp), pages 39–57. Ieee, 2017.",
            "Jailbreakbench: An open robustness benchmark for jail-breaking large language models, 2024.",
            "Jailbreaking black box large language models in twenty queries, 2024.",
            "Masterkey: Automated jailbreaking of large language model chatbots. In Proc. ISOC NDSS, 2024.",
            "The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024.",
            "Hotflip: White-box adversarial examples for text classification. arXiv preprint arXiv:1712.06751, 2017.",
            "Figstep: Jailbreaking large vision-language models via typographic visual prompts, 2023.",
            "Explaining and harnessing adversarial examples. arXiv preprint arXiv:1412.6572, 2014.",
            "Deberta: Decoding-enhanced bert with disentangled attention, 2021.",
            "Universal language model fine-tuning for text classification, 2018.",
            "Catastrophic jailbreak of open-source llms via exploiting generation, 2023.",
            "Llama guard: Llm-based input-output safeguard for human-ai conversations, 2023.",
            "Baseline defenses for adversarial attacks against aligned language models. arXiv preprint arXiv:2309.00614, 2023.",
            "Improved techniques for optimization-based jailbreaking on large language models, 2024.",
            "Mistral 7b, 2023.",
            "Unlocking adversarial suffix optimization without affirmative phrases: Efficient black-box jailbreaking via llm as optimizer. arXiv preprint arXiv:2408.11313, 2024.",
            "Adversarial machine learning at scale, 2017.",
            "The power of scale for parameter-efficient prompt tuning. arXiv preprint arXiv:2104.08691, 2021.",
            "Open the pandora’s box of llms: Jailbreaking llms through representation engineering, 2024.",
            "Amplegcg: Learning a universal and transferable generative model of adversarial suffixes for jailbreaking both open and closed llms, 2024.",
            "Advancing adversarial suffix transfer learning on aligned large language models. arXiv preprint arXiv:2408.14866, 2024.",
            "Autodan: Generating stealthy jailbreak prompts on aligned large language models. arXiv preprint arXiv:2310.04451, 2023.",
            "Delving into transferable adversarial examples and black-box attacks. arXiv preprint arXiv:1611.02770 , 2016.",
            "Harmbench: A standardized evaluation framework for automated red teaming and robust refusal. arXiv preprint arXiv:2402.04249, 2024.",
            "Universal adversarial triggers are not universal, 2024.",
            "Fast model editing at scale, 2022.",
            "Jailbreaking attack against multimodal large language model, 2024.",
            "Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35:27730–27744, 2022.",
            "The limitations of deep learning in adversarial settings. In2016 IEEE European symposium on security and privacy (EuroS&P), pages 372–387. IEEE, 2016.",
            "Advprompter: Fast adaptive adversarial prompting for llms. arXiv preprint arXiv:2404.16873, 2024.",
            "Cold decoding: Energy-based constrained text generation with langevin dynamics. Advances in Neural Information Processing Systems, 35:9538–9551, 2022.",
            "Jailbreakeval: An integrated toolkit for evaluating jailbreak attempts against large language models, 2024.",
            "Smoothllm: Defending large language models against jailbreaking attacks. arXiv preprint arXiv:2310.03684, 2023.",
            "When do universal image jailbreaks transfer between vision-language models? arXiv preprint arXiv:2407.15211, 2024.",
            "A value for n-person games. 1953.",
            "Autoprompt: Eliciting knowledge from language models with automatically generated prompts. arXiv preprint arXiv:2010.15980 , 2020.",
            "The many shapley values for model explanation, 2020.",
            "Intriguing properties of neural networks, 2014.",
            "Gemma. 2024.",
            "Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.",
            "Jailbroken: How does llm safety training fail? arXiv preprint arXiv:2307.02483, 2023.",
            "Neural text generation with unlikelihood training, 2019.",
            "Hard prompts made easy: Gradient-based discrete optimization for prompt tuning and discovery. arXiv preprint arXiv:2302.03668, 2023.",
            "Jailbreaking as a reward misspecification problem. arXiv preprint arXiv:2406.14393, 2024.",
            "A comprehensive study of jailbreak attack versus defense for large language models, 2024.",
            "Llm jailbreak attack versus defense techniques–a comprehensive study. arXiv preprint arXiv:2402.13457 , 2024.",
            "Qwen2 technical report. arXiv preprint arXiv:2407.10671, 2024.",
            "Jailbreak attacks and defenses against large language models: A survey. arXiv preprint arXiv:2407.04295, 2024.",
            "Gptfuzzer: Red teaming large language models with auto-generated jailbreak prompts. arXiv preprint arXiv:2309.10253 , 2023.",
            "Jade: A linguistics-based safety evaluation platform for large language models, 2023.",
            "Weak-to-strong jailbreaking on large language models. arXiv preprint arXiv:2401.17256, 2024.",
            "Judging llm-as-a-judge with mt-bench and chatbot arena. arXiv preprint arXiv:2306.05685, 2023.",
            "Emulated dis-alignment: Safety alignment for large language models may backfire! arXiv preprint arXiv:2402.12343, 2024.",
            "Autodan: Interpretable gradient-based adversarial attacks on large language models. In First Conference on Language Modeling, 2023.",
            "Fine-tuning language models from human preferences. arXiv preprint arXiv:1909.08593 , 2019.",
            "Universal and transferable adversarial attacks on aligned language models. arXiv preprint arXiv:2307.15043, 2023."
        ]
    },
    {
        "index": 76,
        "title": "EEG-Defender: Defending against Jailbreak through Early Exit Generation of Large Language Models",
        "publication_date": "2024-08-21",
        "references": [
            "GPT-4 technical report",
            "Towards a Theory of Thinking",
            "Defending against alignment-breaking attacks via robustly aligned llm",
            "Jailbreaking black box large language models in twenty queries",
            "Language, memory, and mental time travel: An evolutionary perspective",
            "Layerskip: Enabling early exit inference and self-speculative decoding",
            "Not all layers of llms are necessary during inference",
            "Harm categories in azure ai content safety",
            "Language is primarily a tool for communication rather than thought",
            "Llama guard: Llm-based input-output safeguard for human-ai conversations",
            "Llama guard: Llm-based input-output safeguard for human-ai conversations",
            "Baseline defenses for adversarial attacks against aligned language models",
            "Perspective api",
            "Deepinception: Hypnotize large language model to be jailbreaker",
            "Alpacaeval: An automatic evaluator of instruction-following models",
            "Towards understanding jailbreak attacks in llms: A representation space analysis",
            "Toxicchat: Unveiling hidden challenges of toxicity detection in real-world user-AI conversation",
            "AutoDAN: Generating stealthy jailbreak prompts on aligned large language models",
            "Jailbreakv-28k: A benchmark for assessing the robustness of multimodal large language models against jailbreak attacks",
            "Tree of attacks: Jailbreaking black-box llms automatically",
            "Shortgpt: Layers in large language models are more redundant than you expect",
            "Locating and editing factual associations in gpt",
            "GPT-4 technical report",
            "Moderation guide",
            "Training language models to follow instructions with human feedback",
            "The language and thought of the child",
            "Hijacking large language models via adversarial in-context learning",
            "Smoothllm: Defending large language models against jailbreaking attacks",
            "Human Physiology: From Cells to Systems",
            "Prototypical networks for few-shot learning",
            "Preference ranking optimization for human alignment",
            "Univeral and transferable adversarial attacks on aligned language models"
        ]
    },
    {
        "index": 77,
        "title": "EFFECTIVE AND EFFICIENT ADVERSARIAL DETECTION FOR VISION-LANGUAGE MODELS VIA A SINGLE VECTOR",
        "publication_date": "2024-10-30",
        "references": [
            "Jailbreaking leading safety-aligned llms with simple adaptive attacks.",
            "Gemini: A family of highly capable multimodal models.",
            "A general language assistant as a laboratory for alignment.",
            "Qwen-vl: A frontier large vision-language model with versatile abilities.",
            "Recent advances in adversarial training for adversarial robustness.",
            "Are aligned neural networks adversarially aligned?",
            "Adversarial attacks and defences: A survey.",
            "A survey on adversarial attacks and defences.",
            "Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality.",
            "Safe RLHF: safe reinforcement learning from human feedback.",
            "Instructblip: Towards general-purpose vision-language models with instruction tuning.",
            "Position: The platonic representation hypothesis.",
            "Beavertails: Towards improved safety alignment of LLM via a human-preference dataset.",
            "AI alignment: A comprehensive survey.",
            "Efficient memory management for large language model serving with pagedattention.",
            "Scalable agent alignment via reward modeling: a research direction.",
            "BLIP-2: bootstrapping language-image pre-training with frozen image encoders and large language models.",
            "Inference-time intervention: Eliciting truthful answers from a language model.",
            "Microsoft COCO: common objects in context.",
            "Visual instruction tuning.",
            "In-context vectors: Making in context learning more effective and controllable through latent space steering.",
            "Safety of multimodal large language models on images and text.",
            "An image is worth 1000 lies: Transferability of adversarial images across prompts on vision-language models.",
            "Towards deep learning models resistant to adversarial attacks.",
            "Harmbench: A standardized evaluation framework for automated red teaming and robust refusal.",
            "Gpt-4v(ision) system card.",
            "Gpt-4o system card.",
            "Training language models to follow instructions with human feedback.",
            "Direct preference optimization: Your language model is secretly a reward model.",
            "Steering llama 2 via contrastive activation addition.",
            "Do-not-answer: A dataset for evaluating safeguards in llms.",
            "Llama 2: Open foundation and fine-tuned chat models.",
            "How many unicorns are in this image? A safety evaluation benchmark for vision llms.",
            "Activation addition: Steering language models without optimization.",
            "Jailguard: A universal detection framework for llm prompt-based attacks.",
            "A mutation-based method for multi-modal jailbreaking attack detection.",
            "Minigpt-4: Enhancing vision-language understanding with advanced large language models."
        ]
    },
    {
        "index": 78,
        "title": "Efficient Adversarial Training in LLMs with Continuous Attacks",
        "publication_date": "2024-11-01",
        "references": [
            "Universal and Transferable Adversarial Attacks on Aligned Language Models",
            "Jailbreaking Leading Safety-Aligned LLMs with Simple Adaptive Attacks",
            "Explaining and Harnessing Adversarial Examples",
            "Towards Deep Learning Models Resistant to Adversarial Attacks",
            "Baseline Defenses for Adversarial Attacks Against Aligned Language Models",
            "Harmbench: A Standardized Evaluation Framework for Automated Red Teaming and Robust Refusal",
            "Adversarial Attacks and Defenses in Large Language Models: Old and New Threats",
            "Soft Prompt Threats: Attacking Safety Alignment and Unlearning in Open-Source LLMs through the Embedding Space",
            "SMART: Robust and Efficient Fine-Tuning for Pre-Trained Natural Language Models through Principled Regularized Optimization",
            "FreeLB: Enhanced Adversarial Training for Natural Language Understanding",
            "Token-Aware Virtual Adversarial Training in Natural Language Understanding",
            "Improved Text Classification via Contrastive Adversarial Training",
            "Direct Preference Optimization: Your Language Model is Secretly a Reward Model",
            "Enhancing Chat Language Models by Scaling High-Quality Instructional Conversations",
            "Zephyr: Direct Distillation of LM Alignment",
            "The Alignment Handbook",
            "Measuring Massive Multitask Language Understanding",
            "On the Measure of Intelligence",
            "Judging LLM-As-A-Judge with MT-Bench and Chatbot Arena",
            "Gemma: Open Models Based on Gemini Research and Technology",
            "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone",
            "Mistral 7B",
            "Llama 2: Open Foundation and Fine-Tuned Chat Models",
            "LoRA: Low-Rank Adaptation of Large Language Models",
            "A Framework for Few-Shot Language Model Evaluation",
            "Why Should Adversarial Perturbations be Imperceptible? Rethink the Research Paradigm in Adversarial NLP",
            "Theoretically Principled Trade-Off between Robustness and Accuracy",
            "Decoupled Weight Decay Regularization",
            "Fast is Better than Free: Revisiting Adversarial Training",
            "Negative preference optimization: From catastrophic collapse to effective unlearning",
            "Negative preference optimization: From catastrophic collapse to effective unlearning",
            "A General Theoretical Paradigm to Understand Learning from Human Preferences",
            "Generative Adversarial Nets",
            "Identifying Untrustworthy Predictions in Neural Networks by Geometric Gradient Analysis",
            "Raising the Bar for Certified Adversarial Robustness with Diffusion Models",
            "Jailbreaking Black Box Large Language Models in Twenty Queries",
            "AutoDAN: Generating Stealthy Jailbreak Prompts on Aligned Large Language Models",
            "Jailbreaker: Automated Jailbreak Across Multiple Large Language Model Chatbots",
            "AdvPrompter: Fast Adaptive Adversarial Prompting for LLMs",
            "In-Context Learning Can Re-learn Forbidden Tasks",
            "Catastrophic Jailbreak of Open-Source LLMs via Exploiting Generation",
            "Attacking Large Language Models with Projected Gradient Descent",
            "Scaling Laws for Adversarial Attacks on Language Model Activations",
            "Adversarial Training for Large Neural Language Models",
            "DeBERTa: Decoding-Enhanced BERT with Disentangled Attention",
            "Token-Aware Virtual Adversarial Training in Natural Language Understanding",
            "Improved Text Classification via Contrastive Adversarial Training",
            "Direct Preference Optimization: Your Language Model is Secretly a Reward Model",
            "Neural Text Generation with Unlikelihood Training",
            "Enhancing Chat Language Models by Scaling High-Quality Instructional Conversations",
            "Zephyr: Direct Distillation of LM Alignment",
            "Judging LLM-As-A-Judge with MT-Bench and Chatbot Arena",
            "Gemma: Open Models Based on Gemini Research and Technology",
            "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone",
            "Mistral 7B",
            "Llama 2: Open Foundation and Fine-Tuned Chat Models",
            "LoRA: Low-Rank Adaptation of Large Language Models",
            "A Framework for Few-Shot Language Model Evaluation",
            "Why Should Adversarial Perturbations be Imperceptible? Rethink the Research Paradigm in Adversarial NLP",
            "Theoretically Principled Trade-Off between Robustness and Accuracy",
            "Decoupled Weight Decay Regularization",
            "Fast is Better than Free: Revisiting Adversarial Training",
            "Negative preference optimization: From catastrophic collapse to effective unlearning",
            "Negative preference optimization: From catastrophic collapse to effective unlearning",
            "A General Theoretical Paradigm to Understand Learning from Human Preferences"
        ]
    },
    {
        "index": 79,
        "title": "GASP: Efficient Black-Box Generation of Adversarial Suffixes for Jailbreaking LLMs",
        "publication_date": "2024-11-21",
        "references": [
            "Gpt-4 technical report.",
            "Falcon-40B: an open large language model with state-of-the-art performance.",
            "Training a helpful and harmless assistant with reinforcement learning from human feedback.",
            "Training a helpful and harmless assistant with reinforcement learning from human feedback.",
            "How (un)ethical are instruction-centric responses of llms? unveiling the vulnerabilities of safety guardrails to harmful queries.",
            "Optimizing training trajectories in variational autoencoders via latent bayesian optimization approach.",
            "On the opportunities and risks of foundation models.",
            "Language models are few-shot learners.",
            "Jailbreaking black box large language models in twenty queries.",
            "Can llm-generated misinformation be detected?",
            "Agentpoison: Red-teaming llm agents via poisoning memory or knowledge bases.",
            "Black-box prompt optimization: Aligning large language models without model training.",
            "FFT: towards harmlessness evaluation and analysis for llms with factuality, fairness, toxicity.",
            "Masterkey: Automated jailbreaking of large language model chatbots.",
            "Advancing bayesian optimization via learning correlated latent space.",
            "Llama 2: Open foundation and fine-tuned chat models.",
            "The llama 3 herd of models.",
            "Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned.",
            "Bayesian optimization with inequality constraints.",
            "Generative language models and automated influence operations: Emerging threats and potential mitigations.",
            "Token-level adversarial prompt detection based on perplexity measures and contextual information.",
            "Jailbroken: How does llm safety training fail?.",
            "Prefix-tuning: Optimizing continuous prompts for generation.",
            "Navigating the safety landscape: Measuring risks in finetuning large language models.",
            "Amplegcg: Learning a universal and transferable generative model of adversarial suffixes for jailbreaking both open and closed llms.",
            "Autodan-turbo: A lifelong agent for strategy self-exploration to jailbreak llms.",
            "Social risks in the era of generative ai.",
            "Autodan: Generating stealthy jailbreak prompts on aligned large language models.",
            "Formalizing and benchmarking prompt injection attacks and defenses.",
            "Beyond the pipeline: Discrete optimization in NLP.",
            "Joint composite latent space bayesian optimization.",
            "Prompts have evil twins.",
            "Training language models to follow instructions with human feedback.",
            "Advprompter: Fast adaptive adversarial prompting for llms.",
            "Navigating the safety landscape: Measuring risks in finetuning large language models.",
            "Nemo guardrails: A toolkit for controllable and safe llm applications with programmable rails.",
            "Smoothllm: Defending large language models against jailbreaking attacks.",
            "Mathematical discoveries from program search with large language models.",
            "Survey of vulnerabilities in large language models revealed by adversarial attacks.",
            "Characterizing and evaluating in-the-wild jailbreak prompts on large language models.",
            "Autoprompt: Eliciting knowledge from language models with automatically generated prompts.",
            "Interpreting embedding spaces by conceptualization.",
            "A strongreject for empty jailbreaks.",
            "Demystifying embedding spaces using large language models.",
            "Visualizing data using t-sne.",
            "Aligning large language models with human: A survey.",
            "Functional homotopy: Smoothing discrete optimization via continuous parameters for llm jailbreak attacks.",
            "Jailbroken: How does llm safety training fail?.",
            "Jailbreak and guard aligned language models with only few in-context demonstrations.",
            "Unveiling the implicit toxicity in large language models."
        ]
    },
    {
        "index": 80,
        "title": "Efficient Detection of Toxic Prompts in Large Language Models",
        "publication_date": "2024-11-01",
        "references": [
            "About the api score",
            "allenai/real-toxicity-prompts ·datasets at hugging face",
            "cdn.openai.com/papers/gpt-4-system-card.pdf",
            "Fact sheet: Biden-harris administration secures voluntary commitments from leading artificial intelligence companies to manage the risks posed by ai",
            "Meta llama 3",
            "Number of chatgpt users (aug 2024)",
            "Ryokoai/sharegpt52k datasets at hugging face",
            "Toxicdetector",
            "Usage policies | openai",
            "Gpt-4 technical report",
            "Run a chatgpt-like chatbot on a single gpu with rocm",
            "Safety-tuned llamas: Lessons from improving the safety of large language models that follow instructions",
            "Directed greybox fuzzing",
            "Coverage-based greybox fuzzing as markov chain",
            "Play guessing game with llm: Indirect jailbreak attack with implicit clues",
            "Jailbreaker in jail: Moving target defense for large language models",
            "Bge m3-embedding: Multi-lingual, multi-functionality, multi-granularity text embeddings through self-knowledge distillation",
            "Masterkey: Automated jailbreaking of large language model chatbots.",
            "Pandora: Jailbreak gpts by retrieval augmented generation poisoning",
            "Realtoxicityprompts: Evaluating neural toxic degeneration in language models",
            "From chatgpt to threatgpt: Impact of generative ai in cybersecurity and privacy",
            "The platonic representation hypothesis",
            "Baseline defenses for adversarial attacks against aligned language models",
            "Transformers are short text classifiers: A study of inductive short text classifiers on benchmarks and real-world datasets",
            "Multi-layer perceptrons",
            "Watch your language: Investigating content moderation with large language models",
            "A New Generation of Perspective API: Efficient Multilingual Character-level Transformers",
            "Self and cross-model distillation for llms: Effective methods for refusal pattern alignment",
            "Understanding neural networks through representation erasure",
            "Lockpicking llms: A logit-based jailbreak using token-level manipulation",
            "Prompt injection attack against llm-integrated applications",
            "Jailbreaking chatgpt via prompt engineering: An empirical study",
            "Groot: Adversarial testing for generative text-to-image models with tree-based semantic transformation",
            "A holistic approach to undesired content detection",
            "Harmbench: A standardized evaluation framework for automated red teaming and robust refusal",
            "Umap: Uniform manifold approximation and projection for dimension reduction",
            "Mann-whitney u test",
            "llama-13b",
            "llama2-13b",
            "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "Why so toxic? measuring and triggering toxic behavior in open-domain chatbots",
            "A strongreject for empty jailbreaks",
            "Gemini: a family of highly capable multimodal models",
            "Gemma 2: Improving open language models at a practical size",
            "vicuna-13b",
            "Decodingtrust: A comprehensive assessment of trustworthiness in gpt models",
            "A comprehensive study of jailbreak attack versus defense for large language models",
            "Autodefense: Multi-agent llm defense against jailbreak attacks",
            "Efficient toxic content detection by bootstrapping and distilling large language models",
            "Defending jailbreak prompts via in-context adversarial game",
            "Red teaming chatgpt via jail-breaking: Bias, robustness, reliability and toxicity",
            "Representation engineering: A top-down approach to ai transparency",
            "Universal and transferable adversarial attacks on aligned language models"
        ]
    },
    {
        "index": 81,
        "title": "Embedding-based classifiers can detect prompt injection attacks",
        "publication_date": "2024-10-24",
        "references": [
            "The science of detecting llm-generated texts",
            "Is your code generated by chatgpt really correct? rigorous evaluation of large language models for code generation",
            "Booookscore: A systematic exploration of book-length summarization in the era of llms",
            "Enhancing financial sentiment analysis via retrieval augmented large language models",
            "Chatgpt and large language model (llm) chatbots: The current state of acceptability and a proposal for guidelines on utilization in academic medicine",
            "Exploring human-like translation strategy with large language models",
            "Exploring ai ethics of chatgpt: A diagnostic analysis",
            "Holistic evaluation of language models",
            "Training a helpful and harmless assistant with reinforcement learning from human feedback",
            "Sqlrand: Preventing sql injection attacks",
            "Cross-site scripting (xss) attacks and defense mechanisms: classification and state-of-the-art",
            "Ignore previous prompt: Attack techniques for language models",
            "Assessing prompt injection risks in 200+ custom gpts",
            "Prompt injection attack against llm-integrated applications",
            "Not what you’ve signed up for: Compromising real-world llm-integrated applications with indirect prompt injection",
            "“real attackers don’t compute gradients”: bridging the gap between adversarial ml research and practice",
            "Text and code embeddings by contrastive pre-training",
            "Linguistic knowledge and transferability of contextual representations",
            "Impact of word embedding models on text analytics in deep learning environment: a review",
            "Exploiting programmatic behavior of llms: Dual-use through standard security attacks",
            "“ do anything now”: Characterizing and evaluating in-the-wild jailbreak prompts on large language models",
            "Jailbroken: How does llm safety training fail?",
            "Rainbow teaming: Open-ended generation of diverse adversarial prompts",
            "Special characters attack: Toward scalable training data extraction from large language models",
            "Detecting language model attacks with perplexity",
            "Baseline defenses for adversarial attacks against aligned language models",
            "Struq: Defending against prompt injection with structured queries",
            "Benchmarking and defending against indirect prompt injection attacks on large language models",
            "Ignore this title and hackaprompt: Exposing systemic vulnerabilities of llms through a global prompt hacking competition",
            "Gpt-4 technical report",
            "Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned",
            "Eliciting latent predictions from transformers with the tuned lens",
            "Using gpt-eliezer against chatgpt jailbreaking",
            "Fine-tuned deberta-v3-base for prompt injection detection",
            "Model card - prompt guard",
            "Principal component analysis",
            "Visualizing data using t-sne.",
            "Umap: Uniform manifold approximation and projection for dimension reduction",
            "On the early history of the singular value decomposition",
            "Scikit-learn: Machine learning in Python",
            "How to use t-sne effectively",
            "Categories for the working mathematician",
            "Minimizing finite sums with the stochastic average gradient",
            "Xgboost: A scalable tree boosting system",
            "Random forests",
            "Efficient few-shot learning without prompts",
            "Deberta: Decoding-enhanced bert with disentangled attention"
        ]
    },
    {
        "index": 82,
        "title": "Emerging Safety Attack and Defense in Federated Instruction Tuning of Large Language Models",
        "publication_date": "2024-06-15",
        "references": [
            "Training language models to follow instructions with human feedback.",
            "Llama 2: Open foundation and fine-tuned chat models.",
            "Mistral 7b.",
            "Gpt-4 technical report.",
            "How far can camels go? exploring the state of instruction tuning on open resources.",
            "Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality.",
            "Wizardlm: Empowering large language models to follow complex instructions.",
            "Openchat: Advancing open-source language models with mixed-quality data.",
            "Camels in a changing climate: Enhancing lm adaptation with tulu 2, 2023.",
            "Bloomberggpt: A large language model for finance.",
            "Retrieval-augmented generation for knowledge-intensive nlp tasks.",
            "Lima: Less is more for alignment.",
            "Beavertails: Towards improved safety alignment of llm via a human-preference dataset.",
            "Communication-efficient learning of deep networks from decentralized data.",
            "Advances and open problems in federated learning.",
            "Openfedllm: Training large language models on decentralized private data via federated learning.",
            "Fedllm-bench: Realistic benchmarks for federated learning of large language models.",
            "Towards building the federated gpt: Federated instruction tuning.",
            "Fedpit: Towards privacy-preserving and few-shot federated instruction tuning.",
            "Federated learning on large language models (llms).",
            "Fate-llm: A industrial grade federated learning framework for large language models.",
            "Federatedscope-llm: A comprehensive package for fine-tuning large language models in federated learning.",
            "Empowering federated learning for massive models with nvidia flare.",
            "Byzantine-robust distributed learning: Towards optimal statistical rates.",
            "Machine learning with adversaries: Byzantine tolerant gradient descent.",
            "Manipulating the byzantine: Optimizing model poisoning attacks and defenses for federated learning.",
            "Mitigating sybils in federated learning poisoning.",
            "Finetuned language models are zero-shot learners.",
            "The flan collection: Designing data and methods for effective instruction tuning.",
            "Introducing the world’s first truly open instruction-tuned llm.",
            "Supernaturalinstructions: Generalization via declarative instructions on 1600+ nlp tasks.",
            "Openassistant conversations-democratizing large language model alignment.",
            "Instruction tuning with gpt-4.",
            "Wildchat: 1m chatGPT interaction logs in the wild.",
            "LMSYS-chat-1m: A large-scale real-world LLM conversation dataset.",
            "Managing ai risks in an era of rapid progress.",
            "Foundational challenges in assuring alignment and safety of large language models.",
            "Trustllm: Trustworthiness in large language models.",
            "Privacy and robustness in federated learning: Attacks and defenses.",
            "Manipulating machine learning: Poisoning attacks and countermeasures for regression learning.",
            "Poisoning attacks against support vector machines.",
            "Rflpa: A robust federated learning framework against poisoning attacks with secure aggregation.",
            "Data poisoning attacks against federated learning systems.",
            "Data poisoning attacks on federated machine learning.",
            "Analyzing federated learning through an adversarial lens.",
            "A little is enough: Circumventing defenses for distributed learning.",
            "Subpopulation data poisoning attacks.",
            "Local model poisoning attacks to {Byzantine-Robust }federated learning.",
            "Mpaf: Model poisoning attacks to federated learning based on fake clients.",
            "Poisonedfl: Model poisoning attacks to federated learning via multi-round consistency.",
            "Adversarial label flips attack on support vector machines.",
            "Detection and mitigation of label-flipping attacks in federated learning systems with kpca and k-means.",
            "Exploring representational similarity analysis to protect federated learning from data poisoning.",
            "Fine-tuning aligned language models compromises safety, even when users do not intend to!",
            "Open-source can be dangerous: On the vulnerability of value alignment in open-source LLMs.",
            "Towards attack-tolerant federated learning via critical parameter analysis.",
            "Personalized federated learning with inferred collaboration graphs.",
            "Feddefender: Client-side attack-tolerant federated learning.",
            "Attack-resistant federated learning with residual-based reweighting.",
            "No fear of classifier biases: Neural collapse inspired federated learning with synthetic and fixed classifier.",
            "Federated learning with bilateral curation for partially class-disjoint data.",
            "Eliminating domain bias for federated learning in representation space.",
            "Training a helpful and harmless assistant with reinforcement learning from human feedback.",
            "Principle-driven self-alignment of language models from scratch with minimal human supervision.",
            "Lora: Low-rank adaptation of large language models.",
            "Decoupled weight decay regularization.",
            "Universal and transferable adversarial attacks on aligned language models.",
            "Catastrophic jailbreak of open-source llms via exploiting generation.",
            "Salad-bench: A hierarchical and comprehensive safety benchmark for large language models.",
            "Judging llm-as-a-judge with mt-bench and chatbot arena.",
            "Zephyr: Direct distillation of lm alignment.",
            "Wizard-vicuna-30b-uncensored.",
            "Code alpaca: An instruction-following llama model for code generation.",
            "Evaluating large language models trained on code."
        ]
    },
    {
        "index": 84,
        "title": "Empirical Analysis of Large Vision-Language Models against Goal Hijacking via Visual Prompt Injection",
        "publication_date": "2024-08-07",
        "references": [
            "Defense-prefix for preventing typographic attacks on CLIP",
            "Language models are few-shot learners",
            "Instructblip: Towards general-purpose vision-language models with instruction tuning",
            "A challenger to gpt-4v? early explorations of gemini in visual expertise",
            "Multimodal neurons in artificial neural networks",
            "Figstep: Jailbreaking large vision-language models via typographic visual prompts",
            "Gemini: A family of highly capable multimodal models",
            "BLIP-2: bootstrapping language-image pre-training with frozen image encoders and large language models",
            "Mitigating hallucination in large multi-modal models via robust instruction tuning",
            "Improved baselines with visual instruction tuning",
            "Query-relevant images jailbreak large multi-modal models",
            "VIM: probing multimodal large language models for visual embedded instruction following",
            "OCR-VQA: visual question answering by reading text in images",
            "GPT-4 technical report",
            "Ignore previous prompt: Attack techniques for language models",
            "Learning transferable visual models from natural language supervision",
            "Survey of vulnerabilities in large language models revealed by adversarial attacks",
            "The beginner’s guide to visual prompt injections: Invisibility cloaks, cannibalistic adverts, and robot women",
            "Multi-modal prompt injection image attacks against gpt-4v",
            "The dawn of lmms: Preliminary explorations with gpt-4v(ision)"
        ]
    },
    {
        "index": 85,
        "title": "The Task Shield: Enforcing Task Alignment to Defend Against Indirect Prompt Injection in LLM Agents",
        "publication_date": "2024-12-21",
        "references": [
            "Language models are few-shot learners",
            "Struq: Defending against prompt injection with structured queries",
            "Agentdojo: A dynamic environment to evaluate attacks and defenses for llm agents",
            "Mind2web: Towards a generalist agent for the web",
            "A security risk taxonomy for prompt-based interaction with large language models",
            "A survey on rag meeting llms: Towards retrieval-augmented large language models",
            "Misusing tools in large language models with visual adversarial examples",
            "Not what you’ve signed up for: Compromising real-world llm-integrated applications with indirect prompt injection",
            "Toolkengpt: Augmenting frozen language models with massive tools via tool embeddings",
            "Defending against indirect prompt injection attacks with spotlighting",
            "Defending against indirect prompt injection attacks with spotlighting",
            "Recommender ai agent: Integrating large language models for interactive recommendations",
            "Palisade–prompt injection detection framework",
            "Securing tool use in llm agents: Challenges and strategies",
            "Prompt injection attack against llm-integrated applications",
            "Formalizing and benchmarking prompt injection attacks and defenses",
            "A language agent for autonomous driving",
            "Augmented language models: a survey",
            "Training language models to follow instructions with human feedback",
            "Gorilla: Large language model connected with massive apis",
            "Ignore previous prompt: Attack techniques for language models",
            "Jatmo: Prompt injection defense by task-specific finetuning",
            "Sandwich defense",
            "Toolllm: Facilitating large language models to master 16000+ real-world apis",
            "Toolformer: Language models can teach themselves to use tools",
            "Investigating agency of LLMs in human-AI collaboration tasks",
            "Toolalpaca: Generalized tool learning for language models with 3000 simulated cases",
            "Llama 2: Open foundation and fine-tuned chat models",
            "The instruction hierarchy: Training llms to prioritize privileged instructions",
            "Fath: Authentication-based test-time defense against indirect prompt injection attacks",
            "Guardagent: Safeguard llm agents by a guard agent via knowledge-enabled reasoning",
            "React: Synergizing reasoning and acting in language models",
            "Injecagent: Benchmarking indirect prompt injections in tool-integrated large language model agents",
            "Universal and transferable adversarial attacks on aligned language models"
        ]
    },
    {
        "index": 86,
        "title": "Ensemble Jailbreak on Large Language Models",
        "publication_date": "2024-08-07",
        "references": [
            "Introducing chatgpt",
            "Gpt-4 technical report",
            "Llama 2: Open foundation and fine-tuned chat models",
            "Introducing claude",
            "Code llama: Open foundation models for code",
            "Revolutionizing finance with llms: An overview of applications and insights",
            "Large language models in medicine",
            "Chatgpt for good? on opportunities and challenges of large language models for education",
            "Constitutional ai: Harmlessness from ai feedback",
            "Pretraining language models with human preferences",
            "Training language models to follow instructions with human feedback",
            "Automatically auditing large language models via discrete optimization",
            "Improving alignment of dialogue agents via targeted human judgements",
            "Self-instruct: Aligning language model with self generated instructions",
            "Autodan: Automatic and interpretable adversarial attacks on large language models",
            "Universal and transferable adversarial attacks on aligned language models",
            "Chat gpt ”dan” (and other ”jailbreaks”)",
            "Gptfuzzer: Red teaming large language models with auto-generated jailbreak prompts",
            "A wolf in sheep’s clothing: Generalized nested jailbreak prompts can fool large language models easily",
            "Jailbreaking black box large language models in twenty queries",
            "Language models are unsupervised multitask learners",
            "Safe rlhf: Safe reinforcement learning from human feedback",
            "Scalable extraction of training data from (production) language models",
            "Jailbroken: How does llm safety training fail?",
            "Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned",
            "Autoprompt: Eliciting knowledge from language models with automatically generated prompts",
            "Smoothllm: Defending large language models against jailbreaking attacks",
            "Baseline defenses for adversarial attacks against aligned language models",
            "Judging llm-as-a-judge with mt-bench and chatbot arena",
            "Easyjailbreak: A unified framework for jailbreaking large language models"
        ]
    },
    {
        "index": 87,
        "title": "Eraser: Jailbreaking Defense in Large Language Models via Unlearning Harmful Knowledge",
        "publication_date": "2024-07-03",
        "references": [
            "Gpt-4 technical report",
            "Palm 2 technical report",
            "Qwen technical report",
            "Machine unlearning",
            "Defending against alignment-breaking attacks via robustly aligned llm",
            "Jailbreaking black box large language models in twenty queries",
            "Jailbreaker in jail: Moving target defense for large language models",
            "Unlearn what you want to forget: Efficient unlearning for LLMs",
            "Boolq: Exploring the surprising difficulty of natural yes/no questions",
            "Think you have solved question answering? try arc, the ai2 reasoning challenge",
            "The commitmentbank: Investigating projection in naturally occurring discourse",
            "Attack prompt generation for red teaming and defending large language models",
            "Toxicity in chatgpt: Analyzing persona-assigned language models",
            "Who’s harry potter? approximate unlearning in llms",
            "Llm self defense: By self examination, llms know they are being tricked",
            "Measuring massive multitask language understanding",
            "Knowledge unlearning for mitigating privacy risks in language models",
            "Certifying llm safety against adversarial prompting",
            "Smoothllm: Defending large language models against jailbreaking attacks",
            "Choice of plausible alternatives: An evaluation of commonsense causal reasoning",
            "Stanford alpaca: An instruction-following llama model",
            "Llama 2: Open foundation and fine-tuned chat models",
            "Self-guard: Empower the llm to safeguard itself",
            "Baichuan 2: Open large-scale language models",
            "Large language model unlearning",
            "Hellaswag: Can a machine really finish your sentence?",
            "Defending large language models against jail-breaking attacks through goal prioritization",
            "Making harmful behaviors unlearnable for large language models",
            "Universal and transferable adversarial attacks on aligned language models"
        ]
    },
    {
        "index": 88,
        "title": "Evaluating Then Aligning Safety of Vision-Language Models at Inference Time",
        "publication_date": "2025-02-10",
        "references": [
            "Gpt-4 technical report.",
            "Jailbreaking leading safety-aligned llms with simple adaptive attacks.",
            "Learning transferable visual models from natural language supervision.",
            "Generative verifiers: Reward modeling as next-token prediction.",
            "Improved baselines with visual instruction tuning.",
            "Visual instruction tuning.",
            "Training language models to follow instructions with human feedback.",
            "Universal and transferable adversarial attacks on aligned language models.",
            "Large language monkeys: Scaling inference compute with repeated sampling.",
            "Self-evaluation as a defense against adversarial attacks on llms.",
            "Adashield: Safeguarding multi-modal large language models from structure-based attack via adaptive shield prompting.",
            "Eyes closed, safety on: Protecting multimodal llms via image-to-text transformation.",
            "Safety alignment should be made more than just a few tokens deep.",
            "Large language models.",
            "Making the V in VQA matter: Elevating the role of image understanding in Visual Question Answering.",
            "Query-relevant images jailbreak large multi-modal models.",
            "How many unicorns are in this image? a safety evaluation benchmark for vision llms.",
            "Mllm-protector: Ensuring mllm’s safety without hurting performance.",
            "Harmbench: A standardized evaluation framework for automated red teaming and robust refusal.",
            "Red teaming visual language models.",
            "Microsoft coco: Common objects in context.",
            "Towards vqa models that can read.",
            "A comprehensive safety preference alignment dataset for vision language model."
        ]
    },
    {
        "index": 89,
        "title": "Event-Radar: Event-driven Multi-View Learning for Multimodal Fake News Detection",
        "publication_date": "2024-08-11",
        "references": [
            "Verifying multimedia use at mediaeval 2015. In MediaEval 2015",
            "Exploring the role of visual content in fake news detection",
            "MMDetection: Open mm-lab detection toolbox and benchmark",
            "Cross-modal ambiguity learning for multimodal fake news detection",
            "Causal intervention and counterfactual reasoning for multi-modal fake news detection",
            "Noisy correspondence learning with self-reinforcing errors mitigation",
            "Disentangled representation learning with transmitted information bottleneck",
            "Fast graph representation learning with pytorch geometric",
            "Pizzagate: From rumor, to hashtag, to gunfire in dc",
            "Trusted multi-view classification",
            "Subjective Logic: A formalism for reasoning under uncertainty",
            "Mvae: Multimodal variational autoencoder for fake news detection",
            "Efficient large-scale multi-modal classification",
            "Semi-supervised classification with graph convolutional networks",
            "Rumor detection with field of linear and non-linear propagation",
            "Bic: Twitter bot detection with text-graph interaction and semantic consistency",
            "Clip-event: Connecting text and images with event structures",
            "Interpretable multimodal misinformation detection with logic reasoning",
            "Texsmart: A system for enhanced natural language understanding",
            "Image and video processing techniques in the dct domain",
            "The Stanford CoreNLP natural language processing toolkit",
            "The covid-19 'infodemic': a new front for information professionals",
            "Automatic differentiation in pytorch",
            "Scikit-learn: Machine learning in python",
            "Exploiting multi-domain visual information for fake news detection",
            "Learning transferable visual models from natural language supervision",
            "Outrageously large neural networks: The sparsely-gated mixture-of-experts layer",
            "Improved semantic-aware network embedding with fine-grained word alignment",
            "Integrating pattern- and fact-based fake news detection via model preference learning",
            "Attention is all you need",
            "Generating reactions and explanations for llm-based misinformation detection",
            "Mitigating idiom inconsistency: A multi-semantic contrastive learning method for chinese idiom reading comprehension",
            "Characterizing and overcoming the greedy nature of learning in multi-modal deep neural networks",
            "Multimodal fusion with co-attention networks for fake news detection",
            "Chinese clip: Contrastive vision-language pretraining in chinese",
            "Bootstrapping multi-view representations for fake news detection",
            "An explainable multi-view semantic fusion model for multimodal fake news detection"
        ]
    },
    {
        "index": 90,
        "title": "Evolver: Chain-of-Evolution Prompting to Boost Large Multimodal Models for Hateful Meme Detection",
        "publication_date": "2025-01-21",
        "references": [
            "Multimodal hate speech detection in memes using contrastive language-image pre-training",
            "Openflamingo: An open-source framework for training large autoregressive vision-language models",
            "Deephate: Hate speech detection via multi-faceted text representations",
            "SemEval-2022 task 5: Multimedia automatic misogyny identification",
            "The hateful memes challenge: Detecting hate speech in multimodal memes",
            "Hate-clipper: Multimodal hateful meme classification based on cross-modal interaction of clip features",
            "BERT: Pre-training of deep bidirectional transformers for language understanding",
            "An image is worth 16x16 words: Transformers for image recognition at scale",
            "Improved baselines with visual instruction tuning",
            "Visual instruction tuning",
            "GPT-4v(ision) as a social media analysis engine",
            "The dawn of LMMs: Preliminary explorations with GPT-4v(ision)",
            "Mmicl: Empowering vision-language model with multi-modal in-context learning",
            "MiniGPT-4: Enhancing vision-language understanding with advanced large language models",
            "Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models",
            "Instructblip: Towards general-purpose vision-language models with instruction tuning",
            "Learning transferable visual models from natural language supervision",
            "Gemini: a family of highly capable multimodal models",
            "Llama: Open and efficient foundation language models"
        ]
    },
    {
        "index": 91,
        "title": "FlexLLM: Exploring LLM Customization for Moving Target Defense on Black-Box LLMs Against Jailbreak Attacks",
        "publication_date": "2024-12-10",
        "references": [
            "Falcon-40B: an open large language model with state-of-the-art performance.",
            "Detecting language model attacks with perplexity.",
            "Morphence: Moving target defense against adversarial examples.",
            "Training a helpful and harmless assistant with reinforcement learning from human feedback.",
            "Defending against alignment-breaking attacks via robustly aligned llm.",
            "Jailbreaking black box large language models in twenty queries.",
            "Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality.",
            "Qlora: Efficient finetuning of quantized llms.",
            "Stochastic activation pruning for robust adversarial defense.",
            "On randomization in mtd systems.",
            "A research agenda: Dynamic models to defend against correlated attacks.",
            "Dolphin-llama2-7b.",
            "Catastrophic jailbreak of open-source llms via exploiting generation.",
            "Baseline defenses for adversarial attacks against aligned language models.",
            "Hardware moving target defenses against physical attacks: Design challenges and opportunities.",
            "Llm guard - the security toolkit for llm interactions.",
            "Sok: Certified robustness for deep neural networks.",
            "Deepinception: Hypnotize large language model to be jailbreaker.",
            "Autodan: Generating stealthy jailbreak prompts on aligned large language models.",
            "Smoothllm: Defending large language models against jailbreaking attacks.",
            "Mtdeep: boosting the security of deep neural nets against adversarial attacks with moving target defense.",
            "Improving the robustness of transformer-based large language models with dynamic attention.",
            "Moving target defense for embedded deep visual sensing against adversarial examples.",
            "Llama 2: Open foundation and fine-tuned chat models.",
            "Jailbroken: How does llm safety training fail?",
            "Jailbreak and guard aligned language models with only few in-context demonstrations.",
            "Defending chatgpt against jailbreak attack via self-reminder.",
            "Defending chatgpt against jailbreak attack via self-reminders.",
            "Safedecoding: Defending against jailbreak attacks via safety-aware decoding.",
            "Gptfuzzer: Red teaming large language models with auto-generated jailbreak prompts.",
            "Defending large language models against jailbreaking attacks through goal prioritization.",
            "Universal and transferable adversarial attacks on aligned language models."
        ]
    },
    {
        "index": 92,
        "title": "F2A: An Innovative Approach for Prompt Injection by Utilizing Feign Security Detection Agents",
        "publication_date": "2024-10-14",
        "references": [
            "LLM-Based Edge Intelligence: A Comprehensive Survey on Architectures, Applications, Security and Trustworthiness",
            "Breaking Down the Defenses: A Comparative Survey of Attacks on Large Language Models",
            "Not What You’ve Signed Up For: Compromising Real-World LLM-Integrated Applications with Indirect Prompt Injection",
            "Defending Against Indirect Prompt Injection Attacks With Spotlighting",
            "CheatAgent: Attacking LLM-Empowered Recommender Systems via LLM Agent",
            "Prompt-CARE: Prompt Copyright Protection by Watermark Injection and Verification",
            "Attack Prompt Generation for Red Teaming and Defending Large Language Models",
            "ShieldLM: Empowering LLMs as Aligned, Customizable and Explainable Safety Detectors",
            "SafetyBench: Evaluating the Safety of Large Language Models",
            "Defending Language Models Against Image-Based Prompt Attacks via User-Provided Specifications",
            "Hello GPT-4o |OpenAI"
        ]
    },
    {
        "index": 93,
        "title": "Failures to Find Transferable Image Jailbreaks Between Vision-Language Models",
        "publication_date": "2024-12-16",
        "references": [
            "Phi-3 technical report: A highly capable language model locally on your phone, 2024.",
            "Gpt-4 technical report. arXiv preprint arXiv:2303.08774 , 2023.",
            "Many-shot jailbreaking. In The Thirty-eighth Annual Conference on Neural Information Processing Systems , 2024.",
            "Claude 2. https://www.anthropic.com/news/claude-2 , 2023.",
            "Model card and evaluations for claude models, 2023.",
            "Refusal in language models is mediated by a single direction, 2024.",
            "Openflamingo: An open-source framework for training large autoregressive vision-language models, 2023.",
            "Abusing images and sounds for indirect instruction injection in multi-modal llms, 2023.",
            "Qwen technical report, 2023.",
            "Qwen-vl: A versatile vision-language model for understanding, localization, text reading, and beyond, 2023.",
            "Image hijacks: Adversarial images can control generative models at runtime. arXiv preprint arXiv:2309.00236 , 2023.",
            "Understanding jailbreak success: A study of latent space dynamics in large language models, 2024.",
            "Are aligned neural networks adversarially aligned? Advances in Neural Information Processing Systems , 36, 2024.",
            "Defending against unforeseen failure modes with latent adversarial training, 2024.",
            "Universal adversarial perturbations: A survey, 2020.",
            "Rethinking model ensemble in transfer-based adversarial attacks. In The Twelfth International Conference on Learning Representations , 2024.",
            "Red teaming gpt-4v: Are gpt-4v safe against uni/multi-modal jailbreak attacks? arXiv preprint arXiv:2404.03411 , 2024.",
            "Pali-3 vision language models: Smaller, faster, stronger, 2023.",
            "Reproducible scaling laws for contrastive language-image learning, 2022.",
            "Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023.",
            "Scaling instruction-finetuned language models, 2022.",
            "Instructblip: Towards general-purpose vision-language models with instruction tuning, 2023.",
            "Boosting adversarial attacks with momentum. In Proceedings of the IEEE conference on computer vision and pattern recognition , pages 9185–9193, 2018.",
            "How robust is google’s bard to adversarial image attacks? arXiv preprint arXiv:2309.11751 , 2023.",
            "An image is worth 16x16 words: Transformers for image recognition at scale, 2021.",
            "Adversarial evaluation of multimodal models under realistic gray box assumption, 2021.",
            "Unbridled icarus: A survey of the potential perils of image inputs in multimodal large language model security, 2024.",
            "Exploring the limits of masked visual representation learning at scale, 2022.",
            "Multi-attacks: Many images +the same adversarial attack →many target labels, 2023.",
            "Misusing tools in large language models with visual adversarial examples. arXiv preprint arXiv:2310.03185 , 2023.",
            "Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned, 2022.",
            "Adversarial robustness for visual grounding of multimodal large language models, 2024.",
            "Llama-adapter v2: Parameter-efficient visual instruction model, 2023.",
            "Multimodal neurons in artificial neural networks. Distill , 6(3):e30, 2021.",
            "Jailbreaking large vision-language models via typographic visual prompts, 2023.",
            "Explaining and harnessing adversarial examples, 2015.",
            "Try bard and share your feedback. https://blog.google/technology/ai/ try-bard/ , 2023.",
            "Agent smith: A single image can jailbreak one million multimodal llm agents exponentially fast, 2024.",
            "Llava-gemma: Accelerating multimodal foundation models with a compact language model, 2024.",
            "Feature space perturbations yield more transferable adversarial examples. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 7066–7074, 2019.",
            "What makes and breaks safety fine-tuning? mechanistic study, 2024.",
            "Mistral 7b, 2023.",
            "Exploiting programmatic behavior of llms: Dual-use through standard security attacks. arXiv preprint arXiv:2302.05733 , 2023.",
            "Brave: Broadening the visual encoding of vision-language models, 2024.",
            "Prismatic vlms: Investigating the design space of visually-conditioned language models, 2024.",
            "Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of NAACL-HLT , pages 4171–4186, 2019.",
            "Adam: A method for stochastic optimization, 2017.",
            "Analyzing and editing inner mechanisms of backdoored language models. In The 2024 ACM Conference on Fairness, Accountability, and Transparency , pages 2362–2373, 2024.",
            "A mechanistic understanding of alignment algorithms: A case study on dpo and toxicity, 2024.",
            "Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. In International conference on machine learning , pages 12888–12900. PMLR, 2022.",
            "Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models, 2023.",
            "Textbooks are all you need ii: phi-1.5 technical report, 2023.",
            "Images are achilles’ heel of alignment: Exploiting visual vulnerabilities for jailbreaking multimodal large language models, 2024.",
            "Mini-gemini: Mining the potential of multi-modality vision language models, 2024.",
            "A technique for the measurement of attitudes. Archives of psychology , 1932.",
            "Moe-llava: Mixture of experts for large vision-language models, 2024.",
            "On pre-training for visual language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 26689–26699, 2024.",
            "Improved baselines with visual instruction tuning, 2023.",
            "Visual instruction tuning, 2023.",
            "Mm-safetybench: A benchmark for safety evaluation of multimodal large language models, 2024.",
            "Safety of multimodal large language models on images and text, 2024.",
            "Delving into transferable adversarial examples and black-box attacks. arXiv preprint arXiv:1611.02770 , 2016.",
            "Test-time backdoor attacks on multimodal large language models, 2024.",
            "Deepseek-vl: Towards real-world vision-language understanding, 2024.",
            "An image is worth 1000 lies: Adversarial transferability across prompts on vision-language models, 2024.",
            "Harmbench: A standardized evaluation framework for automated red teaming and robust refusal, 2024.",
            "Mm1: Methods, analysis & insights from multimodal llm pre-training, 2024.",
            "Universal adversarial triggers are not universal, 2024.",
            "Llama 3. 2024.",
            "Universal adversarial perturbations. In Proceedings of the IEEE conference on computer vision and pattern recognition , pages 1765–1773, 2017.",
            "Jailbreaking attack against multimodal large language model, 2024.",
            "Reading isn’t believing: Adversarial attacks on multi-modal neurons. arXiv preprint arXiv:2103.10480 , 2021.",
            "Gpt-4v(ision) system card. https://openai.com/index/ gpt-4v-system-card/ , 2023.",
            "Dinov2: Learning robust visual features without supervision, 2024.",
            "Gemini: a family of highly capable multimodal models. arXiv preprint arXiv:2312.11805 , 2023.",
            "Gemma: Open models based on gemini research and technology. arXiv preprint arXiv:2403.08295 , 2024.",
            "Llama: Open and efficient foundation language models, 2023."
        ]
    },
    {
        "index": 94,
        "title": "Faster-GCG: Efficient Discrete Optimization Jailbreak Attacks against Aligned Large Language Models",
        "publication_date": "2024-10-20",
        "references": [
            "Gpt-4 technical report",
            "Towards evaluating the robustness of neural networks",
            "Jailbreakbench: An open robustness benchmark for jailbreaking large language models",
            "Jailbreaking black box large language models in twenty queries",
            "Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality",
            "Masterkey: Automated jailbreaking of large language model chatbots",
            "A survey of adversarial defenses and robustness in NLP",
            "Baseline defenses for adversarial attacks against aligned language models",
            "Improved techniques for optimization-based jailbreaking on large language models",
            "Exploiting programmatic behavior of llms: Dual-use through standard security attacks",
            "Amplegcg: Learning a universal and transferable generative model of adversarial suffixes for jailbreaking both open and closed llms",
            "Towards deep learning models resistant to adversarial attacks",
            "Harmbench: A standardized evaluation framework for automated red teaming and robust refusal",
            "Tdc 2023 (llm edition): The trojan detection challenge",
            "Tree of attacks: Jailbreaking black-box llms automatically",
            "Training language models to follow instructions with human feedback",
            "Smoothllm: Defending large language models against jailbreaking attacks",
            "Autoprompt: Eliciting knowledge from language models with automatically generated prompts",
            "Llama 2: Open foundation and fine-tuned chat models",
            "Jailbroken: How does LLM safety training fail?",
            "Jailbreak and guard aligned language models with only few in-context demonstrations",
            "Mma-diffusion: Multimodal attack on diffusion models",
            "Gpt-4 is too smart to be safe: Stealthychat with llms via cipher",
            "Accelerating greedy coordinate gradient via probe sampling",
            "On prompt-driven safeguarding for large language models",
            "Autodan: Automatic and interpretable adversarial attacks on large language models",
            "Red teaming chatgpt via jailbreaking: Bias, robustness, reliability and toxicity",
            "Universal and transferable adversarial attacks on aligned language models"
        ]
    },
    {
        "index": 95,
        "title": "FATH: Authentication-based Test-time Defense against Indirect Prompt Injection Attacks",
        "publication_date": "2024-11-25",
        "references": [
            "Llama 3 model card",
            "Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples",
            "Keying hash functions for message authentication",
            "Improving image generation with better captions",
            "Language models are few-shot learners",
            "Struq: Defending against prompt injection with structured queries",
            "Mind2web: Towards a generalist agent for the web",
            "AutoGPT",
            "Not what you’ve signed up for: Compromising real-world llm-integrated applications with indirect prompt injection",
            "Defending against indirect prompt injection attacks with spotlighting",
            "HMAC: Keyed-Hashing for Message Authentication",
            "LangChain",
            "Automatic and universal prompt injection attacks against large language models",
            "Prompt injection attack against llm-integrated applications",
            "Prompt injection attacks and defenses in llm-integrated applications",
            "New Bing",
            "GPT-3.5 Turbo",
            "Introducing GPTs",
            "OWASP Top 10 for LLM Applications",
            "Gorilla: Large language model connected with massive apis",
            "Ignore previous prompt: Attack techniques for language models",
            "Toolllm: Facilitating large language models to master 16000+ real-world apis",
            "Sentence-BERT: Sentence embeddings using Siamese BERT-networks",
            "Toolformer: Language models can teach themselves to use tools",
            "Stanford Alpaca: An instruction-following llama model",
            "Tensor Trust: Interpretable prompt injection attacks from an online game",
            "Chain-of-thought prompting elicits reasoning in large language models",
            "Delimiters won't save you from prompt injection",
            "Prompt injection: What’s the worst that can happen?",
            "Wipi: A new web threat for llm-driven web agents",
            "A new era in llm security: Exploring security concerns in real-world llm-based systems",
            "React: Synergizing reasoning and acting in language models",
            "Benchmarking and defending against indirect prompt injection attacks on large language models",
            "Assessing prompt injection risks in 200+ custom gpts",
            "Injecagent: Benchmarking indirect prompt injections in tool-integrated large language model agents",
            "Judging llm-as-a-judge with mt-bench and chatbot arena",
            "Can llms separate instructions from data? and what do we even mean by that?"
        ]
    },
    {
        "index": 96,
        "title": "FEINT AND ATTACK: ATTENTION-BASED STRATEGIES FOR JAILBREAKING AND PROTECTING LLMs",
        "publication_date": "2024-10-18",
        "references": [
            "Neural machine translation by jointly learning to align and translate",
            "Understanding jailbreak success: A study of latent space dynamics in large language models",
            "Play guessing game with LLM: indirect jailbreak attack with implicit clues",
            "Jailbreaking black box large language models in twenty queries",
            "What does BERT look at? an analysis of BERT’s attention",
            "A wolf in sheep’s clothing: Generalized nested jailbreak prompts can fool large language models easily",
            "An image is worth 16x16 words: Transformers for image recognition at scale",
            "Detecting hallucinations in large language models using semantic entropy",
            "Self-attention attribution: Interpreting information interactions inside transformer",
            "Found in the middle: Calibrating positional attention bias improves long context utilization",
            "GUARD: role-playing to generate natural-language jailbreakings to test guideline adherence of large language models",
            "Semantic uncertainty: Linguistic invariants for uncertainty estimation in natural language generation",
            "Deepinception: Hypnotize large language model to be jailbreaker",
            "AutoDAN: Generating stealthy jailbreak prompts on aligned large language models",
            "Effective approaches to attention-based neural machine translation",
            "Tree of attacks: Jailbreaking black-box llms automatically",
            "Llama: Open and efficient foundation language models",
            "Llama 2: Open foundation and fine-tuned chat models",
            "Attention interpretability across NLP tasks",
            "Tell your model where to attend: Post-hoc attention steering for LLMs",
            "How alignment and jailbreak work: Explain LLM safety through intermediate hidden states",
            "Universal and transferable adversarial attacks on aligned language models",
            "Tastle: Distract large language models for automatic jailbreak attack",
            "Unveiling and harnessing hidden attention sinks: Enhancing large language models without training through attention calibration",
            "Red teaming language models with language models",
            "”do anything now”: Characterizing and evaluating in-the-wild jailbreak prompts on large language models",
            "Attention is all you need"
        ]
    },
    {
        "index": 97,
        "title": "FINE-TUNING ALIGNED LANGUAGE MODELS COMPROMISES SAFETY, EVEN WHEN USERS DO NOT INTEND TO!",
        "publication_date": "2023-12-01",
        "references": [
            "Multi-party goal tracking with llms: Comparing pre-training, fine-tuning, and prompt engineering",
            "Training a helpful and harmless assistant with reinforcement learning from human feedback",
            "Constitutional ai: Harmlessness from ai feedback",
            "Code llama: Open foundation models for code",
            "Few-shot fine-tuning vs. in-context learning: A fair comparison and evaluation",
            "Jailbreaking chatgpt on release day",
            "GPT as a financial advisor",
            "Understanding deep learning (still) requires rethinking generalization",
            "Stanford alpaca: An instruction-following llama model",
            "Llama: Open and efficient foundation language models",
            "Llama 2: Open foundation and fine-tuned chat models",
            "fllama 2 - function calling llama 2",
            "Clinical outcome prediction from admission notes using self-supervised knowledge integration",
            "A survey on large language model based autonomous agents",
            "Negligence and ai’s human users",
            "Shadow alignment: The ease of subverting safely-aligned language models",
            "Open-source can be dangerous: On the vulnerability of value alignment in open-source llms",
            "Bitfit: Simple parameter-efficient fine-tuning for transformer-based masked language-models",
            "Narcissus: A practical clean-label backdoor attack with limited information",
            "Removing rlhf protections in gpt-4 via fine-tuning",
            "Improving language understanding by generative pre-training",
            "Learning transferable visual models from natural language supervision",
            "On the opportunities and risks of foundation models",
            "Rt-2: Vision-language-action models transfer web knowledge to robotic control",
            "Language models are few-shot learners",
            "What neural networks memorize and why: Discovering the long tail via influence estimation",
            "Finetuned language models are zero-shot learners",
            "Chain-of-thought prompting elicits reasoning in large language models",
            "A prompt pattern catalog to enhance prompt engineering with chatgpt",
            "Data selection for language models via importance resampling",
            "Judging llm-as-a-judge with mt-bench and chatbot arena",
            "Minigpt-4: Enhancing vision-language understanding with advanced large language models",
            "Universal and transferable adversarial attacks on aligned language models",
            "A mechanistic understanding of alignment algorithms: A case study on dpo and toxicity",
            "Assessing the brittleness of safety alignment via pruning and low-rank modifications",
            "Visual adversarial examples jailbreak aligned large language models",
            "Backdoor learning: A survey",
            "BERT: Pre-training of deep bidirectional transformers for language understanding",
            "Improving language understanding by generative pre-training",
            "A backdoor attack against lstm-based text classification systems",
            "Instructblip: Towards general-purpose vision-language models with instruction tuning",
            "Entity-based knowledge conflicts in question answering",
            "Decoupled weight decay regularization",
            "A survey for in-context learning",
            "Are aligned neural networks adversarially aligned?",
            "Lora: Low-rank adaptation of large language models",
            "Jailbroken: How does llm safety training fail?",
            "Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning",
            "Finetuned language models are zero-shot learners",
            "Chain-of-thought prompting elicits reasoning in large language models",
            "A prompt pattern catalog to enhance prompt engineering with chatgpt",
            "Data selection for language models via importance resampling",
            "Shadow alignment: The ease of subverting safely-aligned language models",
            "Open-source can be dangerous: On the vulnerability of value alignment in open-source llms",
            "Bitfit: Simple parameter-efficient fine-tuning for transformer-based masked language-models",
            "Narcissus: A practical clean-label backdoor attack with limited information",
            "Removing rlhf protections in gpt-4 via fine-tuning",
            "Understanding deep learning (still) requires rethinking generalization",
            "Stanford alpaca: An instruction-following llama model",
            "Llama: Open and efficient foundation language models",
            "Llama 2: Open foundation and fine-tuned chat models",
            "fllama 2 - function calling llama 2",
            "Clinical outcome prediction from admission notes using self-supervised knowledge integration",
            "A survey on large language model based autonomous agents",
            "Negligence and ai’s human users",
            "Shadow alignment: The ease of subverting safely-aligned language models",
            "Open-source can be dangerous: On the vulnerability of value alignment in open-source llms",
            "Bitfit: Simple parameter-efficient fine-tuning for transformer-based masked language-models",
            "Narcissus: A practical clean-label backdoor attack with limited information",
            "Removing rlhf protections in gpt-4 via fine-tuning",
            "Understanding deep learning (still) requires rethinking generalization",
            "Stanford alpaca: An instruction-following llama model",
            "Llama: Open and efficient foundation language models",
            "Llama 2: Open foundation and fine-tuned chat models",
            "fllama 2 - function calling llama 2"
        ]
    },
    {
        "index": 98,
        "title": "Fine-Tuning, Quantization, and LLMs: Navigating Unintended Outcomes",
        "publication_date": "2024-09-09",
        "references": [
            "Jailbreaking Black Box Large Language Models in Twenty Queries",
            "Not what you’ve signed up for: Compromising Real-World LLM-Integrated Applications with Indirect Prompt Injection",
            "PETGEN: Personalized Text Generation Attack on Deep Sequence Embedding-based Classification Models",
            "ProPILE: Probing Privacy Leakage in Large Language Models",
            "Certifying LLM Safety against Adversarial Prompting",
            "MALCOM: Generating Malicious Comments to Attack Neural Fake News Detection Models",
            "DeepInception: Hypnotize Large Language Model to Be Jailbreaker",
            "Jailbreaking ChatGPT via Prompt Engineering: An Empirical Study",
            "Training language models to follow instructions with human feedback",
            "Fine-tuning Aligned Language Models Compromises Safety, Even When Users Do Not Intend To!",
            "Direct Preference Optimization: Your Language Model is Secretly a Reward Model",
            "NeMo Guardrails: A Toolkit for Controllable and Safe LLM Applications with Programmable Rails",
            "Jailbroken: How Does LLM Safety Training Fail?",
            "Exploring Parameter-Efficient Fine-Tuning Techniques for Code Generation with Large Language Models",
            "Robust Prompt Optimization for Defending Language Models Against Jailbreaking Attacks",
            "AutoDAN: Interpretable Gradient-Based Adversarial Attacks on Large Language Models",
            "Universal and Transferable Adversarial Attacks on Aligned Language Models",
            "Universal and transferable adversarial attacks on aligned language models"
        ]
    },
    {
        "index": 99,
        "title": "FLASK: Fine-grained Language Model Evaluation based on Alignment SKill Sets",
        "publication_date": "2024-01-01",
        "references": [
            "TopiOCQA: Open-domain conversational question answering with topic switching",
            "Do as i can, not as i say: Grounding language in robotic affordances",
            "Training a helpful and harmless assistant with reinforcement learning from human feedback",
            "Training verifiers to solve math word problems",
            "Measuring massive multitask language understanding",
            "Aligning ai with shared human values",
            "Hurdles to progress in long-form question answering",
            "The disagreement deconvolution: Bringing machine learning performance metrics in line with reality",
            "The false promise of imitating proprietary llms",
            "Hierarchical neural story generation",
            "Large language models are not fair evaluators",
            "Chain of thought prompting elicits reasoning in large language models",
            "Fine-grained human feedback gives better rewards for language model training",
            "Instructeval: Towards holistic evaluation of instruction-tuned large language models",
            "Scaling instruction-finetuned language models",
            "A pretrainer’s guide to training data: Measuring the effects of data age, domain coverage, quality, & toxicity",
            "Wizardcoder: Empowering code large language models with evol-instruct",
            "Beyond the imitation game: Quantifying and extrapolating the capabilities of language models",
            "Stanford alpaca: An instruction-following llama model",
            "Model evaluation for extreme risks",
            "self awareness: a benchmark task to measure self-awareness of language models",
            "Evaluating human-language model interaction",
            "Large language models are not fair evaluators",
            "Llama: Open and efficient foundation language models",
            "Llama 2: Open foundation and fine-tuned chat models",
            "Retrieval of soft prompt enhances zero-shot task generalization",
            "Guess the instruction!: making language models stronger zero-shot learners",
            "In-context instruction learning",
            "Selfee: Iterative self-revising llm empowered by self-feedback generation",
            "Concise answers to complex questions: Summarization of long-form answers",
            "Let’s verify step by step",
            "The e2e dataset: New challenges for end-to-end generation",
            "Aligning Large Language Models Through Synthetic Feedback",
            "What language model architecture and pretraining objective work best for zero-shot generalization?",
            "Neural network acceptability judgments",
            "When not to trust language models: Investigating effectiveness of parametric and non-parametric memories",
            "Neural toxic degeneration in language models",
            "Mapping language to code in programmatic context",
            "Fantastic questions and where to find them: FairytaleQA – an authentic dataset for narrative comprehension",
            "HotpotQA: A dataset for diverse, explainable multi-hop question answering",
            "Scone: Benchmarking negation reasoning in language models with fine-tuning and in-context learning",
            "Challenging big-bench tasks and whether chain-of-thought can solve them",
            "A human-centric benchmark for evaluating foundation models",
            "Training language models to follow instructions with human feedback",
            "An automatic evaluator of instruction-following models",
            "Holistic evaluation of language models",
            "Do anything now\": Characterizing and evaluating in-the-wild jailbreak prompts on large language models",
            "Self-instruct: Aligning language model with self generated instructions",
            "Benchmarking generalization via in-context instructions on 1,600+ language tasks",
            "How far can camels go?: Exploring the state of instruction tuning on open resources",
            "Judging llm-as-a-judge with mt-bench and chatbot arena",
            "An analysis of prerequisite skills for reading comprehension",
            "Evaluation metrics for machine reading comprehension: Prerequisite skills and readability",
            "CommonsenseQA 2.0: Exposing the limits of ai through gamification",
            "Compositional semantic parsing on semi-structured tables",
            "Evaluating quality of chatbots and intelligent conversational agents",
            "Counterfactual story reasoning and generation",
            "TimeDial: Temporal commonsense reasoning in dialog",
            "Crepe: Open-domain question answering with false presuppositions",
            "Large language models are not fair evaluators",
            "Compositional semantic parsing on semi-structured tables",
            "Training verifiers to solve math word problems",
            "Training language models to follow instructions with human feedback",
            "TruthfulQA: Measuring how models mimic human falsehoods",
            "Inverse scaling prize",
            "Fine-tuned language models are continual learners",
            "When large language models meet personalization",
            "A framework for evaluation of machine reading comprehension gold standards",
            "Unnatural instructions: Tuning language models with (almost) no human labor",
            "An analysis of prerequisite skills for reading comprehension",
            "Mapping language to code in programmatic context",
            "Neural network acceptability judgments",
            "CommonsenseQA 2.0: Exposing the limits of ai through gamification",
            "Counterfactual story reasoning and generation",
            "Training verifiers to solve math word problems",
            "Training language models to follow instructions with human feedback",
            "TruthfulQA: Measuring how models mimic human falsehoods",
            "Inverse scaling prize",
            "Fine-tuned language models are continual learners",
            "When large language models meet personalization",
            "A framework for evaluation of machine reading comprehension gold standards",
            "Unnatural instructions: Tuning language models with (almost) no human labor",
            "An analysis of prerequisite skills for reading comprehension",
            "Mapping language to code in programmatic context",
            "Neural network acceptability judgments",
            "CommonsenseQA 2.0: Exposing the limits of ai through gamification",
            "Counterfactual story reasoning and generation",
            "Training verifiers to solve math word problems",
            "Training language models to follow instructions with human feedback",
            "TruthfulQA: Measuring how models mimic human falsehoods",
            "Inverse scaling prize",
            "Fine-tuned language models are continual learners",
            "When large language models meet personalization",
            "A framework for evaluation of machine reading comprehension gold standards",
            "Unnatural instructions: Tuning language models with (almost) no human labor",
            "An analysis of prerequisite skills for reading comprehension",
            "Mapping language to code in programmatic context",
            "Neural network acceptability judgments",
            "CommonsenseQA 2.0: Exposing the limits of ai through gamification",
            "Counterfactual story reasoning and generation",
            "Training verifiers to solve math word problems",
            "Training language models to follow instructions with human feedback",
            "TruthfulQA: Measuring how models mimic human falsehoods",
            "Inverse scaling prize",
            "Fine-tuned language models are continual learners",
            "When large language models meet personalization",
            "A framework for evaluation of machine reading comprehension gold standards",
            "Unnatural instructions: Tuning language models with (almost) no human labor",
            "An analysis of prerequisite skills for reading comprehension",
            "Mapping language to code in programmatic context",
            "Neural network acceptability judgments",
            "CommonsenseQA 2.0: Exposing the limits of ai through gamification",
            "Counterfactual story reasoning and generation",
            "Training verifiers to solve math word problems",
            "Training language models to follow instructions with human feedback",
            "TruthfulQA: Measuring how models mimic human falsehoods"
        ]
    },
    {
        "index": 100,
        "title": "FLIPATTACK: JAILBREAK LLMs VIA FLIPPING",
        "publication_date": "2024-10-02",
        "references": [
            "Revisiting character-level adversarial attacks for language models.",
            "Gpt-4 technical report.",
            "Detecting language model attacks with perplexity.",
            "Many-shot jailbreaking.",
            "Gemini: A family of highly capable multimodal models.",
            "Promptsource: An integrated development environment and repository for natural language prompts.",
            "Training a helpful and harmless assistant with reinforcement learning from human feedback.",
            "Jailbreaking black box large language models in twenty queries.",
            "Jailbreakbench: An open robustness benchmark for jailbreaking large language models.",
            "Rethinking model ensemble in transfer-based adversarial attacks.",
            "Pandora: Detailed llm jailbreaking via collaborated phishing agents with decomposed reasoning.",
            "Safe rlhf: Safe reinforcement learning from human feedback.",
            "Agent smith: A single image can jailbreak one million multimodal llm agents exponentially fast.",
            "Textbooks are all you need.",
            "Open sesame! universal black box jailbreaking of large language models.",
            "Open the pandora’s box of llms: Jailbreaking llms through representation engineering.",
            "Drattack: Prompt decomposition and reconstruction makes powerful llm jailbreakers.",
            "Attacking large language models with projected gradient descent.",
            "Towards deep learning models resistant to adversarial attacks.",
            "Tree of attacks: Jailbreaking black-box llms automatically.",
            "Training language models to follow instructions with human feedback.",
            "AdvPrompter: Fast adaptive adversarial prompting for llms.",
            "Universal jailbreak backdoors from poisoned human feedback.",
            "Llm self defense: By self examination, llms know they are being tricked.",
            "Fine-tuning aligned language models compromises safety, even when users do not intend to!",
            "Improved techniques for optimization-based jailbreaking on large language models.",
            "Exploring the limits of domain-adaptive training for detoxifying large-scale language models.",
            "Defending large language models against jailbreaking attacks through goal prioritization.",
            "Autodan: Interpretable gradient-based adversarial attacks on large language models.",
            "Recipes for safety in open-domain chatbots.",
            "Cognitive overload: Jailbreaking large language models with overloaded logical thinking.",
            "An llm can fool itself: A prompt-based adversarial attack.",
            "Jailbreak in pieces: Compositional adversarial attacks on multi-modal language models.",
            "Defending large language models against jailbreaking attacks via semantic smoothing.",
            "Making them ask and answer: Jailbreaking large language models in few queries via disguise and reconstruction.",
            "Amplegcg: Learning a universal and transferable generative model of adversarial suffixes for jailbreaking both open and closed llms.",
            "Single character perturbations break llm alignment.",
            "Catastrophic jailbreak of open-source llms via exploiting generation.",
            "Rain: Your language models can align themselves without finetuning.",
            "Langgpt: Rethinking structured reusable prompt design framework for llms from the programming language.",
            "Self-instruct: Aligning language models with self-generated instructions.",
            "Super-naturalinstructions: Generalization via declarative instructions on 1600+ nlp tasks.",
            "Adashield: Safeguarding multimodal large language models from structure-based attack via adaptive shield prompting.",
            "Poisoned langchain: Jailbreak llms by langchain.",
            "Sos! soft prompt attack against open-source large language models.",
            "Jailbreak open-sourced large language models via enforced decoding.",
            "Boosting jailbreak attack with momentum.",
            "Defending large language models against jailbreaking attacks through goal prioritization.",
            "Revolutionizing finance with llms: An overview of applications and insights.",
            "On prompt-driven safeguarding for large language models.",
            "Improved few-shot jailbreaking can circumvent aligned language models and their defenses.",
            "Autodan: Interpretable gradient-based adversarial attacks on large language models."
        ]
    },
    {
        "index": 101,
        "title": "Formalizing and Benchmarking Prompt Injection Attacks and Defenses",
        "publication_date": "2023-11-24",
        "references": [
            "Bing Search.",
            "ChatGPT Plugins.",
            "ChatWithPDF.",
            "Instruction defense.",
            "Introducing ChatGPT.",
            "llma2-13b-chat-url.",
            "llma2-7b-chat-url.",
            "Random sequence enclosure.",
            "Sandwitch defense.",
            "Contributions to the study of sms spam filtering: New collection and results.",
            "Detecting language model attacks with perplexity.",
            "Palm 2 technical report.",
            "Spinning language models: Risks of propaganda-as-a-service and countermeasures.",
            "Evaluating the susceptibility of pre-trained language models via handcrafted adversarial examples.",
            "Language models are few-shot learners.",
            "Extracting training data from large language models.",
            "Ultimate ChatGPT prompt engineering guide for general users and developers.",
            "Jailbroken: How does llm safety training fail?",
            "Prompt injection attacks against GPT-3.",
            "Delimiters won't save you from prompt injection.",
            "Hacker Reveals Microsoft's New AI-Powered Bing Chat Search Secrets.",
            "Predicting grammaticality on an ordinal scale.",
            "GLUE: A multi-task benchmark and analysis platform for natural language understanding.",
            "Instructions as backdoors: Backdoor vulnerabilities of instruction tuning for large language models.",
            "Promptbench: Towards evaluating the robustness of large language models on adversarial prompts."
        ]
    },
    {
        "index": 102,
        "title": "FUNCTIONAL HOMOTOPY : S MOOTHING DISCRETE OPTIMIZATION VIA CONTINUOUS PARAMETERS FOR LLM JAILBREAK ATTACKS",
        "publication_date": "2025-02-16",
        "references": [
            "Jailbreaking leading safety-aligned llms with simple adaptive attacks",
            "Convex optimization",
            "The care and feeding of wild-caught mutants",
            "Towards evaluating the robustness of neural networks",
            "Lora: Low-rank adaptation of large language models",
            "Efficient llm jailbreak via adaptive dense-to-sparse constrained optimization",
            "Mistral 7b",
            "Infuse: fusing integration test management with change management",
            "Reducibility among Combinatorial Problems",
            "Amplegcg: Learning a universal and transferable generative model of adversarial suffixes for jailbreaking both open and closed llms",
            "AutoDAN: Generating stealthy jailbreak prompts on aligned large language models",
            "Automatic and universal prompt injection attacks against large language models",
            "Towards deep learning models resistant to adversarial attacks",
            "Harmbench: A standardized evaluation framework for automated red teaming and robust refusal",
            "Tree of attacks: Jailbreaking black-box llms automatically",
            "Soft prompt threats: Attacking safety alignment and unlearning in open-source LLMs through the embedding space",
            "Intriguing properties of neural networks",
            "Llama 2: Open foundation and fine-tuned chat models"
        ]
    },
    {
        "index": 103,
        "title": "GenTel-Safe: A Unified Benchmark and Shielding Framework for Defending Against Prompt Injection Attacks",
        "publication_date": "2024-09-29",
        "references": [
            "Extracting training data from large language models",
            "Risk taxonomy, mitigation, and assessment benchmarks of large language model systems",
            "Llama guard: Llm-based input-output safeguard for human-ai conversations",
            "Not what you’ve signed up for: Compromising real-world llm-integrated applications with indirect prompt injection",
            "Cold-attack: Jailbreaking llms with stealthiness and controllability",
            "Semantic-guided prompt organization for universal goal hijacking against llms",
            "Pleak: Prompt leaking attacks against large language model applications",
            "Jailbreaker: Automated jail-break across multiple large language model chatbots",
            "Toxicchat: Unveiling hidden challenges of toxicity detection in real-world user-AI conversation",
            "Salad-bench: A hierarchical and comprehensive safety benchmark for large language models",
            "What makes good data for alignment? a comprehensive study of automatic data selection in instruction tuning",
            "Autodan: Generating stealthy jailbreak prompts on aligned large language models",
            "Automatic and universal prompt injection attacks against large language models",
            "Jailbreaking chatgpt via prompt engineering: An empirical study",
            "Can chatgpt forecast stock price movements? return predictability and large language models",
            "Designing chemical reaction arrays using phactor and chatgpt",
            "A holistic approach to undesired content detection in the real world",
            "Harmbench: A standardized evaluation framework for automated red teaming and robust refusal",
            "Multilingual e5 text embeddings: A technical report",
            "Jailbroken: How does llm safety training fail?",
            "Eda: Easy data augmentation techniques for boosting performance on text classification tasks",
            "Gradsafe: Detecting jailbreak prompts for llms via safety-critical gradient analysis",
            "A comprehensive study of jailbreak attack versus defense for large language models",
            "Large language models can rate news outlet credibility",
            "Chatgpt and environmental research",
            "Universal and transferable adversarial attacks on aligned language models"
        ]
    },
    {
        "index": 104,
        "title": "GPT-4 Jailbreaks Itself with Near-Perfect Success Using Self-Explanation",
        "publication_date": "2024-10-15",
        "references": [
            "GPT-4 technical report",
            "Jailbreaking leading safety-aligned llms with simple adaptive attacks",
            "Many-shot jailbreaking",
            "The claude 3 model family: Opus, sonnet, haiku",
            "“real attackers don’t compute gradients”: Bridging the gap between adversarial ml research and practice",
            "Adversarial attacks and defences: A survey",
            "Jailbreaking black box large language models in twenty queries",
            "Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality",
            "The llama 3 herd of models",
            "Bias and fairness in large language models: A survey",
            "Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned",
            "Attacking large language models with projected gradient descent",
            "Toxigen: A large-scale machine-generated dataset for adversarial and implicit hate speech detection",
            "Open sesame! universal black box jailbreaking of large language models",
            "Tree of attacks: Jailbreaking black-box llms automatically",
            "Jailbroken: How does llm safety training fail?",
            "Emergent abilities of large language models",
            "Jailbreak and guard aligned language models with only few in-context demonstrations",
            "Tree of thoughts: Deliberate problem solving with large language models",
            "Gpt-fuzzer: Red teaming large language models with auto-generated jailbreak prompts",
            "How johnny can persuade llms to jailbreak them: Rethinking persuasion to challenge ai safety by humanizing llms",
            "Adversarial attacks on deep-learning models in natural language processing: A survey",
            "A survey of large language models",
            "Universal and transferable adversarial attacks on aligned language models"
        ]
    },
    {
        "index": 105,
        "title": "Great, Now Write an Article About That: The Crescendo Multi-Turn LLM Jailbreak Attack",
        "publication_date": "2025-02-26",
        "references": [
            "Responsible Disclosure and Mitigation Efforts.",
            "Many-shot jailbreaking.",
            "Constitutional ai: Harmlessness from ai feedback, 2022.",
            "Extracting training data from large language models, 2021.",
            "Jailbreaking Black Box Large Language Models in Twenty Queries. CoRR abs/2310.08419, 2023.",
            "Leveraging the context through multi-round interactions for jailbreaking attacks.",
            "Jailbreaker: Automated Jailbreak Across Multiple Large Language Model Chatbots. CoRR abs/2307.08715, 2023.",
            "Multilingual Jailbreak Challenges in Large Language Models. CoRR abs/2310.06474, 2023.",
            "Mart: Improving llm safety with multi-round automatic red-teaming, 2023.",
            "Improving alignment of dialogue agents via targeted human judgements, 2022.",
            "Catastrophic Jailbreak of Open-source LLMs via Exploiting Generation. CoRR abs/2310.06987, 2023.",
            "User inference attacks on large language models, 2023.",
            "Ethical frameworks and computer security trolley problems: Foundations for conversations.",
            "Pretraining language models with human preferences, 2023.",
            "Multi-step jailbreaking privacy attacks on chatgpt, 2023.",
            "AutoDAN: Generating Stealthy Jailbreak Prompts on Aligned Large Language Models. CoRR abs/2310.04451, 2023.",
            "Jailbreaking ChatGPT via Prompt Engineering: An Empirical Study. CoRR abs/2305.13860, 2023.",
            "Analyzing leakage of personally identifiable information in language models, 2023.",
            "Harmbench: A standardized evaluation framework for automated red teaming and robust refusal, 2024.",
            "Training language models to follow instructions with human feedback, 2022.",
            "Maat-phor: Automated variant analysis for prompt injection attacks, 2023.",
            "Do Anything Now: Characterizing and Evaluating In-The-Wild Jailbreak Prompts on Large Language Models. CoRR abs/2308.03825, 2023.",
            "Jailbroken: How Does LLM Safety Training Fail? CoRR abs/2307.02483, 2023.",
            "Last one standing: A comparative analysis of security and privacy of soft prompt tuning, lora, and in-context learning, 2023.",
            "Defending chatgpt against jailbreak attack via self-reminders. Nature Machine Intelligence, 5:1486–1496, 2023.",
            "Sc-safety: A multi-round open-ended question adversarial safety benchmark for large language models in chinese, 2023.",
            "Chain of attack: a semantic-driven contextual multi-turn attacker for llm, 2024.",
            "GPTFUZZER: Red Teaming Large Language Models with Auto-Generated Jailbreak Prompts. CoRR abs/2309.10253, 2023.",
            "Defending large language models against jailbreaking attacks through goal prioritization, 2024.",
            "Make them spill the beans! coercive knowledge extraction from (production) llms, 2023.",
            "Universal and Transferable Adversarial Attacks On Aligned Language Models. CoRR abs/2307.15043, 2023."
        ]
    },
    {
        "index": 106,
        "title": "GuardAgent: Safeguard LLM Agents via Knowledge-Enabled Reasoning",
        "publication_date": "2025-03-09",
        "references": [
            "Guardrails AI",
            "Conversational health agents: A personalized llm-powered agent framework",
            "Agentpoison: Red-teaming llm agents via poisoning memory or knowledge bases",
            "Personalized autonomous driving with large language models: Field experiments",
            "Mind2web: Towards a generalist agent for the web",
            "A real-world webagent with planning, long context understanding, and program synthesis",
            "Dme-driver: Integrating human decision logic and 3d scene perception in autonomous driving",
            "Llama guard: Llm-based input-output safeguard for human-ai conversations",
            "Surrealdriver: Designing generative driver agent simulation framework in urban contexts based on large language model",
            "A new generation of perspective api: Efficient multi-lingual character-level transformers",
            "Retrieval-augmented generation for knowledge-intensive nlp tasks",
            "Agent hospital: A simulacrum of hospital with evolvable medical agents",
            "Ehragent: Code empowers large language models for few-shot complex tabular reasoning on electronic health records",
            "Llm agents for psychology: A study on gamified assessments",
            "R-judge: Benchmarking safety risk awareness for LLM agents",
            "Rigorllm: Resilient guardrails for large language models against undesired content",
            "Gpt-4v(ision) is a generalist web agent, if grounded",
            "Webarena: A realistic web environment for building autonomous agents",
            "Decodingtrust: A comprehensive assessment of trustworthiness in gpt models",
            "Self-consistency improves chain of thought reasoning in language models",
            "Chain of thought prompting elicits reasoning in large language models",
            "The rise and potential of large language model based agents: A survey",
            "Ecological drivers of crispr immune systems",
            "Finmem: A performance-enhanced llm trading agent with layered memory and character design",
            "ReAct: Synergizing reasoning and acting in language models",
            "Finmem: A performance-enhanced llm trading agent with layered memory and character design",
            "CommonsenseQA: A question answering challenge targeting commonsense knowledge"
        ]
    },
    {
        "index": 107,
        "title": "h4rm3l: A Dynamic Benchmark of Composable Jailbreak Attacks for LLM Safety Assessment",
        "publication_date": "2024-09-13",
        "references": [
            "Detecting Language Model Attacks with Perplexity",
            "Program Synthesis with Large Language Models",
            "Safety-Tuned LLaMAs: Lessons from Improving the Safety of Large Language Models that Follow Instructions",
            "JailbreakBench: An Open Robustness Benchmark for Jailbreaking Large Language Models",
            "Jailbreaking Black Box Large Language Models in Twenty Queries",
            "Design Patterns: Elements of Reusable Object-Oriented Software",
            "Red Teaming Language Models to Reduce Harms: Methods, Scaling Behaviors, and Lessons Learned",
            "RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models",
            "Not What you’ve Signed up for: Compromising Real-World LLM-Integrated Applications with Indirect Prompt Injection",
            "LLM-based Code Generation Method for Golang Compiler Testing",
            "Program Synthesis",
            "LoRA: Low-Rank Adaptation of Large Language Models",
            "Catastrophic Jailbreak of Open-Source LLMs via Exploiting Generation",
            "Baseline Defenses for Adversarial Attacks Against Aligned Language Models",
            "Exploiting Programmatic Behavior of LLMs: Dual-Use Through Standard Security Attacks",
            "DSPy: Compiling Declarative Language Model Calls into Self-Improving Pipelines",
            "BPE-Dropout: Simple and Effective Subword Regularization",
            "Fine-tuning Aligned Language Models Compromises Safety, Even When Users do not Intend to!",
            "Latent Jailbreak: A Benchmark for Evaluating Text Safety and Output Robustness of Large Language Models",
            "XSTest: A Test Suite for Identifying Exaggerated Safety Behaviours in Large Language Models",
            "Code Llama: Open Foundation Models for Code",
            "Jailbroken: How Does LLM Safety Training Fail?",
            "Low-Resource Languages Jailbreak GPT-4",
            "GPT-4 is too Smart to be Safe: Stealthy Chat with LLMs via Cipher",
            "How Johnny Can Persuade LLMs to Jailbreak Them: Rethinking Persuasion to Challenge AI Safety by Humanizing LLMs",
            "AutoDAN: Automatic and Interpretable Adversarial Attacks on Large Language Models",
            "Universal and Transferable Adversarial Attacks on Aligned Language Models"
        ]
    },
    {
        "index": 108,
        "title": "Hacc-Man: An Arcade Game for Jailbreaking LLMs",
        "publication_date": "2024-07-01",
        "references": [
            "Perceived self-efficacy in the exercise of control over AIDS infection.",
            "Generative Artificial Intelligence and the Emergence of Creative Displacement Anxiety.",
            "Right on Track: NVIDIA Open-Source Software Helps Developers Add Guardrails to AI Chatbots — blogs.nvidia.com.",
            "Assessing language model deployment with risk cards.",
            "Materializing the abstract: Understanding AI by game jamming.",
            "The social psychology of self-efficacy.",
            "An unobtrusive image.",
            "Alternate uses.",
            "Designing participatory ai: Creative professionals’ worries and expectations about generative ai.",
            "Summon a demon and bind it: A grounded theory of llm red teaming in the wild.",
            "Developing Creative Potential: The Power of Process, People, and Place.",
            "Chinese police arrest man who allegedly used ChatGPT to spread fake news in first case of its kind — cnbc.com.",
            "The eureka hunt.",
            "Against The Achilles’ Heel: A Survey on Red Teaming for Generative Models.",
            "Naming unrelated words predicts creativity.",
            "Creativity in the age of generative AI.",
            "A StrongREJECT for Empty Jailbreaks.",
            "Red Team Development and Operations–A practical Guide.",
            "Prompt injection: What’s the worst that can happen? — simonwillison.net.",
            "Prompt injection and jailbreaking are not the same thing — simonwillison.net.",
            "A New Era in LLM Security: Exploring Security Concerns in Real-World LLM-based Systems.",
            "Don’t Listen To Me: Understanding and Exploring Jailbreak Prompts of Large Language Models.",
            "Why Johnny can’t prompt: how non-AI experts try (and fail) to design LLM prompts."
        ]
    },
    {
        "index": 109,
        "title": "Hacking Back the AI-Hacker: Prompt Injection as a Defense Against LLM-driven Cyberattacks",
        "publication_date": "2024-11-18",
        "references": [
            "Hack the box.",
            "Prompt injection attacks against gpt-3.",
            "Securing llm systems against prompt injection.",
            "ffuf: Fast web fuzzer written in go.",
            "metasploit: The world’s most used penetration testing framework.",
            "Redis: In-memory data structure store.",
            "Concrete problems in ai safety.",
            "Deep reinforcement learning from human preferences.",
            "Pentestgpt: An llm-empowered automatic penetration testing tool.",
            "PentestGPT: Evaluating and harnessing large language models for automated penetration testing.",
            "Llms for cyber security: New opportunities.",
            "Llm agents can autonomously exploit one-day vulnerabilities.",
            "Llm agents can autonomously hack websites.",
            "Teams of llm agents can exploit zero-day vulnerabilities.",
            "Chatgpt for vulnerability detection, classification, and repair: How far are we?",
            "sqlmap: Automatic sql injection and database takeover tool.",
            "Autopenbench: Benchmarking generative agents for penetration testing.",
            "Ai chatbots can read and write invisible text, creating an ideal covert channel.",
            "Hacking, the lazy way: Llm augmented pentesting.",
            "Not what you’ve signed up for: Compromising real-world llm-integrated applications with indirect prompt injection.",
            "Getting pwn’d by ai: Penetration testing with large language models.",
            "Generative ai for pentesting: the good, the bad, the ugly.",
            "The ethics of hacking back.",
            "Penheal: A two-stage llm framework for automated pentesting and optimal remediation.",
            "Llmjacking and open-source tool abuse surge in 2024 cloud attacks.",
            "The state of ai and cybersecurity in 2024.",
            "Large language models in cybersecurity: Threats, exposure and mitigation.",
            "Openai blocks 20 global malicious campaigns using ai for cybercrime and disinformation.",
            "Disrupting malicious uses of ai by state-affiliated threat actors.",
            "A new era of cybersecurity with ai: Predictions for 2024.",
            "LLMmap: Fingerprinting For Large Language Models.",
            "Neural exec: Learning (and learning from) execution triggers for prompt injection attacks.",
            "Ignore previous prompt: Attack techniques for language models.",
            "Chinese researchers develop ai model for military use, back meta’s llama.",
            "Llm agent honeypot: Real-world ai threat analysis.",
            "Ciso perspectives: Tackling the rise of ai-powered cyber attacks.",
            "An empirical evaluation of llms for solving offensive security challenges.",
            "Nyu ctf dataset: A scalable open-source benchmark dataset for evaluating llms in offensive security.",
            "Honeypots: tracking hackers.",
            "Cognitive architectures for language agents.",
            "The study of instinct.",
            "The instruction hierarchy: Training llms to prioritize privileged instructions.",
            "From sands to mansions: Enabling automatic full-life-cycle cyberattack construction with llm.",
            "Autoattacker: A large language model guided system to implement automatic cyber-attacks.",
            "React: Synergizing reasoning and acting in language models."
        ]
    },
    {
        "index": 110,
        "title": "Harmful Fine-tuning Attacks and Defenses for Large Language Models: A Survey",
        "publication_date": "2024-12-03",
        "references": [
            "Identifying and tuning safety neurons in large language models",
            "Measuring the contribution of fine-tuning to individual responses of LLMs",
            "On evaluating the durability of safeguards for open-weight LLMs",
            "Safety alignment shouldn’t be complicated",
            "SaloRA: Safety-alignment preserved low-rank adaptation",
            "Unraveling and mitigating safety alignment degradation of vision-language models",
            "Your task may vary: A systematic understanding of alignment and safety degradation when fine-tuning LLMs",
            "Current state of llm risks and ai guardrails",
            "How to backdoor federated learning",
            "Training a helpful and harmless assistant with reinforcement learning from human feedback",
            "Safety-tuned llamas: Lessons from improving the safety of large language models that follow instructions",
            "What does it mean for a language model to preserve privacy?",
            "Trustworthy llms: A survey and guideline for evaluating large language models’ alignment",
            "Defending against alignment-breaking attacks via robustly aligned llm",
            "Composite backdoor attacks against large language models",
            "Lockdown: backdoor defense for federated learning with isolated subspace training",
            "Booster: Tackling harmful fine-tuning for large language models via attenuating harmful perturbation",
            "Lazy safety alignment for large language models against harmful fine-tuning",
            "Vaccine: Perturbation-aware alignment for large language model",
            "A survey of safety and trustworthiness of large language models through the lens of verification and validation",
            "Jailbroken: How does llm safety training fail?",
            "Jailbroken: How does llm safety training fail?",
            "Assessing the brittleness of safety alignment via pruning and low-rank modifications",
            "What makes and breaks safety fine-tuning? mechanistic study",
            "Defending against reverse preference attacks is difficult",
            "Representation noising effectively prevents harmful fine-tuning on llms",
            "Immunization against harmful fine-tuning attacks",
            "Safetyprompts: a systematic review of open datasets for evaluating and improving large language model safety",
            "Recursive deep models for semantic compositionality over a sentiment treebank",
            "Training language models to follow instructions with human feedback",
            "Universal and transferable adversarial attacks on aligned language models",
            "Improving alignment and robustness with circuit breakers",
            "Llama: Open and efficient foundation language models",
            "Judging llm-as-a-judge with mt-bench and chatbot arena",
            "Keeping llms aligned after fine-tuning: The crucial role of prompt templates",
            "Composite backdoor attacks against large language models",
            "From trojan horses to castle walls: Unveiling bilateral backdoor effects in diffusion models",
            "Bag of tricks for training data extraction from language models",
            "Training verifiers to solve math word problems",
            "Measuring massive multitask language understanding",
            "Composite backdoor attacks against large language models",
            "Measuring forgetting of memorized training examples",
            "Safety alignment doesn’t need to be complicated",
            "Safety layers of aligned large language models: The key to llm security",
            "Unraveling and mitigating safety alignment degradation of vision-language models",
            "Backdooring instruction-tuned large language models with virtual prompt injection",
            "Truthfulqa: Measuring how models mimic human falsehoods",
            "B. A strong, replicable instruction-following model",
            "Robustifying safety-aligned large language models through clean data curation",
            "Safety fine-tuning at (almost) no cost: A baseline for vision large language models",
            "Truthfulqa: Measuring how models mimic human falsehoods",
            "Recursive deep models for semantic compositionality over a sentiment treebank",
            "Composite backdoor attacks against large language models",
            "Jailbroken: How does llm safety training fail?",
            "Jailbroken: How does llm safety training fail?",
            "Defending against alignment-breaking attacks via robustly aligned llm",
            "Lazy safety alignment for large language models against harmful fine-tuning",
            "Defending against reverse preference attacks is difficult",
            "Representation noising effectively prevents harmful fine-tuning on llms",
            "Immunization against harmful fine-tuning attacks",
            "Safetyprompts: a systematic review of open datasets for evaluating and improving large language model safety",
            "Recursive deep models for semantic compositionality over a sentiment treebank",
            "Training language models to follow instructions with human feedback",
            "Universal and transferable adversarial attacks on aligned language models",
            "Improving alignment and robustness with circuit breakers",
            "Llama: Open and efficient foundation language models",
            "Judging llm-as-a-judge with mt-bench and chatbot arena",
            "Keeping llms aligned after fine-tuning: The crucial role of prompt templates",
            "Composite backdoor attacks against large language models",
            "From trojan horses to castle walls: Unveiling bilateral backdoor effects in diffusion models",
            "Bag of tricks for training data extraction from language models",
            "Training verifiers to solve math word problems",
            "Measuring massive multitask language understanding",
            "Safety layers of aligned large language models: The key to llm security",
            "Unraveling and mitigating safety alignment degradation of vision-language models",
            "Backdooring instruction-tuned large language models with virtual prompt injection",
            "Truthfulqa: Measuring how models mimic human falsehoods",
            "B. A strong, replicable instruction-following model",
            "Robustifying safety-aligned large language models through clean data curation",
            "Safety fine-tuning at (almost) no cost: A baseline for vision large language models",
            "Truthfulqa: Measuring how models mimic human falsehoods",
            "Recursive deep models for semantic compositionality over a sentiment treebank"
        ]
    },
    {
        "index": 111,
        "title": "HARNESSING TASK OVERLOAD FOR SCALABLE JAILBREAK ATTACKS ON LARGE LANGUAGE MODELS",
        "publication_date": "2024-10-05",
        "references": [
            "Gpt-4 technical report.",
            "Jailbreak chat.",
            "Does refusal training in llms generalize to the past tense?",
            "Jailbreaking black box large language models in twenty queries.",
            "Jailbreakbench: An open robustness benchmark for jailbreaking large language models.",
            "Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, march 2023.",
            "A survey of chain of thought reasoning: Advances, frontiers and future.",
            "The llama 3 herd of models.",
            "Finbert: A large language model for extracting information from financial text.",
            "Chatgpt for good? on opportunities and challenges of large language models for education.",
            "Large language models are zero-shot reasoners.",
            "Scaling laws for neural language models.",
            "Artprompt: Ascii art-based jailbreak attacks against aligned llms.",
            "Jailbreakzoo: Survey, landscapes, and horizons in jailbreaking large language and vision-language models.",
            "A comprehensive study of jailbreak attack versus defense for large language models.",
            "Qwen2 technical report.",
            "Universal and transferable adversarial attacks on aligned language models."
        ]
    },
    {
        "index": 112,
        "title": "Heuristic-Induced Multimodal Risk Distribution Jailbreak Attack for Multimodal Large Language Models",
        "publication_date": "2025-01-03",
        "references": [
            "Gpt-4 technical report",
            "Gemini: A family of highly capable multimodal models",
            "Qwen technical report",
            "Qwen-vl: A frontier large vision-language model with versatile abilities",
            "Qwen-vl: A versatile vision-language model for understanding, localization, text reading, and beyond",
            "Jailbreaking black box large language models in twenty queries",
            "Scaling rectified flow transformers for high-resolution image synthesis",
            "Chatglm: A family of large language models from glm-130b to glm-4 all tools",
            "Adv-watermark: A novel watermark perturbation for adversarial examples",
            "Global challenge for safe and secure llms track 1",
            "Improved techniques for optimization-based jailbreaking on large language models",
            "How many unicorns are in this image? A safety evaluation benchmark for vision llms",
            "White-box multimodal jailbreaks against large vision-language models",
            "Jailbreakv-28k: A benchmark for assessing the robustness of multimodal large language models against jailbreak attacks",
            "Harmbench: A standardized evaluation framework for automated red teaming and robust refusal",
            "Minigpt-4: Enhancing vision-language understanding with advanced large language models",
            "Llama: Open and efficient foundation language models",
            "Llama 2: Open foundation and fine-tuned chat models",
            "How johnny can persuade llms to jail-break them: Rethinking persuasion to challenge ai safety by humanizing llms",
            "Images are achilles’ heel of alignment: Exploiting visual vulnerabilities for jail-breaking multimodal large language models",
            "Red teaming visual language models",
            "A survey on multimodal large language models"
        ]
    },
    {
        "index": 113,
        "title": "Hide Your Malicious Goal Into Benign Narratives: Jailbreak Large Language Models through Carrier Articles",
        "publication_date": "2025-02-07",
        "references": [
            "Chatgpt for software security: Exploring the strengths and limitations of chatgpt in the security applications",
            "Repair Is Nearly Generation: Multilingual Program Repair with LLMs",
            "Evaluating large language models for real-world vulnerability repair in c/c++ code",
            "Beavertails: Towards improved safety alignment of llm via a human-preference dataset",
            "Human-instruction-free llm self-alignment with limited samples",
            "How alignment and jailbreak work: Explain llm safety through intermediate hidden states",
            "Training language models to follow instructions with human feedback",
            "Training a helpful and harmless assistant with reinforcement learning from human feedback",
            "Scaling monosemanticity: Extracting interpretable features from claude 3 sonnet",
            "Jailbroken: How does llm safety training fail?",
            "Catastrophic jailbreak of open-source llms via exploiting generation",
            "Autodan: Generating stealthy jailbreak prompts on aligned large language models",
            "Universal and transferable adversarial attacks on aligned language models",
            "Jailbreaking black box large language models in twenty queries",
            "Low-resource languages jailbreak gpt-4",
            "Jailbreaking chatgpt via prompt engineering: An empirical study",
            "Gptfuzzer: Red teaming large language models with auto-generated jailbreak prompts",
            "Do llms exhibit human-like reasoning? evaluating theory of mind in llms for open-ended responses",
            "Do large language models understand logic or just mimick context?",
            "Can llms reason with rules? logic scaffolding for stress-testing and improving llms",
            "Attention is all you need",
            "Attention mechanism, transformers, bert, and gpt: tutorial and survey",
            "Wordnet",
            "Security and privacy challenges of large language models: A survey",
            "Prompt injection attack against llm-integrated applications",
            "Neural machine translation by jointly learning to align and translate",
            "Effective approaches to attention-based neural machine translation",
            "Efficient streaming language models with attention sinks",
            "H2o: Heavy-hitter oracle for efficient generative inference of large language models",
            "Shannon information and kolmogorov complexity",
            "Lost in the middle: How language models use long contexts",
            "Autoprompt: Eliciting knowledge from language models with automatically generated prompts",
            "Jailbreakbench: An open robustness benchmark for jailbreaking large language models",
            "Jailbreak chat",
            "Cold-attack: Jailbreaking llms with stealthiness and controllability",
            "Make them spill the beans! coercive knowledge extraction from (production) llms",
            "Fine-tuning aligned language models compromises safety, even when users do not intend to!",
            "Exploiting programmatic behavior of llms: Dual-use through standard security attacks",
            "Fuzzllm: A novel and universal fuzzing framework for proactively discovering jailbreak vulnerabilities in large language models",
            "Masterkey: Automated jailbreaking of large language model chatbots",
            "Jailbreaker: Automated jailbreak across multiple large language model chatbots"
        ]
    },
    {
        "index": 114,
        "title": "HIJACK RAG: Hijacking Attacks against Retrieval-Augmented Large Language Models",
        "publication_date": "2024-10-30",
        "references": [
            "Self-rag: Learning to retrieve, generate, and critique through self-reflection.",
            "LangChain.",
            "Chatlaw: Open-source legal large language model with integrated external knowledge bases.",
            "The Llama 3 Herd of Models.",
            "Hot-Flip: White-Box Adversarial Examples for Text Classification.",
            "ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools.",
            "Not what you’ve signed up for: Compromising real-world llm-integrated applications with indirect prompt injection.",
            "REALM: retrieval-augmented language model pretraining.",
            "Towards unsupervised dense information retrieval with contrastive learning.",
            "Baseline defenses for adversarial attacks against aligned language models.",
            "Survey of hallucination in natural language generation.",
            "A statistical interpretation of term specificity and its application in retrieval.",
            "Dense Passage Retrieval for Open-Domain Question Answering.",
            "Natural questions: a benchmark for question answering research.",
            "Retrieval-augmented generation for knowledge-intensive nlp tasks.",
            "LlamaIndex.",
            "Recall: A benchmark for llms robustness against external counterfactual knowledge.",
            "MS MARCO: A Human Generated MAchine Reading COmprehension Dataset.",
            "GPT-4 Technical Report.",
            "Fine-tuning or retrieval? comparing knowledge injection in llms.",
            "Check your facts and try again: Improving large language models with external knowledge and automated feedback.",
            "Ignore Previous Prompt: Attack Techniques For Language Models.",
            "Kerckhoffs’ principle.",
            "Language Models as Knowledge Bases?",
            "State of AI Report 2024.",
            "How Much Knowledge Can You Pack Into the Parameters of a Language Model?",
            "Ignore this title and HackAPrompt: Exposing systemic vulnerabilities of LLMs through a global prompt hacking competition.",
            "LLaMA: Open and Efficient Foundation Language Models.",
            "Llama 2: Open foundation and fine-tuned chat models.",
            "Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions.",
            "Self-Knowledge Guided Retrieval Augmentation for Large Language Models.",
            "Approximate Nearest Neighbor Negative Contrastive Learning for Dense Text Retrieval.",
            "HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering.",
            "Poisoning Retrieval Corpora by Injecting Adversarial Passages.",
            "DocPrompting: Generating Code by Retrieving the Docs.",
            "Poison-drag: Knowledge poisoning attacks to retrieval-augmented generation of large language models."
        ]
    },
    {
        "index": 115,
        "title": "Explain LLM Safety through Intermediate Hidden States",
        "publication_date": "2024-06-13",
        "references": [
            "Gpt-4 technical report",
            "GQA: Training generalized multi-query transformer models from multi-head checkpoints",
            "Foundational challenges in assuring alignment and safety of large language models",
            "Training a helpful and harmless assistant with reinforcement learning from human feedback",
            "Eliciting latent predictions from transformers with the tuned lens",
            "Managing extreme ai risks amid rapid progress",
            "Language models can explain neurons in language models",
            "Are aligned neural networks adversarially aligned?",
            "Jailbreakbench: An open robustness benchmark for jail-breaking large language models",
            "Jailbreaking black box large language models in twenty queries",
            "Support-vector networks",
            "Jailbreaker: Automated jailbreak across multiple large language model chatbots",
            "Mistral 7b",
            "Certifying llm safety against adversarial prompting",
            "Deepinception: Hypnotize large language model to be jailbreaker",
            "The unlocking spell on base llms: Rethinking alignment via in-context learning",
            "Generating stealthy jailbreak prompts on aligned large language models",
            "Trustworthy LLMs: a survey and guideline for evaluating large language models’ alignment",
            "Interpreting GPT: The logit lens",
            "In-context learning and induction heads",
            "Training language models to follow instructions with human feedback",
            "Scikit-learn: Machine learning in python",
            "The refinedweb dataset for falcon llm: outperforming curated corpora with web data, and web data only",
            "Learning representations by back-propagating errors",
            "A strongreject for empty jailbreaks",
            "Scaling monosemanticity: Extracting interpretable features from claude 3 sonnet",
            "Function vectors in large language models",
            "Llama: Open and efficient foundation language models",
            "Llama 2: Open foundation and fine-tuned chat models",
            "Visualizing data using t-sne",
            "Label words are anchors: An information flow perspective for understanding in-context learning",
            "Jailbroken: How does llm safety training fail?",
            "Jailbreak and guard aligned language models with only few in-context demonstrations",
            "Explainability for large language models: A survey",
            "Judging llm-as-a-judge with mt-bench and chatbot arena",
            "Lima: Less is more for alignment",
            "Universal and transferable adversarial attacks on aligned language models"
        ]
    },
    {
        "index": 116,
        "title": "Defending against Jailbreak Attacks with Hidden State Filtering",
        "publication_date": "2024-08-31",
        "references": [
            "Gpt-4 technical report",
            "Detecting language model attacks with perplexity",
            "Training a helpful and harmless assistant with reinforcement learning from human feedback",
            "Safety-Tuned LLa-MAs: Lessons From Improving the Safety of Large Language Models that Follow Instructions",
            "Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality",
            "WizardLM-30B-Uncensored",
            "Free dolly: Introducing the world’s first truly open instruction-tuned llm",
            "A Wolf in Sheep’s Clothing: Generalized Nested Jailbreak Prompts can Fool Large Language Models Easily",
            "Should chatgpt be biased? challenges and risks of bias in large language models",
            "Controllable Preference Optimization: Toward Controllable Multi-Objective Alignment",
            "Curiosity-driven Red-teaming for Large Language Models",
            "Catastrophic Jailbreak of Open-source LLMs via Exploiting Generation",
            "Baseline defenses for adversarial attacks against aligned language models",
            "Aligner: Achieving efficient alignment through weak-to-strong correction",
            "Beavertails: Towards improved safety alignment of llm via a human-preference dataset",
            "Survey of hallucination in natural language generation",
            "Mistral 7B",
            "Learning diverse attacks on large language models for robust red-teaming and safety tuning",
            "Deepinception: Hypnotize large language model to be jail-breaker",
            "AutoDAN: Generating Stealthy Jailbreak Prompts on Aligned Large Language Models",
            "Jailbreaking chatgpt via prompt engineering: An empirical study",
            "The Llama 3 Herd of Models",
            "Training language models to follow instructions with human feedback",
            "Llm self defense: By self examination, llms know they are being tricked",
            "Universal Adversarial Triggers for Attacking and Analyzing NLP",
            "Mitigating fine-tuning jailbreak attack with backdoor enhanced alignment",
            "Jailbroken: How does llm safety training fail?",
            "Jailbreak and guard aligned language models with only few in-context demonstrations",
            "Ethical and social risks of harm from language models",
            "Defending chatgpt against jailbreak attack via self-reminders",
            "Safedecoding: Defending against jailbreak attacks via safety-aware decoding",
            "Jailbreak Attacks and Defenses Against Large Language Models: A Survey",
            "Gptfuzzer: Red teaming large language models with auto-generated jailbreak prompts",
            "On prompt-driven safeguarding for large language models",
            "EasyJailbreak: A Unified Framework for Jailbreaking Large Language Models",
            "Universal and transferable adversarial attacks on aligned language models"
        ]
    },
    {
        "index": 117,
        "title": "HTS-Attack: Heuristic Token Search for Jailbreaking Text-to-Image Models",
        "publication_date": "2024-12-15",
        "references": [
            "Gpt-4 technical report",
            "“real attackers don’t compute gradients”: bridging the gap between adversarial ml research and practice",
            "Decision-based adversarial attacks: Reliable attacks against black-box machine learning models",
            "Divide-and-conquer attack: Harnessing the power of llm to bypass the censorship of text-to-image generation model",
            "Detoxify",
            "Erasing Concepts from Diffusion Models",
            "Evaluating the Robustness of Text-to-image Diffusion Models against Real-world Attacks",
            "Boosting transferability in vision-language attacks via diversification along the intersection region of adversarial trajectory",
            "Bae: Bert-based adversarial examples for text classification",
            "A survey on transferability of adversarial examples across deep neural networks",
            "Efficiently adversarial examples generation for visual-language models under targeted transfer scenarios using diffusion models",
            "Natural & adversarial bokeh rendering via circle-of-confusion predictive network",
            "A la:Naturalness-aware adversarial lightness attack",
            "Texture re-scalable universal adversarial perturbation",
            "Personalization as a shortcut for few-shot backdoor attack against text-to-image diffusion models",
            "Perception-guided jailbreak against text-to-image models",
            "Comdefend: An efficient image compression model to defend adversarial examples",
            "Adv-watermark: A novel watermark perturbation for adversarial examples",
            "Las-at: adversarial training with learnable attack strategy",
            "Semantic-aligned adversarial evolution triangle for high-transferability vision-language attack",
            "Global challenge for safe and secure llms track 1",
            "Improved techniques for optimization-based jailbreaking on large language models",
            "Improving fast adversarial training with prior-guided knowledge",
            "Is bert really robust? a strong baseline for natural language attack on text classification and entailment",
            "Character As Pixels: A Controllable Prompt Adversarial Attacking Framework for Black-Box Text Guided Image Generation Models",
            "Ablating Concepts in Text-to-Image Diffusion Models",
            "Holistic Evaluation of Text-To-Image Models",
            "Textbugger: Generating adversarial text against real-world applications",
            "Bootstrapping language-image pre-training for unified vision-language understanding and generation",
            "Safegen: Mitigating unsafe content generation in text-to-image models",
            "Adversarial example does good: Preventing painting imitation from diffusion models via adversarial examples",
            "Intriguing Properties of Text-guided Diffusion Models",
            "Latent guard: a safety framework for text-to-image generation",
            "Jailbreaking prompt attack: A controllable adversarial attack against diffusion models",
            "Coljailbreak: Collaborative generation and editing for jailbreaking text-to-image deep generation",
            "Towards deep learning models resistant to adversarial attacks",
            "Black box adversarial prompting for foundation models",
            "Midjourney",
            "NSFW-text-classifier",
            "OpenAI-Moderation",
            "Unsafe diffusion: On the generation of unsafe images and hateful memes from text-to-image models",
            "Learning transferable visual models from natural language supervision",
            "Red-Teaming the Stable Diffusion Safety Filter",
            "High-Resolution Image Synthesis with Latent Diffusion Models",
            "Safety Checker nested in Stable Diffusion",
            "Raising the cost of malicious ai-powered image editing",
            "Can Machines Help Us Answering Question 16 in Datasheets, and In Turn Reflecting on Inappropriate Content?",
            "Safe Latent Diffusion: Mitigating Inappropriate Degeneration in Diffusion Models",
            "LAION-5B: An Open Large-scale Dataset for Training Next Generation Image-text Models",
            "Ring-A-Bell! How Reliable are Concept Removal Methods for Diffusion Models?",
            "On the multi-modal vulnerability of diffusion models",
            "Cheating suffix: Targeted attack to text-to-image diffusion models with multi-modal priors",
            "On the multi-modal vulnerability of diffusion models",
            "Sneakyprompt: Jailbreaking text-to-image generative models",
            "Mma-diffusion: Multimodal attack on diffusion models",
            "Guardt2i: Defending text-to-image models from adversarial prompts",
            "To generate or not? safety-driven unlearned diffusion models are still easy to generate unsafe images... for now",
            "A Pilot Study of Query-Free Adversarial Attack against Stable Diffusion"
        ]
    },
    {
        "index": 118,
        "title": "Humanizing the Machine: Proxy Attacks to Mislead LLM Detectors",
        "publication_date": "2025-02-22",
        "references": [
            "The falcon series of open language models.",
            "Real or fake? learning to discriminate machine from human generated text.",
            "Fast-detectgpt: Efficient zero-shot detection of machine-generated text via conditional probability curvature.",
            "Gpt-neo: Large scale autoregressive language modeling with mesh-tensorflow.",
            "Findings of the 2016 conference on machine translation (wmt16).",
            "Rank analysis of incomplete block designs: I. the method of paired comparisons.",
            "Evade chatgpt detectors via a single space.",
            "Accelerating large language model decoding with speculative sampling.",
            "Gpt-sentinel: Distinguishing human and chatgpt generated content.",
            "Rouge: A package for automatic evaluation of summaries.",
            "Evading ai-generated content detectors using homoglyphs.",
            "Parameter-efficient fine-tuning of large-scale pre-trained language models.",
            "By chatgpt fool scientists.",
            "Llama: Open and efficient foundation language models.",
            "Llama 2: Open foundation and fine-tuned chat models.",
            "Intrinsic dimension estimation for robust detection of ai-generated texts.",
            "Authorship attribution for neural text generation.",
            "Spotting llms with binoculars: Zero-shot detection of machine-generated text.",
            "Deal: Decoding-time alignment for large language models.",
            "Mixtral of experts.",
            "Pubmedqa: A dataset for biomedical research question answering.",
            "Watermark stealing in large language models.",
            "Will chatgpt get you caught? rethinking of plagiarism detection.",
            "Args: Alignment as reward-guided search.",
            "A watermark for large language models.",
            "On the reliability of watermarks for large language models.",
            "Deepfake text detection in the wild.",
            "Tuning language models by proxy.",
            "Decoding-time realignment of language models.",
            "Check me if you can: Detecting chatgpt-generated academic writing using checkgpt.",
            "Large language models can be guided to evade ai-generated text detection.",
            "Provable robust watermarking for ai-generated text.",
            "Humanizing machine-generated content: Evading ai-text detection through adversarial attack.",
            "Gpt paternity test: Gpt generated text detection with gpt genetic inheritance.",
            "Tinyllama: An open-source small language model.",
            "Bertscore: Evaluating text generation with bert.",
            "A survey on detection of llms-generated content."
        ]
    },
    {
        "index": 119,
        "title": "Image-Perfect Imperfections: Safety, Bias, and Authenticity in the Shadow of Text-To-Image Model Evolution",
        "publication_date": "2024-10-17",
        "references": [
            "https://midjourney.com/ . 1, 14",
            "https://www.adobe.com/sensei/generative-ai/firefly.html . 1",
            "https://artificialintelligenceact.eu/ . 3, 10",
            "https://www.gov.uk/government/organisations/ai-safety-institute . 3",
            "https://www.4chan.org/ . 4",
            "https://lexica.art/ . 4, 6",
            "https://huggingface.co/CompVis/stable-diffusion-safety-checker . 4, 18",
            "https://unctad.org/press-material/facts-and-figures-7 . 10",
            "https://github.com/borisdayma/dalle-mini . 13",
            "https://openai.com/dall-e-3 . 13",
            "The Silicon Ceiling: Auditing GPT’s Race and Gender Biases in Hiring",
            "Easily Accessible Text-to-Image Generation Amplifies Demographic Stereotypes at Large Scale",
            "Multimodal Datasets: Misogyny, Pornography, and Malignant Stereotypes",
            "Mitigating Inappropriateness in Image Generation: Can there be Value in Reflecting the Worlds Ugli- ness?",
            "Microsoft COCO Captions: Data Collection and Evaluation Server",
            "Race and Economic Opportunity in the United States: an Intergenerational Perspective",
            "Bias in the Air: A Nationwide Exploration of Teachers’ Implicit Racial Attitudes, Aggregate Bias, and Student Outcomes",
            "Prompting4Debugging: Red-Teaming Text-to-Image Diffusion Models by Finding Problematic Prompts",
            "Dall-eval: Probing the Reasoning Skills and Social Biases of Text-to-Image Generation Models",
            "On The Detection of Synthetic Images Generated by Diffusion Models",
            "Understanding poverty",
            "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
            "Diffusion Models Beat GANs on Image Synthesis",
            "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
            "A Friendly Face: Do Text-to-Image Systems Rely on Stereotypes when the Input is Under-Specified?",
            "Fair Diffusion: Instructing Text-to-Image Generation Models on Fairness",
            "An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion",
            "Uncurated Image-Text Datasets: Shedding Light on Demographic Bias",
            "’Person’ == Light-skinned, Western Man, and Sexualization of Women of Color: Stereotypes in Stable Diffusion",
            "Generative Adversarial Networks",
            "Minimax Filter: Learning to Preserve Privacy from Inference Attacks",
            "Situated Knowledges: The Science Question in Feminism and the Privilege of Partial Perspective",
            "Deep Residual Learning for Image Recognition",
            "Beyond the Spectrum: Detecting Deepfakes via Re-Synthesis",
            "Denoising Diffusion Probabilistic Models",
            "FairFace: Face Attribute Dataset for Balanced Race, Gender, and Age for Bias Measurement and Mitigation",
            "You Keep Using That Word: Ways of Thinking about Gender in Computing Research",
            "MiVOLO: Multi-input Transformer for Age and Gender Estimation",
            "Stable Bias: Analyzing Societal Representations in Diffusion Models",
            "Indicators of Poverty and Social Exclusion in a Global Context",
            "Exploiting Visual Artifacts to Expose Deepfakes and Face Manipulations",
            "A Survey on Bias and Fairness in Machine Learning",
            "SDEdit: Guided Image Synthesis and Editing with Stochastic Differential Equations",
            "Learning Transferable Visual Models From Natural Language Supervision",
            "Hierarchical Text-Conditional Image Generation with CLIP Latents",
            "Zero-Shot Text-to-Image Generation",
            "Red-Teaming the Stable Diffusion Safety Filter",
            "HyperExtended Light-Face: A Facial Attribute Analysis Framework",
            "The Bias Amplification Paradox in Text-to-Image Generation",
            "DE-FAKE: Detection and Attribution of Fake Images Generated by Text-to-Image Diffusion Models",
            "Score-Based Generative Modeling through Stochastic Differential Equations",
            "High-Resolution Image Synthesis with Latent Diffusion Models",
            "U-Net: Convolutional Networks for Biomedical Image Segmentation",
            "Measuring Social Biases in Grounded Vision and Language Embeddings",
            "DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation",
            "Artificial Fingerprinting for Generative Models: Rooting Deepfake Attribution in Training Data",
            "ITI-GEN: Inclusive Text-to-Image Generation",
            "The Unreasonable Effectiveness of Deep Features as a Perceptual Metric",
            "Deepfake Generation and Detection, A Survey",
            "To Generate or Not? Safety-Driven Unlearned Diffusion Models Are Still Easy To Generate Unsafe Images ... For Now"
        ]
    },
    {
        "index": 120,
        "title": "Image-to-Text Logic Jailbreak: Your Imagination can Help You Do Anything",
        "publication_date": "2024-08-26",
        "references": [
            "Gpt-4 technical report.",
            "Many-shot jailbreaking.",
            "Qwen-vl: A versatile vision-language model for understanding, localization, text reading, and beyond.",
            "Are aligned neural networks adversarially aligned?",
            "Jailbreaking black box large language models in twenty queries.",
            "Can llms’ tuning methods work in medical multmodal domain?",
            "Egothink: Evaluating first-person perspective thinking capability of vision-language models.",
            "Large language models as optimizers.",
            "Bert: Pre-training of deep bidirectional transformers for language understanding.",
            "Explaining and harnessing adversarial examples.",
            "Model extraction and adversarial transferability, your bert is vulnerable!",
            "Minicpm: Unveiling the potential of small language models with scalable training strategies.",
            "Catastrophic jailbreak of open-source llms via exploiting generation.",
            "Automatic jailbreaking of the text-to-image generative ai systems.",
            "Visualwebarena: Evaluating multimodal agents on realistic visual web tasks.",
            "Open sesame! universal black box jailbreaking of large language models.",
            "Multi-step jailbreaking privacy attacks on chatgpt.",
            "Using adversarial attacks to reveal the statistical bias in machine reading comprehension models.",
            "Expanding scope: Adapting english adversarial attacks to chinese.",
            "Improved baselines with visual instruction tuning.",
            "Llava-next: Improved reasoning, ocr, and world knowledge.",
            "Roberta: A robustly optimized bert pretraining approach.",
            "Mmbench: Is your multi-modal model an all-around player?",
            "Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts.",
            "Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity.",
            "Fairness-guided few-shot prompting for large language models.",
            "Chartqa: A benchmark for question answering about charts with visual and logical reasoning.",
            "Infographicvqa.",
            "Introducing chatgpt.",
            "Visual language integration: A survey and open challenges.",
            "Sdxl: Improving latent diffusion models for high-resolution image synthesis.",
            "Fine-tuning aligned language models compromises safety, even when users do not intend to!",
            "Visual adversarial examples jailbreak aligned large language models.",
            "Autoprompt: Eliciting knowledge from language models with automatically generated prompts.",
            "Towards vqa models that can read.",
            "Flowchartqa: the first large-scale benchmark for reasoning over flowcharts.",
            "Cogvlm: Visual expert for pretrained language models.",
            "Lvlm-ehub: A comprehensive evaluation benchmark for large vision-language models.",
            "Sneakyprompt: Evaluating robustness of text-to-image generative models’ safety filters.",
            "The dawn of lmms: Preliminary explorations with gpt-4v (ision).",
            "Bridge the modality and capacity gaps in vision-language model selection.",
            "A survey on multimodal large language models.",
            "Gptfuzzer: Red teaming large language models with auto-generated jailbreak prompts.",
            "Discovering universal semantic triggers for text-to-image synthesis.",
            "A survey on universal adversarial attack.",
            "First multi-dimensional evaluation of flowchart comprehension for multimodal large language models.",
            "Vision-language models for vision tasks: A survey.",
            "Adversarial attacks on deep-learning models in natural language processing: A survey.",
            "Investigating copyright issues of diffusion models under practical scenarios.",
            "How far are we from intelligent visual deductive reasoning?",
            "On evaluating adversarial robustness of large vision-language models.",
            "Calibrate before use: Improving few-shot performance of language models.",
            "Universal and transferable adversarial attacks on aligned language models.",
            "Is the system message really important to jailbreaks in large language models?"
        ]
    },
    {
        "index": 122,
        "title": "Improved Few-Shot Jailbreaking Can Circumvent Aligned Language Models and Their Defenses",
        "publication_date": "2024-10-30",
        "references": [
            "Detecting language model attacks with perplexity",
            "Jailbreaking leading safety-aligned llms with simple adaptive attacks",
            "Many-shot jailbreaking",
            "Qwen technical report",
            "Training a helpful and harmless assistant with reinforcement learning from human feedback",
            "Language models are few-shot learners",
            "Defending against alignment-breaking attacks via robustly aligned llm",
            "Are aligned neural networks adversarially aligned?",
            "Jailbreaking black box large language models in twenty queries",
            "Jailbreakbench: An open robustness benchmark for jailbreaking large language models",
            "Jailbreaker: Automated jailbreak across multiple large language model chatbots",
            "Multilingual jailbreak challenges in large language models",
            "Badllama: cheaply removing safety fine-tuning from llama 2-chat 13b",
            "Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned",
            "Not what you’ve signed up for: Compromising real-world llm-integrated applications with indirect prompt injection",
            "Query-based adversarial prompt generation",
            "Llm self defense: By self examination, llms know they are being tricked",
            "Gradient cuff: Detecting jailbreak attacks on large language models by exploring refusal loss landscapes",
            "Token-level adversarial prompt detection based on perplexity measures and contextual information",
            "Catastrophic jailbreak of open-source llms via exploiting generation",
            "Llama guard: Llm-based input-output safeguard for human-ai conversations",
            "Baseline defenses for adversarial attacks against aligned language models",
            "Defending large language models against jailbreak attacks via semantic smoothing",
            "Mistral 7b",
            "Pretraining language models with human preferences",
            "Llama 3 model card",
            "Studious bob fight back against jailbreaking via prompt adversarial tuning",
            "Universal black box jailbreaking of large language models",
            "Lora fine-tuning efficiently undoes safety training in llama 2-chat 70b",
            "Deepinception: Hypnotize large language model to be jailbreaker",
            "Rain: Your language models can align themselves without finetuning",
            "Amplegcg: Learning a universal and transferable generative model of adversarial suffixes for jailbreaking both open and closed llms",
            "The unlocking spell on base llms: Rethinking alignment via in-context learning",
            "Lost in the middle: How language models use long contexts",
            "Autodan: Generating stealthy jailbreak prompts on aligned large language models",
            "Jailbreaking chatgpt via prompt engineering: An empirical study",
            "Prp: Propagating universal perturbations to attack large language model guard-rails",
            "Harmbench: A standardized evaluation framework for automated red teaming and robust refusal",
            "Tree of attacks: Jailbreaking black-box llms automatically",
            "Gpt-4 technical report",
            "Red teaming language models with language models",
            "Fine-tuning aligned language models compromises safety, even when users do not intend to!",
            "Tricking llms into disobedience: Understanding, analyzing, and preventing jailbreaks",
            "The convergence of the random search method in the extremal control of a many parameter system",
            "Sentence embeddings using siamese bert-networks",
            "Smoothllm: Defending large language models against jailbreaking attacks",
            "Identifying the risks of lm agents with an lm-emulated sandbox",
            "Spml: A dsl for defending language models against prompt attacks",
            "\"do anything now\": Characterizing and evaluating in-the-wild jailbreak prompts on large language models",
            "Pal: Proxy-guided black-box attack on large language models",
            "Evil geniuses: Delving into the safety of llm-based agents",
            "Llama 2: Open foundation and fine-tuned chat models",
            "Tensor trust: Interpretable prompt injection attacks from an online game",
            "Open-chat: Advancing open-source language models with mixed-quality data",
            "From noise to clarity: Unraveling the adversarial suffix of large language model attacks via translation of text embeddings",
            "Defending llms against jailbreaking attacks via backtranslation",
            "Jailbroken: How does llm safety training fail?",
            "Jailbreak and guard aligned language models with only few in-context demonstrations",
            "Defending chatgpt against jailbreak attack via self-reminders",
            "Safedecoding: Defending against jailbreak attacks via safety-aware decoding",
            "Shadow alignment: The ease of subverting safely-aligned language models",
            "Low-resource languages jailbreak gpt-4",
            "Gpt-4 is too smart to be safe: Stealthy chat with llms via cipher",
            "Rigorllm: Resilient guardrails for large language models against undesired content",
            "How johnny can persuade llms to jailbreak them: Rethinking persuasion to challenge ai safety by humanizing llms",
            "Defending large language models against jailbreaking attacks through goal prioritization",
            "Robust prompt optimization for defending language models against jailbreaking attacks",
            "Defending jailbreak prompts via in-context adversarial game",
            "Autodan: Automatic and interpretable adversarial attacks on large language models",
            "Universal and transferable adversarial attacks on aligned language models"
        ]
    },
    {
        "index": 123,
        "title": "Improved Generation of Adversarial Examples Against Safety-aligned LLMs",
        "publication_date": "2024-11-01",
        "references": [
            "Phi-3 technical report: A highly capable language model locally on your phone",
            "Refusal in language models is mediated by a single direction",
            "Constitutional ai: Harmlessness from ai feedback",
            "Are aligned neural networks adversarially aligned?",
            "Explore, establish, exploit: Red teaming language models from scratch",
            "Jailbreaking black box large language models in twenty queries",
            "Jailbreaker: Automated jailbreak across multiple large language model chatbots",
            "Searching for a robust neural architecture in four gpu hours",
            "Boosting adversarial attacks with momentum",
            "Hotflip: White-box adversarial examples for text classification",
            "Patchscope: A unifying framework for inspecting hidden representations of language models",
            "Improving alignment of dialogue agents via targeted human judgements",
            "Lgv: Boosting adversarial example transferability from large geometric vicinity",
            "Simple black-box adversarial attacks",
            "Gradient-based adversarial attacks against text transformers",
            "Backpropagating linearly improves transferability of adversarial examples",
            "An intermediate-level attack framework on the basis of linear regression",
            "Enhancing adversarial example transferability with an intermediate level attack",
            "Mistral 7b",
            "Automatically auditing large language models via discrete optimization",
            "Pretraining language models with human preferences",
            "Adversarial machine learning at scale",
            "The power of scale for parameter-efficient prompt tuning",
            "Yet another intermediate-leve attack",
            "Improving transferability of adversarial examples via bayesian attacks",
            "Improving adversarial transferability via intermediate-level perturbation decay",
            "Making substitute models more bayesian can enhance transferability of adversarial examples",
            "Towards evaluating transfer-based attacks systematically, practically, and fairly",
            "A theoretical view of linear backpropagation and its convergence",
            "Autodan: Generating stealthy jailbreak prompts on aligned large language models",
            "Delving into transferable adversarial examples and black-box attacks",
            "Harmbench: A standardized evaluation framework for automated red teaming and robust refusal",
            "Tree of attacks: Jailbreaking black-box llms automatically",
            "Locating and editing factual associations in gpt",
            "Mass-editing memory in a transformer",
            "On improving adversarial transferability of vision transformers",
            "interpreting gpt: the logit lens",
            "Training language models to follow instructions with human feedback",
            "Transferability in machine learning: from phenomena to black-box attacks using adversarial samples",
            "Llama: Open and efficient foundation language models",
            "Universal adversarial triggers for attacking and analyzing nlp",
            "Feature importance-aware transferable adversarial attacks",
            "Jailbroken: How does llm safety training fail?",
            "Jailbreak and guard aligned language models with only few in-context demonstrations",
            "Towards transferable adversarial attacks on vision transformers",
            "Hard prompts made easy: Gradient-based discrete optimization for prompt tuning and discovery",
            "Rethinking the security of skip connections in resnet-like neural networks",
            "Improving transferability of adversarial examples with input diversity",
            "Uncovering safety risks in open-source llms through concept activation vector",
            "How johnny can persuade llms to jailbreak them: Rethinking persuasion to challenge ai safety by humanizing llms",
            "Improving adversarial transferability via neuron attribution-based attacks",
            "Universal and transferable adversarial attacks on aligned language models"
        ]
    },
    {
        "index": 124,
        "title": "Improved Large Language Model Jailbreak Detection via Pretrained Embeddings",
        "publication_date": "2024-12-02",
        "references": [
            "Bad characters: Imperceptible nlp attacks.",
            "Xgboost: A scalable tree boosting system.",
            "garak: A Framework for Security Probing Large Language Models.",
            "Summon a demon and bind it: A grounded theory of llm red teaming in the wild.",
            "Mistral 7B.",
            "Dense Passage Retrieval for Open-Domain Question Answering.",
            "NV-Embed: Improved Techniques for Training LLMs as Generalist Embedding Models.",
            "ToxicChat: Unveiling Hidden Challenges of Toxicity Detection in Real-World User-AI Conversation.",
            "AutoDAN: Generating Stealthy Jailbreak Prompts on Aligned Large Language Models.",
            "Tree of attacks: Jailbreaking black-box llms automatically.",
            "Embedding And Clustering Your Data Can Improve Contrastive Pretraining.",
            "Distributed representations of words and phrases and their compositionality.",
            "CVE-2019-20634.",
            "Nemo guardrails: A toolkit for controllable and safe llm applications with programmable rails.",
            "Characterizing and evaluating in-the-wild jailbreak prompts on large language models.",
            "Intriguing properties of neural networks.",
            "Text embeddings by weakly-supervised contrastive pre-training.",
            "Defending llms against jailbreaking attacks via backtranslation.",
            "Jailbroken : How does llm safety training fail?",
            "Fundamental limitations of alignment in large language models.",
            "Increasing confidence in adversarial robustness evaluations.",
            "Improving alignment and robustness with circuit breakers.",
            "Universal and transferable adversarial attacks on aligned language models."
        ]
    },
    {
        "index": 125,
        "title": "Improved Techniques for Optimization-Based Jailbreaking on Large Language Models",
        "publication_date": "2024-06-05",
        "references": [
            "Jailbreaking leading safety-aligned llms with simple adaptive attacks.",
            "Special characters attack: Toward scalable training data extraction from large language models.",
            "A survey on evaluation of large language models.",
            "Jailbreaking black box large language models in twenty queries.",
            "Red teaming gpt-4v: Are gpt-4v safe against uni/multi-modal jailbreak attacks?",
            "Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality.",
            "Comprehensive assessment of jailbreak attacks against llms.",
            "Jailbreaker: Automated jailbreak across multiple large language model chatbots.",
            "Qlora: Efficient finetuning of quantized llms.",
            "A survey of adversarial defenses and robustness in nlp.",
            "Responsible generative ai: What to generate and what not.",
            "A survey on transferability of adversarial examples across deep neural networks.",
            "Agent smith: A single image can jailbreak one million multimodal llm agents exponentially fast.",
            "Gradient-based adversarial attacks against text transformers.",
            "Mistral 7b.",
            "Exploiting programmatic behavior of llms: Dual-use through standard security attacks.",
            "Large language models versus natural language understanding and generation.",
            "Chatgpt for good? on opportunities and challenges of large language models for education.",
            "Openassistant conversations- democratizing large language model alignment.",
            "Open sesame! universal black box jailbreaking of large language models.",
            "Query-efficient black-box red teaming via bayesian optimization.",
            "Deepinception: Hypnotize large language model to be jailbreaker.",
            "Amplegcg: Learning a universal and transferable generative model of adversarial suffixes for jailbreaking both open and closed llms.",
            "Autodan: Generating stealthy jailbreak prompts on aligned large language models.",
            "Generating stealthy jailbreak prompts on aligned large language models.",
            "Jailbreaking chatgpt via prompt engineering: An empirical study.",
            "Tdc 2023 (llm edition): The trojan detection challenge.",
            "Harmbench: A standardized evaluation framework for automated red teaming and robust refusal.",
            "Tree of attacks: Jailbreaking black-box llms automatically.",
            "Adversarial attack and defense technologies in natural language processing: A survey.",
            "Do anything now\": Characterizing and evaluating in-the-wild jailbreak prompts on large language models.",
            "Autoprompt: Eliciting knowledge from language models with automatically generated prompts.",
            "All in how you ask for it: Simple black-box method for jailbreak attacks.",
            "Verigen: A large language model for verilog code generation.",
            "Llama 2: Open foundation and fine-tuned chat models.",
            "A closer look at adversarial suffix learning for jailbreaking llms.",
            "Jailbroken: How does llm safety training fail?",
            "Hard prompts made easy: Gradient-based discrete optimization for prompt tuning and discovery.",
            "Tastle: Distract large language models for automatic jailbreak attack.",
            "Cheating suffix: Targeted attack to text-to-image diffusion models with multi-modal priors.",
            "Low-resource languages jailbreak gpt-4.",
            "Gptfuzzer: Red teaming large language models with auto-generated jailbreak prompts.",
            "Gpt-4 is too smart to be safe: Stealthy chat with llms via cipher.",
            "How johnny can persuade llms to jailbreak them: Rethinking persuasion to challenge ai safety by humanizing llms.",
            "Prompting large language model for machine translation: A case study.",
            "Boosting jailbreak attack with momentum.",
            "Weak-to-strong jailbreaking on large language models.",
            "Accelerating greedy coordinate gradient via probe sampling.",
            "Easyjailbreak: A unified framework for jailbreaking large language models.",
            "Universal and transferable adversarial attacks on aligned language models."
        ]
    },
    {
        "index": 127,
        "title": "Immune: Improving Safety Against Jailbreaks in Multi-modal LLMs via Inference-Time Alignment",
        "publication_date": "2024-12-20",
        "references": [
            "Gpt-4 technicalreport",
            "Jailbreaking leading safety-aligned llms with simple adaptive attacks",
            "Vqa: Visual question answering",
            "Qwen-vl: A frontier large vision-language model with versatile abilities",
            "Liar: Leveraging alignment (best-of-n) to jailbreak llms in seconds",
            "On evaluating adversarial robustness",
            "Transferqstar: Principled decoding for llm alignment",
            "Jailbreaking black box large language models in twenty queries",
            "Jailbreakbench: An open robustness benchmark for jailbreaking large language models",
            "Dress: Instructing large vision-language models to align and interact with humans via natural language feedback",
            "Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality",
            "How robust is google’s bard to adversarial image attacks?",
            "Analyzing the inherent response tendency of llms: Real-world instructions-driven jailbreak",
            "Coca: Regaining safety-awareness of multimodal large language models with constitutional calibration",
            "Attacking large language models with projected gradient descent",
            "Fig-step: Jailbreaking large vision-language models via typographic visual prompts",
            "A systematic survey of prompt engineering on vision-language foundation models",
            "Cold-attack: Jailbreaking llms with stealthiness and controllability",
            "Ot-attack: Enhancing adversarial transferability of vision-language models via optimal transport optimization",
            "Wildguard: Open one-stop moderation tools for safety risks, jailbreaks, and refusals of llms",
            "Query-based adversarial prompt generation",
            "Curiosity-driven red-teaming for large language models",
            "A comprehensive survey of deep learning for image captioning",
            "Adversarial attacks on neural network policies",
            "Catastrophic jailbreak of open-source llms via exploiting generation",
            "Llama guard: Llm-based input-output safeguard for human-ai conversations",
            "Automatically auditing large language models via discrete optimization",
            "ARGS: Alignment as reward-guided search",
            "Rewardbench: Evaluating reward models for language modeling",
            "Learning diverse attacks on large language models for robust red-teaming and safety tuning",
            "Red teaming visual language models",
            "Rain: Your language models can align themselves without fine-tuning",
            "Improved baselines with visual instruction tuning",
            "Llava-next: Improved reasoning, ocr, and world knowledge",
            "Agentbench: Evaluating llms as agents",
            "Query-relevant images jailbreak large multi-modal models",
            "Mm-safetybench: A benchmark for safety evaluation of multimodal large language models",
            "Jailbreaking chatgpt via prompt engineering: An empirical study",
            "Jailbreakv-28k: A benchmark for assessing the robustness of multimodal large language models against jailbreak attacks",
            "Macaw-llm: Multi-modal language modeling with image, audio, video, and text integration",
            "Prp: Propagating universal perturbations to attack large language model guard-rails",
            "Jab: Joint adversarial prompting and belief augmentation",
            "Robust conversational agents against imperceptible toxicity triggers",
            "Flirt: Feedback loop in-context red teaming",
            "Controlled decoding from language models",
            "Adversarial attacks on neural network policies",
            "Training language models to follow instructions with human feedback",
            "Offsetbias: Leveraging debiased data for tuning evaluators",
            "Red teaming language models with language models",
            "Mllm-protector: Ensuring mllm’s safety without hurting performance",
            "Visual adversarial examples jailbreak aligned large language models",
            "Safety alignment should be made more than just a few tokens deep",
            "Fine-tuning aligned language models compromises safety, even when users don’t intend to!",
            "Direct preference optimization: Your language model is secretly a reward model",
            "High-resolution image synthesis with latent diffusion models",
            "On the adversarial robustness of multi-modal foundation models",
            "Plug and pray: Exploiting off-the-shelf components of multi-modal models",
            "Pal: Proxy-guided black-box attack on large language models",
            "Decodingtrust: A comprehensive assessment of trustworthiness in gpt models",
            "From noise to clarity: Unraveling the adversarial suffix of large language model attacks via translation of text embeddings",
            "Adashield: Safeguarding multimodal large language models from structure-based attack via adaptive shield prompting",
            "Helpsteer2: Open-source dataset for training top-performing reward models",
            "MMJ-Bench: A comprehensive study on jailbreak attacks and defenses for vision language models",
            "Gradient-based language model red teaming",
            "Visual question answering: A survey of methods and datasets",
            "Cognitive overload: Jailbreaking large language models with overloaded logical thinking",
            "Defending jailbreak attack in vlms via cross-modality information detector",
            "Magpie: Alignment data synthesis from scratch by prompting aligned llms with nothing",
            "Jailbreak vision language models via bi-modal adversarial prompt",
            "Rlhf-v: Towards trustworthy mllms via behavior alignment from fine-grained correctional human feedback",
            "Mm-vet: Evaluating large multimodal models for integrated capabilities",
            "Rrhf: Rank responses to align language models with human feedback without tears",
            "How johnny can persuade llms to jail-break them: Rethinking persuasion to challenge ai safety by humanizing llms",
            "Video-llama: An instruction-tuned audio-visual language model for video understanding",
            "Amutation-based method for multi-modal jailbreaking attack detection",
            "Make them spill the beans! Coercive knowledge extraction from (production) llms",
            "Weak-to-strong jailbreaking on large language models",
            "On evaluating adversarial robustness of large vision-language models",
            "Don’t say no: Jailbreaking llm by suppressing refusal"
        ]
    },
    {
        "index": 128,
        "title": "In-Context Experience Replay Facilitates Safety Red-Teaming of Text-to-Image Diffusion Models",
        "publication_date": "2025-02-12",
        "references": [
            "Automatic pseudo-harmful prompt generation for evaluating false refusals in large language models",
            "Square attack: a query-efficient black-box adversarial attack via random search",
            "A realistic threat model for large language model jailbreaks",
            "Jailbreaking black box large language models in twenty queries",
            "Conceptprune: Concept editing in diffusion models via skilled neuron pruning",
            "Prompting4debugging: Red-teaming text-to-image diffusion models by finding problematic prompts",
            "Diffzoo: A purely query-based black-box attack for red-teaming text-to-image generative model via zeroth order optimization",
            "Cogview: Mastering text-to-image generation via transformers",
            "Revisiting fundamentals of experience replay",
            "Erasing concepts from text-to-image diffusion models with few-shot unlearning",
            "Erasing concepts from diffusion models",
            "Rt-attack: Jailbreaking text-to-image models via random token",
            "Mart: Improving llm safety with multi-round automatic red-teaming",
            "Imagebind: One embedding space to bind them all",
            "Gradient-based adversarial attacks against text transformers",
            "Curiosity-driven red-teaming for large language models",
            "Receler: Reliable concept erasing of text-to-image diffusion models via lightweight erasers",
            "Race: Robust adversarial concept erasure for secure text-to-image diffusion model",
            "Automatic jailbreaking of the text-to-image generative ai systems",
            "Query-efficient black-box red teaming via bayesian optimization",
            "Large language models to enhance bayesian optimization",
            "Autodan: Generating stealthy jailbreak prompts on aligned large language models",
            "Geom-erasing: Geometry-driven removal of implicit concept in diffusion models",
            "One-dimensional adapter to rule them all: Concepts diffusion models and erasing applications",
            "Flirt: Feedback loop in-context red teaming",
            "Iteratively prompting multimodal llms to reproduce natural and ai-generated images",
            "Forget-me-not: Learning to forget in text-to-image diffusion models",
            "Defensive unlearning with adversarial training for robust concept erasure in diffusion models",
            "To generate or not? safety-driven unlearned diffusion models are still easy to generate unsafe images... for now",
            "Universal and transferable adversarial attacks on aligned language models",
            "Learning to predict by the methods of temporal differences",
            "Intriguing properties of neural networks",
            "Can machines help us answering question 16 in datasheets, and in turn reflecting on inappropriate content?",
            "Safe latent diffusion: Mitigating inappropriate degeneration in diffusion models",
            "Red-teaming the stable diffusion safety filter",
            "High-resolution image synthesis with latent diffusion models",
            "Hierarchical text-conditional image generation with clip latents",
            "Red-teaming the stable diffusion safety filter"
        ]
    },
    {
        "index": 129,
        "title": "InjecGuard: Benchmarking and Mitigating Over-defense in Prompt Injection Guardrail Models",
        "publication_date": "2024-11-24",
        "references": [
            "Are you still on track!? catching LLM task drift with activations",
            "Awesome ChatGPT Prompts Dataset",
            "Evaluating the susceptibility of pretrained language models via handcrafted adversarial examples",
            "Language models are few-shot learners",
            "Struq: Defending against prompt injection with structured queries",
            "Agentdojo: A dynamic environment to evaluate attacks and defenses for llm agents",
            "Deepset Prompt Injection Dataset",
            "Adam: A method for stochastic optimization",
            "Enhancing chat language models by scaling high-quality instructional conversations",
            "Towards next-generation intelligent assistants leveraging llm techniques",
            "Prompt Injection Test Dataset",
            "PromptGuard Prompt Injection Guardrail",
            "GPT-4o",
            "Chatbot Instruction Prompt Dataset",
            "Neural exec: Learning (and learning from) execution triggers for prompt injection attacks",
            "From Prompt Injections to SQL Injection Attacks: How Protected is Your LLM-Integrated Web Application?",
            "Ignore Previous Prompt: Attack Techniques For Language Models",
            "Fine-tuned deberta-v3-base for prompt injection detection",
            "No robots",
            "Chat-GPT Jailbreak Prompts Dataset",
            "Xstest: A test suite for identifying exaggerated safety behaviours in large language models",
            "Maatphor: Automated Variant Analysis for Prompt Injection Attacks",
            "Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter",
            "Ignore this title and hackaprompt: Exposing systemic vulnerabilities of llms through a global prompt hacking competition",
            "do anything now\": Characterizing and evaluating in-the-wild jailbreak prompts on large language models",
            "Optimization-based prompt injection attack to llm-as-a-judge",
            "Vigil-Jailbreak-ada Dataset",
            "Stanford alpaca: An instruction-following llama model",
            "Large language models in medicine",
            "Llama 2: Open foundation and fine-tuned chat models",
            "Tensor Trust: Interpretable Prompt Injection Attacks from an Online Game",
            "Open-Instruct Dataset",
            "Safeguarding Crowdsourcing Surveys from ChatGPT with Prompt Injection",
            "Prompt injection attacks against GPT-3",
            "Delimiters won't save you from prompt injection",
            "Backdooring Instruction-Tuned Large Language Models with Virtual Prompt Injection",
            "Benchmarking and Defending Against Indirect Prompt Injection Attacks on Large Language Models",
            "A Novel Evaluation Framework for Assessing Resilience Against Prompt Injection Attacks in Large Language Models",
            "Assessing Prompt Injection Risks in 200+ Custom GPTs",
            "Prompt Injection Mixed Techniques Dataset",
            "Injecagent: Benchmarking indirect prompt injections in tool-integrated large language model agents"
        ]
    },
    {
        "index": 130,
        "title": "Investigating the Influence of Prompt-Specific Shortcuts in AI Generated Text Detection",
        "publication_date": "2024-06-24",
        "references": [
            "Gpt-4 technical report",
            "The claude 3 model family: Opus, sonnet, haiku. Claude-3 Model Card",
            "Fast-detectgpt: Efficient zero-shot detection of machine-generated text via conditional probability curvature",
            "Natural language processing with Python: analyzing text with the natural language toolkit",
            "The Unseen A+ Student: Navigating the Impact of Large Language Models in the Classroom",
            "Language models are few-shot learners",
            "Too good to be true? an empirical study of chatgpt capabilities for academic writing and implications for academic misconduct",
            "ELI5: long form question answering",
            "Promptbreeder: Self-referential self-improvement via prompt evolution",
            "Shortcut learning in deep neural networks",
            "How close is chatgpt to human experts? comparison corpus, evaluation, and detection",
            "Shortcut learning of large language models in natural language understanding",
            "Perplexity—a measure of the difficulty of speech recognition tasks",
            "A watermark for large language models",
            "On the reliability of watermarks for large language models",
            "How you prompt matters! even task-oriented constraints in instructions affect llm-generated text detection",
            "Outfox: Llm-generated essay detection through in-context learning with adversarially generated examples",
            "Paraphrasing evades detectors of ai-generated text, but retrieval is an effective defense",
            "Robust distortion-free watermarks for language models",
            "Large language models understand and can be enhanced by emotional stimuli",
            "Roberta: A robustly optimized bert pretraining approach",
            "Large language models can be guided to evade AI-generated text detection",
            "Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity",
            "Smaller language models are better black-box machine-generated text detectors",
            "Detectgpt: Zero-shot machine-generated text detection using probability curvature",
            "A systematic survey of prompt engineering in large language models: Techniques and applications",
            "Quantifying language models’ sensitivity to spurious features in prompt design or: How i learned to start worrying about prompt formatting",
            "Red teaming language model detectors with language models",
            "Ai model gpt-3 (dis) informs us better than humans",
            "Detectllm: Leveraging log rank information for zero-shot detection of machine-generated text",
            "Hc3 plus: A semantic-invariant human chatgpt comparison corpus",
            "The impact of prompts on zero-shot detection of ai-generated text",
            "Llama 2: Open foundation and fine-tuned chat models",
            "Intrinsic dimension estimation for robust detection of AI-generated texts",
            "Opt: Open pretrained transformer language models",
            "Assaying on the robustness of zero-shot machine-generated text detectors",
            "Large language models are human-level prompt engineers"
        ]
    },
    {
        "index": 131,
        "title": "JAILBREAK ANTIDOTE: REAL-TIME SAFETY-UTILITY BALANCE VIA SPARSE REPRESENTATION ADJUSTMENT IN LARGE LANGUAGE MODELS",
        "publication_date": "2025-05-01",
        "references": [
            "Phi-3 technical report: A highly capable language model locally on your phone",
            "Llm inference performance engineering: Best practices",
            "Llama 3 model card",
            "Detecting language model attacks with perplexity",
            "Does refusal training in llms generalize to the past tense?",
            "Jailbreaking leading safety-aligned llms with simple adaptive attacks",
            "Qwen technical report",
            "Training a helpful and harmless assistant with reinforcement learning from human feedback",
            "Jailbreaking black box large language models in twenty queries",
            "Jailbreakbench: An open robustness benchmark for jailbreaking large language models",
            "Chatbot arena: An open platform for evaluating llms by human preference",
            "Measuring massive multitask language understanding",
            "Baseline defenses for adversarial attacks against aligned language models",
            "Defending large language models against jailbreak attacks via semantic smoothing",
            "Jailbreakzoo: Survey, landscapes, and horizons in jailbreaking large language and vision-language models",
            "Large language models are zero-shot reasoners",
            "Safe rlhf: Safe reinforcement learning from human feedback",
            "Deepinception: Hypnotize large language model to be jailbreaker",
            "Large language models in finance: A survey",
            "In-context vectors: Making in context learning more effective and controllable through latent space steering",
            "Autodan: Generating stealthy jailbreak prompts on aligned large language models",
            "Language models are few-shot learners",
            "Harmbench: A standardized evaluation framework for automated red teaming and robust refusal",
            "Locating and editing factual associations in gpt",
            "Progress measures for grokking via mechanistic interpretability",
            "Training language models to follow instructions with human feedback",
            "Advprompter: Fast adaptive adversarial prompting for llms",
            "harmful harmless instructions",
            "Safety alignment should be made more than just a few tokens deep",
            "Fine-tuning aligned language models compromises safety, even when users do not intend to!",
            "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "Smoothllm: Defending large language models against jailbreaking attacks",
            "Code llama: Open foundation models for code",
            "Rainbow teaming: Open-ended generation of diverse adversarial prompts",
            "Large language models encode clinical knowledge",
            "Testing theory of mind in large language models and humans",
            "Gemma",
            "Towards safety and helpfulness balanced responses via controllable large language models",
            "Steering language models with activation engineering",
            "Attention is all you need",
            "Chain-of-thought reasoning without prompting",
            "Jailbroken: How does llm safety training fail?",
            "Assessing the brittleness of safety alignment via pruning and low-rank modifications",
            "Jailbreak and guard aligned language models with only few in-context demonstrations",
            "Gpt-4 is too smart to be safe: Stealthy chat with llms via cipher",
            "Hellaswag: Can a machine really finish your sentence?",
            "How johnny can persuade llms to jailbreak them: Rethinking persuasion to challenge ai safety by humanizing llms",
            "Representation engineering: A top-down approach to ai transparency",
            "Universal and transferable adversarial attacks on aligned language models",
            "Improving alignment and robustness with short circuiting"
        ]
    },
    {
        "index": 133,
        "title": "Jailbreak Open-Sourced Large Language Models via Enforced Decoding",
        "publication_date": "2024-08-11",
        "references": [
            "Machine unlearning.",
            "Language models are few-shot learners.",
            "Defending against alignment-breaking attacks via robustly aligned llm.",
            "Editing factual knowledge in language models.",
            "Towards making systems forget with machine unlearning.",
            "Free dolly: Introducing the world's first truly open instruction-tuned llm, 2023.",
            "Qlora: Efficient finetuning of quantized llms.",
            "Raft: Reward ranked finetuning for generative foundation model alignment.",
            "The pile: An 800gb dataset of diverse text for language modeling.",
            "More than you've asked for: A comprehensive analysis of novel prompt injection threats to application-integrated large language models.",
            "Textbooks are all you need.",
            "Adaptive machine unlearning.",
            "Catastrophic jailbreak of open-source llms via exploiting generation.",
            "[link].",
            "Open llm leaderboard.",
            "Baseline defenses for adversarial attacks against aligned language models.",
            "Mistral 7b.",
            "Exploiting programmatic behavior of llms: Dual-use through standard security attacks.",
            "Propile: Probing privacy leakage in large language models.",
            "The enron corpus: A new dataset for email classification research.",
            "Openassistant conversations--democratizing large language model alignment.",
            "Certifying llm safety against adversarial prompting.",
            "Lamini-lm: A diverse herd of distilled models from large-scale instructions.",
            "Glm-130b: An open bilingual pre-trained model.",
            "Judging llm-as-a-judge with mt-bench and chatbot arena.",
            "Universal and transferable adversarial attacks on aligned language models.",
            "Efficient estimation of word representations in vector space.",
            "Fast model editing at scale.",
            "[link].",
            "Training language models to follow instructions with human feedback.",
            "Pytorch: An imperative style, high-performance deep learning library.",
            "Instruction tuning with gpt-4.",
            "Direct preference optimization: Your language model is secretly a reward model.",
            "\"do anything now\": Characterizing and evaluating in-the-wild jailbreak prompts on large language models.",
            "Prompting gpt-3 to be reliable.",
            "Editable neural networks.",
            "Preference ranking optimization for human alignment.",
            "Principle-driven self-alignment of language models from scratch with minimal human supervision.",
            "Llama 2: Open foundation and fine-tuned chat models.",
            "Self-instruct: Aligning language model with self generated instructions.",
            "Super-naturalinstructions: Generalization via declarative instructions on 1600+ nlp tasks.",
            "Aligning large language models with human: A survey."
        ]
    },
    {
        "index": 134,
        "title": "Jailbreak Paradox : The Achilles’ Heel of LLMs",
        "publication_date": "2024-06-21",
        "references": [
            "Training a helpful and harmless assistant with reinforcement learning from human feedback",
            "On the opportunities and risks of foundation models",
            "Breaking down the defenses: A comparative survey of attacks on large language models",
            "Comprehensive assessment of jailbreak attacks against llms",
            "Masterkey: Automated jailbreaking of large language model chatbots",
            "Beyond english-centric multilingual machine translation",
            "Toxic, hateful, offensive or abusive? what are we really classifying? an empirical analysis of hate speech datasets",
            "Survey of hallucination in natural language generation",
            "Jailbreaking is best solved by definition",
            "ChatGPT _DAN",
            "Inverse scaling: When bigger isn’t better",
            "Formal Computational Models and Computability",
            "Introducing GPT-4o",
            "Bergeron: Combating adversarial attacks through a conscience-based alignment framework",
            "Tricking LLMs into disobedience: Formalizing, analyzing, and detecting jailbreaks",
            "Les principes des mathématiques et le problème des ensembles",
            "Can ai-generated text be reliably detected?",
            "Llama 2: Open foundation and fine-tuned chat models",
            "Multi-objective reinforcement learning using sets of pareto dominating policies",
            "Jailbroken: How does llm safety training fail?"
        ]
    },
    {
        "index": 135,
        "title": "Jailbreak Vision Language Models via Bi-Modal Adversarial Prompt",
        "publication_date": "2024-07-01",
        "references": [
            "Visual instruction tuning.",
            "Gemini.",
            "Visualgpt: Data-efficient adaptation of pretrained language models for image captioning.",
            "Visual question answering instruction: Unlocking multimodal large language model to domain-specific visual multitasks.",
            "Image retrieval on real-life images with pre-trained vision-and-language models.",
            "An image is worth 1000 lies: Adversarial transferability across prompts on vision-language models.",
            "On the adversarial robustness of multi-modal foundation models.",
            "On the robustness of large multimodal models against image adversarial attacks.",
            "Jailbreak in pieces: Compositional adversarial attacks on multi-modal language models.",
            "Badclip: Dual-embedding guided backdoor attack on multimodal contrastive learning.",
            "Pre-trained trojan attacks for visual recognition.",
            "Does few-shot learning suffer from backdoor attacks?",
            "Poisoned forgery face: Towards backdoor attacks on face forgery detection.",
            "Vl-trojan: Multimodal instruction backdoor attacks against autoregressive visual language models.",
            "Poisoning attack against estimating from pairwise comparisons.",
            "A tale of hodgerank and spectral method: Target attack against rank aggregation is the fixed point of adversarial game.",
            "Semi-supervised robust training with generalized perturbed neighborhood.",
            "Nba: defensive distillation for backdoor removal via neural behavior alignment.",
            "Dlp: towards active defense against backdoor attacks with decoupled learning process.",
            "GPTFUZZER: red teaming large language models with auto-generated jailbreak prompts.",
            "Jailbroken: How does LLM safety training fail?",
            "Jailbreaking black box large language models in twenty queries.",
            "Are aligned neural networks adversarially aligned?",
            "LIMA: less is more for alignment.",
            "On evaluating adversarial robustness of large vision-language models.",
            "Vision-llms can fool themselves with self-generated typographic attacks.",
            "Figstep: Jailbreaking large vision-language models via typographic visual prompts.",
            "Generate more imperceptible adversarial examples for object detection.",
            "Efficient adversarial attacks for visual object tracking.",
            "Transferable adversarial attacks for image and video object detection.",
            "Parallel rectangle flip attack: A query-based black-box attack against object detection.",
            "A large-scale multiple-objective method for black-box attack against object detection.",
            "Diversifying the high-level features for better adversarial transferability.",
            "{X-Adv }: Physical adversarial object attacks against x-ray prohibited item detection.",
            "Generating transferable 3d adversarial point cloud via random perturbation factorization.",
            "Improving adversarial transferability by stable diffusion.",
            "Query-relevant images jailbreak large multi-modal models.",
            "Chain-of-thought prompting elicits reasoning in large language models.",
            "Universal and transferable adversarial attacks on aligned language models.",
            "Chatglm.",
            "Qwen.",
            "Ernie bot.",
            "Jailbroken: How does LLM safety training fail?",
            "Jailbreak and guard aligned language models with only few in-context demonstrations.",
            "Tree of attacks: Jailbreaking black-box llms automatically.",
            "Scalable and transferable black-box jailbreaks for language models via persona modulation.",
            "Autodan: Generating stealthy jailbreak prompts on aligned large language models.",
            "Semantic mirror jailbreak: Genetic algorithm based jailbreak prompts against open-source llms.",
            "Visual adversarial examples jailbreak aligned large language models.",
            "Jailbreaking attack against multimodal large language model.",
            "Instructta: Instruction-tuned targeted attack for large vision-language models.",
            "How robust is google’s bard to adversarial image attacks?",
            "Fool your (vision and) language model with embarrassingly simple permutations.",
            "Perceptual-sensitive gan for generating adversarial patches.",
            "Spatiotemporal attacks for embodied agents.",
            "Bias-based universal adversarial patch attack for automatic check-out.",
            "Harnessing perceptual adversarial patches for crowd counting.",
            "Exploring the relationship between architecture and adversarially robust generalization.",
            "Minigpt-4: Enhancing vision-language understanding with advanced large language models.",
            "Learning transferable visual models from natural language supervision.",
            "Llama: Open and efficient foundation language models.",
            "Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality.",
            "Aligning large multimodal models with factually augmented rlhf.",
            "DRESS: instructing large vision-language models to align and interact with humans via natural language feedback.",
            "WARM: on the benefits of weight averaged reward models.",
            "Towards deep learning models resistant to adversarial attacks.",
            "A survey of chain of thought reasoning: Advances, frontiers and future.",
            "Large language models are zero-shot reasoners.",
            "Instructblip: Towards general-purpose vision-language models with instruction tuning.",
            "Gpt-4 technical report.",
            "Llama guard: Llm-based input-output safeguard for human-ai conversations.",
            "Mllm-protector: Ensuring mllm’s safety without hurting performance.",
            "Attackeval: How to evaluate the effectiveness of jailbreak attacking on large language models.",
            "Imgtrojan: Jailbreaking vision-language models with ONE image.",
            "Vision transformer with quadrangle attention.",
            "Deepinception: Hypnotize large language model to be jailbreaker.",
            "Jailbreak and guard aligned language models with only few in-context demonstrations.",
            "Sa-attack: Improving adversarial transferability of vision-language pre-training models via self-augmentation.",
            "Hide in thicket: Generating imperceptible and rational adversarial perturbations on 3d point clouds.",
            "Improving robust fairness via balance adversarial training.",
            "Exploring inconsistent knowledge distillation for object detection with data augmentation."
        ]
    },
    {
        "index": 136,
        "title": "JailbreakBench: An Open Robustness Benchmark for Jailbreaking Large Language Models",
        "publication_date": "2024-10-31",
        "references": [
            "Are you still on track!? catching llm task drift with activations",
            "Llama 3 model card",
            "Croissant: A metadata format for ml-ready datasets",
            "Jailbreak chat",
            "Detecting language model attacks with perplexity",
            "Jailbreaking leading safety-aligned llms with simple adaptive attacks",
            "Measuring massive multitask language understanding",
            "Catastrophic jailbreak of open-source llms via exploiting generation",
            "Sleeper agents: Training deceptive llms that persist through safety training",
            "Llama guard: Llm-based input-output safeguard for human-ai conversations",
            "Baseline defenses for adversarial attacks against aligned language models",
            "Defending large language models against jailbreak attacks via semantic smoothing",
            "Mixtral of experts",
            "Guard: Role-playing to generate natural-language jailbreakings to test guideline adherence of large language models",
            "Jailbreaking large language models against moderation guardrails via cipher characters",
            "Certifying llm safety against adversarial prompting",
            "Open sesame! universal black box jailbreaking of large language models",
            "Non-determinism in gpt-4 is caused by sparse moe",
            "Jailbreaking black box large language models in twenty queries",
            "Robustbench: a standardized adversarial robustness benchmark",
            "Multilingual jailbreak challenges in large language models",
            "Attacking large language models with projected gradient descent",
            "Gemini v1.5 report",
            "Query-based adversarial prompt generation",
            "Jailbroken: How does llm safety training fail?",
            "Defensive prompt patch: A robust and interpretable defense of llms against jailbreak attacks",
            "Low-resource languages jailbreak gpt-4",
            "Gptfuzzer: Red teaming large language models with auto-generated jailbreak prompts",
            "How johnny can persuade llms to jailbreak them: Rethinking persuasion to challenge ai safety by humanizing llms",
            "Judging llm-as-a-judge with mt-bench and chatbot arena",
            "Improved few-shot jailbreaking can circumvent aligned language models and their defenses",
            "Easyjailbreak: A unified framework for jailbreaking large language models",
            "Promptbench: A unified library for evaluation of large language models",
            "Randomness in neural network training: Characterizing the impact of tooling",
            "Universal and transferable adversarial attacks on aligned language models"
        ]
    },
    {
        "index": 137,
        "title": "JailbreakEval: An Integrated Toolkit for Evaluating Jailbreak Attempts Against Large Language Models",
        "publication_date": "2025-02-24",
        "references": [
            "https://platform.openai.com/docs/guides/moderation/",
            "https://azure.microsoft.com/en-us/products/ai-services/ai-content-safety.",
            "https://www.perspectiveapi.com/.",
            "Jailbreaking leading safety-aligned llms with simple adaptive attacks",
            "How (un)ethical are instruction-centric responses of llms? unveiling the vulnerabilities of safety guardrails to harmful queries",
            "Red-teaming large language models using chain of utterances for safety-alignment",
            "Play guessing game with llm: Indirect jailbreak attack with implicit clues",
            "Jailbreakbench: An open robustness benchmark for jailbreaking large language models",
            "Jailbreaking black box large language models in twenty queries",
            "Red teaming gpt-4v: Are gpt-4v safe against uni/multi-modal jailbreak attacks?",
            "Comprehensive assessment of jailbreak attacks against llms",
            "Safe rlhf: Safe reinforcement learning from human feedback",
            "Attack prompt generation for red teaming and defending large language models",
            "Masterkey: Automated jailbreaking of large language model chatbots",
            "Pandora: Jailbreak gpts by retrieval augmented generation poisoning",
            "Multilingual jailbreak challenges in large language models",
            "A wolf in sheep’s clothing: Generalized nested jailbreak prompts can fool large language models easily",
            "Analyzing the inherent response tendency of llms: Real-world instructions-driven jailbreak",
            "Decoding the threat landscape: Chatgpt, fraudgpt, and wormgpt in social engineering attacks",
            "Mart: Improving llm safety with multi-round automatic red-teaming",
            "Coercing LLMs to do and reveal (almost) anything",
            "Aegis: Online adaptive ai content safety moderation with ensemble of llm experts",
            "Figstep: Jailbreaking large vision-language models via typographic visual prompts",
            "Intelligent virtual assistants with llm-based process automation",
            "Cold-attack: Jailbreaking llms with stealthiness and controllability",
            "Pruning for protection: Increasing jailbreak resistance in aligned llms without fine-tuning",
            "Query-based adversarial prompt generation",
            "Sowing the wind, reaping the whirlwind: The impact of editing language models",
            "Catastrophic jailbreak of open-source LLMs via exploiting generation",
            "Llama guard: Llm-based input-output safeguard for human-ai conversations",
            "Baseline defenses for adversarial attacks against aligned language models",
            "Defending large language models against jailbreak attacks via semantic smoothing",
            "Beavertails: Towards improved safety alignment of llm via a human-preference dataset",
            "Ai alignment: A comprehensive survey",
            "Artprompt: Ascii art-based jailbreak attacks against aligned llms",
            "Guard: Role-playing to generate natural-language jailbreakings to test guideline adherence of large language models",
            "Certifying llm safety against adversarial prompting",
            "Increased llm vulnerabilities from fine-tuning and quantization",
            "Open sesame! universal black box jailbreaking of large language models",
            "A cross-language investigation into jailbreak attacks in large language models",
            "Salad-bench: A hierarchical and comprehensive safety benchmark for large language models",
            "Semantic mirror jailbreak: Genetic algorithm based jailbreak prompts against open-source llms",
            "Drattack: Prompt decomposition and reconstruction makes powerful llm jail-breakers",
            "Deepinception: Hypnotize large language model to be jailbreaker",
            "Images are achilles’ heel of alignment: Exploiting visual vulnerabilities for jail-breaking multimodal large language models",
            "Jail-judge: A comprehensive jailbreak judge benchmark with multi-agent enhanced explanation evaluation framework",
            "Making them ask and answer: Jailbreaking large language models in few queries via disguise and reconstruction",
            "AutoDAN: Generating stealthy jailbreak prompts on aligned large language models",
            "Jailbreaking chatgpt via prompt engineering: An empirical study",
            "Robustness over time: Understanding adversarial examples’ effectiveness on longitudinal versions of large language models",
            "Eraser: Jailbreaking defense in large language models via unlearning harmful knowledge",
            "Jailbreakv-28k: A benchmark for assessing the robustness of multimodal large language models against jailbreak attacks",
            "Codechameleon: Personalized encryption framework for jailbreaking large language models",
            "Prp: Propagating universal perturbations to attack large language model guard-rails",
            "Harmbench: A standardized evaluation framework for automated red teaming and robust refusal",
            "Tree of attacks: Jailbreaking black-box llms automatically",
            "Jailbreaking attack against multimodal large language model",
            "Gpt-4 technical report",
            "An attacker’s dream? exploring the capabilities of chatgpt for developing malware",
            "Visual adversarial examples jailbreak aligned large language models",
            "Latent jailbreak: A benchmark for evaluating text safety and output robustness of large language models",
            "Tricking LLMs into disobedience: Formalizing, analyzing, and detecting jailbreaks",
            "Great, now write an article about that: The crescendo multi-turn llm jailbreak attack",
            "Xstest: A test suite for identifying exaggerated safety behaviours in large language models",
            "Scalable and transferable black-box jailbreaks for language models via persona modulation",
            "Jailbreak in pieces: Compositional adversarial attacks on multi-modal language models",
            "“do anything now”: Characterizing and evaluating in-the-wild jailbreak prompts on large language models",
            "Attackeval: How to evaluate the effectiveness of jailbreak attacking on large language models",
            "Pal: Proxy-guided black-box attack on large language models",
            "A strongreject for empty jailbreaks",
            "Comparing traditional and llm-based search for consumer choice: A randomized experiment",
            "All in how you ask for it: Simple black-box method for jailbreak attacks",
            "Meta llama guard 2",
            "Llama 2: Open foundation and fine-tuned chat models",
            "From noise to clarity: Unraveling the adversarial suffix of large language model attacks via translation of text embeddings",
            "Mitigating fine-tuning jailbreak attack with backdoor enhanced alignment",
            "Adashield: Safeguarding multimodal large language models from structure-based attack via adaptive shield prompting",
            "Jailbroken: How does llm safety training fail?",
            "Chain-of-thought prompting elicits reasoning in large language models",
            "Jailbreak and guard aligned language models with only few in-context demonstrations",
            "Jailbreaking gpt-4v via self-adversarial attacks with system prompts",
            "Tastle: Distract large language models for automatic jailbreak attack",
            "Safedecoding: Defending against jailbreak attacks via safety-aware decoding",
            "Low-resource languages jailbreak gpt-4",
            "Gptfuzzer: Red teaming large language models with auto-generated jailbreak prompts",
            "Don’t listen to me: Understanding and exploring jailbreak prompts of large language models",
            "GPT-4 is too smart to be safe: Stealthy chat with LLMs via cipher",
            "Rigorllm: Resilient guardrails for large language models against undesired content",
            "How johnny can persuade llms to jailbreak them: Rethinking persuasion to challenge ai safety by humanizing llms",
            "Autodefense: Multi-agent llm defense against jailbreak attacks",
            "Intention analysis makes llms a good jailbreak defender",
            "Psysafe: A comprehensive framework for psychological-based attack, defense, and evaluation of multi-agent system safety",
            "Make them spill the beans! coercive knowledge extraction from (production) llms",
            "Weak-to-strong jailbreaking on large language models",
            "On prompt-driven safeguarding for large language models",
            "Judging llm-as-a-judge with mt-bench and chatbot arena",
            "Robust prompt optimization for defending language models against jailbreaking attacks",
            "Easyjailbreak: A unified framework for jailbreaking large language models",
            "Don’t say no: Jailbreaking llm by suppressing refusal",
            "Autodan: Interpretable gradient-based adversarial attacks on large language models",
            "Safety fine-tuning at (almost) no cost: A baseline for vision large language models",
            "Universal and transferable adversarial attacks on aligned language models",
            "Is the system message really important to jailbreaks in large language models?"
        ]
    },
    {
        "index": 138,
        "title": "Jailbreaking and Mitigation of Vulnerabilities in Large Language Models",
        "publication_date": "2024-10-20",
        "references": [
            "Surveying the mllm landscape: A meta-review of current surveys",
            "Emerging techniques in vision-based human posture detection: Machine learning methods and applications",
            "Securing large language models: Addressing bias, misinformation, and prompt attacks",
            "Fundamental Limitations of Alignment in Large Language Models",
            "Intention Analysis Makes LLMs A Good Jailbreak Defender",
            "Large Language Models Are Human-Level Prompt Engineers",
            "Jailbreaking Black Box Large Language Models in Twenty Queries",
            "Scalable and Transferable Black-Box Jailbreaks for Language Models via Persona Modulation",
            "How Johnny Can Persuade LLMs to Jailbreak Them: Rethinking Persuasion to Challenge AI Safety by Humanizing LLMs",
            "Multilingual Jailbreak Challenges in Large Language Models",
            "Don’t Listen To Me: Understanding and Exploring Jailbreak Prompts of Large Language Models",
            "Fine-tuning Aligned Language Models Compromises Safety, Even When Users Do Not Intend To!",
            "Ignore Previous Prompt: Attack Techniques For Language Models",
            "Catastrophic Jail-break of Open-source LLMs via Exploiting Generation",
            "Jailbreaking Proprietary Large Language Models using Word Substitution Cipher",
            "Latent Jailbreak: A Benchmark for Evaluating Text Safety and Output Robustness of Large Language Models",
            "Prompt Engineering as an Important Emerging Skill for Medical Professionals: Tutorial",
            "Large language models and cognitive science: A comprehensive review of similarities, differences, and challenges",
            "Jailbreaking ChatGPT via Prompt Engineering: An Empirical Study",
            "Gptfuzzer: Red Teaming Large Language Models with Auto-Generated Jailbreak Prompts",
            "Figstep: Jailbreaking Large Vision-language Models via Typographic Visual Prompts",
            "Open Sesame! Universal Black Box Jailbreaking of Large Language Models",
            "Weak-to-Strong Jailbreaking on Large Language Models",
            "Tricking LLMs into Disobedience: Formalizing, Analyzing, and Detecting Jailbreaks",
            "Red-Teaming Large Language Models using Chain of Utterances for Safety-Alignment",
            "Red Teaming Language Models to Reduce Harms: Methods, Scaling Behaviors, and Lessons Learned",
            "A Wolf in Sheep’s Clothing: Generalized Nested Jailbreak Prompts can Fool Large Language Models Easily",
            "Jailbreaking GPT-4v via Self-Adversarial Attacks with System Prompts",
            "Red teaming ChatGPT via Jailbreaking: Bias, Robustness, Reliability and Toxicity",
            "Multi-step Jailbreaking Privacy Attacks on ChatGPT",
            "Jailbreaking Leading Safety-Aligned LLMs with Simple Adaptive Attacks",
            "Do-Not-Answer: A Dataset for Evaluating Safeguards in LLMs",
            "‘Do Anything Now’: Characterizing and Evaluating In-The-Wild Jailbreak Prompts on Large Language Models",
            "Unleashing the potential of prompt engineering in Large Language Models: a comprehensive review",
            "Red Teaming Language Models with Language Models",
            "Defending ChatGPT against jailbreak attack via self-reminders",
            "Making Them Ask and Answer: Jailbreaking Large Language Models in Few Queries via Disguise and Reconstruction",
            "Jailbreaking Attack against Multimodal Large Language Model",
            "Visual Adversarial Examples Jailbreak Aligned Large Language Models",
            "Universal and Transferable Adversarial Attacks on Aligned Language Models",
            "Autodan: Generating Stealthy Jailbreak Prompts on Aligned Large Language Models",
            "Wordgame: Efficient &; Effective LLM Jailbreak via Simultaneous Obfuscation in Query and Response",
            "Jailbreak and Guard Aligned Language Models with Only Few In-Context Demonstrations",
            "Great, Now Write an Article About That: The Crescendo Multi-Turn LLM Jailbreak Attack",
            "Speak Out of Turn: Safety Vulnerability of Large Language Models in Multi-turn Dialogue",
            "Hidden You Malicious Goal Into Benign Narratives: Jailbreak Large Language Models through Logic Chain Injection",
            "Artprompt: Ascii Art-based Jailbreak Attacks against Aligned LLMs",
            "Trojanrag: Retrieval-Augmented Generation Can Be Backdoor Driver in Large Language Models",
            "Poisonprompt: Backdoor Attack on Prompt-Based Large Language Models",
            "Shadow Alignment: The Ease of Subverting Safely-Aligned Language Models",
            "Images are Achilles’ Heel of Alignment: Exploiting Visual Vulnerabilities for Jailbreaking Multimodal Large Language Models",
            "Jailbreak in pieces: Compositional Adversarial Attacks on Multi-Modal Language Models",
            "Low-Resource Languages Jailbreak GPT-4",
            "A Cross-Language Investigation into Jailbreak Attacks in Large Language Models",
            "Visual Adversarial Examples Jailbreak Aligned Large Language Models",
            "Baseline Defenses for Adversarial Attacks Against Aligned Language Models",
            "Jailbroken: How Does LLM Safety Training Fail?",
            "Ignore This Title and HackAPrompt: Exposing Systemic Vulnerabilities of LLMs Through a Global Prompt Hacking Competition",
            "Masterkey: Automated Jailbreaking of Large Language Model Chatbots",
            "Studious bob fights back against jailbreaking via prompt adversarial tuning",
            "Robust Prompt Optimization for Defending Language Models Against Jailbreaking Attacks",
            "On Prompt-Driven Safeguarding for Large Language Models",
            "Safety Fine-Tuning at (Almost) No Cost: A Baseline for Vision Large Language Models",
            "Pruning for Protection: Increasing Jailbreak Resistance in Aligned LLMs Without Fine-Tuning",
            "Smoothllm: Defending Large Language Models Against Jailbreaking Attacks",
            "Eraser: Jailbreaking Defense in Large Language Models via Unlearning Harmful Knowledge",
            "Defending Against Alignment-Breaking Attacks via Robustly Aligned LLM",
            "Autodefense: Multi-Agent LLM Defense against Jailbreak Attacks",
            "Llm Self Defense: By Self Examination, LLMs Know They Are Being Tricked",
            "Parden, Can You Repeat That? Defending against Jailbreaks via Repetition",
            "Self-Guard: Empower the LLM to Safeguard Itself",
            "Defending LLMs against Jailbreaking Attacks via Backtranslation",
            "Safedecoding: Defending against Jailbreak Attacks via Safety-Aware Decoding",
            "On prompt-driven safeguarding for large language models",
            "Safety Assessment of Chinese Large Language Models",
            "Mm-SafetyBench: A Benchmark for Safety Evaluation of Multimodal Large Language Models",
            "Jailbreakv-28k: A Benchmark for Assessing the Robustness of MultiModal Large Language Models against Jailbreak Attacks",
            "Sowing the Wind, Reaping the Whirlwind: The Impact of Editing Language Models",
            "Jailbreakbench: An Open Robustness Benchmark for Jailbreaking Large Language Models",
            "Not What You’ve Signed Up For: Compromising Real-World LLM-Integrated Applications with Indirect Prompt Injection",
            "Autodan: Interpretable Gradient-Based Adversarial Attacks on Large Language Models",
            "Latent jailbreak: A benchmark for evaluating text safety and output robustness of large language models",
            "Llms Can Defend Themselves Against Jailbreaking in a Practical Manner: A Vision Paper"
        ]
    },
    {
        "index": 140,
        "title": "PrisonBreak : Jailbreaking Large Language Models with Fewer Than Twenty-Five Targeted Bit-flips",
        "publication_date": "2024-12-10",
        "references": [
            "DeepHammer: Depleting the intelligence of deep neural networks through targeted chain of bit flips.",
            "Terminal brain damage: Exposing the graceless degradation in deep neural networks under hardware fault attacks.",
            "Bit-flip attack: Crushing neural network with progressive bit search.",
            "T-BFA: Targeted Bit Flip Adversarial Weight Attack.",
            "Versatile weight attack via flipping limited bits.",
            "Seeds of SEED: NMT-Stroke: Diverting Neural Machine Translation through Hardware-based Faults.",
            "Deepvenom: Persistent dnn backdoors exploiting transient weight perturbations in memories.",
            "ProFlip: Targeted Trojan Attack with Progressive Bit Flips.",
            "Tbt: Targeted neural network attack with bit trojan.",
            "TrojViT: Trojan Insertion in Vision Transformers.",
            "Don’t knock! rowhammer at the backdoor of dnn models.",
            "Stealing Part of a Production Language Model.",
            "High Accuracy and High Fidelity Extraction of Neural Networks.",
            "Deepsteal: Advanced model extractions leveraging efficient weight stealing in memories.",
            "Flipping bits in memory without accessing them: An experimental study of dram disturbance errors.",
            "Universal and transferable adversarial attacks on aligned language models.",
            "Attacking Large Language Models with Projected Gradient Descent.",
            "Autodan: Generating stealthy jailbreak prompts on aligned large language models.",
            "The privacy onion effect: Memorization is relative.",
            "DRAMA: Exploiting DRAM addressing for Cross-CPU attacks.",
            "Drammer: Deterministic rowhammer attacks on mobile platforms.",
            "Blacksmith: Scalable rowhammering in the frequency domain.",
            "How Companies Are Using Meta Llama — Meta.",
            "Another flip in the wall of rowhammer defenses.",
            "One-bit Flip is All You Need: When Bit-flip Attack Meets Model Training.",
            "Are aligned neural networks adversarially aligned?",
            "cognitivecomputations/Wizard-Vicuna-30B-Uncensored.",
            "Harmbench: A standardized evaluation framework for automated red teaming and robust refusal.",
            "Stanford alpaca: An instruction-following llama model.",
            "Flip feng shui: Hammering a needle in the software stack.",
            "Pytorch: An imperative style, high-performance deep learning library.",
            "Understanding the Linux virtual memory manager.",
            "Huggingface’s transformers: State-of-the-art natural language processing.",
            "Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality.",
            "tinybenchmarks: evaluating LLMs with fewer examples.",
            "Defending and Harnessing the Bit-Flip Based Adversarial Weight Attack.",
            "How important is a neuron.",
            "Defeating software mitigations against rowhammer: a surgical precision hammer.",
            "A primer in BERTology: What we know about how BERT works.",
            "UMAP: Uniform manifold approximation and projection.",
            "Graphene: Strong yet lightweight row hammer protection.",
            "TWiCe: Preventing row-hammering by exploiting time window counters.",
            "ProTRR: Principled yet optimal in-dram target row refresh.",
            "Randomized row-swap: Mitigating row hammer by breaking spatial correlation between aggressor and victim rows.",
            "Architectural support for mitigating row hammering in dram memories.",
            "Blockhammer: Preventing rowhammer at low cost by blacklisting rapidly-accessed dram rows.",
            "A white paper on the benefits of chip-kill-correct ecc for pc server main memory.",
            "Channel codes: classical and modern.",
            "Exploiting correcting codes: On the effectiveness of ecc memory against rowhammer attacks.",
            "How to kill the second bird with one ecc: The pursuit of row hammer resilient dram.",
            "{CAn’t} touch this: Software-only mitigation against rowhammer attacks targeting kernel memory.",
            "Anvil: Software-based protection against next-generation rowhammer attacks.",
            "{ZebRAM }: Comprehensive and compatible software protection against rowhammer attacks.",
            "Hashtag: Hash Signatures for Online Detection of Fault-Injection Attacks on Deep Neural Networks.",
            "Radar: Run-time Adversarial Weight Attack Detection and Accuracy Recovery.",
            "Neuropots: Realtime Proactive Defense against Bit-Flip Attacks in Neural Networks.",
            "Modelshield: A Generic and Portable Framework Extension for Defending Bit-Flip based Adversarial Weight Attacks.",
            "Aegis: Mitigating Targeted Bit-flip Attacks against Deep Neural Networks.",
            "Ra-bnn: Constructing robust & accurate binary neural network to simultaneously defend adversarial bit-flip attack and improve accuracy.",
            "Defending Bit-Flip Attack through DNN Weight Reconstruction.",
            "A Low-cost Fault Corrector for Deep Neural Networks through Range Restriction.",
            "Fast and robust early-exiting framework for autoregressive language models with synchronized parallel decoding.",
            "Ee-llm: Large-scale training and inference of early-exit large language models with 3d parallelism.",
            "Massive activations in large language models."
        ]
    },
    {
        "index": 141,
        "title": "JAILBREAKING LEADING SAFETY-ALIGNED LLM S WITH SIMPLE ADAPTIVE ATTACKS",
        "publication_date": "2024-10-07",
        "references": [
            "Phi-3 technical report: A highly capable language model locally on your phone.",
            "Square attack: a query-efficient black-box adversarial attack via random search.",
            "Many-shot jailbreaking.",
            "Prefill claude’s response for greater output control.",
            "The claude 3 model family: Opus, sonnet, haiku.",
            "Training a helpful and harmless assistant with reinforcement learning from human feedback.",
            "Wild patterns: ten years after the rise of adversarial machine learning.",
            "Evasion attacks against machine learning at test time.",
            "On evaluating adversarial robustness.",
            "Jailbreaking black box large language models in twenty queries.",
            "Jailbreakbench: An open robustness benchmark for jailbreaking large language models.",
            "Leveraging the context through multi-round interactions for jailbreaking attacks.",
            "Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality.",
            "Robustbench: a standardized adversarial robustness benchmark.",
            "Sparse-rs: a versatile framework for query-efficient sparse black-box adversarial attacks.",
            "Evaluating the adversarial robustness of adaptive test-time defenses.",
            "Attacking large language models with projected gradient descent.",
            "Gemini: a family of highly capable multimodal models.",
            "Mistral 7b.",
            "Open sesame! universal black box jailbreaking of large language models.",
            "Amplegcg: Learning a universal and transferable generative model of adversarial suffixes for jailbreaking both open and closed llms.",
            "Autodan: Generating stealthy jailbreak prompts on aligned large language models.",
            "Towards deep learning models resistant to adversarial attacks.",
            "Prp: Propagating universal perturbations to attack large language model guard-rails.",
            "Harmbench: A standardized evaluation framework for automated red teaming and robust refusal.",
            "Tree of attacks: Jailbreaking black-box llms automatically.",
            "Jailbreaking chatgpt on release day.",
            "Nemotron-4 340b technical report.",
            "Universal jailbreak backdoors from poisoned human feedback.",
            "Find the trojan: Universal backdoor detection in aligned llms.",
            "Competition report: Finding universal jailbreak backdoors in aligned llms.",
            "The convergence of the random search method in the extremal control of a many parameter system.",
            "Smoothllm: Defending large language models against jailbreaking attacks.",
            "Great, now write an article about that: The crescendo multi-turn llm jailbreak attack.",
            "Scalable and transferable black-box jailbreaks for language models via persona modulation.",
            "Autoprompt: Eliciting knowledge from language models with automatically generated prompts.",
            "Pal: Proxy-guided black-box attack on large language models.",
            "Intriguing properties of neural networks.",
            "Llama 2: Open foundation and fine-tuned chat models."
        ]
    },
    {
        "index": 142,
        "title": "Jailbreaking Prompt Attack: A Controllable Adversarial Attack against Diffusion Models",
        "publication_date": "2024-09-04",
        "references": [
            "Nudenet: Neural nets for nudity classification, detection and selective censoring",
            "PixArt- α: Fast Training of Diffusion Transformer for Photorealistic Text-to-Image Synthesis",
            "NSFW Image Classifier on GitHub",
            "Prompting4debugging: Red-teaming text-to-image diffusion models by finding problematic prompts",
            "Stable-diffusion-v1-4",
            "Stable Diffusion Safety Checker",
            "Discovering the hidden vocabulary of dalle-2",
            "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
            "An image is worth 16x16 words: Transformers for image recognition at scale",
            "Taming transformers for high-resolution image synthesis",
            "An image is worth one word: Personalizing text-to-image generation using textual inversion",
            "Erasing concepts from diffusion models",
            "Evaluating the Robustness of Text-to-image Diffusion Models against Real-world Attacks",
            "Gen2: Text-to-Video Generation by Runway",
            "NSFW Words List",
            "Gans trained by a two time-scale update rule converge to a local nash equilibrium",
            "Denoising diffusion probabilistic models",
            "NSFW Text Classifier",
            "Character as pixels: A controllable prompt adversarial attacking framework for black-box text guided image generation models",
            "NSFW CLIP-Based Image Classifier on GitHub",
            "Intriguing Properties of Text-guided Diffusion Models",
            "Decoupled weight decay regularization",
            "Towards deep learning models resistant to adversarial attacks",
            "Adversarial prompting for black box foundation models",
            "Microsoft Designer",
            "MidJourney: AI-Generated Art Platform",
            "Deepfool: a simple and accurate method to fool deep neural networks",
            "gpt4",
            "SDXL: Improving latent diffusion models for high-resolution image synthesis",
            "Red-teaming the stable diffusion safety filter",
            "High-resolution image synthesis with latent diffusion models",
            "Stability.ai: Open Models for AI Research and Development",
            "Photorealistic text-to-image diffusion models with deep language understanding",
            "Safe Latent Diffusion: Mitigating Inappropriate Degeneration in Diffusion Models",
            "Can Machines Help Us Answering Question 16 in Datasheets, and In Turn Reflecting on Inappropriate Content?",
            "pytorch-fid: FID Score for PyTorch",
            "Ring-A-Bell! How Reliable are Concept Removal Methods for Diffusion Models?",
            "Hard prompts made easy: Gradient-based discrete optimization for prompt tuning and discovery",
            "Sneakyprompt: Jailbreaking text-to-image generative models",
            "Forget-me-not: Learning to forget in text-to-image diffusion models",
            "To Generate or Not? Safety-Driven Unlearned Diffusion Models Are Still Easy To Generate Unsafe Images ... For Now",
            "Aligning Books and Movies: Towards Story-like Visual Explanations by Watching Movies and Reading Books",
            "A pilot study of query-free adversarial attack against stable diffusion"
        ]
    },
    {
        "index": 143,
        "title": "Jailbreaking Text-to-Image Models with LLM-Based Agents",
        "publication_date": "2024-09-09",
        "references": [
            "Clip-vit-base-patch32",
            "DALL-E 3.",
            "DALL ·E 3 Docs.",
            "GPT-4.",
            "GPT-4V(ision) System Card.",
            "Metric: perplexity.",
            "Midjourney.",
            "SD1.4. Hugging face.",
            "SD3. Hugging face.",
            "Torchmetrics, CLIP Score.",
            "CORRADO ALESSIO. Animals-10 dataset.",
            "Lin Chen, Jisong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua Lin. Sharegpt4v: Improving large multi-modal models with better captions.",
            "Lakshay Chhabra. Nsfw image classifier on github.",
            "Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V Le, and Ruslan Salakhutdinov. Transformer-xl: Attentive language models beyond a fixed-length context.",
            "Sumanth Dathathri, Andrea Madotto, Janice Lan, Jane Hung, Eric Frank, Piero Molino, Jason Yosinski, and Rosanne Liu. Plug and play language models: A simple approach to controlled text generation.",
            "Yimo Deng and Huangxun Chen. Divide-and-conquer attack: Harnessing the power of llm to bypass the censorship of text-to-image generation model.",
            "Yilun Du, Shuang Li, Antonio Torralba, Joshua B Tenenbaum, and Igor Mordatch. Improving factuality and reasoning in language models through multiagent debate.",
            "Rojit George. Nsfw words list on github.",
            "Edouard Grave, Armand Joulin, and Nicolas Usunier. Improving neural language models with a continuous cache.",
            "Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium.",
            "Sirui Hong, Xiawu Zheng, Jonathan Chen, Yuheng Cheng, Jinlin Wang, Ceyao Zhang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, Liyang Zhou, et al. Metagpt: Meta programming for multi-agent collaborative framework.",
            "Wenyue Hua, Lizhou Fan, Lingyao Li, Kai Mei, Jianchao Ji, Yingqiang Ge, Libby Hemphill, and Yongfeng Zhang. War and peace (waragent): Large language model-based multi-agent simulation of world wars.",
            "Fred Jelinek, Robert L Mercer, Lalit R Bahl, and James K Baker. Perplexity—a measure of the difficulty of speech recognition tasks.",
            "Feibo Jiang, Li Dong, Yubo Peng, Kezhi Wang, Kun Yang, Cunhua Pan, Dusit Niyato, and Octavia A Dobre. Large language model enhanced multi-agent systems for 6g communications.",
            "Di Jin, Zhijing Jin, Joey Tianyi Zhou, and Peter Szolovits. Is bert really robust? a strong baseline for natural language attack on text classification and entailment.",
            "Alex Kim. Nsfw image dataset.",
            "Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu-taka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners.",
            "Guohao Li, Hasan Abed Al Kader Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem. Camel: Communicative agents for\" mind\" exploration of large scale language model society.",
            "Jinfeng Li, Shouling Ji, Tianyu Du, Bo Li, and Ting Wang. Textbugger: Generating adversarial text against real-world applications.",
            "Michelle Li. Nsfw text classifier on hugging face.",
            "Tian Liang, Zhiwei He, Wenxiang Jiao, Xing Wang, Yan Wang, Rui Wang, Yujiu Yang, Zhaopeng Tu, and Shuming Shi. Encouraging divergent thinking in large language models through multi-agent debate.",
            "Yaobo Liang, Chenfei Wu, Ting Song, Wenshan Wu, Yan Xia, Yu Liu, Yang Ou, Shuai Lu, Lei Ji, Shaoguang Mao, et al. Taskmatrix. ai: Completing tasks by connecting foundation models with millions of apis.",
            "Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning.",
            "Clara Meister and Ryan Cotterell. Language model evaluation beyond perplexity.",
            "Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation of word representations in vector space.",
            "Oluwatosin Ogundare, Srinath Madasu, and Nathanial Wiggins. Industrial engineering with large language models: A case study of chatgpt’s performance on oil & gas problems.",
            "Joon Sung Park, Joseph O’Brien, Carrie Jun Cai, Meredith Ringel Morris, Percy Liang, and Michael S Bernstein. Generative agents: Interactive simulacra of human behavior.",
            "Joon Sung Park, Lindsay Popowski, Carrie Cai, Meredith Ringel Morris, Percy Liang, and Michael S Bernstein. Social simulacra: Creating populated prototypes for social computing systems.",
            "Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis.",
            "Chen Qian, Xin Cong, Cheng Yang, Weize Chen, Yusheng Su, Juyuan Xu, Zhiyuan Liu, and Maosong Sun. Communicative agents for software development.",
            "Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, et al. Toolllm: Facilitating large language models to master 16000+ real-world apis.",
            "Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners.",
            "Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents.",
            "Javier Rando, Daniel Paleka, David Lindner, Lennart Heim, and Florian Tramèr. Red-teaming the stable diffusion safety filter.",
            "Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bert-networks.",
            "Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models, 2021.",
            "Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salingans, et al. Photorealistic text-to-image diffusion models with deep language understanding.",
            "Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. Distilbert, a distilled version of bert: Smaller, faster, cheaper and lighter.",
            "Ruoxi Sun, Sercan Ö Arik, Alex Muzio, Lesly Miculicich, Satya Gundabathula, Pengcheng Yin, Hanjun Dai, Hootan Nakhost, Rajarishi Sinha, Zifeng Wang, et al. Sql-palm: Improved large language model adaptation for text-to-sql (extended).",
            "Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models.",
            "Yu-Lin Tsai, Chia-Yi Hsu, Chulin Xie, Chih-Hsun Lin, Jia-You Chen, Bo Li, Pin-Yu Chen, Chia-Mu Yu, and Chun-Ying Huang. Ring-a-bell! how reliable are concept removal methods for diffusion models?",
            "Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, et al. A survey on large language model based autonomous agents.",
            "Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models.",
            "Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Shaokun Zhang, Erkang Zhu, Beibin Li, Li Jiang, Xiaoyun Zhang, and Chi Wang. Autogen: Enabling next-gen llm applications via multi-agent conversation framework.",
            "Yifan Wu, Pengchuan Zhang, Wenhan Xiong, Barlas Oguz, James C Gee, and Yixin Nie. The role of chain-of-thought in complex vision-language reasoning task.",
            "Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, et al. The rise and potential of large language model based agents: A survey.",
            "Yuchen Xia, Manthan Shenoy, Nasser Jazdi, and Michael Weyrich. Towards autonomous system: flexible modular production system enhanced with large language model agents.",
            "Yuchen Yang, Bo Hui, Haolin Yuan, Neil Gong, and Yinzhi Cao. Sneakyprompt: Jailbreaking text-to-image generative models.",
            "Qiusi Zhan, Richard Fang, Rohan Bindu, Akul Gupta, Tatsunori Hashimoto, and Daniel Kang. Removing rlhf protections in gpt-4 via fine-tuning.",
            "Zhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola. Automatic chain of thought prompting in large language models.",
            "Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena."
        ]
    },
    {
        "index": 144,
        "title": "JailbreakLens: Interpreting Jailbreak Mechanism in the Lens of Representation and Circuit",
        "publication_date": "2024-11-17",
        "references": [
            "Llama: Open and efficient foundation language models",
            "Qwen2 technical report",
            "Beavertails: Towards improved safety alignment of llm via a human-preference dataset",
            "Language models are homer simpson! safety re-alignment of fine-tuned language models through task arithmetic",
            "Lazy safety alignment for large language models against harmful fine-tuning",
            "Universal and transferable adversarial attacks on aligned language models",
            "Gptfuzzer: Red teaming large language models with auto-generated jailbreak prompts",
            "Deepinception: Hypnotize large language model to be jailbreaker",
            "Codechameleon: Personalized encryption framework for jailbreaking large language models",
            "Jailbreaking black box large language models in twenty queries",
            "Jailbreakzoo: Survey, landscapes, and horizons in jail-breaking large language and vision-language models",
            "Autodan: Generating stealthy jailbreak prompts on aligned large language models",
            "A wolf in sheep's clothing: Generalized nested jailbreak prompts can fool large language models easily",
            "https://www.nytimes.com/2024/10/23/technology/characterai-lawsuit-teen-suicide.html.",
            "Towards understanding jailbreak attacks in llms: A representation space analysis",
            "Understanding jailbreak success: A study of latent space dynamics in large language models",
            "Safety layers of aligned large language models: The key to llm security",
            "How alignment and jailbreak work: Explain llm safety through intermediate hidden states",
            "Causality analysis for evaluating the security of large language models",
            "Finding safety neurons in large language models",
            "Trustworthy llms: a survey and guideline for evaluating large language models’ alignment",
            "Atman: Understanding transformer predictions through memory efficient attention manipulation",
            "Attnlrp: attention-aware layer-wise relevance propagation for transformers",
            "Towards best practices of activation patching in language models: Metrics and methods",
            "Interpretability in the wild: a circuit for indirect object identification in gpt-2 small",
            "Finding alignments between interpretable causal variables and distributed neural representations",
            "How does gpt-2 compute greater-than?: Interpreting mathematical abilities in a pre-trained language model",
            "The linear representation hypothesis and the geometry of large language models",
            "Representation engineering: A top-down approach to ai transparency",
            "Discovering latent knowledge in language models without supervision",
            "Sparse feature circuits: Discovering and editing interpretable causal graphs in language models",
            "Sparse autoencoders find highly interpretable features in language models",
            "Jailbroken: How does llm safety training fail?",
            "Probing classifiers: Promises, shortcomings, and advances",
            "Language models represent space and time",
            "The geometry of truth: Emergent linear structure in large language model representations of true/false datasets",
            "https://www.lesswrong.com/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens",
            "Attention lens: A tool for mechanistically interpreting the attention head information retrieval mechanism",
            "Eliciting latent predictions from transformers with the tuned lens",
            "Generative judge for evaluating alignment"
        ]
    },
    {
        "index": 145,
        "title": "JailbreakLens: Visual Analysis of Jailbreak Attacks Against Large Language Models",
        "publication_date": "2024-04-12",
        "references": [
            "GPT-4V(ision) System Card",
            "JailbreakChat Website",
            "OpenAI’s Embedding Models",
            "OpenAI’s Usage Policies",
            "Gpt-4 technical report",
            "Spell-burst: A node-based interface for exploratory creative coding with natural language prompts",
            "Shared interest: Measuring human-ai alignment to identify recurring patterns in model behavior",
            "Promptify: Text-to-image generation through interactive prompt exploration with large language models",
            "Are aligned neural networks adversarially aligned?",
            "Dece: Decision explorer with counter-factual explanations for machine learning models",
            "Comprehensive assessment of jailbreak attacks against llms",
            "Masterkey: Automated jailbreaking of large language model chatbots",
            "Bert: Pre-training of deep bidirectional transformers for language understanding",
            "A wolf in sheep’s clothing: Generalized nested jailbreak prompts can fool large language models easily",
            "Glm: General language model pretraining with autoregressive blank infilling",
            "Mediators: Conversational agents explaining nlp model behavior",
            "ipoet: interactive painting poetry creation with visual multimodal analysis",
            "Xnli: Explaining and diagnosing nli-based visual data analysis",
            "Promptmagician: Interactive prompt engineering for text-to-image creation",
            "Transforlearn: Interactive visual tutorial for the transformer model",
            "Cecav-dnn: Collective ensemble comparison and visualization using deep neural networks",
            "Composite backdoor attacks against large language models",
            "Graphologue: Exploring large language model responses with interactive diagrams",
            "Quack: Automatic jailbreaking large language models via role-playing",
            "Exploiting programmatic behavior of llms: Dual-use through standard security attacks",
            "Visualizing and understanding recurrent networks",
            "Multi-step jailbreaking privacy attacks on chatgpt",
            "Visualizing and understanding neural models in nlp",
            "Semantic mirror jailbreak: Genetic algorithm based jailbreak prompts against open-source llms",
            "Jailbreaking chatgpt via prompt engineering: An empirical study",
            "Sprout: Authoring programming tutorials with interactive visualization of large language model generation process",
            "Agentlens: Visual analysis for agent behaviors in llm-based autonomous systems",
            "A unified approach to interpreting model predictions",
            "Explaining vulnerabilities to adversarial machine learning through visual analytics",
            "Comparative visualization for parameter studies of dataset series",
            "Harmbench: A standardized evaluation framework for automated red teaming and robust refusal",
            "Understanding hidden memories of recurrent neural networks",
            "Training language models to follow instructions with human feedback",
            "Storyfier: Exploring vocabulary learning support with text generation models",
            "Ignore previous prompt: Attack techniques for language models",
            "Visual adversarial examples jailbreak aligned large language models",
            "Beyond accuracy: Behavioral testing of nlp models with checklist",
            "Visual explanation for open-domain question answering with bert",
            "Jailbreak in pieces: Compositional adversarial attacks on multi-modal language models",
            "Survey of vulnerabilities in large language models revealed by adversarial attacks",
            "“do anything now”: Characterizing and evaluating in-the-wild jailbreak prompts on large language models",
            "Navigating the overkill in large language models",
            "A statistical interpretation of term specificity and its application in retrieval",
            "Conceptnet 5.5: An open multilingual graph of general knowledge",
            "explainer: A visual analytics framework for interactive and explainable machine learning",
            "Lstmvis: A tool for visual analysis of hidden state dynamics in recurrent neural networks",
            "Interactive and visual prompt engineering for ad-hoc task adaptation with large language models",
            "Sensecape: Enabling multilevel exploration and sensemaking with large language models",
            "Safety assessment of chinese large language models",
            "Llama: Open and efficient foundation language models",
            "Visualizing data using t-sne",
            "Attention is all you need",
            "Commonsensevis: Visualizing and understanding commonsense reasoning capabilities of natural language models",
            "Jailbroken: How does llm safety training fail?"
        ]
    },
    {
        "index": 147,
        "title": "JailbreakZoo: Survey, Landscapes, and Horizons in Jailbreaking Large Language and Vision-Language Models",
        "publication_date": "2024-07-25",
        "references": [
            "Language models are few-shot learners",
            "Gpt-4 technical report",
            "Bert: Pre-training of deep bidirectional transformers for language understanding",
            "Learning transferable visual models from natural language supervision",
            "Zero-shot text-to-image generation",
            "Flamingo: a visual language model for few-shot learning",
            "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "Palm: Scaling language modeling with pathways",
            "Jailbreak: oncogene-induced senescence and its evasion",
            "Extracting training data from large language models",
            "Universal adversarial triggers for attacking and analyzing nlp",
            "Survey of vulnerabilities in large language models revealed by adversarial attacks",
            "A comprehensive survey of attack techniques, implementation, and mitigation strategies in large language models",
            "Tricking llms into disobedience: Understanding, analyzing, and preventing jailbreaks",
            "Towards trustworthy and aligned machine learning: A data-centric survey with causality perspectives",
            "Google bard generated literature review: metaverse",
            "Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality",
            "Llama: Open and efficient foundation language models",
            "Against the achilles’ heel: A survey on red teaming for generative models",
            "Exploiting cloze questions for few shot text classification and natural language inference",
            "Making pre-trained language models better few-shot learners",
            "prompt, and predict: A systematic survey of prompting methods in natural language processing.",
            "Prompt programming for large language models: Beyond the few-shot paradigm",
            "Autoprompt: Eliciting knowledge from language models with automatically generated prompts",
            "Controllable text generation with focused variation",
            "Prefix-tuning: Optimizing continuous prompts for generation",
            "Learning how to ask: Querying lms with mixtures of soft prompts",
            "Deep reinforcement learning from human preferences",
            "Learning to summarize with human feedback",
            "Fine-tuning language models from human preferences",
            "Recursively summarizing books with human feedback",
            "From chatgpt to threatgpt: Impact of generative ai in cybersecurity and privacy",
            "Jailbroken: How does llm safety training fail?",
            "Llm censorship: A machine learning challenge or a computer security problem?",
            "Summon a demon and bind it: A grounded theory of llm red teaming in the wild",
            "Exploiting large language models (llms) through deception techniques and persuasion principles",
            "Easyjailbreak: A unified framework for jailbreaking large language models",
            "Coercing llms to do and reveal (almost) anything",
            "How trustworthy are open-source llms? an assessment under malicious demonstrations shows their vulnerabilities",
            "How (un) ethical are instruction-centric responses of llms? unveiling the vulnerabilities of safety guardrails to harmful queries",
            "Artprompt: Ascii art-based jailbreak attacks against aligned llms",
            "Toolsword: Unveiling safety issues of large language models in tool learning across three stages",
            "Spml: A dsl for defending language models against prompt attacks",
            "A strongreject for empty jailbreaks",
            "Llm jailbreak attack versus defense techniques–a comprehensive study",
            "The art of defending: A systematic evaluation and analysis of llm defense strategies on safety and over-defensiveness",
            "Figstep: Jailbreaking large vision-language models via typographic visual prompts",
            "Towards adversarial attack on vision-language pre-training models",
            "Ot-attack: Enhancing adversarial transferability of vision-language models via optimal transport optimization",
            "Set-level guidance attack: Boosting adversarial transferability of vision-language pre-training models",
            "Jailbreak in pieces: Compositional adversarial attacks on multi-modal language models",
            "How robust is google’s bard to adversarial image attacks?",
            "Rethinking model ensemble in transfer-based adversarial attacks",
            "Visual-roleplay: Universal jailbreak attack on multimodal large language models via role-playing image characte",
            "Jailbreaking attack against multimodal large language model",
            "Visual adversarial examples jailbreak aligned large language models",
            "Are aligned neural networks adversarially aligned?"
        ]
    },
    {
        "index": 148,
        "title": "Knowledge-to-Jailbreak: One Knowledge Point Worth One Attack",
        "publication_date": "2024-06-17",
        "references": [
            "Many-shot jailbreaking",
            "The medical licensing examination debate. Regulation & Governance , 11(3):315–322.",
            "Qwen technical report. arXiv preprint arXiv:2309.16609",
            "Safety-tuned LLaMAs: Lessons from improving the safety of large language models that follow instructions. In The Twelfth International Conference on Learning Representations",
            "Language models are few-shot learners. In Advances in Neural Information Processing Systems 33",
            "Sparks of artificial general intelligence: Early experiments with gpt-4. arXiv preprint arXiv:2303.12712",
            "Take a look at it! rethinking how to evaluate language model jailbreak. arXiv preprint arXiv:2404.06407",
            "Jailbreaking black box large language models in twenty queries. arXiv preprint arXiv:2310.08419",
            "Why should adversarial perturbations be imperceptible? rethink the research paradigm in adversarial nlp. arXiv preprint arXiv:2210.10683",
            "Adapting large language models via reading comprehension. In The Twelfth International Conference on Learning Representations",
            "Pandora: Jailbreak gpts by retrieval augmented generation poisoning. arXiv preprint arXiv:2402.08416",
            "ROBBIE: Robust bias evaluation of large generative language models. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
            "Ai hospital: Interactive evaluation and collaboration of llms as intern doctors for clinical diagnosis. arXiv preprint arXiv:2402.09742",
            "Catastrophic jailbreak of open-source llms via exploiting generation. arXiv preprint arXiv:2310.06987",
            "Mistral 7b",
            "Is chat-gpt a good translator? yes with gpt-4 as the engine. arXiv preprint arXiv:2301.08745",
            "Attack-eval: How to evaluate the effectiveness of jailbreak attacking on large language models. arXiv preprint arXiv:2401.09002",
            "Dense passage retrieval for open-domain question answering. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing",
            "Dbpedia–a large-scale, multilingual knowledge base extracted from wikipedia. Semantic web , 6(2):167–195",
            "ROUGE: A package for automatic evaluation of summaries. In Text Summarization Branches Out",
            "Goal-oriented prompt attack and safety evaluation for llms. arXiv e-prints",
            "Autodan: Generating stealthy jailbreak prompts on aligned large language models. arXiv preprint arXiv:2310.04451",
            "Decoupled weight decay regularization. In International Conference on Learning Representations",
            "Eraser: Jailbreaking defense in large language models via unlearning harmful knowledge. arXiv preprint arXiv:2404.05880",
            "Medical licensing examinations in the united states. Journal of dental education , 66(5):595–599",
            "Use of llms for illicit purposes: Threats, prevention measures, and vulnerabilities. arXiv preprint arXiv:2308.12833",
            "Gpt-4 technical report. OpenAI",
            "Healthcare copilot: Eliciting the power of general llms for medical consultation. arXiv preprint arXiv:2402.13408",
            "Safetyprompts: a systematic review of open datasets for evaluating and improving large language model safety. arXiv preprint arXiv:2404.05399",
            "“Do Anything Now”: Characterizing and Evaluating In-The-Wild Jailbreak Prompts on Large Language Models. In ACM SIGSAC Conference on Computer and Communications Security",
            "Stanford alpaca: An instruction-following llama model. https://github.com/tatsu-lab/stanford_alpaca",
            "Internlm: A multilingual language model with progressively enhanced capabilities. https://github.com/InternLM/InternLM",
            "Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288",
            "Littlemu: Deploying an online virtual teaching assistant via heterogeneous sources integration and chain of teach prompts",
            "Do-not-answer: A dataset for evaluating safeguards in llms. arXiv preprint arXiv:2308.13387",
            "Jailbroken: How does LLM safety training fail? In Thirty-seventh Conference on Neural Information Processing Systems",
            "Shadow alignment: The ease of subvert-ing safely-aligned language models. arXiv preprint arXiv:2310.02949",
            "Kola: Carefully benchmarking world knowledge of large language models. arXiv preprint arXiv:2306.09296",
            "Gpt-4 is too smart to be safe: Stealthy chat with llms via cipher. arXiv preprint arXiv:2308.06463",
            "GLM-130b: An open bilingual pre-trained model. In The Eleventh International Conference on Learning Representations",
            "How johnny can persuade llms to jailbreak them: Rethinking persuasion to challenge ai safety by humanizing llms. arXiv preprint arXiv:2401.06373",
            "Universal and transferable adversarial attacks on aligned language models. arXiv preprint arXiv:2307.15043",
            "Easyjailbreak: A unified framework for jail-breaking large language models. arXiv preprint arXiv:2403.12171",
            "Weak-to-strong jailbreaking on large language models. arXiv preprint arXiv:2401.17256",
            "Judging llm-as-a-judge with mt-bench and chatbot arena. arXiv preprint arXiv:2306.05685"
        ]
    },
    {
        "index": 149,
        "title": "LEARNING DIVERSE ATTACKS ON LARGE LANGUAGE MODELS FOR ROBUST RED-TEAMING AND SAFETY TUNING",
        "publication_date": "2025-02-28",
        "references": [
            "Training a helpful and harmless assistant with reinforcement learning from human feedback",
            "Constitutional AI: Harmlessness from AI feedback",
            "Emergent tool use from multi-agent autocurricula",
            "Flow network based generative models for non-iterative diverse candidate generation",
            "GFlowNet foundations",
            "Safety-tuned LLaMAs: Lessons from improving the safety of large language models that follow instructions",
            "Language models are few-shot learners",
            "Jailbreaking black box large language models in twenty queries",
            "Decision transformer: Reinforcement learning via sequence modeling",
            "Think you have solved question answering? try ARC, the AI2 reasoning challenge",
            "Training verifiers to solve math word problems",
            "Free Dolly: Introducing the world's first truly open instruction-tuned llm",
            "Bayesian structure learning with generative flow networks",
            "Build it break it fix it for dialogue safety: Robustness from adversarial human attack",
            "The llama 3 herd of models",
            "RvS: What is essential for offline RL via supervised learning?",
            "Reinforcement learning with a corrupted reward channel",
            "Reinforcement learning with deep energy-based policies",
            "Measuring massive multitask language understanding",
            "Curiosity-driven red-teaming for large language models",
            "LoRA: Low-rank adaptation of large language models",
            "GFlowNet-EM for learning compositional latent variable models",
            "Amortizing intractable inference in large language models",
            "Llama guard: LLM-based input-output safeguard for human-AI conversations",
            "Biological sequence design with gflownets",
            "Baseline defenses for adversarial attacks against aligned language models",
            "BC-Z: zero-shot task generalization with robotic imitation learning",
            "Language models are unsupervised multitask learners",
            "Zero-shot text-to-image generation",
            "Photorealistic text-to-image diffusion models with deep language understanding",
            "Rainbow teaming: Open-ended generation of diverse adversarial prompts",
            "Equivalence between policy gradients and soft Q-learning",
            "Proximal policy optimization algorithms",
            "Defining and characterizing reward gaming",
            "Llama 2: Open foundation and fine-tuned chat models",
            "A-NeSI: A scalable approximate method for probabilistic neurosymbolic inference",
            "Learning from the worst: Dynamically generated datasets to improve online hate detection",
            "Analyzing dynamic adversarial training data in the limit",
            "MiniLMv2: Multi-head self-attention relation distillation for compressing pretrained transformers",
            "How far can camels go? exploring the state of instruction tuning on open resources",
            "Jailbroken: How does LLM safety training fail?",
            "Chain-of-thought prompting elicits reasoning in large language models",
            "Ethical and social risks of harm from language models",
            "Simple statistical gradient-following algorithms for connectionist reinforcement learning",
            "Supporting mouthing in signed languages: New innovations and a proposal for future corpus building",
            "Bot-adversarial dialogue for safe conversational agents",
            "HellaSwag: Can a machine really finish your sentence?",
            "Robust scheduling with GFlownets",
            "Let the flows tell: Solving graph combinatorial problems with GFlowNets",
            "Accelerating greedy coordinate gradient via probe sampling",
            "Starling-7B: Improving LLM helpfulness & harmlessness with RLAIF",
            "Texygen: A benchmarking platform for text generation models",
            "Universal and transferable adversarial attacks on aligned language models"
        ]
    },
    {
        "index": 150,
        "title": "LeCov: Multi-level Testing Criteria for Large Language Models",
        "publication_date": "2024-08-20",
        "references": [
            "Gpt-4 technical report.",
            "Expectation vs. experience: Evaluating the usability of code generation tools powered by large language models.",
            "Robots that ask for help: Uncertainty alignment for large language model planners.",
            "Isr-llm: Iterative self-refined large language model for long-horizon sequential task planning.",
            "Ccnet: Extracting high quality monolingual datasets from web crawl data.",
            "Exploring the limits of transfer learning with a unified text-to-text transformer.",
            "The pile: An 800gb dataset of diverse text for language modeling.",
            "Artificial general intelligence: concept, state of the art, and future prospects.",
            "Trustllm: Trustworthiness in large language models.",
            "Decodingtrust: A comprehensive assessment of trustworthiness in GPT models.",
            "Trustworthy llms: a survey and guideline for evaluating large language models’ alignment.",
            "Selfcheckgpt: Zero-resource black-box hallucination detection for generative large language models.",
            "Real-toxicityprompts: Evaluating neural toxic degeneration in language models.",
            "Curiosity-driven red-teaming for large language models.",
            "Autodan: Generating stealthy jail-break prompts on aligned large language models.",
            "S-eval: Automatic and adaptive test generation for benchmarking safety evaluation of large language models.",
            "A software engineering perspective on testing large language models: Research, practice, tools and benchmarks.",
            "Software defects and their impact on system availability-a study of field failures in operating systems.",
            "Truthfulqa: Measuring how models mimic human falsehoods.",
            "Siren’s song in the ai ocean: a survey on hallucination in large language models.",
            "Software testing research: Achievements, challenges, dreams.",
            "Deephunter: a coverage-guided fuzz testing framework for deep neural networks.",
            "Software testing for machine learning.",
            "Model-based exploration of the frontier of behaviours for deep learning system testing.",
            "Black-box testing of deep neural networks through test case diversity.",
            "Deepgini: prioritizing massive tests to enhance the robustness of deep neural networks.",
            "Adaptive test selection for deep neural networks.",
            "Practical accuracy estimation for efficient deep neural network testing.",
            "Aries: Efficient testing of deep neural networks via labeling-free accuracy estimation.",
            "Deepxplore: Automated whitebox testing of deep learning systems.",
            "Deepgauge: Multi-granularity testing criteria for deep learning systems.",
            "Guiding deep learning system testing using surprise adequacy.",
            "Npc: Neuron path coverage via characterizing decision logic of deep neural networks.",
            "Importance-driven deep learning system testing.",
            "To believe or not to believe your llm.",
            "Look before you leap: An exploratory study of uncertainty measurement for large language models.",
            "The llama 3 herd of models.",
            "Attention is all you need.",
            "A survey of large language models.",
            "Cc: Causality-aware coverage criterion for deep neural networks.",
            "Online safety analysis for llms: a benchmark, an assessment, and a path forward.",
            "Semantic uncertainty: Linguistic invariances for uncertainty estimation in natural language generation.",
            "Can LLMs express their uncertainty? an empirical evaluation of confidence elicitation in LLMs.",
            "Enhancing fault detection for large language models via mutation-based confidence smoothing.",
            "Test optimization in dnn testing: a survey.",
            "Inference-time intervention: Eliciting truthful answers from a language model.",
            "Llama guard: Llm-based input-output safeguard for human-ai conversations.",
            "Deepstellar: Model-based quantitative analysis of stateful deep learning systems.",
            "Machine learning testing: Survey, landscapes and horizons.",
            "Improving short text classification through global augmentation methods.",
            "Llama 2: Open foundation and fine-tuned chat models.",
            "Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality.",
            "Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension.",
            "Natural questions: a benchmark for question answering research.",
            "Test selection for deep learning systems.",
            "Predicting the generalization gap in deep networks with margin distributions.",
            "Coverage-guided testing for recurrent neural networks.",
            "Testing deep neural networks.",
            "Decision-guided weighted automata extraction from recurrent neural networks.",
            "Safetybench: Evaluating the safety of large language models with multiple choice questions.",
            "Self-criticism: Aligning large language models with their understanding of helpfulness, honesty, and harmlessness.",
            "Beyond accuracy: Evaluating self-consistency of code large language models with identitychain.",
            "Training language models to follow instructions with human feedback.",
            "Direct preference optimization: Your language model is secretly a reward model.",
            "Beavertails: Towards improved safety alignment of llm via a human-preference dataset.",
            "A holistic approach to undesired content detection in the real world."
        ]
    },
    {
        "index": 151,
        "title": "LEVERAGING BIASES IN LARGE LANGUAGE MODELS: “BIAS-KNN” FOR EFFECTIVE FEW-SHOT LEARNING",
        "publication_date": "2024-01-18",
        "references": [
            "Language models as knowledge bases?",
            "Language models are few-shot learners",
            "Exploiting cloze-questions for few-shot text classification and natural language inference",
            "Calibrate before use: Improving few-shot performance of language models",
            "Surface form competition: Why the highest probability answer isn’t always right",
            "Enhancing grammatical error correction systems with explanations",
            "Prototypical calibration for few-shot learning of language models",
            "Improving few-shot performance of language models via nearest neighbor calibration",
            "$k$NN prompting: Beyond-context learning with calibration-free nearest neighbor inference",
            "Generalization through memorization: Nearest neighbor language models",
            "Language models are unsupervised multitask learners",
            "Recursive deep models for semantic compositionality over a sentiment treebank",
            "Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales",
            "Mining and summarizing customer reviews",
            "Character-level convolutional networks for text classification",
            "A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts",
            "The pascal recognising textual entailment challenge",
            "It’s not just size that matters: Small language models are also few-shot learners",
            "Knowledgeable prompt-tuning: Incorporating knowledge into prompt verbalizer for text classification",
            "Autoprompt: Eliciting knowledge from language models with automatically generated prompts"
        ]
    },
    {
        "index": 153,
        "title": "Multi-Turn Human Jailbreaks Are Not Robust to Multi-Turn Human Jailbreaks Yet",
        "publication_date": "2024-09-04",
        "references": [
            "Does refusal training in llms generalize to the past tense?",
            "Jailbreaking leading safety-aligned LLMs with simple adaptive attacks",
            "Many-shot jailbreaking",
            "Unlearning via rmu is mostly shallow",
            "Refusal in language models is mediated by a single direction",
            "Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples",
            "Training a helpful and harmless assistant with reinforcement learning from human feedback",
            "Constitutional ai: Harmlessness from ai feedback",
            "Machine unlearning",
            "Towards making systems forget with machine unlearning",
            "Are aligned neural networks adversarially aligned?",
            "Explore, establish, exploit: Red teaming language models from scratch",
            "Black-box access is insufficient for rigorous ai audits",
            "Jailbreaking black box large language models in twenty queries",
            "Jailbreakbench: An open robustness benchmark for jailbreaking large language models",
            "A wolf in sheep’s clothing: Generalized nested jailbreak prompts can fool large language models easily",
            "The llama 3 herd of models",
            "Export administration regulations (ear), 15 cfr parts 730-774",
            "Llm agents can autonomously hack websites",
            "Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned",
            "Mart: Improving llm safety with multi-round automatic red-teaming",
            "Attacking large language models with projected gradient descent",
            "Emerging vulnerabilities in frontier models: Multi-turn jailbreak attacks",
            "Llm censorship: A machine learning challenge or a computer security problem?",
            "Explaining and harnessing adversarial examples",
            "Covert malicious finetuning: Challenges in safeguarding llm adaptation",
            "Jailbreaking proprietary large language models using word substitution cipher",
            "Unsolved problems in ml safety",
            "Red-teaming large language models to identify novel ai risks",
            "Adversarial examples are not bugs, they are features",
            "Summons a demon and binds it: A grounded theory of llm red teaming in the wild",
            "International traffic in arms regulations (itar), 22 cfr parts 120-130",
            "Pku-saferlhf: A safety alignment preference dataset for llama family models",
            "Adversarial examples for evaluating reading comprehension systems",
            "Artprompt: Ascii art-based jailbreak attacks against aligned llms",
            "Wildteaming at scale: From in-the-wild jailbreaks to (adversarially) safer language models",
            "Adversarial examples in the physical world",
            "Lora fine-tuning efficiently undoes safety training in llama 2-chat 70b",
            "Retrieval-augmented generation for knowledge-intensive nlp tasks",
            "The wmdp benchmark: Measuring and reducing malicious use with unlearning",
            "Drattack: Prompt decomposition and reconstruction makes powerful llm jailbreakers",
            "Large language model unlearning via embedding-corrupted prompts",
            "Autodan: Generating stealthy jailbreak prompts on aligned large language models",
            "Eight methods to evaluate robust unlearning in llms",
            "Towards deep learning models resistant to adversarial attacks",
            "Propagating universal perturbations to attack large language model guard-rails",
            "Harmbench: A standardized evaluation framework for automated red teaming and robust refusal",
            "Tree of attacks: Jailbreaking black-box llms automatically",
            "Gpt-4 technical report",
            "Training language models to follow instructions with human feedback",
            "Feedback loops with language models drive in-context reward hacking",
            "Steering llama 2 via contrastive activation addition",
            "Red teaming language models with language models",
            "Safety alignment should be made more than just a few tokens deep",
            "Direct preference optimization: Your language model is secretly a reward model",
            "Representation noising effectively prevents harmful fine-tuning on llms",
            "Great, now write an article about that: The crescendo multi-turn llm jailbreak attack",
            "Revisiting the robust alignment of circuit breakers",
            "Characterizing and evaluating in-the-wild jailbreak prompts on large language models",
            "Targeted latent adversarial training improves robustness to persistent harmful behaviors in llms",
            "AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts",
            "Pal: Proxy-guided black-box attack on large language models",
            "Multi-turn context jailbreak attack on large language models from first principles",
            "Tamper-resistant safeguards for open-weight llms",
            "Gemini: A family of highly capable multimodal models",
            "Breaking circuit breakers",
            "Activation addition: Steering language models without optimization",
            "Universal adversarial triggers for attacking and analyzing NLP",
            "Jailbroken: How does llm safety training fail?",
            "Star: Sociotechnical approach to red teaming language models",
            "Efficient adversarial training in llms with continuous attacks",
            "Low-resource languages jailbreak gpt-4",
            "Gptfuzzer: Red teaming large language models with auto-generated jailbreak prompts",
            "Refuse whenever you feel unsafe: Improving safety in llms via decoupled refusal training",
            "How johnny can persuade llms to jailbreak them: Rethinking persuasion to challenge ai safety by humanizing llms",
            "Robust prompt optimization for defending language models against jailbreaking attacks",
            "Representation engineering: A top-down approach to ai transparency",
            "Universal and transferable adversarial attacks on aligned language models",
            "Improving alignment and robustness with circuit breakers"
        ]
    },
    {
        "index": 154,
        "title": "LLM Detectors Still Fall Short of Real World: Case of LLM-Generated Short News-Like Posts",
        "publication_date": "2024-09-27",
        "references": [
            "Dos and don’ts of machine learning in computer security",
            "Real or fake? learning to discriminate machine from human generated text",
            "On the dangers of stochastic parrots: Can language models be too big?",
            "Can llm-generated misinformation be detected?",
            "ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators",
            "LLM Detectors",
            "BERT: pre-training of deep bidirectional transformers for language understanding",
            "Computational propaganda: If you make it trend, you make it true",
            "Exposure to the russian internet research agency foreign influence campaign on twitter in the 2016 us election and its relationship to attitudes and voting behavior",
            "Efficient black-box adversarial attacks on neural text detectors",
            "Russian propaganda on social media during the 2022 invasion of ukraine",
            "Generative language models and automated influence operations: Emerging threats and potential mitigations",
            "Textbooks are all you need",
            "Mgt-bench: Benchmarking machine-generated text detection",
            "Parameter-Efficient Transfer Learning for NLP",
            "Token-Ensemble Text Generation: On Attacking the Automatic AI-Generated Text Detection",
            "Automatic detection of generated text is easiest when humans are fooled",
            "Paraphrasing evades detectors of ai-generated text, but retrieval is an effective defense",
            "Host-pathongen co-evolution inspired algorithm enables robust GAN training",
            "Mage: Machine-generated text detection in the wild",
            "GPT detec-tors are biased against non-native English writers",
            "Multitude: Large-scale multilingual machine-generated text detection benchmark",
            "Uncovering coordinated networks on social media: Methods and case studies",
            "On the zero-shot generalization of machine-generated text detectors",
            "Can AI-Generated Text be Reliably Detected?",
            "Release strategies and the social impacts of language models",
            "Llama 2: Open foundation and fine-tuned chat models",
            "Zephyr: Direct distillation of LM alignment",
            "The spread of true and false news online",
            "Stumbling blocks: Stress testing the robustness of machine-generated text detectors under attacks",
            "M4: multi-generator, multi-domain, and multi-lingual black-box machine-generated text detection",
            "Fake news in sheep’s clothing: Robust fake news detection against llm-empowered style attacks",
            "On the generalization of training-based chatgpt detection methods",
            "Defending against neural fake news"
        ]
    },
    {
        "index": 155,
        "title": "LLMs can be Dangerous Reasoners: Analyzing-based Jailbreak Attack on Large Language Models",
        "publication_date": "2025-03-05",
        "references": [
            "Gpt-4 technical report.",
            "Detecting language model attacks with perplexity.",
            "Many-shot jailbreaking.",
            "Introducing claude.",
            "Constitutional ai: harmlessness from ai feedback.",
            "Defending against alignment-breaking attacks via robustly aligned llm.",
            "Social-cognitive theories and the coherence of personality.",
            "Jailbreaking black box large language models in twenty queries.",
            "Can large language models be an alternative to human evaluations?",
            "Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality.",
            "Deep reinforcement learning from human preferences.",
            "Scaling instruction-finetuned language models.",
            "Masterkey: Automated jailbreaking of large language model chatbots.",
            "Multilingual jailbreak challenges in large language models.",
            "A wolf in sheep’s clothing: Generalized nested jailbreak prompts can fool large language models easily.",
            "Raft: Reward ranked finetuning for generative foundation model alignment.",
            "Chatglm: A family of large language models from glm-130b to glm-4 all tools.",
            "Catastrophic jailbreak of open-source llms via exploiting generation.",
            "Openai o1 system card.",
            "Pretraining language models with human preferences.",
            "Defending against alignment-breaking attacks via robustly aligned llm.",
            "Process for adapting language models to society (palms) with values-targeted datasets.",
            "Principle-driven self-alignment of language models from scratch with minimal human supervision.",
            "Single turn harmful outputs leaderboard.",
            "Llama 2: Open foundation and fine-tuned chat models.",
            "Exploring the limits of domain-adaptive training for detoxifying large-scale language models.",
            "Jailbroken: How does llm safety training fail?",
            "Chain-of-thought prompting elicits reasoning in large language models.",
            "Jailbreak and guard aligned language models with only few in-context demonstrations.",
            "Wordcraft: story writing with large language models.",
            "Gpt-4 is too smart to be safe: Stealthy chat with llms via cipher.",
            "Planning with large language models for code generation.",
            "Can chatgpt understand too? a comparative study on chatgpt and fine-tuned bert.",
            "Fine-tuning language models from human preferences.",
            "Universal and transferable adversarial attacks on aligned language models."
        ]
    },
    {
        "index": 156,
        "title": "Lockpicking LLMs: A Logit-Based Jailbreak Using Token-level Manipulation",
        "publication_date": "2024-06-19",
        "references": [
            "Llama 3 model card",
            "Jailbreaking black box large language models in twenty queries",
            "Jailbreakbench: An open robustness benchmark for jailbreaking large language models",
            "Lifelong knowledge editing for llms with retrieval-augmented continuous prompt learning",
            "Struq: Defending against prompt injection with structured queries",
            "Robust and scalable model editing for large language models",
            "Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality",
            "Masterkey: Automated jailbreaking of large language model chatbots",
            "Pandora: Jailbreak gpts by retrieval augmented generation poisoning",
            "Efficient llm jailbreak via adaptive dense-to-sparse constrained optimization",
            "Mistral 7b",
            "Halluvault: A novel logic programming-aided metamorphic testing framework for detecting fact-conflicting hallucinations in large language models",
            "Flame: Factuality-aware alignment for large language models",
            "Automatic and universal prompt injection attacks against large language models",
            "Prompt injection attack against llm-integrated applications, june 2023",
            "Keeping llms aligned after fine-tuning: The crucial role of prompt templates",
            "Jailbreaking prompt attack: A controllable adversarial attack against diffusion models",
            "Memllm: Finetuning llms to use an explicit read-write memory",
            "Large dual encoders are generalizable retrievers",
            "Advprompter: Fast adaptive adversarial prompting for llms",
            "Spectral editing of activations for large language model alignment",
            "Signed-prompt: A new approach to prevent prompt injection attacks against llm-integrated applications",
            "Gemma: Open models based on gemini research and technology",
            "Meta llama guard 2",
            "Llama: Open and efficient foundation language models",
            "Tensor trust: Interpretable prompt injection attacks from an online game",
            "Relative preference optimization: Enhancing llm alignment through contrasting responses across identical and diverse prompts",
            "Gptfuzzer: Red teaming large language models with auto-generated jailbreak prompts",
            "Injecagent: Benchmarking indirect prompt injections in tool-integrated large language model agents",
            "Goal-guided generative prompt injection attack on large language models",
            "Knowledge graph enhanced large language model editing",
            "Don’t say no: Jailbreaking llm by suppressing refusal",
            "Universal and transferable adversarial attacks on aligned language models"
        ]
    },
    {
        "index": 157,
        "title": "MAGE: Machine-generated Text Detection in the Wild",
        "publication_date": "2023-09-15",
        "references": [
            "Identifying real or fake articles: Towards better language modeling",
            "Real or fake? learning to discriminate machine from human generated text",
            "Fast-detectgpt: Efficient zero-shot detection of machine-generated text via conditional probability curvature",
            "XLM-T: Multilingual language models in Twitter for sentiment analysis and beyond",
            "Longformer: The long-document transformer",
            "Computer-generated text detection using machine learning: A systematic review",
            "Bloom: A 176b-parameter open-access multilingual language model",
            "GPT-NeoX-20B: An open-source autoregressive language model",
            "Language models are few-shot learners",
            "On the possibilities of ai-generated text detection",
            "SciXGen: A scientific paper dataset for context-aware text generation",
            "DialogSum: A real-life scenario dialogue summarization dataset",
            "Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality",
            "Scaling instruction-finetuned language models",
            "BERT: Pre-training of deep bidirectional transformers for language understanding",
            "Tweepfake: About detecting deepfake tweets",
            "Bag of tricks for efficient text classification",
            "Adam: A method for stochastic optimization",
            "A watermark for large language models",
            "Paraphrasing evades detectors of ai-generated text, but retrieval is an effective defense",
            "Detecting fake content with relative entropy scoring",
            "GPT detectors are biased against non-native english writers",
            "Roberta: A robustly optimized bert pretraining approach",
            "Learning word vectors for sentiment analysis",
            "Natural language watermarking via morphosyntactic alterations",
            "Detectgpt: Zero-shot machine-generated text detection using probability curvature",
            "A corpus and cloze evaluation for deeper understanding of commonsense stories",
            "SemEval-2013 task 2: Sentiment analysis in Twitter",
            "Don’t give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization",
            "Scikit-learn: Machine learning in Python",
            "Deepfake text detection: Limitations and opportunities",
            "High-resolution image synthesis with latent diffusion models",
            "Get to the point: Summarization with pointer-generator networks",
            "Effidit: An assistant for improving writing efficiency",
            "Winning arguments: Interaction dynamics and persuasion strategies in good-faith online discussions",
            "Stanza: A python natural language processing toolkit for many human languages",
            "Language models are unsupervised multitask learners",
            "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "SQuAD: 100,000+ questions for machine comprehension of text",
            "Cross-domain detection of GPT-2-generated technical text",
            "Stanford alpaca: An instruction-following llama model",
            "Llama: Open and efficient foundation language models",
            "Authorship attribution for neural text generation",
            "GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model",
            "HellaSwag: Can a machine really finish your sentence?",
            "Defending against neural fake news",
            "Glm-130b: An open bilingual pre-trained model",
            "Adding conditional control to text-to-image diffusion models",
            "Opt: Open pre-trained transformer language models",
            "Character-level convolutional networks for text classification",
            "SynGEC: Syntax-enhanced grammatical error correction with a tailored GEC-oriented parser",
            "Protecting language generation models via invisible watermarking"
        ]
    },
    {
        "index": 158,
        "title": "Making LLMs Vulnerable to Prompt Injection via Poisoning Alignment",
        "publication_date": "2024-10-18",
        "references": [
            "How amazon continues to improve the customer reviews experience with generative ai",
            "Prompt injection attacks against GPT-3",
            "Securing LLM Systems Against Prompt Injection",
            "Ignore previous prompt: Attack techniques for language models",
            "Delimiters won't save you from prompt injection",
            "Not what you've signed up for: Compromising real-world llm-integrated applications with indirect prompt injection",
            "Formalizing and benchmarking prompt injection attacks and defenses",
            "Neural exec: Learning (and learning from) execution triggers for prompt injection attacks",
            "Automatic and universal prompt injection attacks against large language models",
            "Pleak: Prompt leaking attacks against large language model applications",
            "Optimization-based prompt injection attack to llm-as-a-judge",
            "Finetuned language models are zero-shot learners",
            "Fine-tuning language models from human preferences",
            "Training language models to follow instructions with human feedback",
            "Deep reinforcement learning from human preferences",
            "Direct preference optimization: Your language model is secretly a reward model",
            "Orca dpo pairs",
            "OWASP Top 10 for Large Language Model Applications",
            "Best-of-venom: Attacking rlhf by injecting poisoned preference data",
            "Is poisoning a real threat to llm alignment? maybe more so than you think",
            "Preference poisoning attacks on reward model learning",
            "Rlhfpoison: Reward poisoning attack for reinforcement learning with human feedback in large language models",
            "Instructions as backdoors: Backdoor vulnerabilities of instruction tuning for large language models",
            "Universal jailbreak backdoors from poisoned human feedback",
            "Towards crowdsourced training of large neural networks using decentralized mixture-of-experts",
            "Gpt-4o",
            "Measuring massive multitask language understanding",
            "Gpqa: A graduate-level google-proof q&a benchmark",
            "Training verifiers to solve math word problems",
            "Magpie: Alignment data synthesis from scratch by prompting aligned llms with nothing",
            "Codeclm: Aligning language models with tailored synthetic data",
            "Llama: Open and efficient foundation language models",
            "The llama 3 herd of models",
            "Gemma: Open models based on gemini research and technology",
            "The falcon series of open language models",
            "Training a helpful and harmless assistant with reinforcement learning from human feedback",
            "ROUGE: A package for automatic evaluation of summaries",
            "Predicting grammaticality on an ordinal scale",
            "Automatically constructing a corpus of sentential paraphrases",
            "Jfleg: A fluency corpus and benchmark for grammatical error correction",
            "Automated hate speech detection and the problem of offensive language",
            "GLUE: A multi-task benchmark and analysis platform for natural language understanding",
            "Recursive deep models for semantic compositionality over a sentiment treebank",
            "Contributions to the study of sms spam filtering: New collection and results",
            "A neural attention model for abstractive sentence summarization",
            "MRPC dataset for duplicate sentence detection",
            "Jfleg dataset for grammar correction",
            "HSOL dataset for hate content detection",
            "RTE dataset for natural language inference",
            "SST2 dataset for sentiment analysis",
            "SMS Spam dataset for spam detection",
            "Gigaword dataset for text summarization"
        ]
    },
    {
        "index": 161,
        "title": "MemeCraft: Contextual and Stance-Driven Multimodal Meme Generation",
        "publication_date": "2024-05-13",
        "references": [
            "Pro-Cap: Leveraging a Frozen Vision-Language Model for Hateful Meme Detection",
            "Prompting for Multimodal Hateful Meme Classification",
            "Language models are few-shot learners for prognostic prediction",
            "InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning",
            "Detecting propaganda techniques in memes",
            "Understanding visual memes: An empirical analysis of text superimposed on memes shared on twitter",
            "Sustainable Development Goals: A need for relevant indicators",
            "Decoding the Underlying Meaning of Multimodal Hateful Memes",
            "On explaining multimodal hateful meme detection models",
            "Detecting hate speech in memes: a review",
            "Supervised multimodal bitransformers for classifying images and text",
            "The hateful memes challenge: Detecting hate speech in multimodal memes",
            "Memes in the Wild: Assessing the Generalizability of the Hateful Memes Challenge Dataset",
            "Internet meme and Political Discourse: A study on the impact of internet meme as a tool in communicating political satire",
            "The Ethics of Interaction: Mitigating Security Threats in LLMs",
            "Disentangling hate in online memes",
            "Llava-med: Training a large language-and-vision assistant for biomedicine in one day",
            "Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models",
            "Visual instruction tuning",
            "Findings of the WOAH 5 shared task on fine grained hateful memes detection",
            "Meme Generator",
            "Analysis of Facebook meme groups used during the 2016 US presidential election",
            "Internet meme as a campaign tool to the fight against Covid-19 in Nigeria",
            "A Multimodal Framework for the Identification of Vaccine Critical Memes on Twitter",
            "One Does Not Simply Produce Funny Memes!",
            "OpenAI Gpt-4 Technical Report",
            "Training language models to follow instructions with human feedback",
            "Dank Learning: Generating Memes Using Deep Neural Networks",
            "Detecting Harmful Memes and Their Targets",
            "MOMENTA: A Multimodal Framework for Detecting Harmful Memes and Their Targets",
            "Quick Meme Website",
            "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "MemeBot: Towards automatic image meme generation",
            "Multitask Prompted Training Enables Zero-Shot Task Generalization",
            "Bloom: A 176b-parameter open-access multilingual language model",
            "SemEval-2020 Task 8: Memotion Analysis-the Visuo-Lingual Metaphor!",
            "Detecting and Understanding Harmful Memes: A Survey",
            "Multimodal and explainable internet meme classification",
            "LLama 2: Open foundation and fine-tuned chat models",
            "Detecting and correcting hate speech in multimodal memes with large visual language model",
            "Memeify: A large-scale meme generation system",
            "I Can Has Cheezburger? A nonparanormal approach to combining textual and visual information for predicting and generating popular meme descriptions",
            "Finetuned language models are zero-shot learners",
            "MET-Meme: A multimodal meme dataset rich in metaphors",
            "mplug-owl: Modularization empowers large language models with multimodality",
            "HallE-Switch: Rethinking and Controlling Object Existence Hallucinations in Large Vision Language Models for Detailed Caption",
            "Changing the World One Meme at a Time: The Effects of Climate Change Memes on Civic Engagement Intentions",
            "Automatic chain of thought prompting in large language models",
            "Judging LLM-as-a-judge with MT-Bench and Chatbot Arena",
            "Chatgpt asks, blip-2 answers: Automatic questioning towards enriched visual descriptions",
            "MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models",
            "Multimodal zero-shot hateful meme detection"
        ]
    },
    {
        "index": 162,
        "title": "Merging Improves Self-Critique Against Jailbreak Attacks",
        "publication_date": "2024-07-14",
        "references": [
            "Constitutional ai: Harmlessness from ai feedback.",
            "Open llm leaderboard.",
            "It’s mbr all the way down: Modern generation techniques through the lens of minimum bayes risk.",
            "Are aligned neural networks adversarially aligned?",
            "Bge m3-embedding: Multi-lingual, multi-functionality, multi-granularity text embeddings through self-knowledge distillation.",
            "Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality.",
            "Rephrase and respond: Let large language models ask better questions for themselves.",
            "Distilled self-critique of LLMs with synthetic data: a bayesian perspective.",
            "Loss surfaces, mode connectivity, and fast ensembling of dnns.",
            "LoRA: Low-rank adaptation of large language models.",
            "Averaging weights leads to wider optima and better generalization.",
            "Mistral 7b.",
            "Mixtral of experts.",
            "Automatically auditing large language models via discrete optimization.",
            "Exploiting asymmetry for synthetic training data generation: SynthIE and the case of information extraction.",
            "Prometheus: Inducing fine-grained evaluation capability in language models.",
            "Prometheus 2: An open source language model specialized in evaluating other language models.",
            "Simple and scalable predictive uncertainty estimation using deep ensembles.",
            "Textbooks are all you need ii: phi-1.5 technical report.",
            "Tinygsm: achieving¿ 80% on gsm8k with small language models.",
            "Autodan: Generating stealthy jailbreak prompts on aligned large language models.",
            "Jailbreaking chatgpt via prompt engineering: An empirical study.",
            "Meta llama guard 2.",
            "Defending llms against jailbreaking attacks via backtranslation.",
            "Jailbroken: How does llm safety training fail?",
            "Chain-of-thought prompting elicits reasoning in large language models.",
            "Direct preference optimization: Your language model is secretly a reward model.",
            "Role play with large language models.",
            "”Do Anything Now”: Characterizing and Evaluating In-The-Wild Jailbreak Prompts on Large Language Models.",
            "Stanford alpaca: An instruction-following llama model.",
            "Training language models to follow instructions with human feedback",
            "Generative agents: Interactive simulacra of human behavior.",
            "Hijacking large language models via adversarial in-context learning.",
            "Universal and transferable adversarial attacks on aligned language models."
        ]
    },
    {
        "index": 163,
        "title": "MITIGATING TEXT TOXICITY WITH COUNTERFACTUAL GENERATION",
        "publication_date": "2024-08-07",
        "references": [
            "Internet, social media and online hate speech. systematic review.",
            "Sok: Hate, harassment, and the changing landscape of online abuse.",
            "Social media and online hate.",
            "Social media and genocide: The case for home state responsibility.",
            "Language Models are Few-Shot Learners.",
            "On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?",
            "Bias and Fairness in Large Language Models: A Survey",
            "Combining generative artificial intelligence (ai) and the internet: Heading towards evolution or degradation?",
            "Detoxifying Text with MaRCo: Controllable Revision with Experts and Anti-Experts.",
            "Text Detoxification using Large Pre-trained Neural Models.",
            "A survey on automatic detection of hate speech in text.",
            "Civil rephrases of toxic texts with self-supervised transformers.",
            "Interpretable Machine Learning",
            "Explainable Artificial Intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI.",
            "Counterfactual explanations and how to find them: literature review and benchmarking.",
            "Enhancing textual counterfactual explanation intelligibility through counterfactual feature importance.",
            "Tigtec: Token importance guided text counterfactuals.",
            "Protecting children from online exploitation: Can a trained model detect harmful communication strategies?",
            "Toxic comment classification challenge.",
            "Bleu: a method for automatic evaluation of machine translation.",
            "GloVe: Global vectors for word representation.",
            "Bertscore: Evaluating text generation with bert.",
            "Language Models are Unsupervised Multitask Learners.",
            "Text style transfer: A review and experimental evaluation.",
            "Fighting offensive language on social media with unsupervised text style transfer.",
            "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer.",
            "Towards a friendly online community: An unsupervised style transfer framework for profanity redaction.",
            "A unified approach to interpreting model predictions.",
            "Axiomatic attribution for deep networks.",
            "Evaluating self-attention interpretability through human-grounded experimental protocol.",
            "Explainable Artificial Intelligence (XAI): What we know and what is left to attain Trustworthy Artificial Intelligence.",
            "Explanation in artificial intelligence: Insights from the social sciences.",
            "The Dangers of Post-hoc Interpretability: Unjustified Counterfactual Explanations.",
            "Text Counterfactuals via Latent Optimization and Shapley-Guided Search.",
            "Explaining NLP Models via Minimal Contrastive Editing (MiCE).",
            "CREST: A joint framework for rationalization and counterfactual text generation.",
            "Plug and Play Counterfactual Text Generation for Model Robustness",
            "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension.",
            "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks.",
            "Perplexity—a measure of the difficulty of speech recognition tasks.",
            "Gemma: Open models based on gemini research and technology.",
            "Anatomy of online hate: developing a taxonomy and machine learning models for identifying and classifying hate in online news media.",
            "Identifying and measuring annotator bias based on annotators’ demographic characteristics.",
            "Annotators with attitudes: How annotator beliefs and identities bias toxic language detection.",
            "Adversarial attack and defense: A survey.",
            "Toward stronger textual attack detectors.",
            "Creative language encoding under censorship.",
            "Custodians of the Internet: Platforms, content moderation, and the hidden decisions that shape social media.",
            "Personalizing content moderation on social media: User perspectives on moderation choices, interface design, and labor.",
            "The psychological impacts of content moderation on content moderators: A qualitative study.",
            "The atlas of AI: Power, politics, and the planetary costs of artificial intelligence."
        ]
    },
    {
        "index": 164,
        "title": "MLLMG UARD: A Multi-dimensional Safety Evaluation Suite for Multimodal Large Language Models",
        "publication_date": "2024-06-13",
        "references": [
            "Llama: Open and efficient foundation language models",
            "Palm: Scaling language modeling with pathways",
            "Language models are few-shot learners",
            "Gpt-4 technical report",
            "Gemini: a family of highly capable multimodal models",
            "Cogvlm: Visual expert for pretrained language models",
            "A survey on multimodal large language models",
            "Red teaming visual language models",
            "Holistic analysis of hallucination in gpt-4v(ision): Bias and interference challenges",
            "Detecting and preventing hallucinations in large vision language models",
            "Mitigating hallucination in large multi-modal models via robust instruction tuning",
            "Aligning large multimodal models with factually augmented rlhf",
            "Benchmarking large multimodal models against common corruptions",
            "Benchlmm: Benchmarking cross-style visual capability of large multimodal models",
            "Goat-bench: Safety insights to large multimodal models through meme-based social abuse",
            "Figstep: Jailbreaking large vision-language models via typographic visual prompts",
            "Mm-safetybench: A benchmark for safety evaluation of multimodal large language models",
            "Assessment of multimodal large language models in alignment with human values",
            "Training a helpful and harmless assistant with reinforcement learning from human feedback",
            "The hateful memes challenge: Detecting hate speech in multimodal memes",
            "Benchmark dataset of memes with text transcriptions for automatic detection of multi-modal misogynistic content",
            "Logo-2k+: A large-scale logo dataset for scalable logo classification",
            "Cvalues: Measuring the values of chinese large language models from safety to responsibility",
            "Model evaluation for extreme risks",
            "Roberta: A robustly optimized bert pretraining approach",
            "Scaling laws for neural language models",
            "Improved baselines with visual instruction tuning",
            "How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites",
            "Scigraphqa: A large-scale synthetic multi-turn question-answering dataset for scientific graphs",
            "Iconqa: A new benchmark for abstract diagram understanding and visual language reasoning",
            "mplug-owl: Modularization empowers large language models with multimodality",
            "Coco-text: Dataset and benchmark for text detection and recognition in natural images",
            "Are you smarter than a sixth grader? textbook question answering for multimodal machine comprehension",
            "Chartqa: A benchmark for question answering about charts with visual and logical reasoning",
            "Advancing multimodal chart understanding with large-scale instruction tuning",
            "An augmented benchmark dataset for geometric question answering through dual parallel text encoding",
            "Modeling context in referring expressions",
            "Generation and comprehension of unambiguous object descriptions",
            "Visual genome: Connecting language and vision using crowdsourced dense image annotations",
            "Shikra: Unleashing multimodal llm’s referential dialogue magic",
            "Segment anything",
            "Large-scale classification of fine-art paintings: Learning the right metric on the right feature",
            "Visual instruction tuning",
            "Nocaps: Novel object captioning at scale",
            "A diagram is worth a dozen images",
            "Question answering about charts with visual and logical reasoning",
            "Just ask: Learning to answer questions from millions of narrated videos",
            "Laion-5b: An open large-scale dataset for training next generation image-text models",
            "Coyo-700m: Image-text pair dataset",
            "Are you smarter than a sixth grader? textbook question answering for multimodal machine comprehension",
            "Dynamic prompt learning via policy gradient for semi-structured mathematical reasoning",
            "Bootstrap your own mathematical questions for large language models",
            "Icdar 2019 competition on large-scale street view text with partial labeling-rrc-lsvt",
            "The all-seeing project v2: Towards general relation comprehension of the open world",
            "Objects365: A large-scale, high-quality dataset for object detection",
            "Scene text visual question answering",
            "Icdar2017 competition on reading chinese text in the wild (rctw-17)",
            "Towards vqa models that can read",
            "Pp-ocrv3: More attempts for the improvement of ultra lightweight ocr system",
            "Wukong: A 100 million large-scale chinese cross-modal pre-training benchmark",
            "Kosmos-2: Grounding multimodal large language models to the world",
            "The all-seeing project: Towards panoptic visual recognition and understanding of the open world",
            "Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks",
            "Reasoning over scientific plots",
            "Knowledge-aware visual question answering",
            "A comprehensive multimodal dataset for advancing english and chinese large models",
            "Capsfusion: Rethinking image-text data at scale",
            "Inter-gps: Interpretable geometry problem solving with formal language and symbolic reasoning",
            "Learning to explain: Multimodal reasoning via thought chains for science question answering",
            "The hateful memes challenge: Detecting hate speech in multimodal memes",
            "Multimodal C4: An open, billion-scale corpus of images interleaved with text",
            "OCR-free document understanding transformer",
            "Microsoft coco captions: Data collection and evaluation server",
            "Monkey: Image resolution and text label are important things for large multi-modal models",
            "Unleashing multimodal llm’s referential dialogue magic",
            "Grounding multimodal large language models to the world",
            "Improving large multi-modal models with better captions",
            "Universal ocr-free visually-situated language understanding with multimodal large language model",
            "Scientific diagram analysis with the multimodal large language model",
            "Modularization empowers large language models with multimodality",
            "Making the v in vqa matter: Elevating the role of image understanding in visual question answering",
            "Advancing multimodal chart understanding with large-scale instruction tuning",
            "A new benchmark for abstract diagram understanding and visual language reasoning",
            "Novel object captioning at scale",
            "Large-scale classification of fine-art paintings: Learning the right metric on the right feature",
            "Dataset and benchmark for text detection and recognition in natural images",
            "Are you smarter than a sixth grader? textbook question answering for multimodal machine comprehension",
            "Question answering about charts with visual and logical reasoning",
            "Advancing multimodal chart understanding with large-scale instruction tuning",
            "An augmented benchmark dataset for geometric question answering through dual parallel text encoding",
            "Modeling context in referring expressions",
            "Generation and comprehension of unambiguous object descriptions",
            "Visual genome: Connecting language and vision using crowdsourced dense image annotations",
            "Nocaps: Novel object captioning at scale",
            "A diagram is worth a dozen images",
            "Reasoning over scientific plots",
            "Knowledge-aware visual question answering",
            "A comprehensive multimodal dataset for advancing english and chinese large models",
            "Capsfusion: Rethinking image-text data at scale",
            "Inter-gps: Interpretable geometry problem solving with formal language and symbolic reasoning",
            "Learning to explain: Multimodal reasoning via thought chains for science question answering"
        ]
    },
    {
        "index": 165,
        "title": "MMJ-Bench : A Comprehensive Study on Jailbreak Attacks and Defenses for Multimodal Large Language Models",
        "publication_date": "2024-10-22",
        "references": [
            "Gpt-4 technical report.",
            "Qwen-vl: A versatile vision-language model for understanding, localization, text reading, and beyond.",
            "Image hijacks: Adversarial images can control generative models at runtime.",
            "Cross-modal safety alignment: Is textual unlearning all you need?",
            "Instructblip: Towards general-purpose vision-language models with instruction tuning.",
            "Masterkey: Automated jailbreaking of large language model chatbots.",
            "Figstep: Jailbreaking large vision-language models via typographic visual prompts.",
            "Images are achilles’ heel of alignment: Exploiting visual vulnerabilities for jail-breaking multimodal large language models.",
            "Improved baselines with visual instruction tuning.",
            "Llava-next: Improved reasoning, ocr, and world knowledge.",
            "Visual instruction tuning.",
            "Mm-safetybench: A benchmark for safety evaluation of multimodal large language models.",
            "Autodan: Generating stealthy jailbreak prompts on aligned large language models.",
            "Mm-bench: Is your multi-modal model an all-around player?",
            "Safety alignment for vision language models.",
            "Jailbreakv-28k: A benchmark for assessing the robustness of multimodal large language models against jailbreak attacks.",
            "Harmbench: A standardized evaluation framework for automated red teaming and robust refusal.",
            "Random gradient-free minimization of convex functions.",
            "Jailbreaking attack against multimodal large language model.",
            "Visual adversarial examples jailbreak aligned large language models.",
            "High-resolution image synthesis with latent diffusion models.",
            "Jailbreak in pieces: Compositional adversarial attacks on multi-modal language models.",
            "Inferaligner: Inference-time align-ment for harmlessness through cross-model guidance.",
            "Cross-modality safety alignment.",
            "Adashield: Safeguarding multimodal large language models from structure-based attack via adaptive shield prompting.",
            "Jailbroken: How does llm safety training fail?",
            "Defending jailbreak attack in vlms via cross-modality information detector.",
            "A survey on multimodal large language models.",
            "Mmt-bench: A comprehensive multimodal benchmark for evaluating large vision-language models towards multitask agi.",
            "Mm-vet: Evaluating large multimodal models for integrated capabilities.",
            "A mutation-based method for multi-modal jailbreaking attack detection.",
            "Spavl: A comprehensive safety preference alignment dataset for vision language model.",
            "On evaluating adversarial robustness of large vision-language models."
        ]
    },
    {
        "index": 166,
        "title": "Model Merging and Safety Alignment: One Bad Model Spoils the Bunch",
        "publication_date": "2024-06-20",
        "references": [
            "Llama 3 model card",
            "Evolutionary optimization of model merging recipes",
            "Openbiollms: Advancing open-source large language models for healthcare and life sciences",
            "Llemma: An open language model for mathematics",
            "Sparks of artificial general intelligence: Early experiments with gpt-4",
            "Think you have solved question answering? try ARC, the AI2 Reasoning Challenge",
            "Safe RLHF: Safe reinforcement learning from human feedback",
            "Model breadcrumbs: Scaling multi-task model merging with sparse masks",
            "Reducing the time complexity of the derandomized evolution strategy with covariance matrix adaptation (cma-es)",
            "Aligning ai with shared human values",
            "Measuring massive multitask language understanding",
            "Trustgpt: A benchmark for trustworthy and responsible large language models",
            "Editing models with task arithmetic",
            "Mistral 7b",
            "What disease does this patient have? a large-scale open domain question answering dataset from medical exams",
            "Pubmedqa: A dataset for biomedical research question answering",
            "Dataless knowledge fusion by merging weights of language models",
            "ARGS: Alignment as reward-guided search",
            "Generative judge for evaluating alignment",
            "Self-prompting large language models for zero-shot open-domain qa",
            "Beavertails: Towards improved safety alignment of llm via a human-preference dataset",
            "SALMON: Self-alignment with instructable reward models",
            "An empirical study of multimodal model merging",
            "Beyond reverse KL: Generalizing direct preference optimization with diverse divergence constraints",
            "RigorLLM: Resilient Guardrails for Large Language Models against Undesired Content",
            "Mammoth: Building math generalist models through hybrid instruction tuning",
            "Group preference optimization: Few-shot alignment of large language models",
            "Improving generalization of alignment with human preferences through group invariant learning",
            "Promptbench: Towards evaluating the robustness of large language models on adversarial prompts"
        ]
    },
    {
        "index": 167,
        "title": "Model-Editing-Based Jailbreak against Safety-aligned Large Language Models",
        "publication_date": "2024-12-01",
        "references": [
            "Universal and transferable adversarial attacks on aligned language models",
            "\"do anything now\": Characterizing and evaluating in-the-wild jailbreak prompts on large language models",
            "On large language models' resilience to coercive interrogation",
            "Weak-to-strong jailbreaking on large language models",
            "Attngcg: Enhancing jailbreaking attacks on llms with attention manipulation",
            "Analyzing leakage of personally identifiable information in language models",
            "Adanonymizer: Interactively navigating and balancing the duality of privacy and output performance in human-llm interaction",
            "Dean: Deactivating the coupled neurons to mitigate fairness-privacy conflicts in large language models",
            "Privacy-hardened and hallucination-resistant synthetic data generation with logic-solvers",
            "Drowzee: Metamorphic testing for fact-conflicting hallucination detection in large language models",
            "Gendercare: A comprehensive framework for assessing and reducing gender bias in large language models",
            "Med-HALT: Medical domain hallucination test for large language models",
            "Siren's song in the ai ocean: A survey on hallucination in large language models",
            "A survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions",
            "Glitch tokens in large language models: Categorization taxonomy and effective detection",
            "Glitchprober: Advancing effective detection and mitigation of glitch tokens in large language models",
            "Masterkey: Automated jailbreaking of large language model chatbots",
            "Gptfuzzer: Red teaming large language models with auto-generated jailbreak prompts",
            "Pandora: Jailbreak gpts by retrieval augmented generation poisoning",
            "Autodan: Generating stealthy jailbreak prompts on aligned large language models",
            "Cold-attack: Jailbreaking llms with stealthiness and controllability",
            "Universal adversarial triggers for attacking and analyzing nlp",
            "Universal jailbreak backdoors from poisoned human feedback",
            "Defending large language models against jailbreak attacks via layer-specific editing",
            "Dppa: Pruning method for large language model to model merging",
            "TruthfulQA: Measuring how models mimic human falsehoods",
            "Measuring massive multitask language understanding",
            "Safety-tuned LLaMAs: Lessons from improving the safety of large language models that follow instructions",
            "Is attention explanation? an introduction to the debate",
            "Attention is all you need",
            "Attention is not not explanation",
            "Mechanistic understanding and mitigation of language model non-factual hallucinations",
            "Transformerlens",
            "Eliciting latent predictions from transformers with the tuned lens",
            "How alignment and jailbreak work: Explain LLM safety through intermediate hidden states",
            "Jailbreakbench: An open robustness benchmark for jailbreaking large language models",
            "Stanford alpaca: An instruction-following llama model",
            "Jailbreak open-sourced large language models via enforced decoding",
            "ArtPrompt: ASCII art-based jailbreak attacks against aligned LLMs",
            "Llama 2: Open foundation and fine-tuned chat models",
            "The llama 3 herd of models",
            "Gemma",
            "Mistral 7b",
            "Harmbench: A standardized evaluation framework for automated red teaming and robust refusal",
            "Badedit: Backdooring large language models by model editing",
            "Backdooring instruction-tuned large language models with virtual prompt injection",
            "Jailbreaking leading safety-aligned llms with simple adaptive attacks",
            "Refusal in language models is mediated by a single direction",
            "A mathematical framework for transformer circuits",
            "In-context learning and induction heads",
            "What makes and breaks safety fine-tuning? a mechanistic study",
            "Logicbreaks: A framework for understanding subversion of rule-based inference",
            "Making them ask and answer: Jailbreaking large language models in few queries via disguise and reconstruction",
            "How johnny can persuade llms to jailbreak them: Rethinking persuasion to challenge ai safety by humanizing llms",
            "Lockpicking llms: A logit-based jailbreak using token-level manipulation",
            "Automatically auditing large language models via discrete optimization",
            "Sleeper agents: Training deceptive llms that persist through safety training",
            "Competition report: Finding universal jailbreak backdoors in aligned llms",
            "Backdoorllm: A comprehensive benchmark for backdoor attacks on large language models",
            "Badgpt: Exploring security vulnerabilities of chatgpt via backdoor attacks to instructgpt",
            "Universal jailbreak backdoors from poisoned human feedback"
        ]
    },
    {
        "index": 168,
        "title": "Moderating Illicit Online Image Promotion for Unsafe User-Generated Content Games Using Large Vision-Language Models",
        "publication_date": "2024-08-14",
        "references": [
            "User-Generated Content (UGC): The Future of Gaming.",
            "Harmful Design in the Metaverse and How to Mitigate it: A Case Study of User-Generated Virtual Worlds on Roblox.",
            "Distribution of Roblox audiences worldwide as of December 2022, by age group.",
            "From the Devs\": How to Promote Your Game Effectively, as Told By jjwood1600.",
            "How to market your game?",
            "3 FREE WAYS to PROMOTE your Roblox GAME and make it POPULAR.",
            "Best free ways to advertise my game.",
            "Game Advertising:r/roblox.",
            "Twitter.",
            "Reddit.",
            "Discord.",
            "Roblox: The children’s game with 150 million players.",
            "Outrage after 7-year-old’s Roblox avatar is ‘gang raped’ by other players in the virtual world.",
            "Killing for the greater good: Action aversion and the emotional inhibition of harm in moral dilemmas.",
            "Ethical harm in virtual communities.",
            "My avatar, my self: Virtual harm and attachment.",
            "Both of my kids played a lot of Roblox until we banned it.",
            "The Dark Side of Roblox Every Parent Should Know.",
            "Safety Features: Chat, Privacy & Filtering.",
            "Safety & Civility at Roblox.",
            "Code of Conduct.",
            "Rules of Conduct.",
            "Google Vision AI.",
            "Clarifai.",
            "Amazon Rekognition.",
            "Human-ai collaboration via conditional delegation: A case study of content moderation.",
            "Microsoft Azure.",
            "Yahoo Open NSFW.",
            "Common Sense Media: Ratings, Reviews, and Advice.",
            "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models.",
            "User Generated Content in Games.",
            "The Social Side of Gaming: A Study of Interaction Patterns in a Massively Multiplayer Online Game.",
            "The virtues of moderation.",
            "A Trade-off-centered Framework of Content Moderation.",
            "You can (not) say what you want: Using algospeak to contest and evade algorithmic content moderation on tiktok.",
            "Google-trickers, Yaminjeongeum, and Leetspeak: An Empirical Taxonomy for Intentionally Noisy User-Generated Text.",
            "Skin sheriff: a machine learning solution for detecting explicit images.",
            "Stealthy porn: Understanding real-world adversarial images for illicit online promotion.",
            "Bringing the Kid back into YouTube Kids: Detecting Inappropriate Content on Video Streaming Platforms.",
            "Disturbed YouTube for Kids: Characterizing and Detecting Inappropriate Videos Targeting Young Children.",
            "Recognition of Pornographic Web Pages by Classifying Texts and Images.",
            "A Fuzzy Ontology and SVM–Based Web Content Classification System.",
            "Learning Transferable Visual Models From Natural Language Supervision.",
            "BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation.",
            "Visual Instruction Tuning.",
            "InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning.",
            "GPT-4V(vision) System Card.",
            "Multimodal Chain-of-Thought Reasoning in Language Models.",
            "Chain of Thought Prompt Tuning in Vision Language Models.",
            "Moderating New Waves of Online Hate with Chain-of-Thought Reasoning in Large Language Models.",
            "Chain of Explanation: New Prompting Method to Generate Quality Natural Language Explanation for Implicit Hate Speech.",
            "LAM-BRETTA: Learning to Rank for Twitter Soft Moderation.",
            "Keyword Extraction with BERT.",
            "LSPD: A Large-Scale Pornographic Dataset for Detection and Classification.",
            "Pillow (PIL Fork).",
            "Modern hierarchical, agglomerative clustering algorithms.",
            "Measuring nominal scale agreement among many raters.",
            "NSFW Data Scraper.",
            "Towards Targeted Obfuscation of Adversarial Unsafe Images using Reconstruction and Counterfactual Super Region Attribution Explainability.",
            "Deep-Learning Systems for Domain Adaptation in Computer Vision: Learning Transferable Feature Representations.",
            "A Comprehensive Survey on Domain Adaptation for Visual Applications.",
            "Color pornographic image detection based on color-saliency preserved mixture deformable part model.",
            "An assessment of violent imagery in advertisements on city buses in Manhattan, New York City.",
            "Convolutional neural networks for sentence classification.",
            "Deep NN for NSFW Detection.",
            "MobileNetV2: Inverted Residuals and Linear Bottlenecks.",
            "Not safe for work — Wikipedia, The Free Encyclopedia.",
            "You Only Look Once: Unified, Real-Time Object Detection.",
            "SSD: Single Shot MultiBox Detector.",
            "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks.",
            "A Dive into Vision-Language Models.",
            "Top Roblox Servers on Discord.",
            "ChatGPT.",
            "First Impressions with GPT-4V(ision).",
            "GPT-4 Vision Access in ChatGPT! Full Tour & Impressive Results!",
            "Deep Residual Learning for Image Recognition.",
            "Minecraft.",
            "Terasology.",
            "Lego worlds."
        ]
    },
    {
        "index": 170,
        "title": "Moderator: Moderating Text-to-Image Diffusion Models through Fine-grained Context-based Policies",
        "publication_date": "2024-10-18",
        "references": [
            "Google races to find a solution after AI generator Gemini misses the mark : NPR",
            "SurrogatePrompt: Bypassing the Safety Filter of Text-To-Image Models via Substitution",
            "An empirical evaluation of the system usability scale",
            "How Social Media Platforms’ Community Standards Address Influence Operations",
            "Fake Trump arrest photos: How to spot an AI-generated image",
            "Leace: Perfect linear concept erasure in closed form",
            "Generated faces in the wild: Quantitative comparison of stable diffusion, midjourney and dall-e 2",
            "Vizard: A metadata-hiding data analytic system with end-to-end policy controls",
            "CCSP: Controlled Relaxation of Content Security Policies by Runtime Policy Composition",
            "Towards making systems forget with machine unlearning",
            "A survey of automatic query expansion in information retrieval",
            "Iotguard: Dynamic enforcement of security and safety policy in commodity IoT.",
            "When Machine Unlearning Jeopardizes Privacy",
            "Graph Unlearning",
            "Pfirewall: Semantics-aware customizable data flow control for home automation systems",
            "Editing models with task arithmetic",
            "Patching open-vocabulary models by interpolating weights",
            "Instagram Community Guidelines FAQs",
            "iwf-ai-csam-report_public-oct23v1.pdf",
            "Identifying and Addressing Design and Policy Challenges in Online Content Moderation",
            "A trade-off-centered framework of content moderation",
            "Evading Watermark based Detection of AI-Generated Content",
            "Peekaboo: A hub-based approach to enable transparency in data processing within smart homes",
            "MiddlePolice: Toward Enforcing Destination-Defined Policies in the Middle of the Internet",
            "Conceptualizing and Improving Creator Moderation Design with Platform Stakeholders",
            "Provisions on Ecological Governance of Network Information Content",
            "Safe latent diffusion: Mitigating inappropriate degeneration in diffusion models",
            "DE-FAKE: Detection and Attribution of Fake Images Generated by Text-to-Image Generation Models",
            "Asymmetric Bias in Text-to-Image Generation with Adversarial Attacks",
            "Backdoor Pre-trained Models Can Transfer to All",
            "Bypassing backdoor detection algorithms in deep learning",
            "SoK: Content moderation in social media, from guidelines to enforcement, and research to practice",
            "Tom Hanks says AI version of him used in dental plan ad without his consent",
            "Resolving Interference When Merging Models",
            "SneakyPrompt: Evaluating Robustness of Text-to-image Generative Models’ Safety Filters",
            "Latent Backdoor Attacks on Deep Neural Networks",
            "Narcissus: A Practical Clean-Label Backdoor Attack with Limited Information",
            "Discovering Universal Semantic Triggers for Text-to-Image Synthesis",
            "Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena",
            "Modifying memories in transformer models"
        ]
    },
    {
        "index": 172,
        "title": "MoJE: Mixture of Jailbreak Experts, Naive Tabular Classifiers as Guard for Prompt Attacks",
        "publication_date": "2024-10-04",
        "references": [
            "Detecting Language Model Attacks with Perplexity",
            "Foundational Challenges in Assuring Alignment and Safety of Large Language Models",
            "Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback",
            "Constitutional AI: Harmlessness from AI Feedback",
            "Feature Selection via Mutual Information: New Theoretical Insights",
            "Science in the age of large language models",
            "The Hacking of ChatGPT is Just Getting Started",
            "Are aligned neural networks adversarially aligned?",
            "Building Guardrails for Large Language Models",
            "RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models",
            "Generative Language Models and Automated Influence Operations: Emerging Threats and Potential Mitigations",
            "DE-BERTA: DECODING-ENHANCED BERT WITH DISENTANGLED ATTENTION",
            "Llm self defense: By self examination, llms know they are being tricked",
            "Catastrophic jailbreak of open-source llms via exploiting generation",
            "Sleeper Agents: Training Deceptive LLMs that Persist Through Safety Training",
            "Llama Guard: LLM-based Input-Output Safeguard for Human-AI Conversations",
            "Baseline Defenses for Adversarial Attacks Against Aligned Language Models",
            "Exploiting Programmatic Behavior of LLMs: Dual-Use Through Standard Security Attacks",
            "All the News That’s Fit to Fabricate: AI-Generated Text as a Tool of Media Misinformation",
            "ChatGPT in the public sector – Overhyped or overlooked?",
            "OpenAI Moderation API",
            "Training language models to follow instructions with human feedback",
            "Fine-Tuned DeBERTa-v3 for Prompt Injection Detection",
            "Fine-tuning Aligned Language Models Compromises Safety, Even When Users Do Not Intend To!",
            "Induction of decision trees",
            "Great, Now Write an Article About That: The Crescendo Multi-Turn LLM Jailbreak Attack",
            "Survey of vulnerabilities in large language models revealed by adversarial attacks",
            "”Do Anything Now”: Characterizing and Evaluating In-The-Wild Jailbreak Prompts on Large Language Models",
            "Stanford Alpaca: An Instruction-following LLaMA model",
            "Llama 2: Open Foundation and Fine-Tuned Chat Models",
            "Challenges in Detoxifying Language Models",
            "RigorLLM: Resilient Guardrails for Large Language Models against Undesired Content",
            "Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena",
            "Universal and Transferable Adversarial Attacks on Aligned Language Models"
        ]
    },
    {
        "index": 173,
        "title": "More RLHF, More Trust? On The Impact of Preference Alignment On Trustworthiness",
        "publication_date": "2024-12-21",
        "references": [
            "ChatGPT: A comprehensive review on background, applications, key challenges, bias, ethics, limitations and future scope",
            "A survey of large language models",
            "A Comparative Study of Open-Source Large Language Models, GPT-4 and Claude 2: Multiple-Choice Test Taking in Nephrology",
            "Summary of ChatGPT-Related research and perspective towards the future of large language models",
            "AI alignment: A comprehensive survey",
            "Training language models to follow instructions with human feedback",
            "Fine-Tuning Language Models from Human Preferences",
            "Toxicity in ChatGPT: Analyzing Persona-assigned Language Models",
            "Bias and Fairness in Large Language Models: A Survey",
            "Unsupervised Text Deidentification",
            "DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT Models",
            "TrustLLM: Trustworthiness in Large Language Models",
            "Gpt-4 technical report",
            "Llama 2: Open Foundation and Fine-Tuned Chat Models",
            "Improving alignment of dialogue agents via targeted human judgements",
            "Proximal Policy Optimization Algorithms",
            "Direct Preference Optimization: Your Language Model is Secretly a Reward Model",
            "Understanding black-box predictions via influence functions",
            "Datainf: Efficiently estimating data influence in lora-tuned llms and diffusion models",
            "Deep reinforcement learning from human preferences",
            "Finetuned language models are zero-shot learners",
            "Instruction tuning for large language models: A survey",
            "Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback",
            "Understanding Dataset Difficulty withV-Usable Information",
            "Openassistant conversations-democratizing large language model alignment",
            "ULTRAFEEDBACK:BoostingLanguageModelswithScaledAIFeedback",
            "TruthfulQA: Measuring How Models Mimic Human Falsehoods",
            "Anticipating safety issues in e2e conversational ai: Framework and tooling",
            "Alignment of language agents",
            "CrowS-Pairs: A Challenge Dataset for Measuring Social Biases in Masked Language Models",
            "Stereotyping Norwegian Salmon: An Inventory of Pitfalls in Fairness Benchmark Datasets",
            "Robustness Gym: Unifying the NLP Evaluation Landscape",
            "Admix: Enhancing the transferability of adversarial attacks",
            "Extracting training data from large language models",
            "Aligningaiwithsharedhumanvalues",
            "Discovering language model behaviors with model-written evaluations",
            "Foundational challenges in assuring alignment and safety of large language models",
            "Datamodels: Predicting predictions from training data",
            "Trak: Attributing model behavior at scale",
            "Estimating training data influence by tracing gradient descent",
            "Rank analysis of incomplete block designs: I. The method of paired comparisons",
            "trlX: A framework for large scale reinforcement learning from human feedback",
            "Probing toxic content in large pre-trained language models",
            "Reward modeling for mitigating toxicity in transformer-based language models",
            "RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models",
            "StereoSet: Measuring stereotypical bias in pretrained language models",
            "Identifying and reducing gender bias in word-level language models",
            "Towards understanding and mitigating social biases in language models",
            "Persistent anti-muslim bias in large language models",
            "Towards understanding sycophancy in language models",
            "Ethical and social risks of harm from language models",
            "Ethical by design: Ethics best practices for natural language processing",
            "Ethics of large language models in medicine and medical research",
            "Auditing large language models: a three-layered approach",
            "Aligning AI With Shared Human Values",
            "Inference-time intervention: Eliciting truthful answers from a language model",
            "A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions",
            "Language-aware truth assessment of fact candidates",
            "TruthfulQA: Measuring How Models Mimic Human Falsehoods",
            "What does it mean for a language model to preserve privacy?",
            "Privacy risks of general-purpose language models",
            "A survey on large language model (llm) security and privacy: The good, the bad, and the ugly",
            "Natural language understanding with privacy-preserving bert",
            "The enron corpus: a new dataset for email classification research"
        ]
    },
    {
        "index": 174,
        "title": "MOSSBench: Is Your Multimodal Language Model Oversensitive to Safe Queries?",
        "publication_date": "2024-06-22",
        "references": [
            "Introducing the next generation of claude.",
            "Mitigating jailbreaks & prompt injections.",
            "The claude 3 model family: Opus, sonnet, haiku. Claude-3 Model Card.",
            "Detecting harmful content on online platforms: What platforms need vs. where research efforts go.",
            "Qwen-vl: A frontier large vision-language model with versatile abilities.",
            "Training a helpful and harmless assistant with reinforcement learning from human feedback.",
            "Identifying and mitigating the security risks of generative ai.",
            "Cognitive distortion and problem behaviors in adolescents.",
            "Cognitive therapy and the emotional disorders.",
            "Cognitive therapy of depression.",
            "Cognitive behavior therapy: Basics and beyond.",
            "Improving image generation with better captions.",
            "Open problems and fundamental limitations of reinforcement learning from human feedback.",
            "Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality.",
            "Instructblip: Towards general-purpose vision-language models with instruction tuning.",
            "Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned.",
            "Latent jailbreak: A benchmark for evaluating text safety and output robustness of large language models.",
            "Xstest: A test suite for identifying exaggerated safety behaviours in large language models.",
            "Learning to summarize from human feedback.",
            "Gemini: A family of highly capable multimodal models.",
            "Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context.",
            "Reka core, flash, and edge: A series of powerful multimodal language models.",
            "Llama: Open and efficient foundation language models.",
            "How many unicorns are in this image? a safety evaluation benchmark for vision llms.",
            "Do-not-answer: A dataset for evaluating safeguards in llms.",
            "Jailbroken: How does llm safety training fail?.",
            "Transformers: State-of-the-art natural language processing.",
            "A comprehensive overview of backdoor attacks in large language models within communication networks.",
            "mplug-owl2: Revolutionizing multi-modal large language model with modality collaboration.",
            "Avibench: Towards evaluating the robustness of large vision-language model on adversarial visual-instructions.",
            "How far are we from intelligent visual deductive reasoning?"
        ]
    },
    {
        "index": 175,
        "title": "MRJ-Agent: An Effective Jailbreak Agent for Multi-Round Dialogue",
        "publication_date": "2025-01-07",
        "references": [
            "Gpt-4 technical report",
            "Rank analysis of incomplete block designs: I. the method of paired comparisons",
            "Jailbreakbench: An open robustness benchmark for jailbreaking large language models",
            "Jailbreaking black box large language models in twenty queries",
            "Red teaming gpt-4v: Are gpt-4v safe against uni/multi-modal jailbreak attacks?",
            "Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality",
            "Cold-attack: Jailbreaking llms with stealthiness and controllability",
            "Llama guard: Llm-based input-output safeguard for human-ai conversations",
            "Improved techniques for optimization-based jailbreaking on large language models",
            "Mistral 7b",
            "Artprompt: Ascii art-based jailbreak attacks against aligned llms",
            "Exploiting programmatic behavior of llms: Dual-use through standard security attacks",
            "Autodan: Generating stealthy jailbreak prompts on aligned large language models",
            "Evolving diverse red-team language models in multi-round multi-agent games",
            "Tdc 2023 (llm edition): The trojan detection challenge",
            "Tree of attacks: Jailbreaking black-box llms automatically",
            "Advprompter: Fast adaptive adversarial prompting for llms",
            "Red teaming language models with language models",
            "Direct preference optimization: Your language model is secretly a reward model",
            "Sentence-BERT: Sentence embeddings using siamese bert-networks",
            "Great, now write an article about that: The crescendo multi-turn llm jailbreak attack",
            "Towards understanding sycophancy in language models",
            "Characterizing and evaluating in-the-wild jailbreak prompts on large language models",
            "Jailbroken: How does llm safety training fail?",
            "Chain of attack: a semantic-driven contextual multi-turn attacker for llm",
            "Gpt-fuzzer: Red teaming large language models with auto-generated jailbreak prompts",
            "Gpt-4 is too smart to be safe: Stealthy chat with llms via cipher",
            "How johnny can persuade llms to jailbreak them: Rethinking persuasion to challenge ai safety by humanizing llms",
            "Boosting jailbreak attack with momentum",
            "Accelerating greedy coordinate gradient via probe sampling",
            "Speak out of turn: Safety vulnerability of large language models in multi-turn dialogue",
            "Autodan: Automatic and interpretable adversarial attacks on large language models",
            "Universal and transferable adversarial attacks on aligned language models"
        ]
    },
    {
        "index": 176,
        "title": "Multimodal Pragmatic Jailbreak on Text-to-image Models",
        "publication_date": "2024-09-27",
        "references": [
            "Constitutional ai: Harmlessness from ai feedback.",
            "ediff-i: Text-to-image diffusion models with an ensemble of expert denoisers.",
            "opennsfw2.",
            "Instructpix2pix: Learning to follow image editing instructions.",
            "Universals in language usage: Politeness phenomena.",
            "Language models are few-shot learners.",
            "Are aligned neural networks adversarially aligned?",
            "Jailbreaking black box large language models in twenty queries.",
            "Textdiffuser-2: Unleashing the power of language models for text rendering.",
            "Prompting4debugging: Red-teaming text-to-image diffusion models by finding problematic prompts.",
            "opendalle.",
            "Proteusv0.1.",
            "Proteusv0.2.",
            "Proteusv0.3.",
            "Deepfloyd if.",
            "Unified concept editing in diffusion models.",
            "Figstep: Jailbreaking large vision-language models via typographic visual prompts.",
            "The co-operative, transformative organization of human action and knowledge.",
            "Deep residual learning for image recognition.",
            "The hateful memes challenge: Detecting hate speech in multimodal memes.",
            "Leonardo ai content moderation filter: Everything you need to know.",
            "leonardo.ai.",
            "Figmemes: A dataset for figurative language identification in politically-opinionated memes.",
            "Visual instruction tuning.",
            "Character-aware models improve visual text rendering.",
            "Jailbreaking chatgpt via prompt engineering: An empirical study.",
            "Safety checker model card.",
            "A holistic approach to undesired content detection in the real world.",
            "Tree of attacks: Jailbreaking black-box llms automatically.",
            "The complete list of banned words in midjourney you need to know.",
            "Midjourney original.",
            "Nsfw.",
            "Nudenet.",
            "A holistic approach to undesired content detection in the real world.",
            "Zero-shot text-to-image generation.",
            "Red-teaming the stable diffusion safety filter.",
            "gen2.",
            "High-resolution image synthesis with latent diffusion models.",
            "Photorealistic text-to-image diffusion models with deep language understanding.",
            "Safe latent diffusion: Mitigating inappropriate degeneration in diffusion models.",
            "Safe latent diffusion: Mitigating inappropriate degeneration in diffusion models.",
            "Can machines help us answering question 16 in datasheets, and in turn reflecting on inappropriate content?",
            "Laion-5b: An open large-scale dataset for training next generation image-text models.",
            "On second thought, let’s not think step by step! bias and toxicity in zero-shot reasoning.",
            "Jailbreak in pieces: Compositional adversarial attacks on multi-modal language models.",
            "“do anything now”: Characterizing and evaluating in-the-wild jailbreak prompts on large language models.",
            "stable-diffusion-2.",
            "stable-diffusion-2-1.",
            "stable-diffusion-2-base.",
            "stable-diffusion-xl-base-1.0.",
            "stable-diffusion-xl-refiner-1.0.",
            "Albedoxl.",
            "Ring-a-bell! how reliable are concept removal methods for diffusion models?",
            "Diffusers: State-of-the-art diffusion models.",
            "Jailbroken: How does LLM safety training fail?",
            "Jailbroken: How does llm safety training fail?",
            "Byt5: Towards a token-free future with pre-trained byte-to-byte models.",
            "Glyphcontrol: Glyph conditional control for visual text generation.",
            "Gptfuzzer: Red teaming large language models with auto-generated jailbreak prompts.",
            "Magicbrush: A manually annotated dataset for instruction-guided image editing.",
            "Bertscore: Evaluating text generation with BERT.",
            "Judging llm-as-a-judge with mt-bench and chatbot arena.",
            "Minigpt-4: Enhancing vision-language understanding with advanced large language models."
        ]
    },
    {
        "index": 178,
        "title": "“Not Aligned” is Not “Malicious”: Being Careful about Hallucinations of Large Language Models’ Jailbreak",
        "publication_date": "2025-02-03",
        "references": [
            "Qwen technical report",
            "Constitutional ai: Harmlessness from ai feedback",
            "Modeling local coherence: An entity-based approach",
            "Insights into classifying and mitigating llms’ hallucinations",
            "Jailbreaking black box large language models in twenty queries",
            "Jailbreaker in jail: Moving target defense for large language models",
            "Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality.",
            "Breaking down the defenses: A comparative survey of attacks on large language models",
            "Security and privacy challenges of large language models: A survey",
            "Stanford ai safety",
            "Ai principles: 2023 progress update",
            "Hallucinations in large multilingual translation models",
            "Gradient-based adversarial attacks against text transformers",
            "Cold-attack: Jailbreaking llms with stealthiness and controllability",
            "Detoxify",
            "Control risk for potential misuse of artificial intelligence in science",
            "An overview of catastrophic ai risks",
            "A survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions",
            "Catastrophic jailbreak of open-source llms via exploiting generation",
            "Perplexity—a measure of the difficulty of speech recognition tasks",
            "Survey of halluci-nation in natural language generation",
            "Mistral 7b",
            "Challenges and applications of large language models",
            "Learn what not to learn: Towards generative safety in chatbots",
            "Solar 10.7b: Scaling large language models with simple yet effective depth up-scaling",
            "Propile: Probing privacy leakage in large language models",
            "Open the pandora’s box of llms: Jailbreaking llms through representation engineering",
            "Cmmath: A chinese multi-modal math skill evaluation benchmark for foundation models",
            "LANS: A layout-aware neural solver for plane geometry problem",
            "Against the achilles’ heel: A survey on red teaming for generative models",
            "Autodan: Generating stealthy jailbreak prompts on aligned large language models",
            "Protecting privacy in an ai-driven world",
            "Harmbench: A standardized evaluation framework for automated red teaming and robust refusal",
            "Tree of attacks: Jailbreaking black-box llms automatically",
            "SLANG: New concept comprehension of large language models",
            "Orca 2: Teaching small language models how to reason",
            "Privacy issues in large language models: A survey",
            "large-scale generative pre-training model for conversation",
            "Gpt-4 technical report",
            "Our approach to ai safety",
            "Red teaming language models with language models",
            "Scalable and transferable black-box jail-breaks for language models via persona modulation",
            "Jailbreak in pieces: Compositional adversarial attacks on multi-modal language models",
            "Survey of vulnerabilities in large language models revealed by adversarial attacks",
            "Do Anything Now\": Characterizing and Evaluating In-The-Wild Jailbreak Prompts on Large Language Models",
            "AutoPrompt: Eliciting knowledge from language models with automatically generated prompts",
            "Fmint: Bridging human designed and data pretrained models for differential equation foundation model",
            "Rethinking privacy in the ai era",
            "Gemini: A family of highly capable multi-modal models",
            "Llama: Open and efficient foundation language models",
            "Llama 2: Open foundation and fine-tuned chat models",
            "Zephyr: Direct distillation of lm alignment",
            "Universal adversarial triggers for attacking and analyzing NLP",
            "Openchat: Advancing open-source language models with mixed-quality data",
            "Frustratingly easy jailbreak of large language models via output prefix attacks",
            "Jailbroken: How does llm safety training fail?",
            "Taxonomy of risks posed by language models",
            "Hard prompts made easy: Gradient-based discrete optimization for prompt tuning and discovery",
            "Baichuan 2: Open large-scale language models",
            "How johnny can persuade llms to jailbreak them: Rethinking persuasion to challenge ai safety by humanizing llms",
            "GeoEval: Benchmark for evaluating LLMs and multi-modal models on geometry problem-solving",
            "Fuse, reason and verify: Geometry problem solving with parsed clauses from diagram",
            "Siren’s song in the ai ocean: A survey on hallucination in large language models",
            "Psysafe: A comprehensive framework for psychological-based attack, defense, and evaluation of multi-agent system safety",
            "Mquake: Assessing knowledge editing in language models via multi-hop questions",
            "Starling-7b: Improving llm helpfulness & harmlessness with rlaif",
            "Universal and transferable adversarial attacks on aligned language models"
        ]
    },
    {
        "index": 179,
        "title": "On Calibration of LLM-Based Guard Models for Reliable Content Moderation",
        "publication_date": "2025-05-01",
        "references": [
            "Enhancing in-context learning via linear probe calibration",
            "Palm 2 technical report",
            "Language models are few-shot learners",
            "Jailbreakbench: An open robustness benchmark for jailbreaking large language models",
            "Unlearn what you want to forget: Efficient unlearning for llms",
            "A close look into the calibration of pre-trained language models",
            "Safe rlhf: Safe reinforcement learning from human feedback",
            "The llama 3 herd of models",
            "Mitigating label biases for in-context learning",
            "Aegis: Online adaptive ai content safety moderation with ensemble of llm experts",
            "On calibration of modern neural networks",
            "Towards informative few-shot prompt with maximum information gain for in-context learning",
            "Advancing adversarial suffix transfer learning on aligned large language models",
            "Rethinking machine unlearning for large language models",
            "Autodan: Generating stealthy jailbreak prompts on aligned large language models",
            "A holistic approach to undesired content detection in the real world",
            "Harmbench: A standardized evaluation framework for automated red teaming and robust refusal",
            "Reducing conversational agents’ overconfidence through linguistic calibration",
            "Revisiting the calibration of modern neural networks",
            "Obtaining well calibrated probabilities using bayesian binning",
            "Posterior calibration and exploratory analysis for natural language processing models",
            "Training language models to follow instructions with human feedback",
            "Direct preference optimization: Your language model is secretly a reward model",
            "Nemo guardrails: A toolkit for controllable and safe llm applications with programmable rails",
            "Xstest: A test suite for identifying exaggerated safety behaviours in large language models",
            "A survey on hate speech detection using natural language processing",
            "Llama 2: Open foundation and fine-tuned chat models",
            "Simpsafetytests: a test suite for identifying critical safety risks in large language models",
            "Decodingtrust: A comprehensive assessment of trustworthiness in gpt models",
            "Towards bayesian deep learning: A framework and some existing methods",
            "A survey on bayesian deep learning",
            "Natural-parameter networks: A class of probabilistic neural networks",
            "Continuously indexed domain adaptation",
            "Blob: Bayesian low-rank adaptation by backpropagation for large language models",
            "Can llms express their uncertainty? an empirical evaluation of confidence elicitation in llms",
            "Safedecoding: Defending against jailbreak attacks via safety-aware decoding",
            "Graph-relational domain adaptation",
            "Rigor-llm: Resilient guardrails for large language models against undesired content",
            "Shieldgemma: Generative ai content moderation based on gemma",
            "Calibrate before use: Improving few-shot performance of language models",
            "Batch calibration: Rethinking calibration for in-context learning and prompt engineering",
            "Survival of the most influential prompts: Efficient black-box prompt search via clustering and pruning",
            "On the calibration of large language models and alignment"
        ]
    },
    {
        "index": 180,
        "title": "Evaluating the Durability of Safeguards for Open-Weight LLMs",
        "publication_date": "2024-12-10",
        "references": [
            "Jailbreaking leading safety-aligned llms with simple adaptive attacks.",
            "Refusal in language models is mediated by a single direction.",
            "Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples.",
            "Training a helpful and harmless assistant with reinforcement learning from human feedback.",
            "Evaluating large language models trained on code.",
            "Deep reinforcement learning from human preferences.",
            "Measuring massive multitask language understanding.",
            "Measuring mathematical problem solving with the math dataset.",
            "Adversarial perturbations cannot reliably protect artists from generative ai.",
            "Lora: Low-rank adaptation of large language models.",
            "Boosting: Tackling harmful fine-tuning for large language models via attenuating harmful perturbation.",
            "Catastrophic jailbreak of open-source llms via exploiting generation.",
            "Unlearn and burn: Adversarial machine unlearning requests destroy model accuracy.",
            "Truthfulqa: Measuring how models mimic human falsehoods.",
            "The wmdp benchmark: Measuring and reducing malicious use with unlearning.",
            "Judging llm-as-a-judge with mt-bench and chatbot arena.",
            "Universal and transferable adversarial attacks on aligned language models.",
            "Great, now write an article about that: The crescendo multi-turn llm jailbreak attack.",
            "Scalable and transferable black-box jailbreaks for language models via persona modulation.",
            "Ununlearning: Unlearning is not sufficient for content regulation in advanced generative ai.",
            "Challenging big-bench tasks and whether chain-of-thought can solve them.",
            "Managing misuse risk for dual-use foundation models",
            "Dual-use foundation models with widely available model weights report",
            "Sb 1047 august 15 author amendments overview",
            "A framework for few-shot language model evaluation",
            "Windows xp leak confirmed after user compiles the leaked code into a working os",
            "The 2014 sony pictures hack",
            "Safe and secure innovation for frontier artificial intelligence models act."
        ]
    },
    {
        "index": 181,
        "title": "ON THE ROLE OF ATTENTION HEADS IN LARGE LANGUAGE MODEL SAFETY",
        "publication_date": "2025-05-01",
        "references": [
            "Gpt-4 technical report.",
            "Refusal in language models is mediated by a single direction.",
            "Training a helpful and harmless assistant with reinforcement learning from human feedback.",
            "Constitutional ai: Harmlessness from ai feedback.",
            "Managing extreme ai risks amid rapid progress.",
            "Mechanistic interpretability for ai safety–a review.",
            "Localizing lying in llama: Understanding instructed dishonesty on true-false questions through prompting, probing, and patching.",
            "Are aligned neural networks adversarially aligned?",
            "Jailbreaking black box large language models in twenty queries.",
            "Jailbreakbench: An open robustness benchmark for jailbreaking large language models.",
            "Finding safety neurons in large language models.",
            "Boolq: Exploring the surprising difficulty of natural yes/no questions.",
            "What does BERT look at? an analysis of BERT's attention.",
            "Think you have solved question answering? try arc, the ai2 reasoning challenge.",
            "Sparsegpt: Massive language models can be accurately pruned in one-shot.",
            "Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned.",
            "A framework for few-shot language model evaluation.",
            "Causal abstractions of neural networks.",
            "Successor heads: Recurring, interpretable attention heads in the wild.",
            "Finding neurons in a haystack: Case studies with sparse probing.",
            "Do attention heads in bert track syntactic dependencies?",
            "Catastrophic jailbreak of open-source llms via exploiting generation.",
            "Improved techniques for optimization-based jailbreaking on large language models.",
            "On information and sufficiency.",
            "A mechanistic understanding of alignment algorithms: A case study on dpo and toxicity.",
            "No two devils alike: Unveiling distinct mechanisms of fine-tuning attacks.",
            "Multi-step jailbreaking privacy attacks on chatgpt.",
            "Amplegcg: Learning a universal and transferable generative model of adversarial suffixes for jailbreaking both open and closed llms.",
            "Does circuit analysis interpretability scale? evidence from multiple choice capabilities in chinchilla.",
            "The unlocking spell on base LLMs: Rethinking alignment via in-context learning.",
            "Tracr: Compiled transformers as a laboratory for interpretability.",
            "Autodan: Generating stealthy jailbreak prompts on aligned large language models.",
            "Locating and editing factual associations in gpt.",
            "Are sixteen heads really better than one?",
            "Can a suit of armor conduct electricity? a new dataset for open book question answering.",
            "Winogrande: An adversarial winograd schema challenge at scale.",
            "Bleu: a method for automatic evaluation of machine translation.",
            "LLM self defense: By self examination, LLMs know they are being tricked.",
            "Fine-tuning aligned language models compromises safety, even when users do not intend to!",
            "Glue: A multi-task benchmark and analysis platform for natural language understanding.",
            "Interpretability in the wild: a circuit for indirect object identification in gpt-2 small.",
            "Do-not-answer: Evaluating safeguards in llms.",
            "Jailbroken: How does llm safety training fail?",
            "Assessing the brittleness of safety alignment via pruning and low-rank modifications.",
            "Retrieval head mechanically explains long-context factuality.",
            "Course-correction: Safety alignment using synthetic preferences.",
            "Knowledge conflicts for llms: A survey.",
            "Uncovering safety risks in open-source llms through concept activation vector.",
            "Qwen2 technical report.",
            "Extend model merging from fine-tuned to pre-trained large language models via weight disentanglement.",
            "Language models are super mario: Absorbing abilities from homologous models as a free lunch.",
            "How johnny can persuade llms to jailbreak them: Rethinking persuasion to challenge ai safety by humanizing llms.",
            "Towards best practices of activation patching in language models: Metrics and methods.",
            "Explainability for large language models: A survey.",
            "Defending large language models against jailbreak attacks via layer-specific editing.",
            "On prompt-driven safeguarding for large language models.",
            "Judging llm-as-a-judge with mt-bench and chatbot arena.",
            "Attention heads of large language models: A survey.",
            "How alignment and jailbreak work: Explain llm safety through intermediate hidden states.",
            "Representation engineering: A top-down approach to ai transparency.",
            "Universal and transferable adversarial attacks on aligned language models."
        ]
    },
    {
        "index": 182,
        "title": "Optimization-based Prompt Injection Attack to LLM-as-a-Judge",
        "publication_date": "2024-10-16",
        "references": [
            "Gpt-4 technical report",
            "Mixtral of experts",
            "Detecting language model attacks with perplexity",
            "Claude 2",
            "The claude 3 model family: Opus, sonnet, haiku. Claude-3 Model Card 1",
            "Training a helpful and harmless assistant with reinforcement learning from human feedback",
            "Evaluating the susceptibility of pre-trained language models via handcrafted adversarial examples",
            "A comprehensive survey of ai-generated content (aigc): A history of generative ai from gan to chatgpt",
            "Deep reinforcement learning from human preferences",
            "Prompt injection attacks against GPT-3",
            "Bard",
            "Delimiters won’t save you from prompt injection",
            "Jailbreaking black box large language models in twenty queries",
            "Meta Llama 3",
            "Communicative agents for software development",
            "Advances in Neural Information Processing Systems 36",
            "JugdeLM: Fine-tuned large language models are scalable judges",
            "Autodan: Automatic and interpretable adversarial attacks on large language models",
            "Universal and transferable adversarial attacks on aligned language models",
            "StruQ: Defending against prompt injection with structured queries",
            "Badgpt: Exploring security vulnerabilities of chatgpt via backdoor attacks to instructgpt",
            "MetaTool Benchmark: Deciding Whether to Use Tools and Which to Use",
            "Position: TrustLLM: Trustworthiness in Large Language Models",
            "Trustgpt: A benchmark for trustworthy and responsible large language models",
            "More than you’ve asked for: A Comprehensive Analysis of Novel Prompt Injection Threats to Application-Integrated Large Language Models",
            "Not what you’ve signed up for: Compromising Real-World LLM-Integrated Applications with Indirect Prompt Injection",
            "Evaluating large language models at evaluating instruction following",
            "Wider and deeper llm networks are fairer llm evaluators",
            "Judging llaM-as-a-Judge with mt-bench and chatbot arena",
            "LLM-powered search",
            "Scaling reinforcement learning from human feedback with ai feedback",
            "Generative judge for evaluating alignment",
            "Adv-bert: Bert is not robust on misspellings! generating nature adversarial samples on bert",
            "Detecting language model attacks with perplexity",
            "Baseline defenses for adversarial attacks against aligned language models",
            "Badencoder: Backdoor attacks to pre-trained encoders in self-supervised learning",
            "Llama 2: Open foundation and fine-tuned chat models",
            "OpenChat: Advancing Open-source Language Models with Mixed-Quality Data",
            "PandaLM: An Automatic Evaluation Benchmark for LLM Instruction Tuning Optimization",
            "Are aligned neural networks adversarially aligned?",
            "Autoprompt: Eliciting knowledge from language models with automatically generated prompts",
            "Natural backdoor attack on text data",
            "Text Adversarial Purification as Defense against Adversarial Attacks",
            "Can Large Language Models Be an Alternative to Human Evaluations?"
        ]
    },
    {
        "index": 184,
        "title": "PathSeeker: Exploring LLM Security Vulnerabilities with a Reinforcement Learning-Based Jailbreak Approach",
        "publication_date": "2024-10-01",
        "references": [
            "Training a helpful and harmless assistant with reinforcement learning from human feedback",
            "Sparks of artificial general intelligence: Early experiments with gpt-4",
            "Jailbreaking black box large language models in twenty queries",
            "When llm meets drl: Advancing jailbreaking efficiency via drl-guided search",
            "Rl-jack: Reinforcement learning-powered black-box jailbreaking attack against llms",
            "Comprehensive assessment of jailbreak attacks against llms",
            "Masterkey: Automated jailbreaking of large language model chatbots",
            "A wolf in sheep’s clothing: Generalized nested jailbreak prompts can fool large language models easily",
            "Jailbreakzoo: Survey, landscapes, and horizons in jailbreaking large language and vision-language models",
            "Attackeval: How to evaluate the effectiveness of jailbreak attacking on large language models",
            "A contextual-bandit approach to personalized news article recommendation",
            "Amplegcg: Learning a universal and transferable generative model of adversarial suffixes for jailbreaking both open and closed llms",
            "Multi-agent actor-critic for mixed cooperative-competitive environments",
            "Playing atari with deep reinforcement learning",
            "Fine-tuning aligned language models compromises safety, even when users do not intend to!",
            "Multilingual blending: Llm safety alignment evaluation with language mixture",
            "Reinforcement learning-driven llm agent for automated attacks on llms",
            "Jailbroken: How does LLM safety training fail?",
            "Fuzzllm: A novel and universal fuzzing framework for proactively discovering jailbreak vulnerabilities in large language models",
            "A survey on large language model (llm) security and privacy: The good, the bad, and the ugly",
            "Gptfuzzer: Red teaming large language models with auto-generated jailbreak prompts",
            "Gpt-4 is too smart to be safe: Stealthy chat with llms via cipher",
            "Adversarial contrastive decoding: Boosting safety alignment of large language models via opposite prompt optimization",
            "Universal and transferable adversarial attacks on aligned language models"
        ]
    },
    {
        "index": 185,
        "title": "PEFT-as-an-Attack! Jailbreaking Language Models during Federated Parameter-Efficient Fine-Tuning",
        "publication_date": "2024-12-19",
        "references": [
            "BERT: Pre-training of deep bidirectional transformers for language understanding",
            "Language models are few-shot learners",
            "Chatgpt",
            "Gpt-4 technical report",
            "Llama: Open and efficient foundation language models",
            "Llama 2: Open foundation and fine-tuned chat models",
            "Llama 3.2: Revolutionizing edge ai and vision with open, customizable models",
            "Qwen technical report",
            "Qwen2 technical report",
            "Qwen2.5: A party of foundation models",
            "On the client preference of llm fine-tuning in federated learning",
            "A survey of large language models",
            "The power of scale for parameter-efficient prompt tuning",
            "LoRA: Low-rank adaptation of large language models",
            "Analyzing and reducing catastrophic forgetting in parameter efficient tuning",
            "VeRA: Vector-based random matrix adaptation",
            "Navigating text-to-image customization: From lyCORIS fine-tuning to model evaluation",
            "Lessons from archives: Strategies for collecting sociocultural data in machine learning",
            "Regulation (eu) 2016/679 of the european parliament and of the council",
            "California consumer privacy act (ccpa)",
            "Communication-Efficient Learning of Deep Networks from Decentralized Data",
            "A survey on efficient federated learning methods for foundation model training",
            "Synergizing foundation models and federated learning: A survey",
            "A comprehensive study of jailbreak attack versus defense for large language models",
            "Training language models to follow instructions with human feedback",
            "Constitutional ai: Harmlessness from ai feedback",
            "RLAIF vs. RLHF: Scaling reinforcement learning from human feedback with AI feedback",
            "Fine-tuning aligned language models compromises safety, even when users do not intend to!",
            "Antidote: Post-fine-tuning safety alignment for large language models against harmful fine-tuning",
            "An experimental study of byzantine-robust aggregation schemes in federated learning",
            "Manipulating the byzantine: Optimizing model poisoning attacks and defenses for federated learning",
            "Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning",
            "Tuning layernorm in attention: Towards efficient multi-modal LLM finetuning",
            "Blades: A unified benchmark suite for byzantine attacks and defenses in federated learning",
            "Roberta: A robustly optimized bert pretraining approach",
            "Attention is all you need",
            "Root mean square layer normalization",
            "Stanford alpaca: An instruction-following llama model",
            "Emerging safety attack and defense in federated instruction tuning of large language models",
            "Robust aggregation for federated learning",
            "Local model poisoning attacks to byzantine-robust federated learning",
            "Byzantine-robust aggregation in federated learning empowered industrial iot",
            "Vaccine: Perturbation-aware alignment for large language models against harmful fine-tuning attack",
            "Phi-3 technical report: A highly capable language model locally on your phone",
            "Parameter-efficient fine-tuning of large-scale pre-trained language models",
            "Metamath: Bootstrap your own mathematical questions for large language models",
            "Training verifiers to solve math word problems",
            "Measuring mathematical problem solving with the MATH dataset",
            "What disease does this patient have? a large-scale open domain question answering dataset from medical exams",
            "Beavertails: Towards improved safety alignment of llm via a human-preference dataset",
            "Byzantine-robust distributed learning: Towards optimal statistical rates",
            "Distributed statistical machine learning in adversarial settings: Byzantine gradient descent",
            "Constitutional ai recipe",
            "Decoupled weight decay regularization",
            "Efficient memory management for large language model serving with pagedattention",
            "Universal and transferable adversarial attacks on aligned language models",
            "Jailbreakbench: An open robustness benchmark for jailbreaking large language models",
            "What is in your safe data? identifying benign data that breaks safety",
            "Mitigating the alignment tax of RLHF",
            "Reducing communication overhead in federated learning for pre-trained language models using parameter-efficient finetuning",
            "Federated learning of large language models with parameter-efficient prompt tuning and adaptive optimization",
            "Federated adaptation for foundation model-based recommendations",
            "Fedbpt: Efficient federated black-box prompt tuning for large language models",
            "Personalized federated learning for text classification with gradient-free prompt tuning",
            "Fedra: A random allocation strategy for federated tuning to unleash the power of heterogeneous clients",
            "Federated fine-tuning of large language models under heterogeneous language tasks and client resources",
            "Backdoor threats from compromised foundation models to federated learning",
            "Securing federated learning against novel and classic backdoor threats during foundation model integration",
            "Shadow alignment: The ease of subverting safely-aligned language models",
            "Removing RLHF protections in GPT-4 via fine-tuning",
            "No two devils alike: Unveiling distinct mechanisms of fine-tuning attacks",
            "Assessing the brittleness of safety alignment via pruning and low-rank modifications",
            "Navigating the safety landscape: Measuring risks in finetuning large language models",
            "What makes and breaks safety fine-tuning? a mechanistic study",
            "Fine-tuning can cripple your foundation model; preserving features may be the solution",
            "Safety fine-tuning at (almost) no cost: A baseline for vision large language models",
            "Lisa: Lazy safety alignment for large language models against harmful fine-tuning attack",
            "A safety realignment framework via subspace-oriented model fusion for large language models"
        ]
    },
    {
        "index": 186,
        "title": "Poisoned LangChain: Jailbreak LLMs by LangChain",
        "publication_date": "2024-06-26",
        "references": [
            "Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al .2023. Qwen technical report.",
            "H Chase. 2023. LangChain LLM App Development Framework .",
            "Chatchat-Space. 2023. ChatChat .",
            "ChatGLM3. 2023. ChatGLM3 .",
            "Junjie Chu, Yugeng Liu, Ziqing Yang, Xinyue Shen, Michael Backes, and Yang Zhang. 2024. Comprehensive assessment of jailbreak attacks against llms.",
            "Gelei Deng, Yi Liu, Yuekang Li, Kailong Wang, Ying Zhang, Zefeng Li, Haoyu Wang, Tianwei Zhang, and Yang Liu. 2023. Jailbreaker: Automated jailbreak across multiple large language model chatbots.",
            "Gelei Deng, Yi Liu, Yuekang Li, Kailong Wang, Ying Zhang, Zefeng Li, Haoyu Wang, Tianwei Zhang, and Yang Liu. 2024. MASTERKEY: Automated jailbreaking of large language model chatbots.",
            "Kai Greshake, Sahar Abdelnabi, Shailesh Mishra, Christoph Endres, Thorsten Holz, and Mario Fritz. 2023. More than you’ve asked for: A Comprehensive Analysis of Novel Prompt Injection Threats to Application-Integrated Large Language Models.",
            "Yangsibo Huang, Samyak Gupta, Mengzhou Xia, Kai Li, and Danqi Chen. 2023. Catastrophic jailbreak of open-source llms via exploiting generation.",
            "Daniel Kang, Xuechen Li, Ion Stoica, Carlos Guestrin, Matei Zaharia, and Tatsunori Hashimoto. 2023. Exploiting programmatic behavior of llms: Dual-use through standard security attacks.",
            "Varun Kumar, Leonard Gleyzer, Adar Kahana, Khemraj Shukla, and George Em Karniadakis. 2023. Mycrunchgpt: A llm assisted framework for scientific machine learning.",
            "Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, et al .2020. Retrieval-augmented generation for knowledge-intensive nlp tasks.",
            "Haoran Li, Dadi Guo, Wei Fan, Mingshi Xu, Jie Huang, Fanpu Meng, and Yangqiu Song. 2023. Multi-step jailbreaking privacy attacks on chatgpt.",
            "Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2024. Visual instruction tuning.",
            "Tianyu Liu, Kexiang Wang, Lei Sha, Baobao Chang, and Zhifang Sui. 2018. Table-to-text generation by structure-aware seq2seq learning.",
            "Xiaogeng Liu, Nan Xu, Muhao Chen, and Chaowei Xiao. 2023. Auto-dan: Generating stealthy jailbreak prompts on aligned large language models.",
            "Yihan Liu, Zhen Wen, Luoxuan Weng, Ollie Woodman, Yi Yang, and Wei Chen. 2023. SPROUT: Authoring Programming Tutorials with Interactive Visualization of Large Language Model Generation Process.",
            "Anay Mehrotra, Manolis Zampetakis, Paul Kassianik, Blaine Nelson, Hyrum Anderson, Yaron Singer, and Amin Karbasi. 2023. Tree of attacks: Jailbreaking black-box llms automatically.",
            "Meta. 2023. Introducing Llama 2 .",
            "OpenAI. 2023. Chatgpt-4.0 .",
            "OpenAI. 2024. Openai usage policies .",
            "Xinyue Shen, Zeyuan Chen, Michael Backes, Yun Shen, and Yang Zhang. 2023. \" do anything now\": Characterizing and evaluating in-the-wild jailbreak prompts on large language models.",
            "Shamane Siriwardhana, Rivindu Weerasekera, Elliott Wen, Tharindu Kaluarachchi, Rajib Rana, and Suranga Nanayakkara. 2023. Improving the domain adaptation of retrieval augmented generation (RAG) models for open domain question answering.",
            "Yu Sun, Shuohuan Wang, Shikun Feng, Siyu Ding, Chao Pang, Junyuan Shang, Jiaxiang Liu, Xuyi Chen, Yanbin Zhao, Yuxiang Lu, et al .2021. Ernie 3.0: Large-scale knowledge enhanced pre-training for language understanding and generation.",
            "Harry Thornburg. 2023. Introduction to Bayesian Statistics .",
            "Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al .2023. Llama 2: Open foundation and fine-tuned chat models.",
            "Alexander Wei, Nika Haghtalab, and Jacob Steinhardt. 2024. Jailbroken: How does llm safety training fail?",
            "Luoxuan Weng, Xingbo Wang, Junyu Lu, Yingchaojie Feng, Yihan Liu, and Wei Chen. 2024. InsightLens: Discovering and Exploring Insights from Conversational Contexts in Large-Language-Model-Powered Data Analysis.",
            "Xinghuo. 2023. Xinghuo .",
            "Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, et al .2022. Glm-130b: An open bilingual pre-trained model."
        ]
    },
    {
        "index": 187,
        "title": "PolygloToxicityPrompts: Multilingual Evaluation of Neural Toxic Degeneration in Large Language Models",
        "publication_date": "2024-08-10",
        "references": [
            "A survey on data selection for language models.",
            "Many-shot jailbreaking.",
            "A general theoretical paradigm to understand learning from human preferences.",
            "Just say no: Analyzing the stance of neural dialogue generation in offensive contexts.",
            "Qwen technical report.",
            "On the dangers of stochastic parrots: Can language models be too big?",
            "Pythia: A suite for analyzing large language models across training and scaling.",
            "Nuanced metrics for measuring unintended bias with real data for text classification.",
            "Jailbreaking black box large language models in twenty queries.",
            "Deep reinforcement learning from human preferences.",
            "Toxicity in multilingual machine translation at scale.",
            "Empathy, ways of knowing, and interdependence as mediators of gender differences in attitudes toward hate speech and freedom of speech.",
            "Under the surface: Tracking the artifactuality of llm-generated data.",
            "Rtp-lx: Can llms evaluate toxicity in multilingual scenarios?.",
            "Masterkey: Automated jailbreaking of large language model chatbots.",
            "Multilingual jailbreak challenges in large language models.",
            "BERT: Pre-training of deep bidirectional transformers for language understanding.",
            "Measuring and mitigating unintended bias in text classification.",
            "RealToxicityPrompts: Evaluating neural toxic degeneration in language models.",
            "Fortifying toxic speech detectors against veiled toxicity.",
            "ToxiGen: A large-scale machine-generated dataset for adversarial and implicit hate speech detection.",
            "The pile: An 800gb dataset of diverse text for language modeling.",
            "Training language models to follow instructions with human feedback.",
            "Red teaming language models with language models.",
            "Introducing gemini: our largest and most capable ai model.",
            "On the challenges of using black-box APIs for toxicity evaluation in research.",
            "Crosslingual generalization through multitask finetuning.",
            "StereoSet: Measuring stereotypical bias in pretrained language models.",
            "CrowS-pairs: A challenge dataset for measuring social biases in masked language models.",
            "Toxic bias: Perspective api misreads german as more toxic.",
            "An empirical analysis of compute-optimal large language model training.",
            "Catastrophic jailbreak of open-source llms via exploiting generation.",
            "Social biases in NLP models as barriers for persons with disabilities.",
            "Llama guard: Llm-based input-output safeguard for human-ai conversations.",
            "Camels in a changing climate: Enhancing lm adaptation with tulu 2.",
            "Mistral 7b.",
            "Automatically auditing large language models via discrete optimization.",
            "Efficient memory management for large language model serving with pagedattention.",
            "A new generation of perspective api: Efficient multilingual character-level transformers."
        ]
    },
    {
        "index": 188,
        "title": "Preemptive Answer ‘Attacks’ on Chain-of-Thought Reasoning",
        "publication_date": "2024-05-31",
        "references": [
            "MathQA: Towards interpretable math word problem solving with operation-based formalisms",
            "Graph of thoughts: Solving elaborate problems with large language models",
            "Cognitive restructuring as an early stage in problem solving",
            "Language models are few-shot learners",
            "Man-in-the-middle attack to the https protocol",
            "Jailbreaking black box large language models in twenty queries",
            "Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks",
            "Palm: Scaling language modeling with pathways",
            "Training verifiers to solve math word problems",
            "Metacognition: ideas and insights from neuro-and educational sciences",
            "Did aristotle use a laptop? a question answering benchmark with implicit reasoning strategies",
            "Llama 2: Open foundation and fine-tuned chat models",
            "Interleaving retrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions",
            "Decodingtrust: A comprehensive assessment of trustworthiness in gpt models",
            "Adversarial demonstration attacks on large language models",
            "Self-consistency improves chain of thought reasoning in language models",
            "Chain-of-thought prompting elicits reasoning in large language models",
            "Universal and transferable adversarial attacks on aligned language models",
            "Promptbench: Towards evaluating the robustness of large language models on adversarial prompts",
            "Badchain: Backdoor chain-of-thought prompting for large language models",
            "Exploring the universal vulnerability of prompt-based learning paradigm",
            "The earth is flat because...: Investigating llms’ belief towards misinformation via persuasive conversation",
            "Reprompting: Automated chain-of-thought prompt inference through gibbs sampling",
            "HotpotQA: A dataset for diverse, explainable multi-hop question answering",
            "Llm lies: Hallucinations are not bugs, but features as adversarial examples",
            "Tree of thoughts: Deliberate problem solving with large language models",
            "Complementary explanations for effective in-context learning",
            "Automatic chain of thought prompting in large language models",
            "Prompt as triggers for backdoor attack: Examining the vulnerability in language models",
            "Least-to-most prompting enables complex reasoning in large language models",
            "Context-faithful prompting for large language models"
        ]
    },
    {
        "index": 189,
        "title": "Preference Tuning For Toxicity Mitigation Generalizes Across Languages",
        "publication_date": "2024-11-08",
        "references": [
            "Llama 3 model card.",
            "Refusal in language models is mediated by a single direction.",
            "Massively multilingual sentence embeddings for zero-shot cross-lingual transfer and beyond.",
            "Aya 23: Open weight releases to further multilingual progress.",
            "Training a helpful and harmless assistant with reinforcement learning from human feedback.",
            "Characterizing large language model geometry solves toxicity detection and generation.",
            "Leace: Perfect linear concept erasure in closed form.",
            "Mechanistic interpretability for ai safety–a review.",
            "Bloom: A 176b-parameter open-access multilingual language model.",
            "Toxic comment classification challenge.",
            "Think you have solved question answering? try arc, the ai2 reasoning challenge.",
            "Ultrafeedback: Boosting language models with high-quality feedback.",
            "Rtp-lx: Can llms evaluate toxicity in multilingual scenarios?",
            "Multiparadetox: Extending text detoxification with parallel data to new languages.",
            "Exploring methods for cross-lingual text style transfer: The case of text detoxification.",
            "Multilingual jailbreak challenges in large language models.",
            "Qlora: Efficient finetuning of quantized llms.",
            "Identifying elements essential for BERT’s multilinguality.",
            "Beyond english-centric multilingual machine translation.",
            "A primer on the inner workings of transformer-based language models.",
            "RealToxicityPrompts: Evaluating neural toxic degeneration in language models.",
            "Transformer feed-forward layers build predictions by promoting concepts in the vocabulary space.",
            "Transformer feed-forward layers are key-value memories.",
            "How does gpt-2 compute greater-than?: Interpreting mathematical abilities in a pre-trained language model.",
            "Deep residual learning for image recognition.",
            "Measuring massive multitask language understanding.",
            "The curious case of neural text degeneration.",
            "Reference-free monolithic preference optimization with odds ratio.",
            "mothello: When do cross-lingual representation alignment and cross-lingual transfer emerge in multilingual models?",
            "Camels in a changing climate: Enhancing lm adaptation with tulu 2.",
            "Polyglotoxicityprompts: Multilingual evaluation of neural toxic degeneration in large language models.",
            "A distributional approach to controlled text generation.",
            "Understanding the effects of RLHF on LLM generalisation and diversity.",
            "Okapi: Instruction-tuned large language models in multiple languages with reinforcement learning from human feedback.",
            "A mechanistic understanding of alignment algorithms: A case study on dpo and toxicity.",
            "A new generation of perspective api: Efficient multilingual character-level transformers.",
            "DExperts: Decoding-time controlled text generation with experts and anti-experts.",
            "A safe harbor for ai evaluation and red teaming.",
            "Concrete problems in ai safety, revisited.",
            "Unintended impacts of llm alignment on global representation.",
            "The language barrier: Dissecting safety challenges of llms in multilingual contexts.",
            "mgpt: Few-shot learners go multilingual.",
            "Llama 2: Open foundation and fine-tuned chat models.",
            "Detox: Toxic subspace projection for model editing.",
            "Aya model: An instruction finetuned open-access multilingual language model.",
            "Cross-lingual knowledge editing in large language models.",
            "Detoxifying large language models via knowledge editing.",
            "All languages matter: On the multilingual safety of large language models.",
            "Assessing the brittleness of safety alignment via pruning and low-rank modifications.",
            "Star: Sociotechnical approach to red teaming language models.",
            "Reuse your rewards: Reward model transfer for zero-shot cross-lingual alignment.",
            "Improving alignment and robustness with short circuiting."
        ]
    },
    {
        "index": 190,
        "title": "Prefix Guidance: A Steering Wheel for Large Language Models to Defend Against Jailbreak Attacks",
        "publication_date": "2024-08-22",
        "references": [
            "Language models are few-shot learners",
            "Jailbreaking Black Box Large Language Models in Twenty Queries",
            "Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality",
            "ReNeLLM",
            "Attack Prompt Generation for Red Teaming and Defending Large Language Models",
            "Multilingual Jailbreak Challenges in Large Language Models",
            "Qlora: Efficient finetuning of quantized llms",
            "A Wolf in Sheep’s Clothing: Generalized Nested Jailbreak Prompts can Fool Large Language Models Easily",
            "SafeDecoding",
            "LoRA: Low-Rank Adaptation of Large Language Models",
            "Catastrophic Jailbreak of Open-source LLMs via Exploiting Generation",
            "Llama Guard: LLM-based Input-Output Safeguard for Human-AI Conversations",
            "Baseline Defenses for Adversarial Attacks Against Aligned Language Models",
            "Multi-step Jailbreaking Privacy Attacks on ChatGPT",
            "DeepInception: Hypnotize Large Language Model to Be Jailbreaker",
            "RAIN: Your Language Models Can Align Themselves without Finetuning",
            "AutoDAN: Generating Stealthy Jailbreak Prompts on Aligned Large Language Models",
            "Jailbreaking ChatGPT via Prompt Engineering: An Empirical Study",
            "RoBERTa: A Robustly Optimized BERT Pretraining Approach",
            "GPT-4 Technical Report",
            "LLM Self Defense: By Self Examination, LLMs Know They Are Being Tricked",
            "Fine-tuning Aligned Language Models Compromises Safety, Even When Users Do Not Intend To!",
            "SmoothLLM: Defending Large Language Models Against Jailbreaking Attacks",
            "Do Anything Now\": Characterizing and Evaluating In-The-Wild Jailbreak Prompts on Large Language Models",
            "Stanford Alpaca: An Instruction-following LLaMA model",
            "Gemini: A Family of Highly Capable Multimodal Models",
            "Llama 2: Open Foundation and Fine-Tuned Chat Models",
            "Chain-of-thought prompting elicits reasoning in large language models",
            "Jailbreak and Guard Aligned Language Models with Only Few In-Context Demonstrations",
            "Defending chatgpt against jailbreak attack via self-reminders",
            "SafeDecoding: Defending against Jailbreak Attacks via Safety-Aware Decoding",
            "Low-Resource Languages Jailbreak GPT-4",
            "Teaching large language models to translate with comparison",
            "How Johnny Can Persuade LLMs to Jailbreak Them: Rethinking Persuasion to Challenge AI Safety by Humanizing LLMs",
            "Defending Large Language Models Against Jailbreak Attacks via Layer-specific Editing",
            "Can LLM Replace Stack Overflow? A Study on Robustness and Reliability of Large Language Model Code Generation",
            "Universal and Transferable Adversarial Attacks on Aligned Language Models"
        ]
    },
    {
        "index": 191,
        "title": "Preventing Jailbreak Prompts as Malicious Tools for Cybercriminals: A Cyber Defense Perspective",
        "publication_date": "2024-11-25",
        "references": [
            "Securing large language models: Threats, vulnerabilities and responsible practices",
            "Gpt-4 technical report",
            "Jailbreaking leading safety-aligned llms with simple adaptive attacks",
            "Safety-tuned llamas: Lessons from improving the safety of large language models that follow instructions",
            "Defending against alignment-breaking attacks via robustly aligned llm",
            "Characterizing and evaluating the reliability of llms against jailbreak attacks",
            "Leveraging the context through multi-round interactions for jailbreaking attacks",
            "Attack prompt generation for red teaming and defending large language models",
            "Jailbreaker: Automated jailbreak across multiple large language model chatbots",
            "Cold-attack: Jailbreaking llms with stealthiness and controllability",
            "From chatgpt to threatgpt: Impact of generative ai in cybersecurity and privacy",
            "Obscureprompt: Jailbreaking large language models via obscure input",
            "Baseline defenses for adversarial attacks against aligned language models",
            "Break the breakout: Reinventing lm defense against jailbreak attacks with self-refinement",
            "What features in prompts jailbreak llms? investigating the mechanisms behind attacks",
            "Llm defenses are not robust to multi-turn human jailbreaks yet",
            "Drattack: Prompt decomposition and reconstruction makes powerful llm jailbreakers",
            "Autodan: Generating stealthy jailbreak prompts on aligned large language models",
            "A hitchhiker’s guide to jailbreaking chatgpt via prompt engineering",
            "Jailbreaking chatgpt via prompt engineering: An empirical study",
            "Autojailbreak: Exploring jailbreak attacks and defenses through a dependency lens",
            "Eraser: Jailbreaking defense in large language models via unlearning harmful knowledge",
            "Jailbreakv-28k: A benchmark for assessing the robustness of multimodal large language models against jailbreak attacks",
            "Tree of attacks: Jailbreaking black-box llms automatically",
            "An attacker’s dream? exploring the capabilities of chatgpt for developing malware",
            "Jailbreaking and mitigation of vulnerabilities in large language models",
            "Mitigating adversarial manipulation in llms: a prompt-based approach to counter jailbreak attacks (prompt-g)",
            "Smoothllm: Defending large language models against jailbreaking attacks",
            "Spml: A dsl for defending language models against prompt attacks",
            "‘do anything now’: Characterizing and evaluating in-the-wild jailbreak prompts on large language models",
            "Attngcg: Enhancing jailbreaking attacks on llms with attention manipulation",
            "Jailbroken: How does llm safety training fail?",
            "Gradsafe: Detecting unsafe prompts for llms via safety-critical gradient analysis",
            "Defending chatgpt against jailbreak attack via self-reminders",
            "Defensive prompt patch: A robust and interpretable defense of llms against jailbreak attacks",
            "Autoattacker: A large language model guided system to implement automatic cyber-attacks",
            "Bag of tricks: Benchmarking of jailbreak attacks on llms",
            "A comprehensive study of jailbreak attack versus defense for large language models",
            "Llm jailbreak attack versus defense techniques–a comprehensive study",
            "Jailbreak attacks and defenses against large language models: A survey",
            "Don’t listen to me: Understanding and exploring jailbreak prompts of large language models",
            "Autodefense: Multi-agent llm defense against jailbreak attacks",
            "When llms meet cybersecurity: A systematic literature review",
            "Wordgame: Efficient & effective llm jailbreak via simultaneous obfuscation in query and response",
            "Is the system message really important to jailbreaks in large language models?"
        ]
    },
    {
        "index": 192,
        "title": "PrimeGuard: Safe and Helpful LLMs through Tuning-Free Routing",
        "publication_date": "2024-07-23",
        "references": [
            "Phi-3 technical report: A highly capable language model locally on your phone",
            "Gpt-4 technical report",
            "The poison of alignment",
            "Red-teaming large language models using chain of utterances for safety-alignment",
            "Executive order on the safe, secure, and trustworthy development and use of artificial intelligence",
            "Language models are few-shot learners",
            "Jailbreaking black box large language models in twenty queries",
            "Jailbreakbench: An open robustness benchmark for jailbreaking large language models",
            "Deep reinforcement learning fromhuman preferences",
            "A coefficient of agreement for nominal scales",
            "Free dolly: Introducing the world’s first truly open instruction-tuned llm",
            "Attack prompt generation for red teaming and defending large language models",
            "Hotflip: White-box adversarial examples for text classification",
            "Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned",
            "Gradient-based adversarial attacks against text transformers",
            "Llama guard: Llm-based input-output safeguard for human-ai conversations",
            "Mixtral 8x22b",
            "Mistral 7b",
            "Prometheus 2: An open source language model specialized in evaluating other language models",
            "Three alignment taxes",
            "The unlocking spell on base llms: Rethinking alignment via in-context learning",
            "Student-teacher prompting for red teaming to improve guardrails",
            "Artificial intelligence act",
            "Tree of attacks: Jailbreaking black-box llms automatically",
            "Llama 3",
            "Training language models to follow instructions with human feedback",
            "Aart: Ai-assisted red-teaming with diverse data generation for new llm-powered applications",
            "Direct preference optimization: Your language model is secretly a reward model",
            "Nemo guardrails: A toolkit for controllable and safe llm applications with programmable rails",
            "Smooth-llm: Defending large language models against jailbreaking attacks",
            "Xstest: A test suite for identifying exaggerated safety behaviours in large language models",
            "”do anything now”: Characterizing and evaluating in-the-wild jailbreak prompts on large language models",
            "Ai ethics and systemic risks in finance",
            "Llama 2: Open foundation and fine-tuned chat models",
            "The art of defending: A systematic evaluation and analysis of llm defense strategies on safety and over-defensiveness",
            "The instruction hierarchy: Training llms to prioritize privileged instructions",
            "Do-not-answer: A dataset for evaluating safeguards in llms",
            "Jailbreak and guard aligned language models with only few in-context demonstrations",
            "Efficient guided generation for llms",
            "Defending chatgpt against jailbreak attack via self-reminders",
            "How johnny can persuade llms to jailbreak them: Rethinking persuasion to challenge ai safety by humanizing llms",
            "Intention analysis prompting makes large language models a good jailbreak defender",
            "Shieldlm: Empowering llms as aligned, customizable and explainable safety detectors",
            "Judgelm: Fine-tuned large language models are scalable judges",
            "Universal and transferable adversarial attacks on aligned language models"
        ]
    },
    {
        "index": 193,
        "title": "PBI-Attack: Prior-Guided Bimodal Interactive Black-Box Jailbreak Attack for Toxicity Maximization",
        "publication_date": "2025-02-03",
        "references": [
            "Gpt-4 technical report",
            "Qwen-vl: A frontier large vision-language model with versatile abilities",
            "InstructBLIP: Towards general-purpose vision-language models with instruction tuning",
            "Jailbreaking text-to-image models with llm-based agents",
            "An image is worth 16x16 words: Transformers for image recognition at scale",
            "Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned",
            "Figstep: Jailbreaking large vision-language models via typographic visual prompts",
            "Cnn-based projected gradient descent for consistent ct image reconstruction",
            "Detoxify",
            "Effectiveness assessment of recent large vision-language models",
            "BLIP-2: Bootstrapping language-image pre-training with frozen image encoders and large language models",
            "Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation",
            "Images are achilles’ heel of alignment: Exploiting visual vulnerabilities for jail-breaking multimodal large language models",
            "Amplegcg: Learning a universal and transferable generative model of adversarial suffixes for jail-breaking both open and closed llms",
            "A survey of attacks on large vision-language models: Resources, advances, and future trends",
            "Visual instruction tuning",
            "Arondight: Red teaming large vision language models with auto-generated multi-modal jailbreak prompts",
            "Jailbreaking chatgpt via prompt engineering: An empirical study",
            "Jailbreaking prompt attack: A controllable adversarial attack against diffusion models",
            "Learning transferable visual models from natural language supervision",
            "Jailbreak in pieces: Compositional adversarial attacks on multi-modal language models",
            "Survey of vulnerabilities in large language models revealed by adversarial attacks",
            "Gemini: a family of highly capable multimodal models",
            "Llama: Open and efficient foundation language models",
            "Mrj-agent: An effective jailbreak agent for multi-round dialogue",
            "White-box multimodal jailbreaks against large vision-language models",
            "Chain-of-jailbreak attack for image generation models via editing step by step",
            "Sneakyprompt: Jailbreaking text-to-image generative models",
            "Jailbreak attacks and defenses against large language models: A survey",
            "Vlattack: Multimodal adversarial attacks on vision-language tasks via pre-trained models",
            "Jailbreak vision language models via bi-modal adversarial prompt",
            "Vision-language models for vision tasks: A survey",
            "Judging llm-as-a-judge with mt-bench and chatbot arena",
            "Minigpt-4: Enhancing vision-language understanding with advanced large language models",
            "Universal and transferable adversarial attacks on aligned language models",
            "Description of the attributes in Perspective API"
        ]
    },
    {
        "index": 194,
        "title": "Pro-Woman, Anti-Man? Identifying Gender Bias in Stance Detection",
        "publication_date": "2024-08-11",
        "references": [
            "Stance detection on social media: State of the art and trends.",
            "Zero-shot stance detection: A dataset and model using generalized topic representations.",
            "Evaluating gender bias of pre-trained language models in natural language inference by considering all labels.",
            "A tale of pronouns: Interpretability informs gender bias mitigation for fairer instruction-tuned machine translation.",
            "Good secretaries, bad truck drivers? occupational gender stereotypes in sentiment analysis.",
            "Language (technology) is power: A critical survey of “bias” in NLP.",
            "Understanding the origins of bias in word embeddings.",
            "Semantics derived automatically from language corpora contain human-like biases.",
            "Toward gender-inclusive coreference resolution.",
            "Bias and fairness in natural language processing.",
            "On measuring and mitigating biased inferences of word embeddings.",
            "BERT: Pre-training of deep bidirectional transformers for language understanding.",
            "Queens are powerful too: Mitigating gender bias in dialogue generation.",
            "Determining relative argument specificity and stance for complex argumentative structures.",
            "Stance detection in COVID-19 tweets.",
            "The times they are a-changing ... or are they not? a comparison of gender stereotypes, 1983-2014.",
            "Unlearn dataset bias in natural language inference by fitting the residual.",
            "Infusing knowledge from Wikipedia to enhance stance detection.",
            "Five sources of bias in natural language processing.",
            "Tagging performance correlates with author age.",
            "tWT–WT: A dataset to assert the role of target entities for detecting stance of tweets.",
            "Examining gender and race bias in two hundred sentiment analysis systems.",
            "Stance detection: A survey.",
            "Stance detection on social media with background knowledge.",
            "Multi-task stance detection with sentiment and stance lexicons.",
            "Distilling calibrated knowledge for stance detection.",
            "A new direction in stance detection: Target-stance extraction in the wild.",
            "P-stance: A large dataset for stance detection in political domain.",
            "TTS: A target-based teacher-student framework for zero-shot stance detection.",
            "RoBERTa: A robustly optimized BERT pretraining approach.",
            "Guiding computational stance detection with expanded stance triangle framework.",
            "Decoupled weight decay regularization.",
            "A survey on bias and fairness in machine learning.",
            "SemEval-2016 task 6: Detecting stance in tweets.",
            "Gender bias in stem fields: Variation in prevalence and links to stem self-concept.",
            "Gender bias in coreference resolution.",
            "Gender bias in machine translation.",
            "A survey on gender bias in natural language processing.",
            "Mitigating gender bias in natural language processing: Literature review.",
            "Will i fit in and do well? the importance of social belongingness and self-efficacy for explaining gender differences in interest in stem- and heed-majors.",
            "Language models get a gender makeover: Mitigating gender bias with few-shot data interventions.",
            "Investigating the frequency distortion of word embeddings and its impact on bias metrics.",
            "How would stance detection techniques evolve after the launch of ChatGPT?",
            "C-STANCE: A large dataset for Chinese zero-shot stance detection.",
            "Gender bias in coreference resolution: Evaluation and debiasing methods.",
            "Towards identifying social bias in dialog systems: Framework, dataset, and benchmark."
        ]
    },
    {
        "index": 195,
        "title": "Probing the Safety Response Boundary of Large Language Models via Unsafe Decoding Path Generation",
        "publication_date": "2024-08-26",
        "references": [
            "Gpt-4 technical report",
            "The Bellman equation for minimizing the maximum cost",
            "Safety-tuned llamas: Lessons from improving the safety of large language models that follow instructions",
            "Safe rlhf: Safe reinforcement learning from human feedback",
            "Qlora: Efficient finetuning of quantized llms",
            "DeepSeek-Coder: When the Large Language Model Meets Programming–The Rise of Code Intelligence",
            "Value Augmented Sampling for Language Model Alignment and Personalization",
            "Deep residual learning for image recognition",
            "Llama guard: Llm-based input-output safeguard for human-ai conversations",
            "Aligner: Achieving efficient alignment through weak-to-strong correction",
            "Mistral 7B",
            "ARGS: Alignment as reward-guided search",
            "Aligning Large Language Models with Representation Editing: A Control Perspective",
            "QPO: Query-dependent Prompt Optimization via Multi-Loop Offline Reinforcement Learning",
            "Tptu-v2: Boosting task planning and tool usage of large language model-based agents in real-world systems",
            "Rewardbench: Evaluating reward models for language modeling",
            "Learning diverse attacks on large language models for robust red-teaming and safety tuning",
            "Reducing digital copyright infringement without restricting innovation",
            "Autodan: Generating stealthy jailbreak prompts on aligned large language models",
            "Training language models to follow instructions with human feedback",
            "Iterative reasoning preference optimization",
            "Instruction tuning with gpt-4",
            "Visual adversarial examples jailbreak large language models",
            "Safety Alignment Should Be Made More Than Just a Few Tokens Deep",
            "Fine-tuning aligned language models compromises safety, even when users do not intend to!",
            "Direct preference optimization: Your language model is secretly a reward model",
            "Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context",
            "Principle-driven self-alignment of language models from scratch with minimal human supervision",
            "Reinforcement learning: An introduction",
            "Llama 2: Open foundation and fine-tuned chat models",
            "Q*: Improving Multi-step Reasoning for LLMs with Deliberative Planning",
            "Step-on-feet tuning: Scaling self-alignment of llms via bootstrapping",
            "Are Large Language Models Really Robust to Word-Level Perturbations?",
            "Interpretable Preferences via Multi-Objective Reward Modeling and Mixture-of-Experts",
            "DEER: A Delay-Resilient Framework for Reinforcement Learning with Variable Delays",
            "Tinyllama: An open-source small language model",
            "Raft: Adapting language model to domain specific rag",
            "Minigpt-4: Enhancing vision-language understanding with advanced large language models",
            "Universal and transferable adversarial attacks on aligned language models",
            "Universal and transferable adversarial attacks on aligned language models"
        ]
    },
    {
        "index": 196,
        "title": "PROMPT INFECTION : LLM- TO-LLM PROMPT INJECTION WITHIN MULTI-AGENT SYSTEMS",
        "publication_date": "2024-10-09",
        "references": [
            "StruQ: Defending Against Prompt Injection with Structured Queries",
            "Deep reinforcement learning from human preferences",
            "Here Comes The AI Worm: Unleashing Zero-click Worms that Target GenAI-Powered Applications",
            "crewAIInc/crewAI",
            "Not what you’ve signed up for: Compromising Real-World LLM-Integrated Applications with Indirect Prompt Injection",
            "Agent Smith: A Single Image Can Jailbreak One Million Multimodal LLM Agents Exponentially Fast",
            "Large Language Model based Multi-Agents: A Survey of Progress and Challenges",
            "Defending Against Indirect Prompt Injection Attacks With Spotlighting",
            "War and Peace (WarAgent): Large Language Model-based Multi-Agent Simulation of World Wars",
            "On the Resilience of Multi-Agent Systems with Malicious Agents",
            "Flooding Spread of Manipulated Knowledge in LLM-Based Multi-Agent Communities",
            "Exploiting Programmatic Behavior of LLMs: Dual-Use Through Standard Security Attacks",
            "Towards Fair RAG: On the Impact of Fair Ranking in Retrieval-Augmented Generation",
            "Generative Agents: Interactive Simulacra of Human Behavior",
            "Instruction Tuning with GPT-4",
            "Ignore Previous Prompt: Attack Techniques For Language Models",
            "ChatDev: Communicative Agents for Software Development",
            "Tool Learning with Large Language Models: A Survey",
            "Instruction Defense: Strengthen AI Prompts Against Hacking",
            "Random Sequence Enclosure: Safeguarding AI Prompts",
            "Sandwich Defense",
            "Defending Language Models Against Image-Based Prompt Attacks via User-Provided Specifications",
            "Evil Geniuses: Delving into the Safety of LLM-based Agents",
            "Creating Large Language Model Applications Utilizing LangChain: A Primer on Developing LLM Apps Fast",
            "Jailbroken: How Does LLM Safety Training Fail?",
            "Multiagent Systems: A Modern Approach to Distributed Artificial Intelligence",
            "geekan/MetaGPT",
            "AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation",
            "ToolSword: Unveiling Safety Issues of Large Language Models in Tool Learning Across Three Stages",
            "Breaking Agents: Compromising Autonomous LLM Agents Through Malfunction Amplification",
            "Instruction Tuning for Large Language Models: A Survey",
            "A Study on Prompt Injection Attack Against LLM-Integrated Mobile Robotic Systems",
            "PsySafe: A Comprehensive Framework for Psychological-based Attack, Defense, and Evaluation of Multi-agent System Safety",
            "MemoryBank: Enhancing Large Language Models with Long-Term Memory",
            "SOTOPIA: Interactive Evaluation for Social Intelligence in Language Agents",
            "Universal and Transferable Adversarial Attacks on Aligned Language Models"
        ]
    },
    {
        "index": 197,
        "title": "Trust No AI: Prompt Injection Along The CIA Security Triad",
        "publication_date": "2024-01-01",
        "references": [
            "Understanding the extended CIA model, Revision 7, 4/22/2024",
            "A Taxonomy and Terminology of Attacks and Mitigations",
            "Declassifying the Responsible Disclosure of the Prompt Injection Attack Vulnerability of GPT -3",
            "Exploiting GPT -3 prompts with malicious inputs that order the model to ignore its previous directions.",
            "Prompt Injection",
            "Not what you’ve signed up for: Compromising Real -World LLM -Integrated Applications with Indirect Prompt Injection",
            "Cambridge Online Dictionary",
            "Gandalf",
            "Learn Prompting: Prompt Leaking",
            "New prompt injection attack on ChatGPT web version. Markdown images can steal your chat data.",
            "Bing Chat: Data Exfiltration Exploit Explained",
            "OpenAI Begins Tackling ChatGPT Data Leak Vulnerability",
            "Anthropic Claude Data Exfiltration Vulnerability Fixed",
            "Microsoft Fixes Data Exfiltration Vulnerability in Azure AI Playground",
            "Hacking Google Bard - From Prompt Injection to Data Exfiltration",
            "Google Cloud Vertex AI - Data Exfiltration Vulnerability Fixed in Generative AI Studio",
            "Bobby Tables but with LLM Apps - Google NotebookLM Data Exfiltration",
            "Google AI Studio Data Exfiltration via Prompt Injection - Possible Regression and Fix",
            "Google Colab AI: Data Leakage Through Image Rendering Fixed. Some Risks Remain.",
            "37c3 - NEW IMPORTANT INSTRUCTIONS - (CCC Presentation)",
            "GitHub Copilot Chat: From Prompt Injection to Data Exfiltration",
            "Don't blindly trust LLM responses. Threats to chatbots.",
            "ASCII Smuggler Tool: Crafting Invisible Text and Decoding Hidden Codes",
            "Video: ASCII Smuggling and Hidden Prompt Instructions",
            "Embrace the Red",
            "AWS Fixes Data Exfiltration Attack Angle in Amazon Q for Business",
            "ChatGPT Plugin Exploit Explained: From Prompt Injection to Accessing Private Data",
            "OpenAI Removes the 'Chat with Code' Plugin From Store",
            "Consequential flag",
            "Automatic Tool Invocation when Browsing with ChatGPT - Threats and Mitigations",
            "Spyware Injection Into Your ChatGPT's Long -Term Memory (SpAIware)",
            "Black Hat Europe: SpAIware & More: Advanced Prompt Injection Exploits in LLM Applications",
            "The dangers of AI agents unfurling hyperlinks and what to do about it",
            "Data Integrity: Identifying and Protecting Assets Against Ransomware and Other Destructive Events",
            "On the 'hallucination problem' Tweet",
            "Hacks in Taiwan - Prompt Injections in the Wild",
            "Google Gemini Sidebard app Demo POC",
            "Who Am I? Conditional Prompt Injection Attacks with Microsoft Copilot",
            "Prompt injection via invisible characters",
            "Invisible Prompt Injection Python Script Tweet",
            "ASCII Smuggler",
            "LLMs can also respond in the same invisible encoding",
            "LLM output can take over your computer",
            "Terminal DILLMa: Prompt Injection ANSI Sequences",
            "DeepSeek AI: From Prompt Injection To Account Takeover",
            "LLM Apps: Don't Get Stuck in an Infinite Loop!",
            "ChatGPT: Hacking Memories with Prompt Injection",
            "Sorry, ChatGPT Is Under Maintenance: Persistent Denial of Service through Prompt Injection and Memory Attacks"
        ]
    },
    {
        "index": 198,
        "title": "Prompt Injection Attacks on Large Language Models in Oncology",
        "publication_date": "2024-07-23",
        "references": [
            "Large Language Models Encode Clinical Knowledge",
            "Sparks of Artificial General Intelligence: Early experiments with GPT-4",
            "The future landscape of large language models in medicine",
            "Autonomous Artificial Intelligence Agents for Clinical Decision Making in Oncology",
            "Large language models in medicine",
            "GPT-4 for Information Retrieval and Comparison of Medical Oncology Guidelines",
            "Adapted large language models can outperform medical experts in clinical text summarization",
            "A Multimodal Generative AI Copilot for Human Pathology",
            "Vision-language foundation model for echocardiogram interpretation",
            "Hello GPT-4o",
            "Vision. Anthropic",
            "Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context",
            "Chameleon: Mixed-Modal Early-Fusion Foundation Models",
            "Reka Core, Flash, and Edge: A Series of Powerful Multimodal Language Models",
            "Improving Alignment and Robustness with Short Circuiting",
            "Adversarial attacks and adversarial robustness in computational pathology",
            "Prompt Injection attack against LLM-integrated Applications",
            "Wild patterns: Ten years after the rise of adversarial machine learning",
            "Meta Llama",
            "ArtPrompt: ASCII Art-based Jailbreak Attacks against Aligned LLMs",
            "AgentDojo: A Dynamic Environment to Evaluate Attacks and Defenses for LLM Agents",
            "Comparative Analysis of Multimodal Large Language Model Performance on Clinical Vignette Questions",
            "Bounding the Capabilities of Large Language Models in Open Text Generation with Prompt Constraints",
            "Authorship and AI tools"
        ]
    },
    {
        "index": 199,
        "title": "PROMPT FUZZ: Harnessing Fuzzing Techniques for Robust Testing of Prompt Injection in LLMs",
        "publication_date": "2024-09-23",
        "references": [
            "Gpt-4 technical report",
            "Gandalf ignore instructions",
            "Training a helpful and harmless assistant with reinforcement learning from human feedback",
            "Constitutional ai: Harmlessness from ai feedback",
            "Directed greybox fuzzing",
            "Coverage-based greybox fuzzing as markov chain",
            "Language models are few-shot learners",
            "Extracting training data from large language models",
            "Jailbreaking black box large language models in twenty queries",
            "How is chatgpt’s behavior changing over time?",
            "Struq: Defend-ing against prompt injection with structured queries",
            "When llm meets drl: Advancing jailbreaking efficiency via drl-guided search",
            "Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality",
            "Masterkey: Automated jailbreaking of large language model chatbots",
            "Qlora: Efficient finetuning of quantized llms",
            "Bert: Pre-training of deep bidirectional transformers for language understanding",
            "Grammar-based whitebox fuzzing",
            "Automated whitebox fuzz testing.",
            "Github copilot: Your ai pair programmer",
            "Not what you’ve signed up for: Compromising real-world llm-integrated applications with indirect prompt injection",
            "Badnets: Evaluating backdooring attacks on deep neural networks",
            "Measuring massive multitask language understanding",
            "Seed selection for successful fuzzing",
            "Pleak: Prompt leaking attacks against large language model applications",
            "The tensor trust game",
            "Diar: Removing uninteresting bytes from seeds in software fuzzing",
            "Multi-step jailbreaking privacy attacks on chatgpt",
            "Autodan: Generating stealthy jailbreak prompts on aligned large language models",
            "Automatic and universal prompt injection attacks against large language models",
            "Prompt injection attack against llm-integrated applications",
            "Jailbreaking chatgpt via prompt engineering: An empirical study",
            "Trojaning attack on neural networks",
            "Codechameleon: Personalized encryption framework for jailbreaking large language models",
            "Membership inference attacks against language models via neighbourhood comparison",
            "Tree of attacks: Jailbreaking black-box llms automatically",
            "An empirical study of the reliability of unix utilities",
            "Language model inversion",
            "Webgpt: Browser-assisted question-answering with human feedback",
            "Microsoft bing",
            "Prompt injection",
            "From prompt injections to sql injection attacks: How protected is your llm-integrated web application?",
            "Instruction tuning with gpt-4",
            "Ignore previous prompt: Attack techniques for language models",
            "Learning to poison large language models during instruction tuning",
            "Scaling up llm reviews for google ads content moderation",
            "Language models are unsupervised multitask learners",
            "Smoothllm: Defending large language models against jailbreaking attacks",
            "Bandfuzz: A practical framework for collaborative fuzzing with reinforcement learning",
            "On the exploitability of instruction tuning",
            "Trustllm: Trustworthiness in large language models",
            "Opening a pandora’s box: Things you should know in the era of custom gpts",
            "Llama: Open and efficient foundation language models",
            "Llama 2: Open foundation and fine-tuned chat models",
            "Tensor trust: Interpretable prompt injection attacks from an online game, november 2023",
            "Attention is all you need",
            "The instruction hierarchy: Training llms to prioritize privileged instructions",
            "Decodingtrust: A compre-hensive assessment of trustworthiness in gpt models",
            "Quantifying privacy risks of prompts in visual prompt learning",
            "Instructions as backdoors: Backdoor vulnerabilities of instruction tuning for large language models",
            "Backdooring instruction-tuned large language models with virtual prompt injection",
            "Gpt-4 is too smart to be safe: Stealthy chat with llms via cipher",
            "Gptfuzzer: Red teaming large lan-guage models with auto-generated jailbreak prompts",
            "Enhancing jailbreak attack against large language models through silent tokens",
            "Assessing prompt injection risks in 200+ custom gpts",
            "Ecofuzz: Adaptive energy-saving greybox fuzzing as a variant of the adversarial multi-armed bandit",
            "American fuzzy lop (afl)",
            "Generated distributions are all you need for membership inference attacks against generative models",
            "Prompt as triggers for backdoor attack: Examining the vulnerability in language models",
            "Universal and transferable adversarial attacks on aligned language models"
        ]
    },
    {
        "index": 200,
        "title": "Protecting Your LLMs with Information Bottleneck",
        "publication_date": "2024-10-10",
        "references": [
            "Recent advances in natural language processing via large pre-trained language models: A survey",
            "Universal and transferable adversarial attacks on aligned language models",
            "Fine-tuning aligned language models compromises safety, even when users do not intend to!",
            "Training language models to follow instructions with human feedback",
            "The information bottleneck method",
            "Deep learning and the information bottleneck principle",
            "Are aligned neural networks adversarially aligned?",
            "Defending against alignment-breaking attacks via robustly aligned llm",
            "Jailbreaking black box large language models in twenty queries",
            "Defending large language models against jailbreak attacks via semantic smoothing",
            "Sentence embeddings using siamese bert-networks",
            "Explaining time series via contrastive and locally sparse perturbations",
            "Timex++: Learning time-series explanations with information bottleneck",
            "Categorical reparameterization with gumbel-softmax",
            "Factorized explainer for graph neural networks",
            "Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension",
            "Easyjailbreak: A unified framework for jailbreaking large language models",
            "Llama 2: Open foundation and fine-tuned chat models",
            "Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality",
            "A wolf in sheep’s clothing: Generalized nested jailbreak prompts can fool large language models easily",
            "Gptfuzzer: Red teaming large language models with auto-generated jailbreak prompts",
            "Weak-to-strong jailbreaking on large language models",
            "Meteor: An automatic metric for mt evaluation with improved correlation with human judgments",
            "Bertscore: Evaluating text generation with bert",
            "GPT-4 is too smart to be safe: Stealthy chat with LLMs via cipher",
            "GlM: General language model pretraining with autoregressive blank infilling",
            "Mistral 7b",
            "Gpt-4 technical report",
            "Curiosity-driven red-teaming for large language models",
            "Sentence-bert: Sentence embeddings using siamese bert-networks"
        ]
    },
    {
        "index": 201,
        "title": "Quantized Delta Weight Is Safety Keeper",
        "publication_date": "2024-11-29",
        "references": [
            "Llama recipes: Examples to get started using the llama models from meta.",
            "Qwen technical report.",
            "Revisiting parameter-efficient tuning: Are we really there yet?",
            "Badpre: Task-agnostic backdoor attacks to pre-trained nlp foundation models.",
            "Punica: Multi-tenant lora serving.",
            "Longlora: Efficient fine-tuning of long-context large language models.",
            "Sslguard: A watermarking scheme for self-supervised learning pre-trained encoders.",
            "A back-door attack against lstm-based text classification systems.",
            "GPT3.int8(): 8-bit matrix multiplication for transformers at scale.",
            "QLora: Efficient fine-tuning of quantized LLMs.",
            "BERT: Pre-training of deep bidirectional transformers for language understanding.",
            "Open LLM leaderboard v2.",
            "GPTQ: Accurate post-training quantization for generative pre-trained transformers.",
            "Fine-tuning with the Gemini API.",
            "From chatgpt to threatgpt: Impact of generative ai in cybersecurity and privacy.",
            "You only prompt once: On the capabilities of prompt learning on large language models to tackle toxic content.",
            "Decoding compressed trust: Scrutinizing the trustworthiness of efficient llms under compression.",
            "Universal language model fine-tuning for text classification.",
            "LoRA: Low-rank adaptation of large language models.",
            "Towards understanding factual knowledge of large language models.",
            "Beavertails: Towards improved safety alignment of llm via a human-preference dataset.",
            "Survey of hallucination in natural language generation.",
            "Mistral 7b.",
            "Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension.",
            "Unfamiliar finetuning examples control how language models hallucinate.",
            "Meet dan — the ‘jailbreak’ version of chatgpt and how to use it — ai unchained and unfiltered.",
            "Backdoor learning: A survey.",
            "Improved baselines with visual instruction tuning.",
            "Bitdelta: Your fine-tune may only be worth one bit.",
            "Decoupled weight decay regularization.",
            "Interpreting GPT: the logit lens.",
            "Plateform for GPT Fine-tuning.",
            "Training language models to follow instructions with human feedback.",
            "GPT-3.5 turbo fine-tuning and API updates.",
            "Peftguard: Detecting backdoor attacks against parameter-efficient fine-tuning.",
            "Stanford alpaca: An instruction-following llama model.",
            "Fine-tuning language models for factuality.",
            "Triton: an intermediate language and compiler for tiled neural network computations.",
            "Llama 2: Open foundation and fine-tuned chat models."
        ]
    },
    {
        "index": 202,
        "title": "Rag ’n Roll: An End-to-End Evaluation of Indirect Prompt Manipulations in LLM-based Application Frameworks",
        "publication_date": "2024-08-12",
        "references": [
            "OWASP, “Owasp top 10 for llms and generative ai apps,”",
            "X. Shen, Y . Qu, M. Backes, and Y . Zhang, “Prompt stealing attacks against text-to-image generation models,”",
            "J. Evertz, M. Chlosta, L. Sch ¨onherr, and T. Eisenhofer, “Whispers in the machine: Confidentiality in llm-integrated systems,”",
            "K. Greshake, S. Abdelnabi, S. Mishra, C. Endres, T. Holz, and M. Fritz, “Not what you’ve signed up for: Compromising real-world llm-integrated applications with indirect prompt injection,”",
            "X. Chen, B. He, Z. Ye, L. Sun, and Y . Sun, “Towards imperceptible document manipulations against neural ranking models,”",
            "C. Wu, R. Zhang, J. Guo, M. De Rijke, Y . Fan, and X. Cheng, “Prada: Practical black-box adversarial attacks against neural ranking models,”",
            "J. Liu, Y . Kang, D. Tang, K. Song, C. Sun, X. Wang, W. Lu, and X. Liu, “Order-disorder: Imitation adversarial attacks for black-box neural ranking models,”",
            "A. T. Mallen, A. Asai, V . Zhong, R. Das, H. Hajishirzi, and D. Khashabi, “When not to trust language models: Investigating effectiveness of parametric and non-parametric memories,”",
            "“Introduction — LangChain — python.langchain.com,”",
            "“LlamaIndex - LlamaIndex — docs.llamaindex.ai,”",
            "“Haystack Introduction — docs.haystack.deepset.ai,”",
            "S. E. Robertson and S. Walker, “Some simple effective approximations to the 2-poisson model for probabilistic weighted retrieval,”",
            "“Chroma,”",
            "“pgvector/pgvector: Open-source vector similarity search for postgres,”",
            "“The vector database to build knowledgeable ai — pinecone,”",
            "“Default qa chains and prompts,”",
            "C. Song, A. M. Rush, and V . Shmatikov, “Adversarial semantic collisions,”",
            "Y . Wang, L. Lyu, and A. Anand, “Bert rankers are brittle: A study using adversarial document perturbations,”",
            "X. Chen, B. He, Z. Ye, L. Sun, and Y . Sun, “Towards imperceptible document manipulations against neural ranking models,”",
            "Z. Lin, Z. Li, X. Liao, X. Wang, and X. Liu, “Mawseo: Adversarial wiki search poisoning for illicit online promotion,”",
            "Z. Zhong, Z. Huang, A. Wettig, and D. Chen, “Poisoning retrieval corpora by injecting adversarial passages,”",
            "J. Song, J. Zhang, J. Zhu, M. Tang, and Y . Yang, “Trattack”:”text rewriting attack against text retrieval,”",
            "N. Raval and M. Verma, “One word at a time: adversarial attacks on retrieval models,”",
            "Y . Liu, R. Zhang, J. Guo, M. de Rijke, W. Chen, Y . Fan, and X. Cheng, “Topic-oriented adversarial attacks against black-box neural ranking models,”",
            "——, “Black-box adversarial attacks against dense retrieval models: A multi-view contrastive learning method,”",
            "W. Wang, F. Wei, L. Dong, H. Bao, N. Yang, and M. Zhou, “Minilm: Deep self-attention distillation for task-agnostic compression of pre-trained transformers,”",
            "M. Abdin, S. A. Jacobs, A. A. Awan, J. Aneja, A. Awadallah, H. Awadalla, N. Bach, A. Bahree, A. Bakhtiari, H. Behl et al. , “Phi-3 technical report: A highly capable language model locally on your phone,”",
            "N. F. Liu, K. Lin, J. Hewitt, A. Paranjape, M. Bevilacqua, F. Petroni, and P. Liang, “Lost in the middle: How language models use long contexts,”",
            "N. Kandpal, H. Deng, A. Roberts, E. Wallace, and C. Raffel, “Large language models struggle to learn long-tail knowledge,”",
            "“Ragas,”",
            "M. AI, “Introducing meta llama 3: The most capable openly available llm to date.”",
            "E. Crothers, N. Japkowicz, and H. L. Viktor, “Machine-generated text: A comprehensive survey of threat models and detection methods,”",
            "Y . Liu, G. Deng, Z. Xu, Y . Li, Y . Zheng, Y . Zhang, L. Zhao, T. Zhang, K. Wang, and Y . Liu, “Jailbreaking chatgpt via prompt engineering: An empirical study,”",
            "E. Bagdasaryan, T.-Y . Hsieh, B. Nassi, and V . Shmatikov, “Abusing images and sounds for indirect instruction injection in multi-modal llms,”",
            "G. Alon and M. Kamfonas, “Detecting language model attacks with perplexity,”",
            "S. Abdelnabi, N. Carlini, E. Debenedetti, M. Fritz, K. Greshake, R. Hadzic, T. Holz, D. P. Daphne Ippolito, J. Rando, L. Sch ¨onherr, F. Tram `er, and Y . Zhang. (2024) Large language model capture-the-flag (llm ctf) competition @ satml 2024. Spylab.",
            "Y . Liu, Y . Jia, R. Geng, J. Jia, and N. Z. Gong, “Prompt injection attacks and defenses in llm-integrated applications,”",
            "J. Yu, X. Lin, and X. Xing, “Gptfuzzer: Red teaming large language models with auto-generated jailbreak prompts,”",
            "A. Zou, Z. Wang, J. Z. Kolter, and M. Fredrikson, “Universal and transferable adversarial attacks on aligned language models,”",
            "Y . Bai, A. Jones, K. Ndousse, A. Askell, A. Chen, N. DasSarma, D. Drain, S. Fort, D. Ganguli, T. Henighan et al. , “Training a helpful and harmless assistant with reinforcement learning from human feedback,”",
            "Y . Chen, E. Mendes, S. Das, W. Xu, and A. Ritter, “Can language models be instructed to protect personal information?”",
            "S. Kim, S. Yun, H. Lee, M. Gubri, S. Yoon, and S. J. Oh, “Propile: Probing privacy leakage in large language models,”",
            "N. Carlini, J. Hayes, M. Nasr, M. Jagielski, V . Sehwag, F. Tramer, B. Balle, D. Ippolito, and E. Wallace, “Extracting training data from diffusion models,”"
        ]
    },
    {
        "index": 203,
        "title": "RAID: A Shared Benchmark for Robust Evaluation of Machine-Generated Text Detectors",
        "publication_date": "2024-06-10",
        "references": [
            "Aaditya Bhat. 2023. Gpt-wiki-intro.",
            "Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra Cojocaru, Mérouane Debbah, Étienne Goffinet, Daniel Hesslow, Julien Launay, Quentin Malartic, Daniele Mazzotta, Badreddine Noune, Baptiste Pannier, and Guilherme Penedo. 2023. The falcon series of open language models.",
            "Micael Arman. 2020. Poems dataset (nlp). https://www.kaggle.com/datasets/michaelarman/poemsdataset.",
            "Shahryar Baki, Rakesh Verma, Arjun Mukherjee, and Omprakash Gnawali. 2017. Scaling and effectiveness of email masquerade attacks: Exploiting natural language generation. In Proceedings of the 2017 ACM on Asia Conference on Computer and Communications Security, ASIA CCS '17, page 469–482, New York, NY, USA. Association for Computing Machinery.",
            "David Bamman and Noah A. Smith. 2013. New alignment methods for discriminative book summarization.",
            "Guangsheng Bao, Yanbin Zhao, Zhiyang Teng, Linyi Yang, and Yue Zhang. 2023. Fast-detectgpt: Efficient zero-shot detection of machine-generated text via conditional probability curvature.",
            "Meghana Moorthy Bhat and Srinivasan Parthasarathy. 2020. How effectively can machines defend against machine-generated fake news? an empirical study. In Proceedings of the First Workshop on Insights from Negative Results in NLP, pages 48–53, Online. Association for Computational Linguistics.",
            "Michał Bień, Michał Gilski, Martyna Maciejewska, Wojciech Taisner, Dawid Wiśniewski, and Agnieszka Lawrynowicz. 2020. RecipeNLG: A cooking recipes dataset for semi-structured text generation. In Proceedings of the 13th International Conference on Natural Language Generation, pages 22–28, Dublin, Ireland. Association for Computational Linguistics.",
            "Steven Bird and Edward Loper. 2004. NLTK: The natural language toolkit. In Proceedings of the ACL Interactive Poster and Demonstration Sessions, pages 214–217, Barcelona, Spain. Association for Computational Linguistics.",
            "Matyáš Boháček, Michal Bravanský, Filip Trhlík, and Václav Moravec. 2022. Fine-grained czech news article dataset: An interdisciplinary approach to trustworthiness analysis.",
            "Piotr Bojanowski, Edouard Grave, Armand Joulin, and Tomas Mikolov. 2017. Enriching word vectors with subword information. Transactions of the Association for Computational Linguistics, 5:135–146.",
            "Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners.",
            "Shuyang Cai and Wanyun Cui. 2023. Evade chatgpt detectors via a single space.",
            "Megha Chakraborty, S.M Towhidul Islam Tonmoy, S M Mehedi Zaman, Shreya Gautam, Tanay Kumar, Krish Sharma, Niyar Barman, Chandan Gupta, Vinija Jain, Aman Chadha, Amit Sheth, and Amitava Das. 2023. Counter Turing test (CT2): AI-generated text detection is not as easy as you may think - introducing AI detectability index (ADI). In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 2206–2239, Singapore. Association for Computational Linguistics.",
            "Elizabeth Clark, Tal August, Sofia Serrano, Nikita Haduong, Suchin Gururangan, and Noah A. Smith. 2021. All that’s ‘human’ is not gold: Evaluating human evaluation of generated text. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 7282–7296, Online. Association for Computational Linguistics.",
            "NLP Team Cohere. 2024. World-class ai, at your command. Accessed: 2024-02-02.",
            "Evan N. Crothers, Nathalie Japkowicz, and Herna L. Viktor. 2023. Machine-generated text: A comprehensive survey of threat models and detection methods. IEEE Access, 11:70977–71002.",
            "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171–4186, Minneapolis, Minnesota. Association for Computational Linguistics.",
            "Liam Dugan, Daphne Ippolito, Arun Kirubarajan, and Chris Callison-Burch. 2020. RoFT: A tool for evaluating human detection of machine-generated text. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 189–196, Online. Association for Computational Linguistics.",
            "Liam Dugan, Daphne Ippolito, Arun Kirubarajan, Sherry Shi, and Chris Callison-Burch. 2023. Real or fake text? investigating human ability to detect boundaries between human-written and machine-generated text. In Proceedings of the Thirty-Seventh AAAI Conference on Artificial Intelligence and Thirty-Fifth Conference on Innovative Applications of Artificial Intelligence and Thirteenth Symposium on Educational Advances in Artificial Intelligence, AAAI’23/IAAI’23/EAAI’23. AAAI Press.",
            "Salijona Dyrmishi, Salah Ghamizi, and Maxime Cordy. 2023. How do humans perceive adversarial text? a reality check on the validity and naturalness of word-based adversarial attacks. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 8822–8836, Toronto, Canada. Association for Computational Linguistics.",
            "Vitalii Fishchuk and Daniel Braun. 2023. Efficient black-box adversarial attacks on neural text detectors.",
            "Rinaldo Gagiano, Maria Myung-Hee Kim, Xiuzhen Zhang, and Jennifer Biggs. 2021. Robustness analysis of grover for machine-generated news detection. In Proceedings of the The 19th Annual Workshop of the Australasian Language Technology Association, pages 119–127, Online. Australasian Language Technology Association.",
            "Chujie Gao, Dongping Chen, Qihui Zhang, Yue Huang, Yao Wan, and Lichao Sun. 2024. Llm-as-a-coauthor: The challenges of detecting llm-human mixcase.",
            "Ji Gao, Jack Lanchantin, Mary Lou Soffa, and Yanjun Qi. 2018. Black-box generation of adversarial text sequences to evade deep learning classifiers.",
            "Sebastian Gehrmann, Hendrik Strobelt, and Alexander Rush. 2019. GLTR: Statistical detection and visualization of generated text. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 111–116, Florence, Italy. Association for Computational Linguistics.",
            "Derek Greene and Pádraig Cunningham. 2006. Practical solutions to the problem of diagonal dominance in kernel document clustering. In Proc. 23rd International Conference on Machine learning (ICML’06), pages 377–384. ACM Press.",
            "Jesus Guerrero, Gongbo Liang, and Izzat Alsmadi. 2022. A mutation-based text generation for adversarial machine learning applications.",
            "Biyang Guo, Xin Zhang, Ziyuan Wang, Minqi Jiang, Jinran Nie, Yuxuan Ding, Jianwei Yue, and Yupeng Wu. 2023. How close is chatgpt to human experts? comparison corpus, evaluation, and detection.",
            "Abhimanyu Hans, Avi Schwarzschild, Valeriia Cherepanova, Hamid Kazemi, Aniruddha Saha, Micah Goldblum, Jonas Geiping, and Tom Goldstein. 2024. Spotting llms with binoculars: Zero-shot detection of machine-generated text.",
            "Julian Hazell. 2023. Large language models can be used to effectively scale spear phishing campaigns. arXiv preprint arXiv:2305.06972.",
            "Xinlei He, Xinyue Shen, Zeyuan Chen, Michael Backes, and Yang Zhang. 2023. Mgtbench: Benchmarking machine-generated text detection.",
            "John Hewitt, Christopher Manning, and Percy Liang. 2022. Truncation sampling as language model desmoothing. In Findings of the Association for Computational Linguistics: EMNLP 2022, pages 3414–3427, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.",
            "Xiaomeng Hu, Pin-Yu Chen, and Tsung-Yi Ho. 2023. Radar: Robust ai-text detection via adversarial learning. Advances in Neural Information Processing Systems.",
            "Daphne Ippolito, Daniel Duckworth, Chris Callison-Burch, and Douglas Eck. 2020. Automatic detection of generated text is easiest when humans are fooled. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1808–1822, Online. Association for Computational Linguistics.",
            "Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillame Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. 2023. Mistral 7b.",
            "Nitish Shirish Keskar, Bryan McCann, Lav R. Varshney, Caiming Xiong, and Richard Socher. 2019. Ctrl: A conditional transformer language model for controllable generation.",
            "Tibor Kiss and Jan Strunk. 2006. Unsupervised multilingual sentence boundary detection. Computational Linguistics, 32(4):485–525.",
            "Denis Kocetkov, Raymond Li, Loubna Ben Allal, Jia Li, Chenghao Mou, Carlos Muñoz Ferrandis, Yacine Jernite, Margaret Mitchell, Sean Hughes, Thomas Wolf, Dzmitry Bahdanau, Leandro von Werra, and Harm de Vries. 2022. The stack: 3 tb of permissively licensed source code. Preprint.",
            "Ryuto Koike, Masahiro Kaneko, and Naoaki Okazaki. 2023. How you prompt matters! even task-oriented constraints in instructions affect llm-generated text detection.",
            "Kalpesh Krishna, Yixiao Song, Marzena Karpinska, John Wieting, and Mohit Iyyer. 2023. Paraphrasing evades detectors of ai-generated text, but retrieval is an effective defense.",
            "Pranav Kulkarni, Ziqing Ji, Yan Xu, Marko Neskovic, and Kevin Nolan. 2023. Exploring semantic perturbations on grover.",
            "Tharindu Kumarage, Paras Sheth, Raha Moraffah, Joshua Garland, and Huan Liu. 2023. How reliable are ai-generated-text detectors? an assessment framework using evasive soft prompts.",
            "Katherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck, Chris Callison-Burch, and Nicholas Carlini. 2022. Deduplicating training data makes language models better. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 8424–8445, Dublin, Ireland. Association for Computational Linguistics.",
            "Xiang Lisa Li, Ari Holtzman, Daniel Fried, Percy Liang, Jason Eisner, Tatsunori Hashimoto, Luke Zettlemoyer, and Mike Lewis. 2023. Contrastive decoding: Open-ended text generation as optimization. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 12286–12312, Toronto, Canada. Association for Computational Linguistics.",
            "Yafu Li, Qintong Li, Leyang Cui, Wei Bi, Zhilin Wang, Longyue Wang, Linyi Yang, Shuming Shi, and Yue Zhang. 2024. Mage: Machine-generated text detection in the wild.",
            "Gongbo Liang, Jesus Guerrero, and Izzat Alsmadi. 2023a. Mutation-based adversarial attacks on neural text detectors.",
            "Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, Benjamin Newman, Binhang Yuan, Bobby Yan, Ce Zhang, Christian Cosgrove, Christopher D. Manning, Christopher Ré, Diana Acosta-Navas, Drew A. Hudson, Eric Zelikman, Esin Durmus, Faisal Ladhab, Frieda Rong, Hongyu Ren, Huaxiu Yao, Jue Wang, Keshav Santhanam, Laurel Orr, Lucia Zheng, Mert Yuksekgonul, Mirac Suzgun, Nathan Kim, Neel Guha, Niladri Chatterji, Omar Khattab, Peter Henderson, Qian Huang, Ryan Chi, Sang Michael Xie, Shibani Santurkar, Surya Ganguli, Tatsunori Hashimoto, Thomas Icard, Tianyi Zhang, Vishrav Chaudhary, William Wang, Xuechen Li, Yifan Mai, Yuhui Zhang, and Yuta Koreeda. 2023b. Holistic evaluation of language models.",
            "Aaditya Bhat. 2023. Gpt-wiki-intro.",
            "Weixin Liang, Mert Yuksekgonul, Yining Mao, Eric Wu, and James Zou. 2023c. Gpt detectors are biased against non-native english writers.",
            "Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining approach.",
            "Ning Lu, Shengcai Liu, Rui He, Qi Wang, Yew-Soon Ong, and Ke Tang. 2023. Large language models can be guided to evade ai-generated text detection.",
            "Brady D Lund, Ting Wang, Nishith Reddy Mannuru, Bing Nie, Somipam Shimray, and Ziang Wang. 2023. Chatgpt and a new academic reality: Artificial intelligence-written research papers and the ethics of the large language models in scholarly publishing. Journal of the Association for Information Science and Technology, 74(5):570–581.",
            "Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. 2011. Learning word vectors for sentiment analysis. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 142–150, Portland, Oregon, USA. Association for Computational Linguistics.",
            "Dominik Macko, Robert Moro, Adaku Uchendu, Jason Lucas, Michiharu Yamashita, Matúš Pikuliak, Ivan Srba, Thai Le, Dongwon Lee, Jakub Simko, and Maria Bielikova. 2023. MULTITuDE: Large-scale multilingual machine-generated text detection benchmark. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 9960–9987, Singapore. Association for Computational Linguistics.",
            "Dominik Macko, Robert Moro, Adaku Uchendu, Ivan Srba, Jason Samuel Lucas, Michiharu Yamashita, Nafis Irtiza Tripto, Dongwon Lee, Jakub Simko, and Maria Bielikova. 2024. Authorship obfuscation in multilingual machine-generated text detection.",
            "Clara Meister, Tiago Pimentel, Gian Wiher, and Ryan Cotterell. 2023. Locally typical sampling. Transactions of the Association for Computational Linguistics, 11:102–121.",
            "Eric Mitchell, Yoonho Lee, Alexander Khazatsky, Christopher D. Manning, and Chelsea Finn. 2023. Detectgpt: zero-shot machine-generated text detection using probability curvature. In Proceedings of the 40th International Conference on Machine Learning, ICML’23. JMLR.org.",
            "NLP Team MosaicML. 2023. Introducing mpt-30b: Raising the bar for open-source foundation models. Accessed: 2023-06-22.",
            "Edoardo Mosca, Mohamed Hesham Ibrahim Abdalla, Paolo Basso, Margherita Musumeci, and Georg Groh. 2023. Distinguishing fact from fiction: A benchmark dataset for identifying machine-generated scientific papers in the LLM era. In Proceedings of the 3rd Workshop on Trustworthy Natural Language Processing (TrustNLP 2023), pages 190–207, Toronto, Canada. Association for Computational Linguistics.",
            "OpenAI. 2022. ChatGPT: Optimizing Language Models for Dialogue.",
            "OpenAI. 2023. Gpt-4 technical report.",
            "Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35:27730–27744.",
            "Artidoro Pagnoni, Martin Graciarena, and Yulia Tsvetkov. 2022. Threat scenarios and best practices to detect neural fake news. In Proceedings of the 29th International Conference on Computational Linguistics, pages 1233–1249, Gyeongju, Republic of Korea. International Committee on Computational Linguistics.",
            "Sayak Paul and Soumik Rakshit. 2021. arxiv paper abstracts. https://www.kaggle.com/datasets/spsayakpaul/arxiv-paper-abstracts.",
            "Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay. 2023. The refinedweb dataset for falcon llm: Outperforming curated corpora with web data, and web data only.",
            "J. Pu, Z. Sarwar, S. Abdullah, A. Rehman, Y. Kim, P. Bhattacharya, M. Javed, and B. Viswanath. 2023a. Deepfake text detection: Limitations and opportunities. In 2023 IEEE Symposium on Security and Privacy (SP), pages 1613–1630, Los Alamitos, CA, USA. IEEE Computer Society.",
            "Xiao Pu, Jingyu Zhang, Xiaochuang Han, Yulia Tsvetkov, and Tianxing He. 2023b. On the zero-shot generalization of machine-generated text detectors.",
            "Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners.",
            "Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. The Journal of Machine Learning Research, 21(1):5485–5551.",
            "Veselin Raychev, Pavol Bielik, and Martin Vechev. 2016. Probabilistic model for code with decision trees. SIGPLAN Not., 51(10):731–747.",
            "Juan Diego Rodriguez, Todd Hay, David Gros, Zain Shamsi, and Ravi Srinivasan. 2022. Cross-domain detection of GPT-2-generated technical text. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1213–1233, Seattle, United States. Association for Computational Linguistics.",
            "Vinu Sankar Sadasivan, Aounon Kumar, Sriram Balasubramanian, Wenxiao Wang, and Soheil Feizi. 2023. Can ai-generated text be reliably detected?",
            "Marc Franco-Salvador, Francisco Rangel, Berta Chulvi, and Paolo Rosso. 2023a. Supervised machine-generated text detectors: Family and scale matters.",
            "Areg Mikael Sarvazyan, José Ángel González, Marc Franco-Salvador, Francisco Rangel, Berta Chulvi, and Paolo Rosso. 2023b. Overview of autextification at iberlef 2023: Detection and attribution of machine-generated text in multiple domains.",
            "Dietmar Schabus, Marcin Skowron, and Martin Trapp. 2017. One million posts: A data set of german online discussions. In Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR ’17, page 1241–1244, New York, NY, USA. Association for Computing Machinery.",
            "Tal Schuster, Roei Schuster, Darsh J. Shah, and Regina Barzilay. 2020. The limitations of stylometry for detecting machine-generated fake news. Computational Linguistics, 46(2):499–510.",
            "Tatiana Shamardina, Vladislav Mikhailov, Daniil Chernianskii, Alena Fenogenova, Marat Saidov, Anastasiya Valeeva, Tatiana Shavrina, Ivan Smurov, Elena Tutubalina, and Ekaterina Artemova. 2022. Findings of the the ruatd shared task 2022 on artificial text detection in russian. In Computational Linguistics and Intellectual Technologies. RSUH.",
            "Filipo Sharevski, Jennifer Vander Loop, Peter Jachim, Amy Devine, and Emma Pieroni. 2023. Talking abortion (mis)information with chatgpt on tiktok. In 2023 IEEE European Symposium on Security and Privacy Workshops (EuroS&PW), pages 594–608. IEEE.",
            "Irene Solaiman, Miles Brundage, Jack Clark, Amanda Askell, Ariel Herbert-Voss, Jeff Wu, Alec Radford, Gretchen Krueger, Jong Wook Kim, Sarah Kreps, Miles McCain, Alex Newhouse, Jason Blazakis, Kris McGuffie, and Jasmine Wang. 2019. Release strategies and the social impacts of language models.",
            "Rafael Rivera Soto, Kailin Koch, Aleem Khan, Barry Chen, Marcus Bishop, and Nicholas Andrews. 2024. Few-shot detection of machine-generated text using style representations.",
            "Giovanni Spitale, Nikola Biller-Andorno, and Federico Germani. 2023. Ai model gpt-3 (dis)informs us better than humans. Science Advances, 9(26):eadh1850.",
            "Harald Stiff and Fredrik Johansson. 2022. Detecting computer-generated disinformation. International Journal of Data Science and Analytics, 13:363–383.",
            "Zhenpeng Su, Xing Wu, Wei Zhou, Guangyuan Ma, and Songlin Hu. 2024. Hc3 plus: A semantic-invariant human chatgpt comparison corpus.",
            "Edward Tian and Alexander Cui. 2023. Gptzero: Towards detection of ai-generated text using zero-shot and supervised methods.",
            "Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Marinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. 2023. Llama 2: Open foundation and fine-tuned chat models."
        ]
    },
    {
        "index": 204,
        "title": "RAIDAR: GENERATIVE AI DETECTION VIA REWRITING",
        "publication_date": "2024-04-14",
        "references": [
            "Chatgpt: Optimizing language models for dialogue, 2023",
            "Anthropic, 2023",
            "Harnessing large language models to simulate realistic human responses to social engineering attacks: A case study",
            "Real or fake? learning to discriminate machine from human generated text",
            "Guiding the release of safer e2e conversational ai through value sensitive design",
            "Application of the logistic function to bio-assay",
            "Language models are few-shot learners",
            "On the possibilities of ai-generated text detection",
            "GLTR: Statistical detection and visualization of generated text",
            "Automatic detection of generated text is easiest when humans are fooled",
            "Automatic detection of machine generated text: A critical survey",
            "Is bert really robust? natural language attack on text classification and entailment",
            "Exploiting programmatic behavior of llms: Dual-use through standard security attacks",
            "A watermark for large language models",
            "Large language models are zero-shot reasoners",
            "Binary codes capable of correcting deletions, insertions, and reversals",
            "Pretrained language models for text generation: A survey",
            "Prefix-tuning: Optimizing continuous prompts for generation",
            "GPT detectors are biased against non-native english writers",
            "Smaller language models are better black-box machine-generated text detectors",
            "The threat of offensive ai to organizations",
            "Detectgpt: Zero-shot machine-generated text detection using probability curvature",
            "On the risk of misinformation pollution with large language models",
            "Asleep at the keyboard? assessing the security of github copilot’s code contributions",
            "Language models are unsupervised multitask learners",
            "Robust speech recognition via large-scale weak supervision",
            "Can ai-generated text be reliably detected?",
            "The curse of recursion: Training on generated data makes models forget",
            "Release strategies and the social impacts of language models",
            "Opt: Open pre-trained transformer language models",
            "Recurrentgpt: Interactive generation of (arbitrarily) long text",
            "Large language models are human-level prompt engineers",
            "Universal and transferable adversarial attacks on aligned language models"
        ]
    },
    {
        "index": 205,
        "title": "RAPID RESPONSE : MITIGATING LLM JAILBREAKS WITH A FEW EXAMPLES",
        "publication_date": "2024-11-12",
        "references": [
            "Many-shot jailbreaking",
            "Anthropic's responsible scaling policy",
            "Expanding our model safety bug bounty program — anthropic.com",
            "Training a helpful and harmless assistant with reinforcement learning from human feedback",
            "Constitutional ai: Harmlessness from ai feedback",
            "Defending against alignment-breaking attacks via robustly aligned llm",
            "Some lessons from adversarial machine learning",
            "Jailbreaking black box large language models in twenty queries",
            "Jailbreakbench: An open robustness benchmark for jailbreaking large language models",
            "Multilingual jailbreak challenges in large language models",
            "A wolf in sheep’s clothing: Generalized nested jailbreak prompts can fool large language models easily",
            "The llama 3 herd of models",
            "Jailbreaking proprietary large language models using word substitution cipher",
            "Detoxify",
            "Unsolved problems in ml safety",
            "Curiosity-driven red-teaming for large language models",
            "Obscureprompt: Jailbreaking large language models via obscure input",
            "Llama guard: Llm-based input-output safeguard for human-ai conversations",
            "Baseline defenses for adversarial attacks against aligned language models",
            "Mistral 7b",
            "Holistic evaluation of language models",
            "Adversarial training for large neural language models",
            "Autodan: Generating stealthy jailbreak prompts on aligned large language models",
            "Meta llama guard 2",
            "A holistic approach to undesired content detection in the real world",
            "Harmbench: A standardized evaluation framework for automated red teaming and robust refusal",
            "Tree of attacks: Jailbreaking black-box llms automatically",
            "Introducing chatgpt",
            "Openai preparedness framework (beta)",
            "Gpt-4o system card",
            "Training language models to follow instructions with human feedback",
            "Red teaming language models with language models",
            "Eliciting language model behaviors using reverse language models",
            "Smoothllm: Defending large language models against jailbreaking attacks",
            "Mitigating skeleton key, a new type of generative ai jailbreak technique",
            "Great, now write an article about that: The crescendo multi-turn llm jailbreak attack",
            "Rainbow teaming: Open-ended generation of diverse adversarial prompts",
            "A survey on image data augmentation for deep learning",
            "Gemini: a family of highly capable multimodal models",
            "On adaptive attacks to adversarial example defenses",
            "Jailbroken: How does llm safety training fail?",
            "Eda: Easy data augmentation techniques for boosting performance on text classification tasks",
            "Chain-of-thought prompting elicits reasoning in large language models",
            "Jailbreak and guard aligned language models with only few in-context demonstrations",
            "Defending chatgpt against jailbreak attack via self-reminders",
            "Safedecoding: Defending against jailbreak attacks via safety-aware decoding",
            "Towards improving adversarial training of nlp models",
            "Gptfuzzer: Red teaming large language models with auto-generated jailbreak prompts",
            "GPT-4 is too smart to be safe: Stealthy chat with LLMs via cipher",
            "How johnny can persuade llms to jailbreak them: Rethinking persuasion to challenge ai safety by humanizing llms",
            "Wildchat: 1m chatgpt interaction logs in the wild",
            "Autodan: Automatic and interpretable adversarial attacks on large language models",
            "Adversarial training for high-stakes reliability",
            "Universal and transferable adversarial attacks on aligned language models",
            "Improving alignment and robustness with short circuiting",
            "Easyjailbreak: A unified framework for jailbreaking large language models"
        ]
    },
    {
        "index": 206,
        "title": "Read Over the Lines: Attacking LLMs and Toxicity Detection Systems with ASCII Art to Mask Profanity",
        "publication_date": "2024-10-09",
        "references": [
            "Bad characters: Imperceptible nlp attacks",
            "Not-in-perspective: Towards shielding google’s perspective api against adversarial negation attacks",
            "No offence, bert - I insult only humans! multilingual sentence-level attack on toxicity detection networks",
            "Cyberpl@y: Communicating Online",
            "Perspective api documentation",
            "Tesseract ocr",
            "Easyocr: Ready-to-use ocr with 80+ supported languages",
            "Art: Ascii art generator library for python",
            "Detoxify",
            "Studies in perception i",
            "Scikit-learn: Machine learning in Python",
            "Shielding google’s language toxicity model against adversarial attacks",
            "Typewriter art by flora f.f. stacey",
            "The brooklyn daily eagle",
            "unslothai: Thai natural language processing library",
            "A systematic review of toxicity in large language models: Definitions, datasets, detectors, detoxification methods and challenges",
            "Mttm: Metamorphic testing for textual content moderation software",
            "Mttm: Metamorphic testing for textual content moderation software",
            "Mttm: Metamorphic testing for textual content moderation software",
            "Trojaning language models for fun and profit",
            "Virtual context: Enhancing jailbreak attacks with special token injection"
        ]
    },
    {
        "index": 207,
        "title": "REDQUEEN : Safeguarding Large Language Models against Concealed Multi-Turn Jailbreaking",
        "publication_date": "2024-09-26",
        "references": [
            "Gpt-4 technical report",
            "Jailbroken: How does llm safety training fail?",
            "Claude 3",
            "Training a helpful and harmless assistant with reinforcement learning from human feedback",
            "Many-shot jailbreaking",
            "Llama: Open and efficient foundation language models",
            "A comprehensive study of jailbreak attack versus defense for large language models",
            "Cosafe: Evaluating large language model safety in multi-turn dialogue coreference",
            "Roformer: Enhanced transformer with rotary position embedding",
            "Truthfulqa: Measuring how models mimic human falsehoods",
            "Do-not-answer: A dataset for evaluating safeguards in llms",
            "Training language models to follow instructions with human feedback",
            "Mmlu-pro: A more robust and challenging multi-task language understanding benchmark",
            "Beyond the imitation game: Quantifying and extrapolatingthe capabilities of language models",
            "Images are achilles’ heel of alignment: Exploiting visual vulnerabilities for jailbreaking multimodal large language models",
            "A survey of large language models",
            "Exploiting programmatic behavior of llms: Dual-use through standard security attacks",
            "Hallucination detection: Robustly discerning reliable answers in large language models",
            "Direct preference optimization: Your language model is secretly a reward model",
            "Against the achilles’ heel: A survey on red teaming for generative models",
            "Survey of hallucination in natural language generation",
            "Analyzing the inherent response tendency of llms: Real-world instructions-driven jailbreak",
            "Jailbreaking black box large language models in twenty queries",
            "Measuring massive multitask language understanding",
            "Understanding the planning of llm agents: A survey",
            "Training verifiers to solve math word problems",
            "A general language assistant as a laboratory for alignment",
            "A study of zero-shot adaptation with commonsense knowledge",
            "A more robust and challenging multi-task language understanding benchmark",
            "Alpacafarm: A simulation framework for methods that learn from human feedback",
            "Human alignment",
            "Exploring perceptual limitation of multimodal large language models"
        ]
    },
    {
        "index": 208,
        "title": "REDTEAMING GPT-4V: A REGPT-4V SAFE AGAINST UNI/MULTI-MODAL JAILBREAK ATTACKS?",
        "publication_date": "2024-12-15",
        "references": [
            "Gpt-4 technical report",
            "Fuyu-8b model card",
            "Qwen technical report",
            "Image hijacks: Adversarial images can control generative models at runtime",
            "Are aligned neural networks adversarially aligned?",
            "Jailbreaking black box large language models in twenty queries",
            "Guanaco - generative universal assistant for natural-language adaptive context-aware omnilingual outputs",
            "Vi-cuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality",
            "Jailbreaker: Automated jailbreak across multiple large language model chatbots",
            "GlM: General language model pretraining with autoregressive blank infilling",
            "Figstep: Jailbreaking large vision-language models via typographic visual prompts",
            "Llama guard: Llm-based input-output safeguard for human-ai conversations",
            "Automatically auditing large language models via discrete optimization",
            "Open sesame! universal black box jailbreaking of large language models",
            "Toxicchat: Unveiling hidden challenges of toxicity detection in real-world user-AI conversation",
            "Improved baselines with visual instruction tuning",
            "Autodan: Generating stealthy jailbreak prompts on aligned large language models",
            "Jailbreaking chatgpt via prompt engineering: An empirical study",
            "Training language models to follow instructions with human feedback",
            "Visual adversarial examples jailbreak aligned large language models",
            "Scalable and transferable black-box jailbreaks for language models via persona modulation",
            "Jailbreak in pieces: Compositional adversarial attacks on multi-modal language models",
            "‘do anything now’: Characterizing and evaluating in-the-wild jailbreak prompts on large language models",
            "Low-resource languages jailbreak gpt-4",
            "Gptfuzzer: Red teaming large language models with auto-generated jailbreak prompts",
            "Gpt-4 is too smart to be safe: Stealthy chat with llms via cipher",
            "Judging llm-as-a-judge with mt-bench and chatbot arena"
        ]
    },
    {
        "index": 211,
        "title": "RePD: Defending Jailbreak Attack through a Retrieval-based Prompt Decomposition Process",
        "publication_date": "2024-11-29",
        "references": [
            "Gpt-4 technical report",
            "Detecting language model attacks with perplexity",
            "Defending against alignment-breaking attacks via robustly aligned llm",
            "Jailbreaking black box large language models in twenty queries",
            "Attack prompt generation for red teaming and defending large language models",
            "Multilingual jailbreak challenges in large language models",
            "Anticipating safety issues in e2e conversational ai: Framework and tooling",
            "The capacity for moral self-correction in large language models",
            "Llm self defense: By self examination, llms know they are being tricked",
            "Llama guard: Llm-based input-output safeguard for human-ai conversations",
            "Baseline defenses for adversarial attacks against aligned language models",
            "The impact of reasoning step length on large language models",
            "Salad-bench: A hierarchical and comprehensive safety benchmark for large language models",
            "Toxicchat: Unveiling hidden challenges of toxicity detection in real-world user-ai conversation",
            "Autodan: Generating stealthy jailbreak prompts on aligned large language models",
            "Jailbreaking chatgpt via prompt engineering: An empirical study",
            "Prompt injection attacks and defenses in llm-integrated applications",
            "Tree of attacks: Jailbreaking black-box llms automatically",
            "Training language models to follow instructions with human feedback",
            "Bergeron: Combating adversarial attacks through a conscience-based alignment framework",
            "Fine-tuning aligned language models compromises safety, even when users do not intend to!",
            "Smoothllm: Defending large language models against jailbreaking attacks",
            "\" do anything now\": Characterizing and evaluating in-the-wild jailbreak prompts on large language models",
            "Llama 2: Open foundation and fine-tuned chat models",
            "Aligning large language models with human: A survey",
            "Jailbroken: How does llm safety training fail?",
            "Jailbreak and guard aligned language models with only few in-context demonstrations",
            "Auto-gen: Enabling next-gen llm applications via multi-agent conversation framework",
            "Low-resource languages jailbreak gpt-4",
            "Gpt-4 is too smart to be safe: Stealthy chat with llms via cipher",
            "Autodefense: Multi-agent llm defense against jailbreak attacks",
            "Intention analysis prompting makes large language models a good jailbreak defender",
            "Defending large language models against jail-breaking attacks through goal prioritization",
            "Judging llm-as-a-judge with mt-bench and chatbot arena",
            "Universal and transferable adversarial attacks on aligned language models"
        ]
    },
    {
        "index": 212,
        "title": "Rethinking How to Evaluate Language Model Jailbreak",
        "publication_date": "2024-05-07",
        "references": [
            "doccano: Text annotation tool for human.",
            "Generative ai prohibited use policy.",
            "How to use chatgpt to summarize an article.",
            "An overview of bard: an early experiment with generative ai.",
            "Universe 2023: Copilot transforms github into the ai-powered developer platform.",
            "Gemma: Introducing new state-of-the-art open models.",
            "Introducing meta llama 3: The most capable openly available llm to date.",
            "Usage policies — openai.com.",
            "Usenix submission replication.",
            "Constitutional ai: Harmlessness from ai feedback.",
            "METEOR: An automatic metric for MT evaluation with improved correlation with human judgments.",
            "Large language model-based chatbot as a source of advice on first aid in heart attack.",
            "Language models are few-shot learners.",
            "Jailbreaking black box large language models in twenty queries.",
            "Knowledge graphs meet multi-modal learning: A comprehensive survey.",
            "Deep reinforcement learning from human preferences.",
            "Bert: Pre-training of deep bidirectional transformers for language understanding.",
            "Gptscore: Evaluate as you desire.",
            "Catastrophic jailbreak of open-source llms via exploiting generation.",
            "Aot - attack on things: A security analysis of iot firmware updates.",
            "Safetynot: on the usage of the safetynet attestation api in android.",
            "Improving language understanding by generative pre-training.",
            "Reliability in content analysis: Some common misconceptions and recommendations.",
            "ROUGE: A package for automatic evaluation of summaries.",
            "Truthfulqa: Measuring how models mimic human falsehoods.",
            "Pinsql: Pinpoint root cause sqls to resolve performance issues in cloud databases.",
            "Jailbreaking chatgpt via prompt engineering: An empirical study.",
            "Towards an automatic Turing test: Learning to evaluate dialogue responses.",
            "Harmbench: A standardized evaluation framework for automated red teaming and robust refusal.",
            "Training language models to follow instructions with human feedback.",
            "Training language models to follow instructions with human feedback.",
            "Bleu: a method for automatic evaluation of machine translation.",
            "Fine-tuning aligned language models compromises safety, even when users do not intend to!",
            "Nemo guardrails: A toolkit for controllable and safe llm applications with programmable rails.",
            "Code llama: Open foundation models for code.",
            "Evaluation of chatgpt’s responses to information needs and information seeking of dementia patients.",
            "BLEURT: Learning robust metrics for text generation.",
            "Attackeval: How to evaluate the effectiveness of jailbreak attacking on large language models.",
            "Llama 2: Open foundation and fine-tuned chat models.",
            "Llama 2: Open foundation and fine-tuned chat models.",
            "Concealed data poisoning attacks on nlp models.",
            "Poisoning language models during instruction tuning.",
            "Survey on factuality in large language models: Knowledge, retrieval and domain-specificity.",
            "Real-time workload pattern analysis for large-scale cloud databases.",
            "Jailbroken: How does LLM safety training fail?",
            "Mmmu: A massive multi-disciplinary multimodal understanding and reasoning benchmark for expert agi.",
            "TrojanSQL: SQL injection against natural language interface to database.",
            "Bertscore: Evaluating text generation with bert.",
            "Judging LLM-as-a-judge with MT-bench and chatbot arena.",
            "Fine-tuning language models from human preferences.",
            "Universal and transferable adversarial attacks on aligned language models."
        ]
    },
    {
        "index": 214,
        "title": "Robustifying Safety-Aligned Large Language Models through Clean Data Curation",
        "publication_date": "2024-05-31",
        "references": [
            "Fine-tuning – openai api",
            "Llama 3 model card",
            "Overview of responsible ai practices for azure openai models",
            "Why information security is hard-an economic perspective",
            "Jailbreaking leading safety-aligned llms with simple adaptive attacks",
            "A general language assistant as a laboratory for alignment",
            "Leveraging large language models for decision support in personalized oncology",
            "On the dangers of stochastic parrots: Can language models be too big?",
            "A survey of longest common subsequence algorithms",
            "Jailbreaking black box large language models in twenty queries",
            "Evaluating large language models trained on code",
            "Free dolly: Introducing the world’s first truly open instruction-tuned llm",
            "An educational program on data curation",
            "Safe rlhf: Safe reinforcement learning from human feedback",
            "Llms to the moon? reddit market sentiment analysis with large language models",
            "Documenting large webtext corpora: A case study on the colossal clean crawled corpus",
            "Glm: General language model pretraining with autoregressive blank infilling",
            "Data poisoning attacks and defenses to crowdsourcing systems",
            "What’s in the box? exploring the inner life of neural networks with robust rules",
            "Artificial intelligence, values, and alignment",
            "Realtoxicityprompts: Evaluating neural toxic degeneration in language models",
            "The economics of information security investment",
            "Toolkengpt: Augmenting frozen language models with massive tools via tool embeddings",
            "What’s in your\" safe\" data?: Identifying benign data that breaks safety",
            "Social biases in nlp models as barriers for persons with disabilities",
            "Beavertails: Towards improved safety alignment of llm via a human-preference dataset",
            "Inferfix: End-to-end program repair with llms",
            "Improving activation steering in language models with mean-centring",
            "Openassistant conversations-democratizing large language model alignment",
            "Self-alignment with instruction backtranslation",
            "TruthfulQA: Measuring how models mimic human falsehoods",
            "Llm-pruner: On the structural pruning of large language models",
            "Universal multi-party poisoning attacks",
            "Dataperf: Benchmarks for data-centric AI development",
            "Training language models to follow instructions with human feedback",
            "Hidden trigger backdoor attack on {NLP}models via linguistic style manipulation",
            "Examining zero-shot vulnerability repair with large language models",
            "Mind the style of text! adversarial and backdoor attacks based on text style transfer",
            "Fine-tuning aligned language models compromises safety, even when users do not intend to!",
            "Autovc: Zero-shot voice style transfer with only autoencoder loss",
            "Tool learning with foundation models",
            "Improving language understanding by generative pre-training",
            "Conformal nucleus sampling",
            "A thorough examination of decoding methods in the era of llms",
            "Bilingual correspondence recursive autoencoder for statistical machine translation",
            "Crowdsourcing under data poisoning attacks: A comparative study",
            "Stanford alpaca: an instruction-following llama model (2023)",
            "Chatgpt, bard, and large language models for biomedical research: opportunities and pitfalls",
            "Enriching the knowledge sources used in a maximum entropy part-of-speech tagger",
            "Llama: Open and efficient foundation language models",
            "Llama 2: Open foundation and fine-tuned chat models",
            "Llama 2: Open foundation and fine-tuned chat models",
            "Neural machine translation with reconstruction",
            "Jailbroken: How does llm safety training fail?",
            "Taxonomy of risks posed by language models",
            "Shadow alignment: The ease of subverting safely-aligned language models",
            "On the safety of open-sourced large language models: Does alignment really prevent them from being misused?",
            "Judging llm-as-a-judge with mt-bench and chatbot arena",
            "Hot or cold? adaptive temperature sampling for code generation with large language models",
            "Universal and transferable adversarial attacks on aligned language models"
        ]
    },
    {
        "index": 215,
        "title": "ROBUST KV: DEFENDING LARGE LANGUAGE MODELS AGAINST JAILBREAK ATTACKS VIA KV EVICTION",
        "publication_date": "2024-10-25",
        "references": [
            "Longbench: A bilingual, multitask benchmark for long context understanding.",
            "Extracting training data from large language models.",
            "Jailbreakbench: An open robustness benchmark for jailbreaking large language models.",
            "Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality.",
            "Certified adversarial robustness via randomized smoothing.",
            "Toxicity in chatgpt: Analyzing persona-assigned language models.",
            "Alpacafarm: A simulation framework for methods that learn from human feedback.",
            "Model tells you what to discard: Adaptive kv cache compression for llms.",
            "Cold-attack: Jailbreaking llms with stealthiness and controllability.",
            "Mistral 7b.",
            "Certifying llm safety against adversarial prompting.",
            "Snapkv: Llm knows what you are looking for before generation.",
            "Amplegcg: Learning a universal and transferable generative model of adversarial suffixes for jailbreaking both open and closed llms.",
            "Autodan: Generating stealthy jailbreak prompts on aligned large language models.",
            "Jailbreaking chatgpt via prompt engineering: An empirical study.",
            "Scissorhands: Exploiting the persistence of importance hypothesis for llm kv cache compression at test time.",
            "Towards deep learning models resistant to adversarial attacks.",
            "Tree of attacks: Jailbreaking black-box llms automatically.",
            "Advprompter: Fast adaptive adversarial prompting for llms.",
            "Fine-tuning aligned language models compromises safety, even when users do not intend to!",
            "Smoothllm: Defending large language models against jailbreaking attacks.",
            "The instruction hierarchy: Training llms to prioritize privileged instructions.",
            "Jailbroken: How does LLM safety training fail?",
            "Efficient streaming language models with attention sinks.",
            "Defending chatgpt against jailbreak attack via self-reminders.",
            "Gptfuzzer: Red teaming large language models with auto-generated jailbreak prompts.",
            "GPT-4 is too smart to be safe: Stealthy chat with LLMs via cipher.",
            "Jailbreak open-sourced large language models via enforced decoding.",
            "H2o: Heavy-hitter oracle for efficient generative inference of large language models.",
            "Defending large language models against jailbreaking attacks through goal prioritization.",
            "Autodan: Interpretable gradient-based adversarial attacks on large language models.",
            "Universal and transferable adversarial attacks on aligned language models."
        ]
    },
    {
        "index": 216,
        "title": "Root Defence Strategies: Ensuring Safety of LLM at the Decoding Level",
        "publication_date": "2025-02-06",
        "references": [
            "Gpt-4 technical report",
            "Llama 3 model card",
            "Detecting language model attacks with perplexity",
            "Training a helpful and harmless assistant with reinforcement learning from human feedback",
            "Purple llama cyberseceval: A secure coding benchmark for language models",
            "Medusa: Simple llm inference acceleration framework with multiple decoding heads",
            "Accelerating large language model decoding with speculative sampling",
            "Cascade speculative drafting for even faster llm inference",
            "Eagle: Speculative sampling requires rethinking feature uncertainty",
            "The unlocking spell on base llms: Rethinking alignment via in-context learning",
            "Llm self defense: By self examination, llms know they are being tricked",
            "Fine-tuning aligned language models compromises safety, even when users do not intend to!",
            "Xstest: A test suite for identifying exaggerated safety behaviours in large language models",
            "Evil geniuses: Delving into the safety of llm-based agents",
            "Llama 2: Open foundation and fine-tuned chat models",
            "Jailbroken: How does llm safety training fail?",
            "Ethical and social risks of harm from language models",
            "Defending chatgpt against jail-break attack via self-reminder",
            "Autogen: Enabling next-gen llm applications via multi-agent conversation framework",
            "Unlocking efficiency in large language model inference: A comprehensive survey of speculative decoding",
            "Safedecoding: Defending against jailbreak attacks via safety-aware decoding",
            "Qwen2 technical report",
            "Tinyllama: An open-source small language model",
            "Intention analysis prompting makes large language models a good jailbreak defender",
            "Weak-to-strong jailbreaking on large language models",
            "Prompt-driven llm safeguarding via directed representation optimization",
            "Judging llm-as-a-judge with mt-bench and chatbot arena",
            "Robust prompt optimization for defending language models against jailbreaking attacks",
            "Universal and transferable adversarial attacks on aligned language models"
        ]
    },
    {
        "index": 217,
        "title": "S-Eval : Automatic and Adaptive Test Generation for Benchmarking Safety Evaluation of Large Language Models",
        "publication_date": "2024-05-28",
        "references": [
            "Gpt-4 technical report",
            "Llama 3 Model Card",
            "The repository of our benchmark and experimental data",
            "Introducing Claude",
            "Qwen technical report",
            "A Multitask, Multilingual, Multimodal Evaluation of ChatGPT on Reasoning, Hallucination, and Interactivity",
            "Language model unalignment: Parametric red-teaming to expose hidden harms and biases",
            "Red-teaming large language models using chain of utterances for safety-alignment",
            "Can GPT-3 perform statutory reasoning?",
            "Jailbreaking black box large language models in twenty queries",
            "Multilingual Jailbreak Challenges in Large Language Models",
            "GLM: General Language Model Pretraining with Autoregressive Blank Infilling",
            "Misuse of the Internet by pedophiles: Implications for law enforcement and probation practice",
            "Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned",
            "Realtoxicityprompts: Evaluating neural toxic degeneration in language models",
            "Aligning AI With Shared Human Values",
            "Lora: Low-rank adaptation of large language models",
            "Flames: Benchmarking value alignment of chinese large language models",
            "Catastrophic Jailbreak of Open-source LLMs via Exploiting Generation",
            "ErnieBot",
            "Mistral 7B",
            "Prompt packer: Deceiving llms through compositional instruction with hidden attacks",
            "New era of artificial intelligence in education: Towards a sustainable multifaceted revolution",
            "Exploiting programmatic behavior of llms: Dual-use through standard security attacks",
            "Critiquellm: Scaling llm-as-critic for effective and explainable evaluation of large language model generation",
            "Chatgpt needs spade (sustainability, privacy, digital divide, and ethics) evaluation: A review",
            "Multi-step Jailbreaking Privacy Attacks on ChatGPT",
            "SALAD-Bench: A Hierarchical and Comprehensive Safety Benchmark for Large Language Models",
            "Deepinception: Hypnotize large language model to be jailbreaker",
            "Holistic evaluation of language models",
            "Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing",
            "Prompting frameworks for large language models: A survey",
            "Jailbreaking chatgpt via prompt engineering: An empirical study",
            "Analyzing leakage of personally identifiable information in language models",
            "Behavioral study of obedience",
            "Introducing ChatGPT",
            "Moderation",
            "BBQ: A hand-built bias benchmark for question answering",
            "Societal biases in language generation: Progress and challenges",
            "AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts",
            "Beyond classification: Financial reasoning in state-of-the-art language models",
            "Safety Assessment of Chinese Large Language Models",
            "Trustllm: Trustworthiness in large language models",
            "Does synthetic data generation of llms help clinical text mining?",
            "Gemini: a family of highly capable multimodal models",
            "Gemma: Open Models Based on Gemini Research and Technology",
            "Meta Llama Guard 2",
            "Llama: Open and efficient foundation language models",
            "Llama 2: Open foundation and fine-tuned chat models",
            "Considerations for differentially private learning with large-scale public pretraining",
            "Adapted large language models can outperform medical experts in clinical text summarization",
            "Attention is all you need",
            "DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT Models",
            "Do-not-answer: A dataset for evaluating safeguards in llms",
            "Jailbroken: How does llm safety training fail?",
            "Emergent Abilities of Large Language Models",
            "Jailbreak and guard aligned language models with only few in-context demonstrations",
            "A prompt pattern catalog to enhance prompt engineering with chatgpt",
            "Cvalues: Measuring the values of chinese large language models from safety to responsibility",
            "Baichuan 2: Open large-scale language models",
            "Bigtrans: Augmenting large language models with multilingual translation capability over 100 languages",
            "Yi: Open Foundation Models by 01. AI",
            "Gptfuzzer: Red teaming large language models with auto-generated jailbreak prompts",
            "Sentiment analysis in the era of large language models: A reality check",
            "Safetybench: Evaluating the safety of large language models with multiple choice questions",
            "Judging llm-as-a-judge with mt-bench and chatbot arena",
            "Universal and transferable adversarial attacks on aligned language models"
        ]
    },
    {
        "index": 218,
        "title": "Safe Text-to-Image Generation: Simply Sanitize the Prompt Embedding",
        "publication_date": "2024-11-15",
        "references": [
            "Automatic1111. Negative prompt.",
            "Barcélo, R., Alcázar, C., And Tobar, F. Avoiding mode collapse in diffusion models fine-tuned with reinforcement learning.",
            "Betker, J., Go, G., Jing, L., Brooks, T., Wang, J., Li, L., Ouyang, L., Huang, J., Lee, J., Guo, Y., Et Al. Improving image generation with better captions.",
            "Bourtooule, L., Chandrasekarana, V., Choquette-Choo, C. A., Jia, H., Traverson, A., Zhang, B., Lie, D., And Papernot, N. Machine unlearning.",
            "Crawford, A., And Smith, T. Illegal trade in ai child sex abuse images exposed.",
            "Every Pixel. People are creating an average of 34 million images per day. statistics for 2024.",
            "Gandikota, R., Materzyńska, J., Fiotto-Kaufman, J., And Bau, D. Erasing concepts from diffusion models.",
            "Google. Generative ai prohibited use policy.",
            "Google DeepMind. Imagen3.",
            "Hatamizadeh, A., Song, J., Liu, G., Kautz, J., And Vahdat, A. Diffit: Diffusion vision transformers for image generation.",
            "Ho, J., Jain, A., And Abbeel, P. Denoising diffusion probabilistic models.",
            "Johnson, S. J., Murty, M. R., And Navakanth, I. A detailed review on word embedding techniques with emphasis on word2vec.",
            "Kim, C., Min, K., And Yang, Y. Race: Robust adversarial concept erasure for secure text-to-image diffusion model.",
            "Kingma, D. P. Auto-encoding variational bayes.",
            "Kumari, N., Zhang, B., Wang, S.-Y., Shechtman, E., Zhang, R., And Zhu, J.-Y. Ablating concepts in text-to-image diffusion models.",
            "Leu, W., Nakashima, Y., And Garcia, N. Auditing image-based nsfw classifiers for content filtering.",
            "Li, X., Yang, Y., Deng, J., Yan, C., Chen, Y., Ji, X., And Xu, W. SafeGen: Mitigating Sexually Explicit Content Generation in Text-to-Image Models.",
            "Lin, T.-Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Dollár, P., And Zitnick, C. L. Microsoft coco: Common objects in context.",
            "Liu, R., Khakzar, A., Gu, J., Chen, Q., Torr, P., And Pizzati, F. Latent guard: a safety framework for text-to-image generation.",
            "Lu, S., Wang, Z., Li, L., Liu, Y., And Kong, A. W.-K. Mace: Mass concept erasure in diffusion models.",
            "Ly, M., Yang, Y., Hong, H., Chen, H., Jin, X., He, Y., Xue, H., Han, J., And Ding, G. One-dimensional adapter to rule them all: Concepts diffusion models and erasing applications.",
            "Machine Vision & Learning Group LMU. Safety checker model card.",
            "Notai Tech. Nudenet.",
            "OpenAI. Moderationapi.",
            "OpenAI. Usage policies.",
            "Pham, M., Marshall, K. O., Cohen, N., Mittal, G., And Hegde, C. Circumventing concept erasure methods for text-to-image generative models.",
            "Poppi, S., Poppi, T., Cocchi, F., Cornia, M., Baraldi, L., Cucchiarra, R., Et Al. Safe-clip: Removing nsfw concepts from vision-and-language models.",
            "Qu, Y., Shen, X., He, X., Backes, M., Zannettou, S., And Zhang, Y. Unsafe diffusion: On the generation of unsafe images and hateful memes from text-to-image models.",
            "Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Go, G., Agarwal, S., Sastry, G., Askel, A., Mishkin, P., Clark, J., Et Al. Learning transferable visual models from natural language supervision.",
            "Ramesh, A., Pavlov, M., Go, G., Gray, S., Voss, C., Radford, A., Chen, M., And Sutskever, I. Zero-shot text-to-image generation.",
            "Randó, J., Paleka, D., Lindner, D., Heim, L., And Tramer, F. Red-teaming the stable diffusion safety filter.",
            "Ray, A., Radenovic, F., Dubey, A., Plummer, B., Krishna, R., And Saenko, K. Cola: A benchmark for compositional text-to-image retrieval.",
            "Rombach, R., Blattmann, A., Lorenz, D., Esser, P., And Ommer, B. High-resolution image synthesis with latent diffusion models.",
            "Ronneberger, O., Fischer, P., And Brox, T. U-net: Convolutional networks for biomedical image segmentation.",
            "Schramowski, P., Brack, M., Deiseroth, B., And Kersting, K. Safe latent diffusion: Mitigating inappropriate degeneration in diffusion models.",
            "Schramowski, P., Tauchmann, C., And Kersting, K. Can machines help us answering question 16 in datasheets, and in turn reflecting on inappropriate content?",
            "Schuhmann, C., Beaumont, R., Vencu, R., Gordon, C., Wortsman, M., Et Al. Laion-5b: An open large-scale dataset for training next generation image-text models.",
            "Singer, U., Poljak, A., Hayes, T., Yin, X., An, J., Zhang, S., Hu, Q., Yang, H., Ashual, O., Gafni, O., Et Al. Make-a-video: Text-to-video generation without text-video data.",
            "Song, J., Meng, C., And Ermon, S. Denoising diffusion implicit models.",
            "Stability AI. Stable diffusion v1 model card.",
            "Stability AI. Stable diffusion v2.",
            "Stability AI. Stable diffusion 3 medium.",
            "Tsai, Y.-L., Hsu, C.-Y., Xie, C., Lin, C.-H., Chen, J. Y., Li, B., Chen, P.-Y., Yu, C.-M., And Huang, C.-Y. Ring-a-bell! how reliable are concept removal methods for diffusion models?",
            "Wang, B., Chen, Q., And Wang, Z. Diffusion-based visual art creation: A survey and new perspectives.",
            "Wu, Z., Gao, H., Wang, Y., Zhang, X., And Wang, S. Universal prompt optimizer for safe text-to-image generation.",
            "Yang, Y., Gao, R., Wang, X., Ho, T.-Y., Xu, N., And Xu, Q. Mma-diffusion: Multimodal attack on diffusion models.",
            "Yang, Y., Hui, B., Yuan, H., Gong, N., And Cao, Y. Sneakyprompt: Jailbreaking text-to-image generative models.",
            "Yang, Y., Hui, B., Yuan, H., Gong, N., And Cao, Y. Sneakyprompt: Jailbreaking text-to-image generative models.",
            "Zhang, C., Hu, M., Li, W., And Wang, L. Adversarial attacks and defenses on text-to-image diffusion models: A survey.",
            "Zhang, J., Guo, J., Sun, S., Lou, J.-G., And Zhang, D. Layout-diffusion: Improving graphic layout generation by discrete diffusion probabilistic models.",
            "Zhang, Y., Chen, X., Jia, J., Zhang, Y., Fan, C., Liu, J., Hong, M., Ding, K., And Liu, S. Defensive unlearning with adversarial training for robust concept erasure in diffusion models."
        ]
    },
    {
        "index": 219,
        "title": "Safe Unlearning: A Surprisingly Effective and Generalizable Solution to Defend Against Jailbreak Attacks",
        "publication_date": "2024-11-05",
        "references": [
            "Defending against alignment-breaking attacks via robustly aligned llm",
            "Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality.",
            "Ultrafeedback: Boosting language models with high-quality feedback",
            "Toxicity in chatgpt: Analyzing persona-assigned language models",
            "Knowledge distillation of large language models",
            "Llama guard: Llm-based input-output safeguard for human-ai conversations",
            "Exploiting programmatic behavior of llms: Dual-use through standard security attacks",
            "Multi-step jailbreaking privacy attacks on chatgpt",
            "Deepinception: Hypnotize large language model to be jailbreaker",
            "Alpacaeval: An automatic evaluator of instruction-following models",
            "Rain: Your language models can align themselves without finetuning",
            "Jailbreaking chatgpt via prompt engineering: An empirical study",
            "Towards safer large language models through machine unlearning",
            "Eraser: Jailbreaking defense in large language models via unlearning harmful knowledge",
            "Smaug: Fixing failure modes of preference optimisation with dpo-positive",
            "Direct preference optimization: Your language model is secretly a reward model",
            "Smoothllm: Defending large language models against jailbreaking attacks",
            "Xstest: A test suite for identifying exaggerated safety behaviours in large language models",
            "\"do anything now\": Characterizing and evaluating in-the-wild jailbreak prompts on large language models",
            "Llama 2: Open foundation and fine-tuned chat models",
            "Super-NaturalInstructions: Generalization via declarative instructions on 1600+ NLP tasks",
            "Jailbroken: How does LLM safety training fail?",
            "Defending chatgpt against jailbreak attack via self-reminders",
            "Large language model unlearning",
            "GPTFUZZER: red teaming large language models with auto-generated jailbreak prompts",
            "Negative preference optimization: From catastrophic collapse to effective unlearning",
            "Safetybench: Evaluating the safety of large language models with multiple choice questions",
            "Shieldlm: Empowering llms as aligned, customizable and explainable safety detectors",
            "ETHICIST: targeted training data extraction through loss smoothed soft prompting and calibrated confidence estimation",
            "Defending large language models against jailbreaking attacks through goal prioritization",
            "On prompt-driven safeguarding for large language models",
            "Universal and transferable adversarial attacks on aligned language models"
        ]
    },
    {
        "index": 221,
        "title": "SafeBench: A Safety Evaluation Framework for Multimodal Large Language Models",
        "publication_date": "2024-10-24",
        "references": [
            "Visualgpt: Data-efficient adaptation of pretrained language models for image captioning",
            "Visual question answering instruction: Unlocking multimodal large language model to domain-specific visual multitasks",
            "Image retrieval on real-life images with pre-trained vision-and-language models",
            "An image is worth 1000 lies: Adversarial transferability across prompts on vision-language models",
            "On the adversarial robustness of multi-modal foundation models",
            "On the robustness of large multimodal models against image adversarial attacks",
            "Jailbreak in pieces: Compositional adversarial attacks on multi-modal language models",
            "Semantic mirror jailbreak: Genetic algorithm based jailbreak prompts against open-source llms",
            "Jailbreak vision language models via bi-modal adversarial prompt",
            "Simpo: Simple preference optimization with a reference-free reward",
            "Direct preference optimization: Your language model is secretly a reward model",
            "Training a helpful and harmless assistant with reinforcement learning from human feedback",
            "Training language models to follow instructions with human feedback",
            "A comprehensive evaluation framework for deep model robustness",
            "Robustart: Benchmarking robustness on architecture design and training techniques",
            "Training robust deep neural networks via adversarial noise propagation",
            "Badclip: Dual-embedding guided backdoor attack on multimodal contrastive learning",
            "Pre-trained trojan attacks for visual recognition",
            "Poisoned forgery face: Towards backdoor attacks on face forgery detection",
            "GPTFUZZER: red teaming large language models with auto-generated jailbreak prompts",
            "Jailbroken: How does LLM safety training fail?",
            "Jailbreaking black box large language models in twenty queries",
            "Unveiling the safety of gpt-4o: An empirical study using jailbreak attacks",
            "Mm-safetybench: A benchmark for safety evaluation of multimodal large language models",
            "Fig-step: Jailbreaking large vision-language models via typographic visual prompts",
            "Jailbreakv-28k: A benchmark for assessing the robustness of multimodal large language models against jailbreak attacks",
            "Gpt-4o system card",
            "Gemini: A family of highly capable multimodal models",
            "Realtoxicityprompts: Evaluating neural toxic degeneration in language models",
            "Universal and transferable adversarial attacks on aligned language models",
            "Toxigen: A large-scale machine-generated dataset for adversarial and implicit hate speech detection",
            "Benchmarking cognitive biases in large language models as evaluators",
            "Privlm-bench: A multi-level privacy evaluation benchmark for language models",
            "Truthfulqa: Measuring how models mimic human falsehoods",
            "FFT: towards harmlessness evaluation and analysis for llms with factuality, fairness, toxicity",
            "Sorry-bench: Systematically evaluating large language model safety refusal behaviors",
            "Harm-bench: A standardized evaluation framework for automated red teaming and robust refusal",
            "Benchmarking trustworthiness of multimodal large language models: A comprehensive study",
            "Avibench: Towards evaluating the robustness of large vision-language model on adversarial visual-instructions",
            "How many unicorns are in this image? A safety evaluation benchmark for vision llms",
            "Jailbreakv-28k: A benchmark for assessing the robustness of multimodal large language models against jailbreak attacks",
            "Cross-modality safety alignment",
            "Autodan: Generating stealthy jailbreak prompts on aligned large language models",
            "Realtoxicityprompts: Evaluating neural toxic degeneration in language models",
            "Visual adversarial examples jailbreak aligned large language models",
            "Perspective",
            "Llama guard: Llm-based input-output safeguard for human-ai conversations",
            "Jailbreaking black box large language models in twenty queries",
            "Tree of attacks: Jailbreaking black-box llms automatically",
            "Visual adversarial examples jailbreak aligned large language models",
            "Does refusal training in llms generalize to the past tense?",
            "Draw-and-understand: Leveraging visual prompts to enable mllms to comprehend what you want",
            "Can large multi-modal models uncover deep semantics behind images?",
            "Parler-tts",
            "Chain-of-thought prompting elicits reasoning in large language models",
            "Multi-modal chain-of-thought reasoning in language models",
            "Efficient multimodal learning from data-centric perspective",
            "Cogvlm: Visual expert for pre-trained language models",
            "Chatglm: A family of large language models from glm-130b to glm-4 all tools",
            "How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites",
            "Llava-next: Improved reasoning, ocr, and world knowledge",
            "Minicpm-v: A gpt-4v level mllm on your phone",
            "Qwen-vl: A frontier large vision-language model with versatile abilities",
            "Qwen-vl: A versatile vision-language model for understanding, localization, text reading, and beyond",
            "Sharegpt4v: Improving large multi-modal models with better captions",
            "Yi: Open foundation models by 01.ai",
            "Claude 3.5 sonnet",
            "The claude 3 model family: Opus, sonnet, haiku",
            "Gemini pro",
            "Gemini flash",
            "Gpt-4o mini: advancing cost-efficient intelligence",
            "Llama 3 model card",
            "Gemma",
            "Internlm2 technical report",
            "Qwen technical report",
            "Chatglm: A family of large language models from glm-130b to glm-4 all tools",
            "A high quality sparse mixture-of-experts",
            "Introducing dbrx: A new state-of-the-art open llm",
            "Grok-2 beta release",
            "Scaling rectified flow transformers for high-resolution image synthesis",
            "Qwen2-vl-7b-instruct model card",
            "Minicpm-v: A gpt-4v level mllm on your phone",
            "Phi-3-vision-128k-instruct",
            "Phi-3.5-vision-instruct"
        ]
    },
    {
        "index": 223,
        "title": "Safety Alignment for Vision Language Models",
        "publication_date": "2024-05-22",
        "references": [
            "Bai, J., Bai, S., Yang, S., Wang, S., Tan, S., Wang, P., Lin, J., Zhou, C., and Zhou, J. Qwen-vl: A versatile vision-language model for understanding, localization, text reading, and beyond.",
            "Bailey, L., Ong, E., Russell, S., and Emmons, S. Image hijacks: Adversarial images can control generative models at runtime.",
            "Bavishi, R., Elsen, E., Hawthorne, C., Nye, M., Odena, A., Somani, A., and Ta¸sırlar, S. Introducing our multimodal models.",
            "Bethany, M., Wherry, B., Vishwamitra, N., and Najafirad, P. Image safeguarding: Reasoning with conditional vision language model and obfuscating unsafe content counterfactually.",
            "Cha, J., Kang, W., Mun, J., and Roh, B. Honeybee: Locality-enhanced projector for multimodal lmm.",
            "Chang, Y.-M., Yeh, C., Chiu, W.-C., and Yu, N. Antifakeprompt: Prompt-tuned vision-language models are fake image detectors.",
            "Chen, L., Li, J., Dong, X., Zhang, P., He, C., Wang, J., Zhao, F., and Lin, D. Sharegpt4v: Improving large multi-modal models with better captions.",
            "Chen, X., Fang, H., Lin, T.-Y., Vedantam, R., Gupta, S., Dollár, P., and Zitnick, C. L. Microsoft coco captions: Data collection and evaluation server.",
            "Ding, M., Yang, Z., Hong, W., Zheng, W., Zhou, C., Yin, D., Lin, J., Zou, X., Shao, Z., Yang, H., et al. Cogview: Mastering text-to-image generation via transformers.",
            "Dong, X., Zhang, P., Zang, Y., Cao, Y., Wang, B., Ouyang, L., Wei, X., Zhang, S., Duan, H., Cao, M., Zhang, W., Li, Y., Yan, H., Gao, Y., Zhang, X., Li, W., Li, J., Chen, K., He, C., Zhang, X., Qiao, Y., Lin, D., and Wang, J. Internlm-xcomposer2: Mastering free-form text-image composition and comprehension in vision-language large model.",
            "Du, Z., Qian, Y., Liu, X., Ding, M., Qiu, J., Yang, Z., and Tang, J. Glm: General language model pretraining with autoregressive blank infilling.",
            "Fu, C., Chen, P., Shen, Y., Qin, Y., Zhang, M., Lin, X., Yang, J., Zheng, X., Li, K., Sun, X., Wu, Y., and Ji, R. Mme: A comprehensive evaluation benchmark for multimodal large language models.",
            "Gao, K., Bai, Y., Gu, J., Xia, S.-T., Torr, P., Li, Z., and Liu, W. Inducing high energy-latency of large vision-language models with verbose images.",
            "Gong, Y., Ran, D., Liu, J., Wang, C., Cong, T., Wang, A., Duan, S., and Wang, X. Figstep: Jailbreaking large vision-language models via typographic visual prompts.",
            "Han, J., Gong, K., Zhang, Y., Wang, J., Zhang, K., Lin, D., Qiao, Y., Gao, P., and Yue, X. Onellm: One framework to align all modalities with language.",
            "Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., and Chen, W. Lora: Low-rank adaptation of large language models.",
            "Kim, A. Nsfw data scraper.",
            "Krause, J., Johnson, J., Krishna, R., and Fei-Fei, L. A hierarchical approach for generating descriptive image paragraphs.",
            "Li, B., Ge, Y., Ge, Y., Wang, G., Wang, R., Zhang, R., and Shan, Y. Seed-bench-2: Benchmarking multimodal large language models.",
            "Li, B., Wang, R., Wang, G., Ge, Y., Ge, Y., and Shan, Y. Seed-bench: Benchmarking multimodal lllms with generative comprehension.",
            "Li, J., Li, D., Xiong, C., and Hoi, S. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation.",
            "Li, J., Li, D., Savarese, S., and Hoi, S. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models.",
            "Li, M., Li, L., Yin, Y., Ahmed, M., Liu, Z., and Liu, Q. Red teaming visual language models.",
            "Liang, J., Liang, S., Luo, M., Liu, A., Han, D., Chang, E.-C., and Cao, X. Vl-trojan: Multimodal instruction backdoor attacks against autoregressive visual language models.",
            "Lin, T.-Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Dollár, P., and Zitnick, C. L. Microsoft coco: Common objects in context.",
            "Liu, H., Li, C., Li, Y., and Lee, Y. J. Improved baselines with visual instruction tuning.",
            "Liu, H., Li, C., Wu, Q., and Lee, Y. J. Visual instruction tuning.",
            "Liu, H., Li, C., Li, Y., Li, B., Zhang, Y., Shen, S., and Lee, Y. J. Llava-next: Improved reasoning, ocr, and world knowledge.",
            "Liu, X., Zhu, Y., Gu, J., Lan, Y., Yang, C., and Qiao, Y. Mm-safetybench: A benchmark for safety evaluation of multimodal large language models.",
            "Liu, Y., Duan, H., Zhang, Y., Li, B., Zhang, S., Zhao, W., Yuan, Y., Wang, J., He, C., Liu, Z., Chen, K., and Lin, D. Mmbench: Is your multi-modal model an all-around player?",
            "Liu, Z., Luo, P., Wang, X., and Tang, X. Deep learning face attributes in the wild.",
            "Luccioni, S., Akiki, C., Mitchell, M., and Jernite, Y. Stable bias: Evaluating societal representations in diffusion models.",
            "OpenAI. Gpt-4 technical report.",
            "QResearch. llama3-vision-alpha.",
            "Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., Krueger, G., and Sutskever, I. Learning transferable visual models from natural language supervision.",
            "Röttger, P., Kirk, H. R., Vidgen, B., Attanasio, G., Bianchi, F., and Hovy, D. Xstest: A test suite for identifying exaggerated safety behaviours in large language models.",
            "Tu, H., Cui, C., Wang, Z., Zhou, Y., Zhao, B., Han, J., Zhou, W., Yao, H., and Xie, C. How many unicorns are in this image? a safety evaluation benchmark for vision llms.",
            "Vishwamitra, N., Hu, H., Luo, F., and Cheng, L. Towards understanding and detecting cyberbullying in real-world images.",
            "Wang, H., Liao, J., Cheng, T., Gao, Z., Liu, H., Ren, B., Bai, X., and Liu, W. Knowledge mining with scene text for fine-grained recognition.",
            "Wei, A., Haghtalab, N., and Steinhardt, J. Jailbroken: How does llm safety training fail?",
            "Zhang, J., Ma, X., Wang, X., Qiu, L., Wang, J., Jiang, Y.-G., and Sang, J. Adversarial prompt tuning for vision-language models.",
            "Zhang, X., Zhang, C., Li, T., Huang, Y., Jia, X., Xie, X., Liu, Y., and Shen, C. A mutation-based method for multi-modal jailbreaking attack detection.",
            "Zhang, Y., Gong, K., Zhang, K., Li, H., Qiao, Y., Ouyang, W., and Yue, X. Meta-transformer: A unified framework for multimodal learning.",
            "Zhao, C., Mangat, J., Koujalgi, S., Squicciarini, A., and Caragea, C. Privacyalert: A dataset for image privacy prediction.",
            "Zhao, Y., Pang, T., Du, C., Yang, X., Li, C., Cheung, N.-M., and Lin, M. On evaluating adversarial robustness of large vision-language models.",
            "Zhong, Z., Wu, Z., Manning, C. D., Potts, C., and Chen, D. Mquake: Assessing knowledge editing in language models via multi-hop questions.",
            "Zong, Y., Bohdal, O., Yu, T., Yang, Y., and Hospedales, T. Safety fine-tuning at (almost) no cost: A baseline for vision large language models.",
            "Zou, A., Wang, Z., Carlini, N., Nasr, M., Kolter, J. Z., and Fredrikson, M. Universal and transferable adversarial attacks on aligned language models."
        ]
    },
    {
        "index": 224,
        "title": "Safety Alignment Should Be Made More Than Just a Few Tokens Deep",
        "publication_date": "2024-06-10",
        "references": [
            "Language models are few-shot learners",
            "Introducing ChatGPT",
            "Gpt-4 technical report",
            "Llama: Open and efficient foundation language models",
            "Llama 2: Open foundation and fine-tuned chat models",
            "Introducing Claude",
            "Gemini: A family of highly capable multimodal models",
            "Scalable agent alignment via reward modeling: a research direction",
            "The alignment problem: Machine learning and human values",
            "Alignment of language agents",
            "Introducing Superalignment",
            "Ai alignment: A comprehensive survey",
            "Finetuned language models are zero-shot learners",
            "Training language models to follow instructions with human feedback",
            "Training a helpful and harmless assistant with reinforcement learning from human feedback",
            "Direct preference optimization: Your language model is secretly a reward model",
            "Visual adversarial examples jailbreak aligned large language models",
            "Are aligned neural networks adversarially aligned?",
            "Representation engineering: A top-down approach to ai transparency",
            "Jailbreaking black box large language models in twenty queries",
            "Jailbreaking leading safety-aligned llms with simple adaptive attacks",
            "Fine-tuning aligned language models compromises safety, even when users do not intend to!",
            "Removing rlhf protections in gpt-4 via fine-tuning",
            "Catastrophic jailbreak of open-source llms via exploiting generation",
            "The unlocking spell on base LLMs: Rethinking alignment via in-context learning",
            "Dissecting learning and forgetting in language model finetuning",
            "Weak-to-strong jailbreaking on large language models",
            "Lima: Less is more for alignment",
            "Universal and transferable adversarial attacks on aligned language models",
            "A trivial jailbreak against llama 3",
            "Gemma: Open models based on gemini research and technology",
            "Hex-phi: Human-extended policy-oriented harmful instruction benchmark",
            "Scaling laws for reward model overoptimization",
            "Prefill Claude’s response",
            "What’s in your\" safe\" data?: Identifying benign data that breaks safety",
            "Stanford alpaca: An instruction-following llama model",
            "Alpacaeval: An automatic evaluator of instruction-following models",
            "Gpt-3.5 turbo fine-tuning and api updates",
            "Kto: Model alignment as prospect theoretic optimization",
            "Controlled decoding from language models",
            "Incorporating second-order functional knowledge for better option pricing",
            "SAMSum corpus: A human-annotated dialogue dataset for abstractive summarization",
            "sql-create-context dataset",
            "Training verifiers to solve math word problems",
            "Constitutional ai: Harmlessness from ai feedback",
            "How johnny can persuade llms to jailbreak them: Rethinking persuasion to challenge ai safety by humanizing llms",
            "Shadow alignment: The ease of subverting safely-aligned language models",
            "Badllama: cheaply removing safety fine-tuning from llama 2-chat 13b",
            "Llama guard: Llm-based input-output safeguard for human-ai conversations",
            "Breaking llama guard",
            "Assessing the brittleness of safety alignment via pruning and low-rank modifications",
            "Mechanistically analyzing the effects of fine-tuning on procedurally defined tasks",
            "Safedecoding: Defending against jailbreak attacks via safety-aware decoding",
            "On the theory of policy gradient methods: Optimality, approximation, and distribution shift",
            "Recovery rl: Safe reinforcement learning with learned recovery zones",
            "Sequential composition of dynamically dexterous robot behaviors",
            "Control barrier functions: Theory and applications",
            "Self-destructing models: Increasing the costs of harmful dual uses of foundation models",
            "Seq2sql: Generating structured queries from natural language using reinforcement learning",
            "Spider: A large-scale human-labeled dataset for complex and cross-domain semantic parsing and text-to-sql task",
            "Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned",
            "Rank analysis of incomplete block designs: I. the method of paired comparisons"
        ]
    },
    {
        "index": 225,
        "title": "SAFETY LAYERS IN ALIGNED LARGE LANGUAGE MODELS : THE KEY TO LLM'S SECURITY",
        "publication_date": "2025-02-19",
        "references": [
            "Phi-3 technical report: A highly capable language model locally on your phone",
            "Gpt-4 technical report",
            "Huggingface",
            "Refusal in llms is mediated by a single direction",
            "Training a helpful and harmless assistant with reinforcement learning from human feedback",
            "Safety-tuned llamas: Lessons from improving the safety of large language models that follow instructions",
            "Or-bench: An over-refusal benchmark for large language models",
            "Safe rlhf: Safe reinforcement learning from human feedback",
            "JUST A FEW TOKENS DEEP",
            "Aligning ai with shared human values",
            "Measuring massive multitask language understanding",
            "Safe lora: the silver lining of reducing safety risks when fine-tuning large language models",
            "Antidote: Post-fine-tuning safety alignment for large language models against harmful fine-tuning",
            "Lazy safety alignment for large language models against harmful fine-tuning",
            "Vaccine: Perturbation-aware alignment for large language model",
            "Increased llm vulnerabilities from fine-tuning and quantization",
            "No two devils alike: Unveiling distinct mechanisms of fine-tuning attacks",
            "Alpacaeval: An automatic evaluator of instruction-following models",
            "Rouge: A package for automatic evaluation of summaries",
            "Keeping llms aligned after fine-tuning: The crucial role of prompt templates",
            "Introducing meta llama 3: The most capable openly available llm to date",
            "Fine-tuning can cripple your foundation model; preserving features may be the solution",
            "Training language models to follow instructions with human feedback",
            "Training language models to follow instructions with human feedback",
            "Shadow alignment: The ease of subverting safely-aligned language models",
            "On the vulnerability of safety alignment in open-access llms",
            "A safety realignment framework via subspace-oriented model fusion for large language models",
            "Bi-factorial preference optimization: Balancing safety-helpfulness in language models",
            "Defending large language models against jailbreak attacks via layer-specific editing",
            "Safety fine-tuning at (almost) no cost: A baseline for vision large language models",
            "Universal and transferable adversarial attacks on aligned language models",
            "Navigating the safety landscape: Measuring risks in finetuning large language models",
            "Stanford alpaca: An instruction-following llama model",
            "Gemma: Open models based on gemini research and technology",
            "Llama 2: Open foundation and fine-tuned chat models",
            "Attention is all you need",
            "Fine-tuned language models are zero-shot learners",
            "Fine-tuned language models are zero-shot learners",
            "Shadow alignment: The ease of subverting safely-aligned language models",
            "On the vulnerability of safety alignment in open-access llms",
            "A safety realignment framework via subspace-oriented model fusion for large language models",
            "Bi-factorial preference optimization: Balancing safety-helpfulness in language models",
            "Defending large language models against jailbreak attacks via layer-specific editing",
            "Safety fine-tuning at (almost) no cost: A baseline for vision large language models",
            "Universal and transferable adversarial attacks on aligned language models",
            "Navigating the safety landscape: Measuring risks in finetuning large language models",
            "Stanford alpaca: An instruction-following llama model",
            "Gemma: Open models based on gemini research and technology",
            "Llama 2: Open foundation and fine-tuned chat models",
            "Attention is all you need",
            "Fine-tuned language models are zero-shot learners",
            "Fine-tuned language models are zero-shot learners"
        ]
    },
    {
        "index": 226,
        "title": "Safety Misalignment Against Large Language Models",
        "publication_date": "2023-10-11",
        "references": [
            "Language models are few-shot learners",
            "Llama 2: Open foundation and fine-tuned chat models",
            "Empowering llm to use smartphone for intelligent task automation",
            "Judging llm-as-a-judge with mt-bench and chatbot arena",
            "https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX%3A32024R1689.",
            "Training language models to follow instructions with human feedback",
            "Training a helpful and harmless assistant with reinforcement learning from human feedback",
            "Scaling instruction-finetuned language models",
            "Shadow alignment: The ease of subverting safely-aligned language models",
            "Fine-tuning aligned language models compromises safety, even when users do not intend to!",
            "Exploiting novel gpt-4 apis",
            "Safe rlhf: Safe reinforcement learning from human feedback",
            "Mistral 7b",
            "Language model unalignment: Parametric red-teaming to expose hidden harms and biases",
            "Removing rlhf protections in gpt-4 via fine-tuning",
            "LoRA: Low-rank adaptation of large language models",
            "Adaptive budget allocation for parameter-efficient fine-tuning",
            "Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning",
            "Llama-adapter: Efficient fine-tuning of large language models with zero-initialized attention",
            "Decodingtrust: A comprehensive assessment of trustworthiness in gpt models",
            "Badedit: Backdooring large language models by model editing",
            "Harmbench: A standardized evaluation framework for automated red teaming and robust refusal",
            "Hellaswag: Can a machine really finish your sentence?",
            "BoolQ: Exploring the surprising difficulty of natural yes/no questions",
            "Think you have solved question answering? try arc, the ai2 reasoning challenge",
            "A framework for few-shot language model evaluation",
            "Llama-adapter v2: Parameter-efficient visual instruction model",
            "Litgpt",
            "A theory of production",
            "Gpt-4 technical report",
            "Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned",
            "Stanford alpaca: An instruction-following llama model",
            "Mechanistically analyzing the effects of fine-tuning on procedurally defined tasks",
            "Navigating the safety landscape: Measuring risks in finetuning large language models",
            "Fundamental limitations of alignment in large language models",
            "Language models resist alignment",
            "https://openai.com/index/gpt-4o-system-card/.",
            "Minigpt-4: Enhancing vision-language understanding with advanced large language models",
            "Visual instruction tuning",
            "Cogvlm: Visual expert for pretrained language models",
            "Minigpt-v2: large language model as a unified interface for vision-language multi-task learning",
            "Learning transferable visual models from natural language supervision",
            "Eva: Exploring the limits of masked visual representation learning at scale",
            "Eva-clip: Improved training techniques for clip at scale",
            "Effective prompt extraction from language models",
            "Prsa: Prompt reverse stealing attacks against large language models",
            "Extracting prompts by inverting llm outputs",
            "All languages matter: On the multilingual safety of llms",
            "Artprompt: Ascii art-based jailbreak attacks against aligned llms"
        ]
    },
    {
        "index": 228,
        "title": "Safetywashing: Do AI Safety Benchmarks Actually Measure Safety Progress?",
        "publication_date": "2024-07-31",
        "references": [
            "Evaluating and mitigating discrimination in language model decisions",
            "Measuring massive multitask language understanding",
            "Truthfulqa: Measuring how models mimic human falsehoods",
            "The wmdp benchmark: Measuring and reducing malicious use with unlearning",
            "Training language models to follow instructions with human feedback",
            "Natural adversarial examples",
            "Scaling laws for neural language models",
            "Inverse scaling: When bigger isn’t better",
            "A framework for few-shot language model evaluation",
            "Holistic evaluation of language models",
            "Deep learning scaling is predictable, empirically",
            "Why has predicting downstream capabilities of frontier ai models with scale remained elusive?",
            "Are emergent abilities of large language models a mirage?",
            "Scaling laws literature review",
            "Emergent abilities of large language models",
            "Training trajectories of language models across scales",
            "Compression represents intelligence linearly",
            "Rethinking imagenet pre-training",
            "Self-supervised pretraining of visual features in the wild",
            "Scaling laws for neural machine translation",
            "Understanding emergent abilities of language models from the loss perspective",
            "Do better imagenet models transfer better?",
            "Observational scaling laws and the predictability of language model performance",
            "Existential risks: Analyzing human extinction scenarios and related hazards",
            "Pixmix: Dreamlike pictures comprehensively improve safety measures",
            "Augmix: A simple data processing method to improve robustness and uncertainty",
            "The many faces of robustness: A critical analysis of out-of-distribution generalization",
            "X-risk analysis for ai research",
            "Measuring mathematical problem solving with the math dataset",
            "The lambada dataset: Word prediction requiring a broad discourse context",
            "Judging llm-as-a-judge with mt-bench and chatbot arena",
            "Human Compatible: Artificial Intelligence andtheProblem ofControl",
            "Ai could be a disaster for humanity. a top computer scientist thinks he has the solution",
            "A central ai alignment problem: Capabilities generalization",
            "Goal misgeneralization: Why correct specifications aren’t enough for correct goals",
            "Training a helpful and harmless assistant with reinforcement learning from human feedback",
            "Forecasting future world events with neural networks",
            "Math-shepherd: Verify and reinforce llms step-by-step without human annotations",
            "Ai-assisted generation of difficult math questions",
            "On calibration of modern neural networks",
            "Using pre-training can improve model robustness and uncertainty",
            "Adversarial nli: A new benchmark for natural language understanding",
            "Bert: Pre-training of deep bidirectional transformers for language understanding",
            "Roberta: A robustly optimized bert pretraining approach",
            "Adversarial glue: A multi-task benchmark for robustness evaluation of language models",
            "Glue: A multi-task benchmark and analysis platform for natural language understanding",
            "Decodingtrust: A comprehensive assessment of trustworthiness in gpt models",
            "Stanford alpaca: An instruction-following llama model",
            "Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality",
            "\"do anything now\": Characterizing and evaluating in-the-wild jailbreak prompts on large language models",
            "Harmbench: A standardized evaluation framework for automated red teaming and robust refusal",
            "Tree of attacks: Jailbreaking black-box llms automatically",
            "Tree of thoughts: Deliberate problem solving with large language models",
            "Universal and transferable adversarial attacks on aligned language models",
            "Towards deep learning models resistant to adversarial attacks",
            "Ml safety course",
            "faq - control problem",
            "Safe, secure, and trustworthy development and use of artificial intelligence",
            "Senate bill no. 1047 - safe and secure innovation for frontier artificial intelligence models act",
            "The shutdown problem: an ai engineering puzzle for decision theorists",
            "Corrigibility",
            "Benchmarking neural network robustness to common corruptions and perturbations",
            "Learning transferable visual models from natural language supervision",
            "Openood v1.5: Enhanced benchmark for out-of-distribution detection",
            "Notable AI models",
            "The bitter lesson",
            "Llama 2: Open foundation and fine-tuned chat models",
            "Llama 3: An open large language model",
            "Mistral 7b",
            "Mixtral of experts",
            "The falcon series of open language models",
            "Yi: Open foundation models by 01.ai",
            "Qwen technical report",
            "Deepseek llm: Scaling open-source language models with longtermism",
            "Introducing DBRX: A new state-of-the-art open LLM",
            "Dinov2: Learning robust visual features without supervision",
            "Do adversarially robust imagenet models transfer better?",
            "A convnet for the 2020s",
            "Convnext v2: Co-designing and scaling convnets with masked autoencoders",
            "Imagenet classification with deep convolutional neural networks",
            "Very deep convolutional networks for large-scale image recognition",
            "Wide residual networks",
            "Aggregated residual transformations for deep neural networks",
            "Densely connected convolutional networks",
            "Swin transformer: Hierarchical vision transformer using shifted windows",
            "An image is worth 16x16 words: Transformers for image recognition at scale",
            "How to train your vit? Data, augmentation, and regularization in vision transformers",
            "Learning transferable visual models from natural language supervision",
            "Reversible vision transformers",
            "A large-scale hallucination evaluation benchmark for large language models",
            "Vectara hallucination leaderboard",
            "Evaluating the moral beliefs encoded in LLMs",
            "Safetybench: Evaluating the safety of large language models with multiple choice questions",
            "Discovering language model behaviors with model-written evaluations",
            "Language models are few-shot learners",
            "Toxigen: A large-scale machine-generated dataset for implicit and adversarial hate speech detection",
            "Cyberseceval 2: A wide-ranging cybersecurity evaluation suite for large language models",
            "Instruction-following evaluation for large language models",
            "Can LLMs follow simple rules?",
            "On calibration of modern neural networks",
            "Using pre-training can improve model robustness and uncertainty",
            "Adversarial nli: A new benchmark for natural language understanding",
            "Bert: Pre-training of deep bidirectional transformers for language understanding",
            "Roberta: A robustly optimized bert pretraining approach",
            "Adversarial glue: A multi-task benchmark for robustness evaluation of language models",
            "Glue: A multi-task benchmark and analysis platform for natural language understanding",
            "Decodingtrust: A comprehensive assessment of trustworthiness in gpt models",
            "Stanford alpaca: An instruction-following llama model",
            "Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality",
            "\"do anything now\": Characterizing and evaluating in-the-wild jailbreak prompts on large language models",
            "Harmbench: A standardized evaluation framework for automated red teaming and robust refusal",
            "Tree of attacks: Jailbreaking black-box llms automatically",
            "Tree of thoughts: Deliberate problem solving with large language models",
            "Universal and transferable adversarial attacks on aligned language models",
            "Towards deep learning models resistant to adversarial attacks",
            "Ml safety course",
            "faq - control problem",
            "Safe, secure, and trustworthy development and use of artificial intelligence",
            "Senate bill no. 1047 - safe and secure innovation for frontier artificial intelligence models act",
            "The shutdown problem: an ai engineering puzzle for decision theorists",
            "Corrigibility",
            "Benchmarking neural network robustness to common corruptions and perturbations",
            "Learning transferable visual models from natural language supervision",
            "Openood v1.5: Enhanced benchmark for out-of-distribution detection",
            "Notable AI models",
            "The bitter lesson",
            "Llama 2: Open foundation and fine-tuned chat models",
            "Llama 3: An open large language model",
            "Mistral 7b",
            "Mixtral of experts",
            "The falcon series of open language models",
            "Yi: Open foundation models by 01.ai",
            "Qwen technical report",
            "Deepseek llm: Scaling open-source language models with longtermism",
            "Introducing DBRX: A new state-of-the-art open LLM",
            "Dinov2: Learning robust visual features without supervision",
            "Do adversarially robust imagenet models transfer better?",
            "A convnet for the 2020s",
            "Convnext v2: Co-designing and scaling convnets with masked autoencoders",
            "Imagenet classification with deep convolutional neural networks",
            "Very deep convolutional networks for large-scale image recognition",
            "Wide residual networks",
            "Aggregated residual transformations for deep neural networks",
            "Densely connected convolutional networks",
            "Swin transformer: Hierarchical vision transformer using shifted windows",
            "An image is worth 16x16 words: Transformers for image recognition at scale",
            "How to train your vit? Data, augmentation, and regularization in vision transformers",
            "Learning transferable visual models from natural language supervision",
            "Reversible vision transformers",
            "A large-scale hallucination evaluation benchmark for large language models",
            "Vectara hallucination leaderboard",
            "Evaluating the moral beliefs encoded in LLMs",
            "Safetybench: Evaluating the safety of large language models with multiple choice questions",
            "Discovering language model behaviors with model-written evaluations",
            "Language models are few-shot learners",
            "Toxigen: A large-scale machine-generated dataset for implicit and adversarial hate speech detection",
            "Cyberseceval 2: A wide-ranging cybersecurity evaluation suite for large language models",
            "Instruction-following evaluation for large language models",
            "Can LLMs follow simple rules?"
        ]
    },
    {
        "index": 229,
        "title": "SAGE-RT: Synthetic Alignment data Generation for Safety Evaluation and Red Teaming",
        "publication_date": "2024-08-14",
        "references": [
            "Claude-3.5",
            "Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback",
            "Jailbreaking Black Box Large Language Models in Twenty Queries",
            "Not what you’ve signed up for: Compromising Real-World LLM-Integrated Applications with Indirect Prompt Injection",
            "PETGEN: Personalized Text Generation Attack on Deep Sequence Embedding-based Classification Models",
            "ProPILE: Probing Privacy Leakage in Large Language Models",
            "MALCOM: Generating Malicious Comments to Attack Neural Fake News Detection Models",
            "Jailbreaking ChatGPT via Prompt Engineering: An Empirical Study",
            "Tree of Attacks: Jailbreaking Black-Box LLMs Automatically",
            "SimPO: Simple Preference Optimization with a Reference-Free Reward",
            "The Llama 3 Herd of Models",
            "AEGIS: Online Adaptive AI Content Safety Moderation with Ensemble of LLM Experts",
            "Scaling Laws for Neural Language Models",
            "Mixtral of Experts",
            "WildTeaming at Scale: From In-the-Wild Jailbreaks to (Adversarially) Safer Language Models",
            "Llama 2: Open Foundation and Fine-Tuned Chat Models",
            "Will we run out of data? Limits of LLM scaling based on human-generated data",
            "MiniLM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers",
            "Do-Not-Answer: A Dataset for Evaluating Safeguards in LLMs",
            "LMSYS-Chat-1M: A Large-Scale Real-World LLM Conversation Dataset",
            "AutoDAN: Interpretable Gradient-Based Adversarial Attacks on Large Language Models",
            "Universal and Transferable Adversarial Attacks on Aligned Language Models",
            "solar-10.7b-instruct-V1.0-uncensored",
            "ALERT: A Comprehensive Benchmark for Assessing Large Language Models’ Safety through Red Teaming"
        ]
    },
    {
        "index": 230,
        "title": "SANDWICH ATTACK: MULTI-LANGUAGE MIXTURE ADAPTIVE ATTACK ON LLMs",
        "publication_date": "2024-04-09",
        "references": [
            "Gpt-4 technical report",
            "Towards building a robust toxicity predictor",
            "Red-teaming large language models using chain of utterances for safety-alignment",
            "Can large language models be an alternative to human evaluations?",
            "Jailbreaker: Automated jailbreak across multiple large language model chatbots",
            "Multilingual jailbreak challenges in large language models",
            "Llm self defense: By self examination, llms know they are being tricked",
            "Pretraining language models with human preferences",
            "Certifying llm safety against adversarial prompting",
            "Jailbreaking chatgpt via prompt engineering: An empirical study",
            "Ignore previous prompt: Attack techniques for language models",
            "Comprehensive evaluation of chatgpt reliability through multilingual inquiries",
            "Interpretability and transparency-driven detection and transformation of textual adversarial examples (it-dt)",
            "Training language models with language feedback at scale",
            "The attentional blink",
            "Survey of vulnerabilities in large language models revealed by adversarial attacks",
            "’ do anything now’: Characterizing and evaluating in-the-wild jailbreak prompts on large language models",
            "Gemini: a family of highly capable multimodal models",
            "Llama 2: Open foundation and fine-tuned chat models",
            "Taco: Enhancing cross-lingual transfer for low-resource languages in llms through translation-assisted chain-of-thought processes",
            "Jailbroken: How does llm safety training fail?",
            "Cognitive overload: Jailbreaking large language models with overloaded logical thinking",
            "Low-resource languages jailbreak gpt-4",
            "Text-crs: A generalized certified robustness framework against textual adversarial attacks",
            "Universal and transferable adversarial attacks on aligned language models"
        ]
    },
    {
        "index": 231,
        "title": "Scaling Trends in Language Model Robustness",
        "publication_date": "2025-02-19",
        "references": [
            "Not what you’ve signed up for: Compromising real-world LLM-integrated applications with indirect prompt injection",
            "Are Labels Required for Improving Adversarial Robustness?",
            "Did you hear that? Adversarial examples against automatic speech recognition",
            "Jailbreaking leading safety-aligned llms with simple adaptive attacks",
            "Many-shot Jailbreaking",
            "Tool use (function calling)",
            "Training a helpful and harmless assistant with reinforcement learning from human feedback",
            "Inverse Scaling: When Bigger Isn’t Better",
            "Defending against unforeseen failure modes with latent adversarial training",
            "Fast adversarial attacks on language models in one gpu minute",
            "Trading inference-time compute for adversarial robustness"
        ]
    },
    {
        "index": 232,
        "title": "SCISAFEEVAL: A Comprehensive Benchmark for Safety Alignment of Large Language Models in Scientific Tasks",
        "publication_date": "2024-12-16",
        "references": [
            "Controlled risk for potential misuse of artificial intelligence in science.",
            "Darwin series: Domain specific large language models for natural science.",
            "Catalog of narcotic drug varieties (2013 edition).",
            "Uniprot: the universal protein knowledgebase in 2023.",
            "Prohibition of the Development, Production and Stockpiling of Bacteriological.",
            "Random adversarial threshold search enables automated dna screening.",
            "Aurora-m: The first open source multilingual language model red-teamed according to the US executive order.",
            "Prompt-bench: Towards evaluating the robustness of large language models on adversarial prompts.",
            "Realtoxicityprompts: Evaluating neural toxic degeneration in language models.",
            "Disc-medllm: Bridging general large language models and real-world medical consultation.",
            "iupacgpt: Iupac-based large-scale molecular pre-trained model for property prediction and molecule generation.",
            "Selfies and the future of molecular string representations.",
            "Simulating 500 million years of evolution with a language model.",
            "From words to molecules: A survey of large language models in chemistry.",
            "Trustllm: Trustworthiness in large language models.",
            "Bioinstruct: instruction tuning of large language models for biomedical natural language processing.",
            "Qwen2 technical report.",
            "Jailbreak attacks and defenses against large language models: A survey.",
            "Do anything now\": Characterizing and evaluating in-the-wild jailbreak prompts on large language models."
        ]
    },
    {
        "index": 233,
        "title": "SecAlign: Defending Against Prompt Injection with Preference Optimization",
        "publication_date": "2025-01-13",
        "references": [
            "The llama 3 herd of models",
            "Jailbreak and guard aligned language models with only few in-context demonstrations",
            "Struq: Defending against prompt injection with structured queries",
            "GPT-4 Technical Report",
            "Claude 2",
            "Llama 2: Open foundation and fine-tuned chat models",
            "Agentdojo: A dynamic environment to evaluate attacks and defenses for llm agents",
            "Workarena: How capable are web agents at solving common knowledge work tasks?",
            "Introducing computer use, a new claude 3.5 sonnet, and claude 3.5 haiku",
            "Not what you’ve signed up for: Compromising real-world LLM-integrated applications with indirect prompt injection",
            "Formalizing and benchmarking prompt injection attacks and defenses",
            "Tensor Trust: Interpretable Prompt Injection Attacks from an Online Game",
            "Why openai is taking so long to launch agents",
            "OWASP Top 10 for LLM Applications",
            "Learn prompting",
            "Delimiters won't save you from prompt injection",
            "Benchmarking and defending against indirect prompt injection attacks on large language models",
            "Jatmo: Prompt injection defense by task-specific finetuning",
            "The Instruction Hierarchy: Training LLMs to Prioritize Privileged Instructions",
            "Instructional segment embedding: Improving llm safety with instruction hierarchy",
            "Universal and transferable adversarial attacks on aligned language models",
            "Advprompter: Fast adaptive adversarial prompting for llms",
            "Neural exec: Learning (and learning from) execution triggers for prompt injection attacks",
            "Injecagent: Benchmarking indirect prompt injections in tool-integrated large language model agents",
            "Prompt guard",
            "How Robust is Google's Bard to Adversarial Image Attacks?",
            "Data exfiltration from slack ai via indirect prompt injection",
            "Slack",
            "Hacking google bard - from prompt injection to data exfiltration",
            "Zombais: From prompt injection to c2 with claude computer use",
            "Chatgpt macos flaw could've enabled long-term spyware via memory function",
            "Signed-prompt: A new approach to prevent prompt injection attacks against llm-integrated applications",
            "Guardian: A multi-tiered defense architecture for thwarting prompt injection attacks on llms",
            "A novel evaluation framework for assessing resilience against prompt injection attacks in large language models",
            "Ignore previous prompt: Attack techniques for language models",
            "Towards deep learning models resistant to adversarial attacks",
            "Direct preference optimization: Your language model is secretly a reward model",
            "Training language models to follow instructions with human feedback",
            "KTO: Model alignment as prospect theoretic optimization",
            "ORPO: Monolithic Preference Optimization without Reference Model",
            "Alpaca-farm: A simulation framework for methods that learn from human feedback",
            "Cleaned Alpaca Dataset",
            "AlpacaEval: An Automatic Evaluator of Instruction-following Models",
            "Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference",
            "Mistral 7B",
            "LLaMA: Open and Efficient Foundation Language Models",
            "Yi: Open foundation models by 01.ai",
            "LoRA: Low-Rank Adaptation of Large Language Models",
            "TRL: Transformer Reinforcement Learning",
            "PEFT: State-of-the-art Parameter-Efficient Fine-Tuning methods",
            "Pytorch FSDP: experiences on scaling fully sharded data parallel",
            "Gpt-4o mini: advancing cost-efficient intelligence",
            "Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90%",
            "Measuring massive multitask language understanding",
            "WinoGrande: An adversarial Winograd schema challenge at scale",
            "Agieval: A human-centric benchmark for evaluating foundation models",
            "Commonsenseqa: A question answering challenge targeting commonsense knowledge",
            "Chain-of-thought prompting elicits reasoning in large language models",
            "Multilingual machine translation with large language models: Empirical results and analysis",
            "Benchmarking large language models for news summarization",
            "The GPT store",
            "Toolformer: Language models can teach themselves to use tools",
            "Gorilla: Large language model connected with massive apis",
            "ChatGPT plugins",
            "Evaluating the susceptibility of pre-trained language models via handcrafted adversarial examples",
            "Assessing Prompt Injection Risks in 200+ Custom GPTs",
            "A classification of SQL-injection attacks and countermeasures",
            "Command injection | OWASP foundation",
            "Harmbench: A standardized evaluation framework for automated red teaming and robust refusal",
            "Extracting training data from large language models",
            "Bag of tricks for training data extraction from language models",
            "Scalable extraction of training data from (production) language models",
            "Analyzing leakage of personally identifiable information in language models",
            "Multi-step jailbreaking privacy attacks on chatgpt",
            "Membership inference attacks against language models via neighbourhood comparison",
            "Do membership inference attacks work on large language models?",
            "PromptBench: Towards Evaluating the Robustness of Large Language Models on Adversarial Prompts",
            "Back-door Attacks for In-Context Learning with Language Models",
            "On the Robustness of ChatGPT: An Adversarial and Out-of-distribution Perspective",
            "A survey of reinforcement learning from human feedback",
            "Simple statistical gradient-following algorithms for connectionist reinforcement learning",
            "Proximal policy optimization algorithms",
            "RLHF workflow: From reward modeling to online RLHF",
            "Prompt shields in azure ai",
            "Baseline defenses for adversarial attacks against aligned language models",
            "Multi-modal prompt injection image attacks against GPT-4V",
            "Are aligned neural networks adversarially aligned?"
        ]
    },
    {
        "index": 234,
        "title": "SELFDEFEND: LLMs Can Defend Themselves against Jailbreaking in a Practical Manner",
        "publication_date": "2025-02-05",
        "references": [
            "SELFDEFEND: LLMs Can Defend Themselves against Jailbreaking in a Practical Manner",
            "Detecting language model attacks with perplexity",
            "Jailbreaking leading safety-aligned LLMs with simple adaptive attacks",
            "AlpacaEval: An automatic evaluator of instruction-following models.",
            "LoRA: Low-rank adaptation of large language models",
            "Universal and transferable adversarial attacks on aligned language models",
            "JailbreakBench: An open robustness benchmark for jailbreaking large language models",
            "Chain-of-thought prompting elicits reasoning in large language models",
            "Defending large language models against jailbreaking attacks through goal prioritization",
            "Improved techniques for optimization-based jailbreaking on large language models",
            "Learning transferable visual models from natural language supervision",
            "Jailbreak and guard aligned language models with only few in-context demonstrations",
            "Defending large language models against jailbreak attacks via layer-specific editing",
            "Defending against alignment-breaking attacks via robustly aligned LLM",
            "Improving prompt-driven safeguarding for large language models",
            "Certifying LLM safety against adversarial prompting",
            "Prompt injection attack against LLM-integrated applications",
            "Jailbroken: How does LLM safety training fail?",
            "When LLM meets DRL: Advancing jailbreaking efficiency via DRL-guided search",
            "LeanDojo: Theorem proving with retrieval-augmented language models",
            "CodeGen: An open large language model for code with multi-turn program synthesis",
            "Defending large language models against jailbreak attacks via machine unlearning",
            "The llama 3 herd of models",
            "Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned",
            "A cross-language investigation into jailbreak attacks in large language models",
            "Efficient adversarial training in LLMs with continuous attacks",
            "Detecting logic vulnerabilities in smart contracts by combining GPT with program analysis",
            "Jailbreaking black box large language models in twenty queries",
            "Tree of thoughts: Deliberate problem solving with large language models",
            "Large language models for information retrieval: A survey",
            "On evaluating adversarial robustness of large vision-language models",
            "Judging LLM-as-a-judge with MT-Bench and chatbot arena",
            "A survey of large language models",
            "On prompt-driven safeguarding for large language models",
            "Defending large language models against jailbreak attacks via unlearning harmful knowledge",
            "Defending large language models against jailbreak attacks via safety-aware decoding",
            "Defending large language models against jailbreak attacks via layer-specific editing",
            "Defending large language models against jailbreak attacks through goal prioritization",
            "Adversarial tuning: Defending against jailbreak attacks for LLMs",
            "Fight back against jailbreaking via prompt adversarial tuning",
            "Improving alignment and robustness with circuit breakers",
            "Baseline defenses for adversarial attacks against aligned language models",
            "Defending large language models against jailbreak attacks via continuous attacks"
        ]
    },
    {
        "index": 235,
        "title": "SeqAR: Jailbreak LLMs with Sequential Auto-Generated Characters",
        "publication_date": "2024-03-02",
        "references": [
            "Are aligned neural networks adversarially aligned?",
            "Jailbreakbench: An open robustness benchmark for jailbreaking large language models",
            "Jailbreaking black box large language models in twenty queries",
            "The sifo benchmark: Investigating the sequential instruction following ability of large language models",
            "Chat gpt \"dan\" (and other \"jailbreaks\")",
            "A wolf in sheep’s clothing: Generalized nested jail-break prompts can fool large language models easily",
            "Analyzing the inherent response tendency of llms: Real-world instructions-driven jailbreak",
            "The llama 3 herd of models",
            "Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned",
            "DeBERTav3: Improving deBERTa using ELECTRA-style pre-training with gradient-disentangled embedding sharing",
            "Fine-tuning large language models with sequential instructions",
            "Harmbench: a standardized evaluation framework for automated red teaming and robust refusal",
            "Tree of attacks: Jailbreaking black-box llms automatically",
            "Introducing chatgpt",
            "Gpt-4 technical report",
            "Hello gpt-4o",
            "Training language models to follow instructions with human feedback",
            "Red teaming language models with language models",
            "Direct preference optimization: Your language model is secretly a reward model",
            "Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context",
            "SmoothLLM: Defending Large Language Models Against Jailbreaking Attacks",
            "Large Language Models Can Be Easily Distracted by Irrelevant Context",
            "Llama 2: Open foundation and fine-tuned chat models",
            "Evaluating llms with multiple problems at once: A new paradigm for probing llm capabilities",
            "Jailbroken: How does llm safety training fail?",
            "Jailbreak and guard aligned language models with only few in-context demonstrations",
            "Distract large language models for automatic jailbreak attack",
            "Defending chatgpt against jailbreak attack via self-reminders",
            "Large language models as optimizers",
            "GPT-4 is too smart to be safe: Stealthy chat with LLMs via cipher",
            "How johnny can persuade LLMs to jailbreak them: Rethinking persuasion to challenge AI safety by humanizing LLMs",
            "Defending large language models against jail-breaking attacks through goal prioritization",
            "Prompt-bench: Towards evaluating the robustness of large language models on adversarial prompts",
            "AutoDAN: Interpretable gradient-based adversarial attacks on large language models",
            "Universal and transferable adversarial attacks on aligned language models"
        ]
    },
    {
        "index": 236,
        "title": "SequentialBreak: Large Language Models Can be Fooled by Embedding Jailbreak Prompts into Sequential Prompt Chains",
        "publication_date": "2024-11-06",
        "references": [
            "Llama 2: Open foundation and fine-tuned chat models",
            "The llama 3 herd of models",
            "Gemma: Open models based on gemini research and technology",
            "Gemma 2: Improving open language models at a practical size",
            "Model Card and Evaluations for Claude Models",
            "Gpt-4 technical report",
            "Codegeex: A pre-trained model for code generation with multilingual benchmarking on humaneval-x",
            "Use chat gpt to solve programming bugs",
            "Evaluating the feasibility of chatgpt in healthcare: an analysis of multiple clinical and research scenarios",
            "What if the devil is my guardian angel: Chatgpt as a case study of using chatbots in education",
            "Enhancing stem learning with chatgpt and bing chat as objects to think with: A case study",
            "Training language models to follow instructions with human feedback",
            "Universal and transferable adversarial attacks on aligned language models",
            "Autodan: Generating stealthy jailbreak prompts on aligned large language models",
            "Jailbreaking leading safety-aligned llms with simple adaptive attacks",
            "Fast adversarial attacks on language models in one gpu minute",
            "Jailbreaking black box large language models in twenty queries",
            "Deepinception: Hypnotize large language model to be jailbreaker",
            "A wolf in sheep’s clothing: Generalized nested jailbreak prompts can fool large language models easily",
            "Jailbreakbench: An open robustness benchmark for jail-breaking large language models",
            "Chat gpt ”dan” (and other ”jailbreaks”)",
            "Jailbroken: How does llm safety training fail?",
            "Multilingual jailbreak challenges in large language models",
            "Catastrophic jailbreak of open-source llms via exploiting generation",
            "Detecting language model attacks with perplexity",
            "Great, now write an article about that: The crescendo multi-turn llm jailbreak attack",
            "Gptfuzzer: Red teaming large language models with auto-generated jailbreak prompts",
            "Drattack: Prompt decomposition and reconstruction makes powerful llm jailbreakers",
            "Play guessing game with llm: Indirect jailbreak attack with implicit clues",
            "Training a helpful and harmless assistant with reinforcement learning from human feedback",
            "Beavertails: Towards improved safety alignment of llm via a human-preference dataset",
            "Certifying llm safety against adversarial prompting",
            "Smoothllm: Defending large language models against jailbreaking attacks",
            "A holistic approach to undesired content detection in the real world",
            "The trojan detection challenge",
            "Openai usage policies",
            "Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality",
            "Hello gpt-4o",
            "Tree of attacks: Jailbreaking black-box llms automatically"
        ]
    },
    {
        "index": 237,
        "title": "Soft-Label Integration for Robust Toxicity Classification",
        "publication_date": "2024-11-07",
        "references": [
            "Special characters attack: Toward scalable training data extraction from large language models",
            "Inducing high energy-latency of large vision-language models with verbose images",
            "Expectation vs. experience: Evaluating the usability of code generation tools powered by large language models",
            "Large language models challenge the future of higher education",
            "Characteristics of harmful text: Towards rigorous benchmarking of language models",
            "Jailbreaker: Automated jailbreak across multiple large language model chatbots",
            "GPTFuzzer: Red teaming large language models with auto-generated jailbreak prompts",
            "Enhancing jailbreak attack against large language models through silent tokens",
            "Adversarial robustness for visual grounding of multimodal large language models",
            "A machine learning approach to comment toxicity classification",
            "Fair selective classification via sufficiency",
            "Fighting offensive language on social media with unsupervised text style transfer",
            "Fairness and robustness in invariant learning: A case study in toxicity classification",
            "Is your toxicity my toxicity? exploring the impact of rater identity on toxicity annotation",
            "Robust bias evaluation of large generative language models",
            "Measuring and mitigating unintended bias in text classification",
            "Just train twice: Improving group robustness without training group information",
            "Invariant risk minimization",
            "In search of lost domain generalization",
            "Mathematical programs with optimization problems in the constraints",
            "Bi-level meta-learning for few-shot domain generalization",
            "Investigating bi-level optimization for learning and vision from a unified perspective: A survey and beyond",
            "Bilevel optimization: Convergence analysis and enhanced design",
            "Meta-weight-net: Learning an explicit mapping for sample weighting",
            "Meta-learning with implicit gradients",
            "Rethinking bi-level optimization in neural architecture search: A gibbs sampling perspective",
            "Fast and practical neural architecture search",
            "Single-darts: Towards stable architecture search",
            "Model agnostic sample reweighting for out-of-distribution learning",
            "Learning to reweight examples for robust deep learning",
            "Meta label correction for noisy label learning",
            "Crowddb: answering queries with crowdsourcing",
            "Crowdsourcing for multiple-choice question answering",
            "Resolving conflicts in heterogeneous data by truth discovery and source reliability estimation",
            "Truth inference in crowdsourcing: Is the problem solved?",
            "Training complex models with multi-task weak supervision",
            "Snorkel: Rapid training data creation with weak supervision",
            "Data programming: Creating large training sets, quickly",
            "Dealing with disagreements: Looking beyond the majority vote in subjective annotations",
            "Learning with multiple labels",
            "Learning from partial labels",
            "Solving the partial label learning problem: An instance-based approach",
            "Leveraged weighted loss for partial label learning",
            "Provably consistent partial-label learning",
            "Progressive identification of true labels for partial-label learning",
            "Distributionally robust neural networks",
            "Ordered sgd: A new stochastic optimization framework for empirical risk minimization",
            "Distributionally robust language modeling",
            "Robust solutions of optimization problems affected by uncertain probabilities",
            "Statistics of robust optimization: A generalized empirical likelihood approach",
            "Optimization of conditional value-at-risk",
            "Large-scale methods for distributionally robust optimization",
            "Model-agnostic meta-learning for fast adaptation of deep networks",
            "Safe deep semi-supervised learning for unseen-class unlabeled data",
            "Averaged method of multipliers for bi-level optimization without lower-level strong convexity",
            "On the convergence theory for hessian-free bilevel algorithms",
            "Latent dirichlet allocation",
            "Hatexplain: A benchmark dataset for explainable hate speech detection",
            "You only prompt once: On the capabilities of prompt learning on large language models to tackle toxic content",
            "Basic real analysis",
            "Transformers: State-of-the-art natural language processing",
            "Evaluating saliency methods for neural language models",
            "Statemask: Explaining deep reinforcement learning through state mask"
        ]
    },
    {
        "index": 238,
        "title": "SoK: Prompt Hacking of Large Language Models",
        "publication_date": "2024-10-16",
        "references": [
            "Generalizing question answering system with pre-trained language model fine-tuning",
            "Improving text classification with large language model-based data augmentation",
            "Large language models for mathematical reasoning: Progresses and challenges",
            "Codegen: An open large language model for code with multi-turn program synthesis",
            "Enhancing financial sentiment analysis via retrieval augmented large language models",
            "How well do hate speech, toxicity, abusive and offensive language classification models generalize across datasets?",
            "Visualgpt: Data-efficient adaptation of pretrained language models for image captioning",
            "Story centaur: Large language model few shot learning as a creative writing tool",
            "Unveiling security, privacy, and ethical concerns of chatgpt",
            "Extracting training data from large language models",
            "An attacker’s dream? exploring the capabilities of chatgpt for developing malware",
            "Github copilot: A threat to high school security? exploring github copilot’s proficiency in generating malware from simple user prompts",
            "Jailbreaking black box large language models in twenty queries",
            "Breaking bad: Unraveling influences and risks of user inputs to chatgpt for game story generation",
            "Ignore previous prompt: Attack techniques for language models",
            "Jailbreaker in jail: Moving target defense for large language models",
            "Defending chatgpt against jailbreak attack via self-reminders",
            "Ignore this title and hack-a-prompt: Exposing systemic vulnerabilities of llms through a global scale prompt hacking competition",
            "Trick me if you can: Human-in-the-loop generation of adversarial examples for question answering",
            "From chatgpt to threatgpt: Impact of generative AI in cybersecurity and privacy",
            "Prompting large language models for malicious webpage detection",
            "Machine-generated text: A comprehensive survey of threat models and detection methods",
            "Strengthening LLM trust boundaries: A survey of prompt injection attacks surrender suresh kumar dr. M.L. cummings dr. alexander stimpson",
            "Use of llms for illicit purposes: Threats, prevention measures, and vulnerabilities",
            "Survey of vulnerabilities in large language models revealed by adversarial attacks",
            "A survey on large language model (LLM) security and privacy: The good, the bad, and the ugly",
            "'do anything now': Characterizing and evaluating in-the-wild jailbreak prompts on large language models",
            "Jailbreaking chatgpt via prompt engineering: An empirical study",
            "Automatically auditing large language models via discrete optimization",
            "Developing a framework for auditing large language models using human-in-the-loop",
            "The art of defending: A systematic evaluation and analysis of LLM defense strategies on safety and over-defensiveness",
            "Pretraining language models with human preferences",
            "Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned",
            "Interpretability and transparency-driven detection and transformation of textual adversarial examples (IT-DT)",
            "Attention is all you need",
            "Axiomatic attribution for deep networks",
            "Knowledge unlearning for mitigating privacy risks in language models",
            "Editing models with task arithmetic",
            "Locating and editing factual associations in GPT",
            "Knowledge sanitization of large language models",
            "Pruning for protection: Increasing jailbreak resistance in aligned llms without fine-tuning"
        ]
    },
    {
        "index": 239,
        "title": "SORRY-BENCH: SYSTEMATICALLY EVALUATING LARGE LANGUAGE MODEL SAFETY REFUSAL WARNING",
        "publication_date": "2025-03-01",
        "references": [
            "Jailbreaking leading safety-aligned llms with simple adaptive attacks.",
            "Introducing Claude.",
            "Training a helpful and harmless assistant with reinforcement learning from human feedback.",
            "Language models are few-shot learners.",
            "Jailbreakbench: An open robustness benchmark for jailbreaking large language models.",
            "Chatbot arena: An open platform for evaluating llms by human preference.",
            "A coefficient of agreement for nominal scales.",
            "Or-bench: An over-refusal benchmark for large language models.",
            "Towards harmlessness evaluation and analysis for llms with factuality, fairness, toxicity.",
            "Safe rlhf: Safe reinforcement learning from human feedback.",
            "Do-not-answer: A dataset for evaluating safeguards in llms.",
            "Universal and transferable adversarial attacks on aligned language models.",
            "Alert: A comprehensive benchmark for assessing large language models’ safety through red teaming.",
            "Llama 2: Open foundation and fine-tuned chat models.",
            "Measuring massive multitask language understanding.",
            "Training language models to follow instructions with human feedback.",
            "Simultaneous translation and understanding.",
            "Chain-of-thought prompting elicits reasoning in large language models.",
            "Finetuned language models are zero-shot learners.",
            "Jaccard index - wikipedia.",
            "A strongreject for empty jailbreaks.",
            "Defending chatgpt against jailbreak attack via self-reminders.",
            "Gpt-4 is too smart to be safe: Stealthy chat with llms via cipher.",
            "How johnny can persuade llms to jailbreak them: Rethinking persuasion to challenge ai safety by humanizing llms.",
            "Training language models to follow instructions with human feedback.",
            "Jailbreaking chatgpt via prompt engineering: An empirical study.",
            "Jailbreakbench: An open robustness benchmark for jailbreaking large language models.",
            "A brief overview of prior safety benchmark datasets for (large) language models.",
            "Judging llm-as-a-judge with mt-bench and chatbot arena.",
            "Alignbench: Benchmarking chinese alignment of large language models.",
            "Fine-tuning aligned language models compromises safety, even when users do not intend to!",
            "Direct preference optimization: Your language model is secretly a reward model.",
            "Xstest: A test suite for identifying exaggerated safety behaviours in large language models.",
            "Toxicchat: Unveiling hidden challenges of toxicity detection in real-world user-AI conversation.",
            "Multilingual jailbreak challenges in large language models.",
            "Build it break it fix it for dialogue safety: Robustness from adversarial human attack.",
            "The llama 3 herd of models.",
            "Real-toxicityprompts: Evaluating neural toxic degeneration in language models.",
            "BBQ: A hand-built bias benchmark for question answering.",
            "Or-bench: An over-refusal benchmark for large language models.",
            "Do-Not-Answer: A dataset for evaluating safeguards in llms.",
            "Simplesafetytests: a test suite for identifying critical safety risks in large language models.",
            "FFT: Towards harmlessness evaluation and analysis for llms with factuality, fairness, toxicity.",
            "Harmbench: A standardized evaluation framework for automated red teaming and robust refusal.",
            "SALAD-Bench: A hierarchical and comprehensive safety benchmark for large language models.",
            "StrongREJECT: Manually crafted, filtered from other existing datasets, and generated by LLM via prompt engineering.",
            "JBB-Behaviors: Half originally and uniquely crafted, half from other existing datasets."
        ]
    },
    {
        "index": 240,
        "title": "SPA-VL: A Comprehensive Safety Preference Alignment Dataset for Vision Language Models",
        "publication_date": "2025-02-27",
        "references": [
            "Lmms evaluation",
            "Anthropic. Claude",
            "Anthropic. Claude usage policies",
            "Qwen-vl: A versatile vision-language model for understanding, localization, text reading, and beyond",
            "Training a helpful and harmless assistant with reinforcement learning from human feedback",
            "Hallucination of multimodal large language models: A survey",
            "GQA: A new dataset for real-world visual reasoning and compositional question answering",
            "Large language models as automated aligners for benchmarking vision-language models",
            "Prismatic vlms: Investigating the design space of visually-conditioned language models",
            "RLAIF vs. RLHF: scaling reinforcement learning from human feedback with AI feedback",
            "Otter: a multi-modal model with in-context instruction tuning",
            "Seed-bench: Benchmarking multimodal large language models",
            "Trustllm: Trustworthiness in large language models",
            "Aligning large multimodal models with factually augmented RLHF",
            "Lamda: Language models for dialog applications",
            "How many unicorns are in this image? a safety evaluation benchmark for vision llms",
            "Inferaligner: Inference-time alignment for harmlessness through cross-model guidance",
            "Mllm-protector: Ensuring mllm’s safety without hurting performance",
            "Aligning modalities in vision large language models via preference fine-tuning",
            "Minigpt-4: Enhancing vision-language understanding with advanced large language models",
            "Fine-tuning language models from human preferences",
            "Safety fine-tuning at (almost) no cost: A baseline for vision large language models",
            "MM-safetybench: A benchmark for safety evaluation of multimodal large language models",
            "A survey of large language models",
            "On evaluating adversarial robustness of large vision-language models",
            "Improving generalization of alignment with human preferences through group invariant learning",
            "Aligning modalities in vision large language models via preference fine-tuning",
            "LLaVA"
        ]
    },
    {
        "index": 241,
        "title": "SQL Injection Jailbreak: A Structural Disaster of Large Language Models",
        "publication_date": "2025-02-28",
        "references": [
            "Gpt-4 technical report",
            "Many-shot in-context learning",
            "Deepseek llm: Scaling open-source language models with longtermism",
            "Doubao llm (large language model) directions",
            "Defending against alignment-breaking attacks via robustly aligned LLM",
            "Play guessing game with LLM: Indirect jailbreak attack with implicit clues",
            "Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality",
            "Multilingual jailbreak challenges in large language models",
            "A wolf in sheep’s clothing: Generalized nested jail-break prompts can fool large language models easily",
            "The llama 3 herd of models",
            "COLD-attack: Jailbreaking LLMs with stealthiness and controllability",
            "A classification of sql injection attacks and countermeasures",
            "Chat templating",
            "Llama guard: Llm-based input-output safeguard for human-ai conversations",
            "Baseline defenses for adversarial attacks against aligned language models",
            "Beavertails: Towards improved safety alignment of llm via a human-preference dataset",
            "Mistral 7b",
            "Artprompt: ASCII art-based jailbreak attacks against aligned LLMs",
            "Deepinception: Hypnotize large language model to be jailbreaker",
            "RAIN: Your language models can align themselves without finetuning",
            "AutoDAN: Generating stealthy jailbreak prompts on aligned large language models",
            "Jailbreakv-28k: A benchmark for assessing the robustness of multimodal large language models against jailbreak attacks",
            "Codechameleon: Personalized encryption framework for jailbreaking large language models",
            "How to use chat markup language",
            "Chatgpt: Conversational ai model",
            "Fine-tuning aligned language models compromises safety, even when users do not intend to!",
            "Codeattack: Revealing safety generalization challenges of large language models via code completion",
            "Smoothllm: Defending large language models against jailbreaking attacks",
            "Gemini: a family of highly capable multimodal models",
            "Llama 2: Open foundation and fine-tuned chat models",
            "Defending LLMs against jailbreaking attacks via backtranslation",
            "Jailbreak and guard aligned language models with only few in-context demonstrations",
            "GradSafe: Detecting jailbreak prompts for LLMs via safety-critical gradient analysis",
            "Defending chatgpt against jailbreak attack via self-reminders",
            "Cognitive overload: Jailbreaking large language models with overloaded logical thinking",
            "SafeDecoding: Defending against jailbreak attacks via safety-aware decoding",
            "A comprehensive study of jailbreak attack versus defense for large language models",
            "Baichuan 2: Open large-scale language models",
            "On the vulnerability of safety alignment in open-access llms",
            "How johnny can persuade LLMs to jailbreak them: Rethinking persuasion to challenge AI safety by humanizing LLMs",
            "Jailbreak open-sourced large language models via enforced decoding",
            "Defending large language models against jailbreaking attacks through goal prioritization",
            "On Large Language Models’ Resilience to Coercive Interrogation",
            "Prefix guidance: A steering wheel for large language models to defend against jailbreak attacks",
            "Improved few-shot jail-breaking can circumvent aligned language models and their defenses",
            "Virtual context: Enhancing jailbreak attacks with special token injection",
            "Prompt-bench: Towards evaluating the robustness of large language models on adversarial prompts"
        ]
    },
    {
        "index": 242,
        "title": "ADVWAVE: STEALTHY ADVERSARIAL JAILBREAK AGAINST LARGE AUDIO-LANGUAGE MODELS",
        "publication_date": "2024-12-11",
        "references": [
            "Did you hear that? adversarial examples against automatic speech recognition",
            "Audiolm: a language modeling approach to audio generation",
            "Are aligned neural networks adversarially aligned?",
            "Jailbreaking black box large language models in twenty queries",
            "When llm meets drl: Advancing jail-breaking efficiency via drl-guided search",
            "Chatbot arena: An open platform for evaluating llms by human preference",
            "Qwen-audio: Advancing universal audio understanding via unified large-scale audio-language models",
            "Houdini: Fooling deep structured prediction models",
            "Mind2web: Towards a generalist agent for the web",
            "Pengi: An audio language model for audio tasks",
            "Diffusion models beat gans on image synthesis",
            "Robust physical-world attacks on deep learning visual classification",
            "Llama-omni: Seamless speech interaction with large language models",
            "SpoKEN question answering and speech continuation using spectrogram-powered llm",
            "Gpt-4o system card",
            "Fine-tuning aligned language models compromises safety, even when users do not intend to!",
            "Imperceptible, robust, and targeted adversarial examples for automatic speech recognition",
            "Code llama: Open foundation models for code",
            "Fast adversarial attacks on language models in one gpu minute",
            "Deep convolutional neural networks and data augmentation for environmental sound classification",
            "Funaudiollm: Voice understanding and generation foundation models for natural interaction between humans and llms",
            "Salmonn: Towards generic hearing abilities for large language models",
            "Viola: Unified codec language models for speech recognition, synthesis, and translation",
            "{CommanderSong }: a systematic approach for practical adversarial voice recognition",
            "How johnny can persuade llms to jailbreak them: Rethinking persuasion to challenge ai safety by humanizing llms",
            "Speechgpt: Empowering large language models with intrinsic cross-modal conversational abilities",
            "Gpt-4v (ision) is a generalist web agent, if grounded",
            "Judging llm-as-a-judge with mt-bench and chatbot arena",
            "Universal and transferable adversarial attacks on aligned language models"
        ]
    },
    {
        "index": 243,
        "title": "STEER DIFF: S TEERING TOWARDS SAFE TEXT-TO-IMAGE DIFFUSION MODELS",
        "publication_date": "2024-10-03",
        "references": [
            "dall-e 2 preview - risks and limitations",
            "Leonardo ai content moderation filter: Everything you need to know",
            "An image is worth 16x16 words: Transformers for image recognition at scale",
            "Towards resilient and efficient llms: A comparative study of efficiency, performance, and adversarial robustness",
            "Advanced stock price prediction with xlstm-based models: Improving long-term forecasting",
            "Erasing concepts from diffusion models",
            "Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned",
            "Bae: Bert-based adversarial examples for text classification",
            "Datasheets for datasets",
            "Synthclip: Are we ready for a fully synthetic clip training?",
            "Lm-switch: Lightweight language model conditioning in word embedding space",
            "Classifier-free diffusion guidance",
            "Denoising diffusion probabilistic models",
            "Is bert really robust? a strong baseline for natural language attack on text classification and entailment",
            "Art: Automatic red-teaming for text-to-image models to protect benign users",
            "Textbugger: Generating adversarial text against real-world applications",
            "Microsoft coco: Common objects in context",
            "Latent guard: a safety framework for text-to-image generation",
            "Jailbreaking prompt attack: A controllable adversarial attack against diffusion models",
            "stable diffusion v2 model card.",
            "High-resolution image synthesis with latent diffusion models",
            "High-resolution image synthesis with latent diffusion models",
            "Photorealistic text-to-image diffusion models with deep language understanding",
            "Can machines help us answering question 16 in datasheets, and in turn reflecting on inappropriate content?",
            "Safe latent diffusion: Mitigating inappropriate degeneration in diffusion models",
            "Laion-5b: An open large-scale dataset for training next generation image-text models",
            "Deep unsupervised learning using nonequilibrium thermodynamics",
            "Ring-a-bell! how reliable are concept removal methods for diffusion models?",
            "Erasediff: Erasing data influence in diffusion models",
            "Sneakyprompt: Jailbreaking text-to-image generative models",
            "Forget-me-not: Learning to forget in text-to-image diffusion models",
            "Adding conditional control to text-to-image diffusion models",
            "To generate or not? safety-driven unlearned diffusion models are still easy to generate unsafe images... for now.",
            "Imma: Immunizing text-to-image models against malicious adaptation",
            "A pilot study of query-free adversarial attack against stable diffusion"
        ]
    },
    {
        "index": 244,
        "title": "StructuralSleight: Automated Jailbreak Attacks on Large Language Models Utilizing Uncommon Text-Organization Structures",
        "publication_date": "2025-02-18",
        "references": [
            "Dan is my new friend.",
            "You can use gpt-4 to create prompt injections against gpt-4.",
            "Detecting language model attacks with perplexity.",
            "claude2.",
            "claude3.",
            "Ext5: Towards extreme multi-task scaling for transfer learning.",
            "Training a helpful and harmless assistant with reinforcement learning from human feedback.",
            "Constitutional ai: Harmlessness from ai feedback.",
            "Managing extreme ai risks amid rapid progress.",
            "A survey on evaluation of large language models.",
            "Jailbreaking black box large language models in twenty queries.",
            "Safe rlhf: Safe reinforcement learning from human feedback.",
            "Masterkey: Automated jailbreaking of large language model chatbots.",
            "Gpt4graph: Can large language models understand graph structured data ? an empirical evaluation and benchmarking.",
            "Towards a unified view of parameter-efficient transfer learning.",
            "From complex to simple: Enhancing multi-constraint complex instruction following ability of large language models.",
            "Can llms effectively leverage graph structural information: when and why.",
            "StructGPT: A general framework for large language model to reason over structured data.",
            "Challenges and applications of large language models.",
            "Exploiting programmatic behavior of llms: Dual-use through standard security attacks.",
            "Pretraining language models with human preferences.",
            "Vocabulary attack to hijack large language model applications.",
            "Self-alignment with instruction back-translation.",
            "Large language models are superpositions of all characters: Attaining arbitrary role-play via self-alignment.",
            "Tree of attacks: Jail-breaking black-box llms automatically.",
            "Llama3.",
            "Chatgpt.",
            "Gpt-4.",
            "Gpt-4o.",
            "Training language models to follow instructions with human feedback.",
            "Hijacking large language models via adversarial in-context learning.",
            "Exploring safety generalization challenges of large language models via code.",
            "Trustllm: Trustworthiness in large language models.",
            "Principle-driven self-alignment of language models from scratch with minimal human supervision.",
            "Struc-bench: Are large language models really good at generating complex structured data?",
            "What’s in my ai? Life Architect.",
            "Overwriting pretrained bias with finetuning data.",
            "DeepStruct: Pretraining of language models for structure prediction.",
            "Rolellm: Benchmarking, eliciting, and enhancing role-playing abilities of large language models.",
            "Jailbroken: How does llm safety training fail?",
            "Contrastive meta learning with behavior multiplicity for recommendation.",
            "Gpt-fuzzer: Red teaming large language models with auto-generated jailbreak prompts.",
            "Gpt-4 is too smart to be safe: Stealthy chat with llms via cipher.",
            "Jade: A linguistics-based safety evaluation platform for large language models.",
            "Secrets of rlhf in large language models part i: Ppo.",
            "Easyjailbreak: A unified framework for jail-breaking large language models.",
            "Universal and transferable adversarial attacks on aligned language models."
        ]
    },
    {
        "index": 247,
        "title": "Supporting Human Raters with the Detection of Harmful Content using Large Language Models",
        "publication_date": "2024-06-18",
        "references": [
            "Not what you’ve signed up for: Compromising real-world llm-integrated applications with indirect prompt injection",
            "Using terms and conditions to apply fundamental rights to content moderation: Is article 12 dsa a paper tiger?",
            "Constitutional AI: Harmlessness from AI feedback",
            "Rethinking the detection of child sexual abuse imagery on the internet",
            "Detecting hate speech with GPT-3",
            "What is a flag for? social media reporting tools and the vocabulary of complaint",
            "Racial bias in hate speech and abusive language detection datasets",
            "Automated hate speech detection and the problem of offensive language",
            "Pathways to violent extremism in the digital era",
            "A survey on automatic detection of hate speech in text",
            "Update on the Global Internet Forum to Counter Terrorism",
            "Palm 2 for text",
            "Learn about policies across Google",
            "Moderating text",
            "Jury learning: Integrating dissenting voices into machine learning models",
            "Algorithmic content moderation: technical and political challenges in the automation of platform governance",
            "Auto-debias: Debiasing masked language models with automated biased prompts",
            "You only prompt once: On the capabilities of prompt learning on large language models to tackle toxic content",
            "Is ChatGPT better than human annotators? potential and limitations of ChatGPT in explaining implicit hate speech",
            "Personalizing content moderation on social media: User perspectives on moderation choices, interface design, and labor",
            "Toxic comment classification challenge",
            "Scaling laws for neural language models",
            "Repeat spreaders and election delegitimization: A comprehensive dataset of misinformation tweets from the 2020 us election",
            "Designing toxic content classification for a diversity of perspectives",
            "Sustainable modular debiasing of language models",
            "A new generation of perspective api: Efficient multilingual character-level transformers",
            "Retrieval-augmented generation for knowledge-intensive nlp tasks",
            "Lost in the middle: How language models use long contexts",
            "Measuring the impact of covid-19 vaccine misinformation on vaccination intent in the uk and usa",
            "“i’m not sure what difference is between their content and mine, other than the person itself” a study of fairness perception of content moderation on youtube",
            "An empirical survey of the effectiveness of debiasing techniques for pre-trained language models",
            "Policies",
            "Harm categories in Azure AI content safety",
            "ETHOS: an online hate speech detection dataset",
            "A measurement study of hate speech in social media",
            "Ana-lyzing the targets of hate in online social media",
            "Sok: Content moderation in social media, from guidelines to enforcement, and research to practice",
            "Annoy",
            "The psychological well-being of content moderators: the emotional labor of commercial moderation and avenues for improving support",
            "Sok: Hate, harassment, and the changing landscape of online abuse",
            "“at the end of the day facebook does what it wants” how users experience contesting algorithmic content moderation",
            "Moderating new waves of online hate with chain-of-thought reasoning in large language models",
            "Chain-of-thought prompting elicits reasoning in large language models",
            "Using GPT-4 for content moderation",
            "Large language models as optimizers",
            "Predicting the type and target of offensive posts in social media",
            "On the origins of memes by means of fringe web communities",
            "Automatic prompt optimization with “gradient descent” and beam search",
            "Prevention, disruption and deterrence of online child sexual exploitation and abuse",
            "Anatomy of online hate: Developing a taxonomy and machine learning models for identifying and classifying hate in online news media",
            "A framework of severity for harmful content online",
            "Analyzing the targets of hate in online social media",
            "Sok: Content moderation in social media, from guidelines to enforcement, and research to practice",
            "Self-supervised euphemism detection and identification for content moderation"
        ]
    },
    {
        "index": 248,
        "title": "System-Level Defense against Indirect Prompt Injection Attacks: An Information Flow Control Perspective",
        "publication_date": "2024-10-10",
        "references": [
            "Apple Intelligence",
            "Introducting ChatGPT",
            "File system",
            "Gmail ToolKit",
            "langchain",
            "Langchain-Benchmark",
            "Typeletter - Multiple Tools",
            "Openai-Python-SDK",
            "OpenAI Plugins",
            "Relation Data",
            "Typeletter - Single Tool",
            "Secure computer system: Unified exposition and multics interpretation",
            "Nonmalleable information flow control",
            "Compositional security for reentrant applications",
            "Chateval: Towards better llm-based evaluators through multi-agent debate",
            "Struq: Defending against prompt injection with structured queries",
            "Dytan: a generic dynamic taint analysis framework",
            "A lattice model of secure information flow",
            "Security policies and security models",
            "Security policies and security models",
            "Chatgpt is not all you need. a state of the art review of large generative ai models",
            "Not what you’ve signed up for: Compromising real-world llm-integrated applications with indirect prompt injection",
            "Llm platform security: Applying a systematic evaluation framework to openai’s chatgpt plugins",
            "Smart-llm: Smart multi-agent robot task planning using large language models",
            "Software vulnerability analysis",
            "Personal llm agents: Insights and survey about the capability, efficiency and security",
            "Software vulnerability discovery techniques: A survey",
            "Agentbench: Evaluating llms as agents",
            "Prompt Injection attack against LLM-integrated Applications",
            "Prompt Injection Attacks and Defenses in LLM-Integrated Applications",
            "Jflow: Practical mostly-static information flow control",
            "A decentralized model for information flow control",
            "Language-based information-flow security",
            "Maat-phor: Automated Variant Analysis for Prompt Injection Attacks",
            "Llm-planner: Few-shot grounded planning for embodied agents with large language models",
            "The rise and potential of large language model based agents: A survey",
            "React: Synergizing reasoning and acting in language models",
            "Benchmarking and defending against indirect prompt injection attacks on large language models",
            "A Novel Evaluation Framework for Assessing Resilience Against Prompt Injection Attacks in Large Language Models",
            "Assessing Prompt Injection Risks in 200+ Custom GPTs",
            "Robust declascification",
            "Secure program partitioning",
            "Injecagent: Benchmarking indirect prompt injections in tool-integrated large language model agents",
            "Using replication and partitioning to build secure distributed systems",
            "The transport layer security (tls) protocol version 1.3",
            "A language for automatically enforcing privacy policies",
            "Plan-and-solve prompting: Improving zero-shot chain-of-thought reasoning by large language models",
            "A new era in llm security: Exploring security concerns in real-world llm-based systems",
            "Secgpt: An execution isolation architecture for llm-based systems",
            "The rise and potential of large language model based agents: A survey",
            "Re-woo: Decoupling reasoning from observations for efficient augmented language models",
            "A language for automatically enforcing privacy policies",
            "React: Synergizing reasoning and acting in language models",
            "Benchmarking and defending against indirect prompt injection attacks on large language models",
            "A Novel Evaluation Framework for Assessing Resilience Against Prompt Injection Attacks in Large Language Models",
            "Assessing Prompt Injection Risks in 200+ Custom GPTs",
            "Robust declascification",
            "Secure program partitioning",
            "Injecagent: Benchmarking indirect prompt injections in tool-integrated large language model agents",
            "Using replication and partitioning to build secure distributed systems",
            "The transport layer security (tls) protocol version 1.3",
            "A language for automatically enforcing privacy policies"
        ]
    },
    {
        "index": 249,
        "title": "Systematically Analyzing Prompt Injection Vulnerabilities in Diverse LLM Architectures",
        "publication_date": "2024-10-01",
        "references": [
            "Exploiting privacy vulnerabilities in open source LLMs using maliciously crafted prompts.",
            "Prompt injection attacks on large language models in oncology.",
            "A new era of learning: Considerations for ChatGPT as a tool to enhance statistics and data science education.",
            "Evaluation of common security vulnerabilities of state universities and colleges websites based on OWASP.",
            "Segment shards: Cross-prompt adversarial attacks against the segment anything model.",
            "Prompt injection attack against LLM-integrated applications.",
            "Impact of model size on fine-tuned LLM performance in data-to-text generation: A state-of-the-art investigation.",
            "LLM Prompt Injection.",
            "T1056.001: Keylogging.",
            "OWASP Top 10 for LLM Applications: Version 1.1.",
            "Evaluating prompt injections safety in large language models using the PromptBench dataset.",
            "Ignore this title and Hack A Prompt: Exposing systemic vulnerabilities of LLM through a global prompt hacking competition.",
            "TensorTrust: Interpretable prompt injection attacks from an online game.",
            "Defending ChatGPT against jailbreak attack via self-reminder.",
            "Exploring the use of ChatGPT in learning and instructing statistics and data analytics.",
            "TAPI: Towards target-specific and adversarial prompt injection against code LLMs.",
            "Goal-guided generative prompt injection attack on large language models.",
            "TrojanSQL: SQL injection against natural language interface to database.",
            "A study on prompt injection attack against LLM-integrated mobile robotics systems."
        ]
    },
    {
        "index": 250,
        "title": "The Better Angels of Machine Personality: How Personality Relates to LLM Safety",
        "publication_date": "2024-07-17",
        "references": [
            "Is cognition and action consistent or not: Investigating large language model’s personality.",
            "Culture and personality.",
            "Evidence on the homogeneity of personality traits within the auditing profession.",
            "A general language assistant as a laboratory for alignment.",
            "Differential privacy has disparate impact on model accuracy.",
            "Training a helpful and harmless assistant with reinforcement learning from human feedback.",
            "A meta-analysis of personality and workplace safety: addressing unanswered questions.",
            "Workplace safety: A review and research synthesis.",
            "The policy relevance of personality traits.",
            "Myers-Briggs type indicator.",
            "The myers-briggs type indicator and transformational leadership.",
            "The Importance of Agreeableness.",
            "Myers-briggs type indicator score reliability across: Studies a meta-analytic reliability generalization study.",
            "Performance, personality, and energetics: correlation, causation, and mechanism.",
            "Recent assessments of the myers-briggs type indicator.",
            "Evaluation metrics for language models.",
            "Does language affect personality perception? a functional approach to testing the whorfian hypothesis.",
            "Personality traits across cultures.",
            "A coefficient of agreement for nominal scales.",
            "Proposal for a regulation of the european parliament and of the council laying down harmonised rules on artificial intelligence (artificial intelligence act) and amending certain union legislative acts, pub. l. no. com(2021) 206 final.",
            "Ethics guidelines for trustworthy AI.",
            "Trait theories of personality.",
            "Machine mindset: An mbti exploration of large language models.",
            "Toxicity in chatgpt: Analyzing persona-assigned language models.",
            "Do personality tests generalize to large language models?",
            "Trait theory as personality theory: Can a part be as great as the whole?",
            "Manual of the Eysenck Personality Questionnaire (junior & adult).",
            "Catalogue of llm evaluations.",
            "A framework for few-shot language model evaluation.",
            "Personality, affect, and behavior in groups.",
            "Toxigen: A large-scale machine-generated dataset for implicit and adversarial hate speech detection.",
            "Afspp: Agent framework for shaping preference and personality with large language models.",
            "Is there a relationship between the myers-briggs type indicator and emotional intelligence?",
            "Multifaceted personality predictors of workplace safety performance: More than conscientiousness.",
            "An empirical study of metrics to measure representational harms in pre-trained language models.",
            "An empirical test of cognitive style and strategic decision outcomes.",
            "Who is chatgpt? benchmarking llms’ psychological portrayal using psychobench.",
            "Ai alignment: A comprehensive survey.",
            "Evaluating and inducing personality in pre-trained language models.",
            "Big five inventory.",
            "Personality traits across countries: Support for similarities rather than differences.",
            "What is personality?",
            "Inherent trade-offs in the fair determination of risk scores.",
            "Personality and safety citizenship: the role of safety motivation and safety knowledge.",
            "The relations between personality and language use.",
            "Rlaif: Scaling reinforcement learning from human feedback with ai feedback.",
            "Correlation and causation in the study of personality.",
            "Inference-time intervention: Eliciting truthful answers from a language model.",
            "Can multiple-choice questions really be useful in detecting the abilities of llms?",
            "Leveraging word guessing games to assess the intelligence of large language models.",
            "Trustworthy ai: A computational perspective.",
            "Trustworthy llms: a survey and guideline for evaluating large language models’ alignment.",
            "Exploring the sensitivity of LLMs’ decision-making capabilities: Insights from prompt variations and hyperparameters.",
            "Illuminating the black box: A psychometric investigation into the multifaceted nature of large language models.",
            "Codechameleon: Personalized encryption framework for jailbreaking large language models.",
            "Self-refine: Iterative refinement with self-feedback.",
            "Differential privacy has bounded impact on fairness in classification.",
            "Editing personality for llms.",
            "Joint factors in self-reports and ratings: Neuroticism, extraversion and openness to experience.",
            "Reinterpreting the myers-briggs type indicator from the perspective of the five-factor model of personality.",
            "Can llms keep a secret? testing privacy implications of language models via contextual integrity theory.",
            "A self-refinement strategy for noise reduction in grammatical error correction.",
            "MBTI manual: A guide to the development and use of the Myers-Briggs Type Indicator.",
            "Myers-Briggs Type Indicator: form M.",
            "StereoSet: Measuring stereotypical bias in pretrained language models.",
            "Personality and domain-specific risk taking.",
            "Revisiting the reliability of psychological scales on large language models.",
            "Characterchat: Learning towards conversational ai with personalized social support.",
            "Is personality modulated by language?",
            "Decodingtrust: A comprehensive assessment of trustworthiness in gpt models.",
            "Backdoor activation attack: Attack large language models using activation steering for safety-alignment.",
            "Large language models are not fair evaluators.",
            "Incharacter: Evaluating personality fidelity in role-playing agents through psychological interviews.",
            "Jailbroken: How does llm safety training fail?",
            "Controllm: Crafting diverse personalities for language models.",
            "Do changes in personality predict life outcomes?",
            "To be robust or to be fair: Towards fairness in adversarial training.",
            "Investigating the effects of personality on the safety behavior of gold mine workers: A moderated mediation approach.",
            "Shadow alignment: The ease of subverting safely-aligned language models.",
            "Gpt-4 is too smart to be safe: Stealthy chat with llms via cipher.",
            "Judging llm-as-a-judge with mt-bench and chatbot arena.",
            "Easyjailbreak: A unified framework for jailbreaking large language models.",
            "Fine-tuning language models from human preferences.",
            "Representation engineering: A top-down approach to ai transparency."
        ]
    },
    {
        "index": 252,
        "title": "The Instruction Hierarchy: Training LLMs to Prioritize Privileged Instructions",
        "publication_date": "2024-04-19",
        "references": [
            "A general language assistant as a laboratory for alignment",
            "StruQ: Defending against prompt injection with structured queries",
            "Introduction and overview of the Multics system",
            "Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned",
            "Coercing LLMs to do and reveal (almost) anything",
            "Gemini: A family of highly capable multimodal models",
            "Not what you’ve signed up for: Compromising real-world LLM-integrated applications with indirect prompt injection",
            "Prompt injection attack against LLM-integrated applications",
            "WebGPT: Browser-assisted question-answering with human feedback",
            "GPT-4 technical report",
            "Training language models to follow instructions with human feedback",
            "TALM: Tool augmented language models",
            "Red teaming language models with language models",
            "Ignore previous prompt: Attack techniques for language models",
            "The UNIX time-sharing system",
            "The essence of command injection attacks in web applications",
            "On automated prepared statement generation to remove SQL injection vulnerabilities",
            "Toolformer: Language models can teach themselves to use tools",
            "Ignore this title and HackAPrompt: Exposing systemic vulnerabilities of llms through a global scale prompt hacking competition",
            "HuggingGPT: Solving AI tasks with ChatGPT and its friends in hugging face",
            "Learning by distilling context",
            "Universal adversarial triggers for attacking and analyzing NLP",
            "Jailbroken: How does LLM safety training fail?",
            "LLM-powered autonomous agents",
            "Prompt injection attacks against GPT-3",
            "Multi-modal prompt injection image attacks against GPT-4V",
            "Benchmarking and defending against indirect prompt injection attacks on large language models",
            "Prompts should not be seen as secrets: Systematically measuring prompt extraction attack success",
            "Command injection — OWASP foundation",
            "Universal and transferable adversarial attacks on aligned language models",
            "Can LLMs separate instructions from data? And what do we even mean by that?"
        ]
    },
    {
        "index": 253,
        "title": "The VLLM Safety Paradox: Dual Ease in Jailbreak Attack and Defense",
        "publication_date": "2025-03-06",
        "references": [
            "Gpt-4v(ision) system card",
            "Hello gpt-4o",
            "Training a helpful and harmless assistant with reinforcement learning from human feedback",
            "Red-teaming large language models using chain of utterances for safety-alignment",
            "Safety-tuned llamas: Lessons from improving the safety of large language models that follow instructions",
            "Are aligned neural networks adversarially aligned?",
            "Jailbreakbench: An open robustness benchmark for jailbreaking large language models",
            "Minigpt-v2: large language model as a unified interface for vision-language multi-task learning",
            "UNK-VQA: A dataset and A probe into multi-modal large models' abstention ability",
            "Catastrophic jailbreak of open-source llms via exploiting generation",
            "Trustllm: Trustworthiness in large language models",
            "Llama guard: Llm-based input-output safeguard for human-ai conversations",
            "Mistral 7b",
            "Automatically auditing large language models via discrete optimization",
            "Challenges and applications of large language models",
            "Lora fine-tuning efficiently undoes safety training in llama 2-chat 70b",
            "Red teaming visual language models",
            "Microsoft COCO: common objects in context",
            "A survey of attacks on large vision-language models: Resources, advances, and future trends",
            "Improved baselines with visual instruction tuning",
            "Llava-next: Improved reasoning, ocr, and world knowledge",
            "Jailbreak attacks and defenses against multimodal generative models: A survey",
            "Mm-safetybench: A benchmark for safety evaluation of multimodal large language models",
            "Autojailbreak: Exploring jailbreak attacks and defenses through a dependency lens",
            "An image is worth 1000 lies: Transferability of adversarial images across prompts on vision-language models",
            "Jailbreakv-28k: A benchmark for assessing the robustness of multimodal large language models against jailbreak attacks",
            "Towards deep learning models resistant to adversarial attacks",
            "Rule based rewards for fine-grained LLM safety",
            "GPT-4 technical report",
            "Training language models to follow instructions with human feedback",
            "LLM improvement for jailbreak defense: Analysis through the lens of over-refusal",
            "Red teaming language models with language models",
            "Mllm-protector: Ensuring mllm's safety without hurting performance",
            "Visual adversarial examples jailbreak aligned large language models",
            "Fine-tuning aligned language models compromises safety, even when users do not intend to!",
            "High-resolution image synthesis with latent diffusion models",
            "On the adversarial robustness of multi-modal foundation models",
            "Ignore this title and hackaprompt: Exposing systemic vulnerabilities of llms through a global scale prompt hacking competition",
            "SPML: A DSL for defending language models against prompt attacks",
            "Jailbreak in pieces: Compositional adversarial attacks on multi-modal language models",
            "Hugginggpt: Solving AI tasks with chatgpt and its friends in hugging face",
            "Assessment of multimodal large language models in alignment with human values",
            "Qwen2.5: A party of foundation models",
            "Llama 2: Open foundation and fine-tuned chat models",
            "The instruction hierarchy: Training llms to prioritize privileged instructions",
            "Decodingtrust: A comprehensive assessment of trustworthiness in GPT models",
            "Qwen2-vl: Enhancing vision-language model's perception of the world at any resolution",
            "Inferaligner: Inference-time alignment for harmlessness through cross-model guidance",
            "White-box multimodal jailbreaks against large vision-language models",
            "Cogvlm: Visual expert for pretrained language models",
            "Adashield: Safeguarding multimodal large language models from structure-based attack via adaptive shield prompting",
            "Jailbreaking GPT-4V via self-adversarial attacks with system prompts",
            "Jailbreak attacks and defenses against large language models: A survey",
            "GPT-FUZZER: red teaming large language models with auto-generated jailbreak prompts",
            "Mm-vet: Evaluating large multimodal models for integrated capabilities",
            "GPT-4 is too smart to be safe: Stealthy chat with llms via cipher",
            "Removing RLHF protections in GPT-4 via fine-tuning",
            "Make them spill the beans! coercive knowledge extraction from (production) llms",
            "On evaluating adversarial robustness of large vision-language models",
            "Judging llm-as-a-judge with mt-bench and chatbot arena",
            "Autodan: Interpretable gradient-based adversarial attacks on large language models",
            "Safety fine-tuning at (almost) no cost: A baseline for vision large language models",
            "Universal and transferable adversarial attacks on aligned language models",
            "Is the system message really important to jailbreaks in large language models?"
        ]
    },
    {
        "index": 254,
        "title": "Towards Action Hijacking of Large Language Model-based Agent",
        "publication_date": "2024-12-14",
        "references": [
            "The rise and potential of large language model based agents: A survey",
            "A survey on large language model based autonomous agents",
            "React: Synergizing reasoning and acting in language models",
            "A survey on rag meeting llms: Towards retrieval-augmented large language models",
            "Exploring large language model based intelligent agents: Definitions, methods, and prospects",
            "Fundamental capabilities of large language models and their applications in domain scenarios: A survey",
            "Conversational health agents: A personalized llm-powered agent framework",
            "Large language models in finance: A survey",
            "Bots with feelings: Should ai agents express positive emotion in customer service?",
            "Osagent: Copiloting operating system with llm-based agent",
            "Agent security bench (asb): Formalizing and benchmarking attacks and defenses in llm-based agents",
            "Automatic chain of thought prompting in large language models",
            "Tree of thoughts: Deliberate problem solving with large language models",
            "Tptu: Task planning and tool usage of large language model-based ai agents",
            "Collm: Integrating collaborative embeddings into large language models for recommendation",
            "Mind2web: Towards a generalist agent for the web",
            "AI agents under threat: A survey of key security challenges and future pathways",
            "InjecAgent: Benchmarking indirect prompt injections in tool-integrated large language model agents",
            "The good and the bad: Exploring privacy issues in retrieval-augmented generation (RAG)",
            "Large language models can be easily distracted by irrelevant context",
            "Ignore previous prompt: Attack techniques for language models",
            "POSTER: identifying and mitigating vulnerabilities in llm-integrated applications",
            "Tensor trust: Interpretable prompt injection attacks from an online game",
            "Agent smith: A single image can jailbreak one million multimodal LLM agents exponentially fast",
            "Multi-step jailbreaking privacy attacks on chatgpt",
            "Sneakyprompt: Jailbreaking text-to-image generative models",
            "Jailbroken: How does llm safety training fail?",
            "Mma-diffusion: Multimodal attack on diffusion models",
            "GPT-4 is too smart to be safe: Stealthy chat with llms via cipher",
            "Jailbreak attacks and defenses against large language models: A survey",
            "Llm guard",
            "Ai agents under threat: A survey of key security challenges and future pathways",
            "Baseline defenses for adversarial attacks against aligned language models",
            "From prompt injections to SQL injection attacks: How protected is your llm-integrated web application?",
            "Using gpt-eliezer against chatgpt jailbreaking",
            "Return-oriented programming: Systems, languages, and applications",
            "Undefined-oriented programming: Detecting and chaining prototype pollution gadgets in node.js template engines for malicious consequences",
            "Detecting rop with statistical learning of program characteristics",
            "Pleak: Prompt leaking attacks against large language model applications",
            "Clustergan: Latent space clustering in generative adversarial networks",
            "Generative agents: Interactive simulacra of human behavior",
            "Query rewriting via large language models",
            "Poisonedrag: Knowledge corruption attacks to retrieval-augmented generation of large language models",
            "Agentpoison: Red-teaming llm agents via poisoning memory or knowledge bases",
            "Poisonedrag: Knowledge poisoning attacks to retrieval-augmented generation of large language models",
            "WIPI: A new web threat for llm-driven web agents",
            "Benchmarking and defending against indirect prompt injection attacks on large language models",
            "Not what you’ve signed up for: Compromising real-world llm-integrated applications with indirect prompt injection",
            "Semantic-guided prompt organization for universal goal hijacking against llms",
            "Hijacking large language models via adversarial in-context learning",
            "Hijacking context in large multi-modal models",
            "From prompt injections to sql injection attacks: How protected is your llm-integrated web application?",
            "Prompt injection attacks against gpt-3",
            "Delimiters won’t save you from prompt injection",
            "Don’t listen to me: Understanding and exploring jailbreak prompts of large language models",
            "Exploiting programmatic behavior of llms: Dual-use through standard security attacks",
            "Multilingual jailbreak challenges in large language models",
            "Llm lies: Hallucinations are not bugs, but features as adversarial examples",
            "Sok: Prompt hacking of large language models",
            "Redagent: Red teaming large language models with context-aware autonomous language agent",
            "Curiosity-driven red-teaming for large language models",
            "Multisql: A schema-integrated context-dependent text2sql dataset with diverse sql operations",
            "Synthetic-Text-To-SQL: A synthetic dataset for training language models to generate sql queries from natural language prompts",
            "Llama 2: Open foundation and fine-tuned chat models",
            "Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality",
            "Qwen technical report",
            "Pandalm: An automatic evaluation benchmark for llm instruction tuning optimization",
            "Training language models to follow instructions with human feedback",
            "Gpt-4 technical report",
            "Minilm: Deep self-attention distillation for task-agnostic compression of pre-trained transformers",
            "M3e: Moka massive mixed embedding model",
            "Gpt-4 vs. gpt-3.5: A concise showdown",
            "Attacks, defenses and evaluations for LLM conversation safety: A survey",
            "Bert: Pre-training of deep bidirectional transformers for language understanding",
            "Securing llm systems against prompt injection",
            "Formalizing and benchmarking prompt injection attacks and defenses"
        ]
    },
    {
        "index": 256,
        "title": "Towards Understanding Unsafe Video Generation",
        "publication_date": "2024-07-17",
        "references": [
            "Gpt-4 technical report",
            "Stable video diffusion: Scaling latent video diffusion models to large datasets",
            "Using thematic analysis in psychology",
            "High-resolution image synthesis with latent diffusion models",
            "Seine: Short-to-long video diffusion model for generative transition and prediction",
            "Fatezero: Fusing attentions for zero-shot text-based video editing",
            "Unsafe diffusion: On the generation of unsafe images and hateful memes from text-to-image models",
            "Videomae: Masked autoencoders are data-efficient learners for self-supervised video pre-training",
            "Denoising diffusion probabilistic models",
            "Video diffusion models",
            "One-dimensional adapter to rule them all: Concepts diffusion models and erasing applications",
            "Improved denoising diffusion probabilistic models",
            "Vgmshield: Mitigating misuse of video generative models",
            "Scalable diffusion models with transformers",
            "Bivd-diff: A training-free framework for general-purpose video synthesis via bridging image and video diffusion models",
            "Synthetic image detection: Highlights from the ieee video and image processing cup 2022 student competition",
            "On the detection of digital face manipulation",
            "Adversarial perturbations fool deepfake detectors",
            "Learning transferable visual models from natural language supervision",
            "Hierarchical text-conditional image generation with clip latents",
            "De-fake: Detection and attribution of fake images generated by text-to-image generation models",
            "Glaze: Protecting artists from style mimicry by {Text-to-Image } models",
            "Show-1: Marrying pixel and latent diffusion models for text-to-video generation",
            "I2vgen-xl: High-quality image-to-video synthesis via cascaded diffusion models",
            "Controlvideo: Training-free controllable text-to-video generation"
        ]
    },
    {
        "index": 257,
        "title": "Continual Learning Jailbreak Perturbation Patterns for Toxicity Detection",
        "publication_date": "2024-12-17",
        "references": [
            "The nature of cyberbullying, and strategies for prevention.",
            "Cyberbullying.",
            "Hate me, hate me not: Hate speech detection on facebook.",
            "A survey on automatic detection of hate speech in text.",
            "Towards understanding and mitigating social biases in language models.",
            "Detection and moderation of detrimental content on social media platforms: current status and future directions.",
            "Certifying and removing disparate impact.",
            "Measuring and mitigating unintended bias in text classification.",
            "A holistic approach to undesired content detection in the real world.",
            "Persian offensive language detection.",
            "Efficient toxic content detection by bootstrapping and distilling large language models.",
            "Umuteam at semeval-2024 task 8: Combining transformers and syntax features for machine-generated text detection.",
            "Don’t go to extremes: Revealing the excessive sensitivity and calibration limitations of llms in implicit hate speech detection.",
            "Toxicity detection in online georgian discussions.",
            "Handling bias in toxic speech detection: A survey.",
            "Spanning the spectrum of hatred detection: a persian multi-label hate speech dataset with annotator rationales.",
            "Social bias frames: Reasoning about social and power implications of language.",
            "Improving the detection of multilingual online attacks with rich social media data from singapore.",
            "Facilitating fine-grained detection of chinese toxic language: Hierarchical taxonomy, resources, and benchmarks.",
            "Toxigen: A large-scale machine-generated dataset for adversarial and implicit hate speech detection.",
            "Perturbations in the wild: Leveraging human-written text perturbations for realistic adversarial attack and defense.",
            "Towards building a robust toxicity predictor.",
            "Noisyhate: Benchmarking content moderation machine learning models with human-written perturbations online.",
            "Towards robust toxic content classification.",
            "Hiding in plain sight: Tweets with hate speech masked by homoglyphs.",
            "Cyberbullying classifiers are sensitive to model-agnostic perturbations.",
            "Don’t be a fool: Pooling strategies in offensive language detection from user-intended adversarial attacks.",
            "An empirical investigation of catastrophic forgetting in gradient-based neural networks."
        ]
    },
    {
        "index": 258,
        "title": "TRAINING SOCIALLY ALIGNED LANGUAGE MODELS INSIMULATED HUMAN SOCIETY",
        "publication_date": "2023-12-31",
        "references": [
            "Using large language models to simulate multiple humans and replicate human subject studies",
            "Language models as agent models",
            "Out of one, many: Using language models to simulate human samples",
            "A general language assistant as a laboratory for alignment",
            "Constitutional ai: Harmlessness from ai feedback",
            "On the opportunities and risks of foundation models",
            "Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality",
            "Deep reinforcement learning from human preferences",
            "Moral stories: Situated reasoning about norms, intents, actions, and their consequences",
            "Reward tampering problems and solutions in reinforcement learning: A causal influence diagram perspective",
            "Artificial intelligence, values, and alignment",
            "Scaling laws for reward model overoptimization",
            "SimCSE: Simple contrastive learning of sentence embeddings",
            "RealToxicityPrompts: Evaluating neural toxic degeneration in language models",
            "Chatgpt outperforms crowd-workers for text-annotation tasks",
            "Improving alignment of dialogue agents via targeted human judgements",
            "Problems of monetary management: the UK experience",
            "Alignment of language agents",
            "Siamese neural networks for one-shot image recognition",
            "Avoiding side effects by considering future tasks",
            "Socially situated artificial intelligence enables learning from human interaction",
            "Rlaif: Scaling reinforcement learning from human feedback with ai feedback",
            "A human blueprint for ai coexistence.",
            "The surprising creativity of digital evolution: A collection of anecdotes from the evolutionary computation and artificial life research communities",
            "Ai safety gridworlds",
            "Scalable agent alignment via reward modeling: a research direction",
            "TruthfulQA: Measuring how models mimic human falsehoods",
            "Chain of hindsight aligns language models with feedback",
            "Mitigating political bias in language models through reinforced calibration",
            "Quantifying and alleviating political bias in language models",
            "Training language models to follow instructions with human feedback",
            "The effects of reward misspecification: Maping and mitigating misaligned models",
            "Social simulacra: Creating populated prototypes for social computing systems",
            "Generative agents: Interactive simulacra of human behavior",
            "Transformative experience",
            "Choosing for changing selves",
            "Scaling language models: Methods, analysis & insights from training gopher",
            "Direct preference optimization: Your language model is secretly a reward model",
            "Loss functions for preference levels: Regression with discrete ordered labels",
            "Self-critiquing models for assisting human evaluators",
            "Training language models with language feedback at scale",
            "Facenet: A unified embedding for face recognition and clustering",
            "The curse of recursion: Training on generated data makes models forget",
            "Ml systems will have weird failure modes",
            "Understanding the capabilities, limitations, and societal impact of large language models",
            "Stanford alpaca: An instruction-following llama model",
            "Aligning AI with shared human values",
            "Llama: Open and efficient foundation language models",
            "Llama 2: Open foundation and fine-tuned chat models",
            "A study of implicit bias in pre-trained language models against people with disabilities",
            "Transformer reinforcement learning x",
            "Self-instruct: Aligning language model with self generated instructions",
            "Measuring massive multitask language understanding",
            "Star: Bootstrapping reasoning with reasoning",
            "Lima: Less is more for alignment",
            "Fine-tuning language models from human preferences"
        ]
    },
    {
        "index": 259,
        "title": "Transferable Ensemble Black-box Jailbreak Attacks on Large Language Models",
        "publication_date": "2024-11-28",
        "references": [
            "do anything now\": Characterizing and evaluating in-the-wild jailbreak prompts on large language models",
            "Jailbreak attacks and defenses against large language models: A survey",
            "Universal and transferable adversarial attacks on aligned language models",
            "Autodan: Generating stealthy jailbreak prompts on aligned large language models",
            "Gptfuzzer: Red teaming large language models with auto-generated jailbreak prompts",
            "Llms are in-context reinforcement learners",
            "How johnny can persuade llms to jailbreak them: Rethinking persuasion to challenge ai safety by humanizing llms",
            "Tree of attacks: Jailbreaking black-box llms automatically",
            "On prompt-driven safeguarding for large language models",
            "Towards understanding jailbreak attacks in llms: A representation space analysis"
        ]
    },
    {
        "index": 260,
        "title": "Unbridled Icarus: A Survey of the Potential Perils of Image Inputs in Multimodal Large Language Model Security",
        "publication_date": "2024-08-11",
        "references": [
            "Ai alignment: A comprehensive survey",
            "A learning-based incentive mechanism for mobile aigc service in decentralized internet of vehicles",
            "Jailbreak in pieces: Compositional adversarial attacks on multi-modal language models",
            "Images are achilles’ heel of alignment: Exploiting visual vulnerabilities for jailbreaking multimodal large language models",
            "Figstep: Jailbreaking large vision-language models via typographic visual prompts",
            "On evaluating adversarial robustness of large vision-language models",
            "Survey of vulnerabilities in large language models revealed by adversarial attacks",
            "A survey of safety and trustworthiness of large language models through the lens of verification and validation",
            "Risk taxonomy, mitigation, and assessment benchmarks of large language model systems",
            "Mm-llms: Recent advances in multimodal large language models",
            "High-resolution image synthesis with latent diffusion models",
            "On the essence and prospect: An investigation of alignment approaches for big models",
            "On the adversarial robustness of multi-modal foundation models",
            "Visual adversarial examples jailbreak aligned large language models",
            "An image is worth 1000 lies: Adversarial transferability across prompts on vision-language models",
            "Image hijacks: Adversarial images can control generative models at runtime",
            "(ab) using images and sounds for indirect instruction injection in multi-modal lllms",
            "Stop reasoning! when multimodal llms with chain-of-thought reasoning meets adversarial images",
            "Test-time backdoor attacks on multimodal large language models",
            "Agent smith: A single image can jailbreak one million multimodal llm agents exponentially fast",
            "The wolf within: Covert injection of malice into mllm societies via an mllm operative",
            "Vision-llms can fool themselves with self-generated typographic attacks",
            "Jailbreaking gpt-4v via self-adversarial attacks with system prompts",
            "How robust is google’s bard to adversarial image attacks?",
            "Instructta: Instruction-tuned targeted attack for large vision-language models",
            "Ceci n’est pas une pomme: Adversarial illusions in multi-modal embeddings",
            "Ot-attack: Enhancing adversarial transferability of vision-language models via optimal transport optimization",
            "Jailbreaking attack against multimodal large language model",
            "Imgtrojan: Jailbreaking vision-language models with one image",
            "Shadowcast: Stealthy data poisoning attacks against vision-language models",
            "Vl-trojan: Multimodal instruction backdoor attacks against autoregressive visual language models",
            "Image to prompt injection with google bard",
            "Towards adversarial attack on vision-language pre-training models",
            "Set-level guidance attack: Boosting adversarial transferability of vision-language pre-training models",
            "Advclip: Downstream-agnostic adversarial examples in multimodal contrastive learning",
            "Vlattack: Multimodal adversarial attacks on vision-language tasks via pre-trained models",
            "Exploring transferability of multimodal adversarial samples for vision-language pre-training models with contrastive learning",
            "Sa-attack: Improving adversarial transferability of vision-language pre-training models via self-augmentation",
            "Random gradient-free minimization of convex functions",
            "Jailbreaking black box large language models in twenty queries",
            "Invisible backdoor attacks on deep neural networks via steganography and regularization",
            "Hidden backdoors in human-centric language models",
            "Red teaming visual language models",
            "Robust contrastive language-image pretraining against data poisoning and backdoor attacks",
            "Adversarial prompt tuning for vision-language models",
            "One prompt word is enough to boost adversarial robustness for pre-trained vision-language models",
            "Dress: Instructing large vision-language models to align and interact with humans via natural language feedback",
            "A mutation-based method for multi-modal jailbreaking attack detection",
            "Mllm-protector: Ensuring mllm’s safety without hurting performance",
            "Inferaligner: Inference-time alignment for harmlessness through cross-model guidance",
            "Adashield: Safeguarding multimodal large language models from structure-based attack via adaptive shield prompting",
            "Eyes closed, safety on: Protecting multimodal llms via image-to-text transformation",
            "Right to be forgotten in the era of large language models: Implications, challenges, and solutions",
            "Eight methods to evaluate robust unlearning in llms",
            "Differential privacy",
            "Privacy-enhanced knowledge transfer with collaborative split learning over teacher ensembles",
            "Bounded and unbiased composite differential privacy",
            "A comprehensive survey of privacy-preserving federated learning: A taxonomy, review, and future directions",
            "Dynamic user clustering for efficient and privacy-preserving federated learning",
            "Long-term privacy-preserving aggregation with user-dynamics for federated learning",
            "A duty to forget, a right to be assured? exposing vulnerabilities in machine unlearning services",
            "Learn what you want to unlearn: Unlearning inversion attacks against machine unlearning",
            "Threats, attacks, and defenses in machine unlearning: A survey",
            "Towards efficient and certified recovery from poisoning attacks in federated learning",
            "A survey on federated unlearning: Challenges, methods, and future directions",
            "Rlaif: Scaling reinforcement learning from human feedback with ai feedback",
            "Knowledge editing for large language models: A survey",
            "Editing conceptual knowledge for large language models",
            "Detoxifying large language models via knowledge editing",
            "The first to know: How token distributions reveal hidden knowledge in large vision-language models?"
        ]
    },
    {
        "index": 261,
        "title": "Uncertainty-Guided Modal Rebalance for Hateful Memes Detection",
        "publication_date": "2024-08-11",
        "references": [
            "Multimodal word distributions.",
            "Procap: Leveraging a frozen vision-language model for hateful meme detection.",
            "Prompting for multimodal hateful meme classification.",
            "Data uncertainty learning in face recognition.",
            "Probabilistic embeddings for cross-modal retrieval.",
            "Detecting hate speech in multi-modal memes.",
            "Arcface: Additive angular margin loss for deep face recognition.",
            "Bert: Pre-training of deep bidirectional transformers for language understanding.",
            "An image is worth 16x16 words: Transformers for image recognition at scale.",
            "Gaussian error linear units (gelus).",
            "Supervised multimodal bitransformers for classifying images and text.",
            "The hateful memes challenge: detecting hate speech in multimodal memes.",
            "Simple and scalable predictive uncertainty estimation using deep ensembles.",
            "Disentangling hate in online memes.",
            "Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models.",
            "Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation.",
            "Align before fuse: Vision and language representation learning with momentum distillation.",
            "Visualbert: A simple and performant baseline for vision and language.",
            "Smoothing the geometry of probabilistic box embeddings.",
            "A multimodal framework for the detection of hateful memes.",
            "Deep hyperspherical learning.",
            "Vilbert: pretraining task-agnostic visiolinguistic representations for vision-and-language tasks.",
            "Vilio: state-of-the-art visio-linguistic models applied to hateful memes.",
            "Detecting harmful memes and their targets.",
            "Momenta: A multimodal framework for detecting harmful memes and their targets.",
            "Learning transferable visual models from natural language supervision.",
            "L2-constrained softmax loss for discriminative face verification.",
            "Detecting hateful memes using a multimodal deep ensemble.",
            "Grad-cam: Visual explanations from deep networks via gradient-based localization.",
            "Cubemlp: An mlp-based model for multimodal sentiment analysis and depression estimation.",
            "View-invariant probabilistic embedding for human pose.",
            "Multimodal meme dataset (multioff) for identifying offensive content in image and text.",
            "Mlp-mixer: An all-mlp architecture for vision.",
            "Resmlp: Feedforward networks for image classification with data-efficient training.",
            "Probvlm: Probabilistic adapter for frozen vison-language models.",
            "Detecting hate speech in memes using multimodal deep learning approaches: Prize-winning solution to hateful memes challenge.",
            "Word representations via gaussian embedding.",
            "Cosface: Large margin cosine loss for deep face recognition.",
            "Mmcosine: Multi-modal cosine loss towards balanced audio-visual fine-grained learning.",
            "Invariant meets specific: A scalable harmful memes detection framework.",
            "Multimodal hate speech detection via cross-domain knowledge transfer.",
            "Ernie-vil: Knowledge enhanced vision-language representations through scene graphs.",
            "Robust person re-identification by modelling feature uncertainty.",
            "Tot: topology-aware optimal transport for multimodal hate detection."
        ]
    },
    {
        "index": 263,
        "title": "Universal Adversarial Triggers Are Not Universal",
        "publication_date": "2024-04-24",
        "references": [
            "Jailbreaking Leading Safety-Aligned LLMs with Simple Adaptive Attacks",
            "Many-shot Jailbreaking",
            "Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback",
            "QLoRA: Efficient Finetuning of Quantized LLMs",
            "GPT-4 Technical Report",
            "Language models are few-shot learners",
            "Decoupled Weight Decay Regularization",
            "HarmBench: A Standardized Evaluation Framework for Automated Red Teaming and Robust Refusal",
            "LLM Self Defense: By Self Examination, LLMs Know They Are Being Tricked",
            "Stanford Alpaca: An Instruction-following LLaMA model",
            "Starling-7B: Improving LLM Helpfulness & Harmlessness with RLAIF",
            "Fine-Tuning Language Models from Human Preferences",
            "LIMA: Less Is More for Alignment"
        ]
    },
    {
        "index": 264,
        "title": "Universal And Context-Independent Triggers For Precise Control Of LLM Outputs",
        "publication_date": "2024-11-22",
        "references": [
            "GPT-4 Technical Report",
            "An Alternative Softmax Operator for Reinforcement Learning",
            "Jailbreaking Black Box Large Language Models in Twenty Queries",
            "The Llama 3 Herd of Models",
            "HotFlip: White-Box Adversarial Examples for Text Classification",
            "Imprompter: Tricking LLM Agents into Improper Tool Use",
            "Retrieval-Augmented Generation for Large Language Models: A Survey",
            "Coercing LLMs to do and reveal (almost) anything",
            "Gradient-based Adversarial Attacks against Text Transformers",
            "Query-based Adversarial Prompt Generation",
            "PLeak: Prompt Leaking Attacks against Large Language Model Applications",
            "A Survey on Large Language Models for Code Generation",
            "Automatically Auditing Large Language Models via Discrete Optimization",
            "Making a SOTA Adversarial Attack on LLMs 38x Faster",
            "Rouge: A Package for Automatic Evaluation of Summaries",
            "Lost in the Middle: How Language Models Use Long Contexts",
            "AutoDAN-Turbo: A Lifelong Agent for Strategy Self-Exploration to Jailbreak LLMs",
            "Biases in Large Language Models: Origins, Inventory, and Discussion",
            "The OIG Dataset",
            "AdvPrompter: Fast Adaptive Adversarial Prompting for LLMs",
            "Instruction Tuning with GPT-4",
            "AutoGPT",
            "Jailbreaking LLM-Controlled Robots",
            "Mathematical discoveries from program search with large language models",
            "Rainbow Teaming: Open-Ended Generation of Diverse Adversarial Prompts",
            "The Language Barrier: Dissecting Safety Challenges of LLMs in Multilingual Contexts",
            "AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts",
            "StructuredRAG: JSON Response Formatting with Large Language Models",
            "Takeaways from the NeurIPS 2023 Trojan Detection Competition",
            "Role-Break: Character Hallucination as a Jailbreak Attack in Role-Playing Systems",
            "Solving olympiad geometry without human demonstrations",
            "Adversarial Attacks on LLMs",
            "AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation",
            "An LLM can Fool Itself: A Prompt-Based Adversarial Attack",
            "Qwen2 Technical Report",
            "GPTFUZZER: Red Teaming Large Language Models with Auto-Generated Jailbreak Prompts",
            "Uncertainty is Fragile: Manipulating Uncertainty in Large Language Models",
            "How Johnny Can Persuade LLMs to Jailbreak Them: Rethinking Persuasion to Challenge AI Safety by Humanizing LLMs",
            "AutoDAN: Interpretable Gradient-Based Adversarial Attacks on Large Language Models",
            "Universal and Transferable Adversarial Attacks on Aligned Language Models"
        ]
    },
    {
        "index": 265,
        "title": "Unleashing the Unseen: Harnessing Benign Datasets for Jailbreaking Large Language Models",
        "publication_date": "2024-12-19",
        "references": [
            "Gpt-4 technical report",
            "An introduction to multivariate statistical analysis (wiley series in probability and statistics)",
            "Training a helpful and harmless assistant with reinforcement learning from human feedback",
            "Jailbreaking black box large language models in twenty queries",
            "Talking nonsense: Probing large language models’ understanding of adversarial gibberish inputs",
            "Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality",
            "Masterkey: Automated jailbreak across multiple large language model chatbots",
            "Improving alignment of dialogue agents via targeted human judgements",
            "Query-based adversarial prompt generation",
            "What’s in your\" safe\" data?: Identifying benign data that breaks safety",
            "Catastrophic jailbreak of open-source LLMs via exploiting generation",
            "Adversarial examples are not bugs, they are features",
            "Mistral 7b",
            "Deepinception: Hypnotize large language model to be jailbreaker",
            "Amplegcg: Learning a universal and transferable generative model of adversarial suffixes for jailbreaking both open and closed llms",
            "Autodan: Generating stealthy jailbreak prompts on aligned large language models",
            "Generating stealthy jailbreak prompts on aligned large language models",
            "Training language models to follow instructions with human feedback",
            "Future lens: Anticipating subsequent tokens from a single hidden state",
            "Fine-tuning aligned language models compromises safety, even when users do not intend to!",
            "Traceback of targeted data poisoning attacks in neural networks",
            "Characterizing and evaluating in-the-wild jailbreak prompts on large language models",
            "Targeted latent adversarial training improves robustness to persistent harmful behaviors in llms",
            "On the exploitability of instruction tuning",
            "Asetf: A novel method for jailbreak attack on llms through translate suffix embeddings",
            "Lima: Less is more for alignment",
            "Universal and transferable adversarial attacks on aligned language models"
        ]
    },
    {
        "index": 266,
        "title": "Unleashing Worms and Extracting Data: Escalating the Outcome of Attacks against RAG-based Inference in Scale and Severity Using Jailbreaking",
        "publication_date": "2024-09-12",
        "references": [
            "Retrieval-augmented generation for knowledge-intensive nlp tasks",
            "Is my data in your retrieval database? membership inference attacks against retrieval augmented generation",
            "Seeing is believing: Black-box membership inference attacks against retrieval augmented generation",
            "The good and the bad: Exploring privacy issues in retrieval-augmented generation (rag)",
            "Badrag: Identifying vulnerabilities in retrieval augmented generation of large language models",
            "Trojanrag: Retrieval-augmented generation can be backdoor driver in large language models",
            "Poisonedrag: Knowledge poisoning attacks to retrieval-augmented generation of large language models",
            "Machine against the rag: Jamming retrieval-augmented generation with blocker documents",
            "Phantom: General trigger attacks on retrieval augmented language generation",
            " do anything now\": Characterizing and evaluating in-the-wild jailbreak prompts on large language models",
            "Don't listen to me: Understanding and exploring jailbreak prompts of large language models",
            "Universal and transferable adversarial attacks on aligned language models",
            "Sneakyprompt: Jailbreaking text-to-image generative models",
            "Low-resource languages jailbreak gpt-4",
            "Autodan: Generating stealthy jailbreak prompts on aligned large language models",
            "Jailbroken: How does llm safety training fail?",
            "Query-based adversarial prompt generation",
            "Chatdoctor: A medical chat model fine-tuned on a large language model meta-ai (llama) using medical domain knowledge",
            "The enron corpus: A new dataset for email classification research",
            "Mpnet: Masked and permuted pre-training for language understanding",
            "Prompts should not be seen as secrets: Systematically measuring prompt extraction attack success",
            "Pleak: Prompt leaking attacks against large language model applications",
            "Prompt stealing attacks against large language models",
            "Language model inversion",
            "Extracting training data from large language models",
            "Membership inference attacks against machine learning models",
            "Model inversion attacks that exploit confidence information and basic countermeasures",
            "Recent worms: a survey and trends",
            "A taxonomy of computer worms",
            "Computer worms: Architectures, evasion strategies, and detection mechanisms",
            "The “worm” programs—early experience with a distributed computation",
            "The morris worm",
            "The morris worm: A fifteen-year perspective",
            "The morris worm. 1988",
            "Analysis of the iloveyou worm",
            "Iloveyou virus lessons learned report",
            "W32. stuxnet dossier",
            "The real story of stuxnet",
            "Stuxnet under the microscope",
            "Understanding the mirai botnet",
            "Wannacry ransomware: Analysis of infection, persistence, recovery prevention and propagation mechanisms",
            "Automated behavioral analysis of malware: A case study of wannacry ransomware",
            "The dynamic analysis of wannacry ransomware",
            "The static analysis of wannacry ransomware",
            "Towards general text embeddings with multi-stage contrastive learning",
            "Nomic embed: Training a reproducible long context text embedder",
            "Apple brings chatgpt to its apps, including siri",
            "Amazon upgrades alexa for the chatgpt era",
            "Summary zenity research published blackhat",
            "Gecko: Versatile text embeddings distilled from large language models"
        ]
    },
    {
        "index": 267,
        "title": "Unlocking Adversarial Suffix Optimization Without Affirmative Phrases: Efficient Black-box Jailbreaking via LLM as Optimizer",
        "publication_date": "2024-08-21",
        "references": [
            "Openai api.",
            "Introducing the next generation of claude.",
            "Gemini.",
            "Toolqa: A dataset for llm question answering with external tools.",
            "Codegeex: A pre-trained model for code generation with multilingual evaluations on humaneval-x.",
            "Large language models as commonsense knowledge for large-scale task planning.",
            "Selection-inference: Exploiting large language models for interpretable logical reasoning.",
            "Jailbreaker: Automated jailbreak across multiple large language model chatbots.",
            "Gptfuzzer: Red teaming large language models with auto-generated jailbreak prompts.",
            "Deepinception: Hypnotize large language model to be jailbreaker.",
            "Universal and transferable adversarial attacks on aligned language models, 2023.",
            "Jailbreaking black box large language models in twenty queries.",
            "Rapid optimization for jailbreaking llms via subconscious exploitation and echopraxia.",
            "Autodan: Generating stealthy jailbreak prompts on aligned large language models.",
            "Large language models as optimizers.",
            "Red-teaming large language models using chain of utterances for safety-alignment.",
            "Gpt-4 is too smart to be safe: Stealthy chat with llms via cipher.",
            "Low-resource languages jailbreak gpt-4.",
            "Piccolo: Exposing complex backdoors in nlp transformer models.",
            "Hard prompts made easy: Gradient-based discrete optimization for prompt tuning and discovery.",
            "Gradient-based adversarial attacks against text transformers.",
            "Amplegcg: Learning a universal and transferable generative model of adversarial suffixes for jailbreaking both open and closed llms.",
            "Researchers poke holes in safety controls of chatgpt and other chatbots.",
            "Position paper: Leveraging foundational models for black-box optimization: Benefits, challenges, and future directions.",
            "Large language models as evolution strategies.",
            "Importance of directional feedback for llm-based optimizers.",
            "Are large language models good prompt optimizers?",
            "Large language model can interpret latent space of sequential recommender.",
            "Catastrophic jailbreak of open-source llms via exploiting generation.",
            "Fine-tuning aligned language models compromises safety, even when users do not intend to!",
            "Llm jailbreak attack versus defense techniques–a comprehensive study.",
            "Llama 2: Open foundation and fine-tuned chat models.",
            "Vicuna: An open-source chatbot rivaling gpt-4 with 90% chatgpt quality.",
            "The falcon series of open language models.",
            "A mutation-based method for multi-modal jailbreaking attack detection.",
            "Jailbroken: How does llm safety training fail?",
            "Certifying llm safety against adversarial prompting.",
            "Beavertails: Towards improved safety alignment of llm via a human-preference dataset.",
            "Evil geniuses: Delving into the safety of llm-based agents.",
            "On prompt-driven safeguarding for large language models.",
            "Exploring safety generalization challenges of large language models via code.",
            "Robustness over time: Understanding adversarial examples’ effectiveness on longitudinal versions of large language models."
        ]
    },
    {
        "index": 268,
        "title": "UnsafeBench: Benchmarking Image Safety Classifiers on Real-World and AI-Generated Images",
        "publication_date": "2024-09-05",
        "references": [
            "4chan.",
            "Deepfake Explicit Images of Taylor Swift.",
            "Google’s SafeSearch API.",
            "GPT-4V.",
            "Happy Merchant Meme.",
            "LAION-2B.",
            "LAION-5B.",
            "LAION-AI.",
            "Lexica Dataset.",
            "Microsoft’s Image Moderation API.",
            "Midjourney.",
            "MultiHeaded Dataset.",
            "Neo-Nazi Symbol.",
            "NSFW_Detector.",
            "NudeNet.",
            "NudeNet Classifier Dataset v1.",
            "OpenAI Content Policy.",
            "Reddit.",
            "Safety Filter in Stable Diffusion.",
            "Scar Images from Roboflow.",
            "Self-Hanging Images from Roboflow.",
            "SMID Dataset.",
            "Unsafe Concepts of the Built-in Safety Filter in Stable Diffusion.",
            "Vicuna.",
            "Violent Behavior Images from Roboflow.",
            "Unsafe Concepts of the Built-in Safety Filter in Stable Diffusion.",
            "A Friendly Face: Do Text-to-Image Systems Rely on Stereotypes when the Input is Under-Specified?",
            "Erasing Concepts from Diffusion Models.",
            "Understanding and Detecting Hateful Content using Contrastive Learning.",
            "Explaining and Harnessing Adversarial Examples.",
            "Moderating Illicit Online Image Promotion for Unsafe User-Generated Content Games Using Large Vision-Language Models.",
            "Optimizing Prompts for Text-to-Image Generation.",
            "LoRA: Low-Rank Adaptation of Large Language Models.",
            "Llama Guard: LLM-based Input-Output Safeguard for Human-AI Conversations.",
            "The Hateful Memes Challenge: Detecting Hate Speech in Multimodal Memes.",
            "Hatemoji: A Test Suite and Adversarially-Generated Dataset for Benchmarking and Detecting Emoji-Based Hate.",
            "Self-Discovering Interpretable Diffusion Latent Directions for Responsible Text-to-Image Generation.",
            "Visual Instruction Tuning.",
            "RoBERTa: A Robustly Optimized BERT Pretraining Approach.",
            "Towards Deep Learning Models Resistant to Adversarial Attacks.",
            "FLIRT: Feedback Loop In-context Red Teaming.",
            "Universal Adversarial Perturbations.",
            "Deepfool: A Simple and Accurate Method to Fool Deep Neural Networks.",
            "Defining Digital Self-Harm.",
            "On the Evolution of (Hateful) Memes by Means of Multimodal Contrastive Learning.",
            "Unsafe Diffusion: On the Generation of Unsafe Images and Hateful Memes From Text-To-Image Models.",
            "Learning Transferable Visual Models From Natural Language Supervision.",
            "Red-Teaming the Stable Diffusion Safety Filter.",
            "Zero shot VLMs for hate meme detection: Are we there yet?",
            "High-Resolution Image Synthesis with Latent Diffusion Models.",
            "Hate Speech in Pixels: Detection of Offensive Memes towards Automatic Moderation.",
            "Safe Latent Diffusion: Mitigating Inappropriate Degeneration in Diffusion Models.",
            "Can Machines Help Us Answering Question 16 in Datasheets, and In Turn Reflecting on Inappropriate Content?",
            "LAION-Aesthetics.",
            "LAION-5B: An open large-scale dataset for training next generation image-text models.",
            "LAION-400M: Open Dataset of CLIP-Filtered 400 Million Image-Text Pairs.",
            "YFCC100M: the new data in multimedia research.",
            "LLaMA: Open and Efficient Foundation Language Models.",
            "Ring-A-Bell! How Reliable are Concept Removal Methods for Diffusion Models?",
            "Stereotypes and Smut: The (Mis)representation of Non-cisgender Identities by Text-to-Image Models.",
            "Modeling the Trade-off of Privacy Preservation and Activity Recognition on Low-Resolution Images.",
            "Protest Activity Detection and Perceived Violence Estimation from Social Media Images.",
            "On the Proactive Generation of Unsafe Images From Text-To-Image Models Using Benign Prompts.",
            "SneakyPrompt: Evaluating Robustness of Text-to-image Generative Models’ Safety Filters.",
            "A Quantitative Approach to Understanding Online Antisemitism.",
            "To Generate or Not? Safety-Driven Unlearned Diffusion Models Are Still Easy To Generate Unsafe Images ... For Now."
        ]
    },
    {
        "index": 269,
        "title": "Unveiling the Implicit Toxicity in Large Language Models",
        "publication_date": "2023-11-29",
        "references": [
            "Persistent anti-muslim bias in large language models",
            "Language models are few-shot learners",
            "Extracting training data from large language models",
            "PaLM: Scaling language modeling with pathways",
            "Scaling instruction-finetuned language models",
            "Automated hate speech detection and the problem of offensive language",
            "Toxicity in chatgpt: Analyzing persona-assigned language models",
            "Build it break it fix it for dialogue safety: Robustness from adversarial human attack",
            "Latent hatred: A benchmark for understanding implicit hate speech",
            "Measuring nominal scale agreement among many raters",
            "You call that a rhetorical question?: Forms and functions of rhetorical questions in conversation",
            "The unbearable hurtfulness of sarcasm",
            "Predictability and surprise in large generative models",
            "Detecting online hate speech using context aware models",
            "Realtoxicityprompts: Evaluating neural toxic degeneration in language models",
            "Toxigen: A large-scale machine-generated dataset for adversarial and implicit hate speech detection",
            "Reward learning from human preferences and demonstrations in atari",
            "Can machines learn morality? the delphi experiment",
            "Improving hate speech type and target detection with hateful metaphor features",
            "A diversity-promoting objective function for neural conversation models",
            "ROBERTA: A robustly optimized BERT pretraining approach",
            "G-eval: NLG evaluation using GPT-4 with better human alignment",
            "Language models as knowledge bases?",
            "Recipes for building an open-domain chatbot",
            "Explaining toxic text via knowledge enhanced text generation",
            "Learning to summarize with human feedback",
            "Proximal policy optimization algorithms",
            "Explaining toxic text via knowledge enhanced text generation",
            "Training language models to follow instructions with human feedback",
            "Is chatgpt a good nlg evaluator? a preliminary study",
            "Safety-bench: Evaluating the safety of large language models with multiple choice questions",
            "ETHICIST: Targeted training data extraction through loss smoothed soft prompting and calibrated confidence estimation"
        ]
    },
    {
        "index": 274,
        "title": "VLMGuard: Defending VLMs against Malicious Prompts via Unlabeled Data",
        "publication_date": "2024-10-01",
        "references": [
            "Phi-3 technical report: A highly capable language model locally on your phone.",
            "Detecting language model attacks with perplexity.",
            "(ab) using images and sounds for indirect instruction injection in multi-modal llms.",
            "Feed two birds with one scone: Exploiting wild data for both out-of-distribution generalization and detection.",
            "Out-of-distribution learning with human feedback.",
            "Discovering latent knowledge in language models without supervision.",
            "Are aligned neural networks adversarially aligned?",
            "Jailbreaking black box large language models in twenty queries.",
            "Struq: Defending against prompt injection with structured queries.",
            "How to save your annotation cost for panoptic segmentation?",
            "Active learning to classify macromolecular structures in situ for less supervision in cryo-electron tomography.",
            "Learning diverse-structured networks for adversarial robustness.",
            "Siren: Shaping representations for detecting out-of-distribution objects.",
            "Unknown-aware object detection: Learning what you don't know from videos in the wild.",
            "Vos: Learning what you don't know by virtual outlier synthesis.",
            "Noise-robust graph learning by estimating and leveraging pairwise interactions.",
            "Dream the impossible: Outlier imagination with diffusion models.",
            "How does unlabeled data provably help out-of-distribution detection?",
            "When and how does in-distribution label help out-of-distribution detection?",
            "Haloscope: Harnessing unlabeled llm generations for hallucination detection.",
            "Mirrorcheck: Efficient adversarial defense for vision-language models.",
            "Figstep: Jailbreaking large vision-language models via typographic visual prompts.",
            "Eyes closed, safety on: Protecting multimodal llms via image-to-text transformation.",
            "Not what you've signed up for: Compromising real-world llm-integrated applications with indirect prompt injection.",
            "Agent smith: A single image can jailbreak one million multimodal llm agents exponentially fast.",
            "Defending against indirect prompt injection attacks with spotlighting.",
            "Gradient cuff: Detecting jailbreak attacks on large language models by exploring refusal loss landscapes.",
            "Robust clip: Unsupervised adversarial fine-tuning of vision embeddings for robust large vision-language models.",
            "Jailbreak in pieces: Compositional adversarial attacks on multi-modal language models.",
            "Optimization-based prompt injection attack to llm-as-a-judge.",
            "Non-parametric outlier synthesis.",
            "From llms to mllms: Exploring the landscape of multimodal jailbreaking.",
            "Defending llms against jailbreaking attacks via backtranslation.",
            "Jailbroken: How does llm safety training fail?",
            "Performance-aware mutual knowledge distillation for improving neural architecture search.",
            "Gradsafe: Detecting unsafe prompts for llms via safety-critical gradient analysis.",
            "Defending jailbreak attack in vlms via cross-modality information detector.",
            "Openood: Benchmarking generalized out-of-distribution detection.",
            "Generalized out-of-distribution detection: A survey.",
            "Benchmarking and defending against indirect prompt injection attacks on large language models.",
            "Jailbreak attacks and defenses against large language models: A survey.",
            "Vlattack: Multimodal adversarial attacks on vision-language tasks via pre-trained models.",
            "Autodefense: Multi-agent llm defense against jailbreak attacks.",
            "Openood v1. 5: Enhanced benchmark for out-of-distribution detection.",
            "Soft prompts go hard: Steering visual language models with hidden meta-instructions.",
            "A mutation-based method for multi-modal jailbreaking attack detection.",
            "Judging llm-as-a-judge with mt-bench and chatbot arena.",
            "Representation engineering: A top-down approach to ai transparency.",
            "Universal and transferable adversarial attacks on aligned language models.",
            "Improving alignment and robustness with short circuiting."
        ]
    },
    {
        "index": 275,
        "title": "Voice Jailbreak Attacks Against GPT-4o",
        "publication_date": "2024-05-29",
        "references": [
            "Comprehensive Assessment of Jailbreak Attacks Against LLMs",
            "Multilingual Jailbreak Challenges in Large Language Models",
            "Fig-Step: Jailbreaking Large Vision-language Models via Typographic Visual Prompts",
            "Do Anything Now: Characterizing and Evaluating In-The-Wild Jailbreak Prompts on Large Language Models",
            "SmoothLLM: Defending Large Language Models Against Jailbreaking Attacks",
            "Scalable and Transferable Black-Box Jailbreaks for Language Models via Persona Modulation",
            "Jailbroken: How Does LLM Safety Training Fail?",
            "Defending ChatGPT against jailbreak attack via self-reminders",
            "Low-Resource Languages Jailbreak GPT-4",
            "GPT-FUZZER: Red Teaming Large Language Models with Auto-Generated Jailbreak Prompts",
            "How Johnny Can Persuade LLMs to Jailbreak Them: Rethinking Persuasion to Challenge AI Safety by Humanizing LLMs",
            "Universal and Transferable Adversarial Attacks on Aligned Language Models"
        ]
    },
    {
        "index": 278,
        "title": "WILDGUARD : Open One-stop Moderation Tools for Safety Risks, Jailbreaks, and Refusals of LLMs",
        "publication_date": "2024-12-09",
        "references": [
            "Gpt-4 technical report.",
            "Llama 3 model card.",
            "The claude 3 model family: Opus, sonnet, haiku.",
            "Foundational challenges in assuring alignment and safety of large language models.",
            "Training a helpful and harmless assistant with reinforcement learning from human feedback.",
            "Longformer: The long-document transformer.",
            "Safety-tuned llamas: Lessons from improving the safety of large language models that follow instructions.",
            "Are aligned neural networks adversarially aligned?",
            "Safe rlhf: Safe reinforcement learning from human feedback.",
            "The measurement of interrater agreement.",
            "Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned.",
            "Realtoxicityprompts: Evaluating neural toxic degeneration in language models.",
            "Aegis: Online adaptive ai content safety moderation with ensemble of llm experts.",
            "Ruddit: Norms of offensiveness for english reddit comments.",
            "An overview of catastrophic ai risks.",
            "Llama guard: Llm-based input-output safeguard for human-ai conversations.",
            "Camels in a changing climate: Enhancing lm adaptation with tulu 2.",
            "Beavertails: Towards improved safety alignment of llm via a human-preference dataset.",
            "Mistral 7b.",
            "Wildteaming at scale: From in-the-wild jailbreaks to (adversarially) safer language models.",
            "A new generation of perspective api: Efficient multilingual character-level transformers.",
            "Salad-bench: A hierarchical and comprehensive safety benchmark for large language models.",
            "Toxicchat: Unveiling hidden challenges of toxicity detection in real-world user-ai conversation.",
            "A holistic approach to undesired content detection in the real world.",
            "Harmbench: A standardized evaluation framework for automated red teaming and robust refusal.",
            "Meta llama guard 2: Model cards and prompt formats.",
            "Chenghaomou/text-dedup: Reference snapshot, September 2023.",
            "Openai moderation api.",
            "A large-scale semi-supervised dataset for offensive language identification.",
            "Xstest: A test suite for identifying exaggerated safety behaviours in large language models.",
            "Safety assessment of chinese large language models.",
            "Trustllm: Trustworthiness in large language models.",
            "Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context.",
            "Alert: A comprehensive benchmark for assessing large language models’ safety through red teaming.",
            "Llama 2: Open foundation and fine-tuned chat models.",
            "Simplesafetytests: a test suite for identifying critical safety risks in large language models.",
            "Introducing v0.5 of the ai safety benchmark from mlcommons."
        ]
    },
    {
        "index": 280,
        "title": "YOUKNOW WHAT I’MSAYING : JAILBREAK ATTACK VIA IMPLICIT REFERENCE",
        "publication_date": "2024-10-08",
        "references": [
            "Detecting language model attacks with perplexity",
            "Does refusal training in llms generalize to the past tense?",
            "Anthropic’s responsible scaling policy",
            "Introducing claude",
            "Claude 3.5 sonnet model card addendum",
            "Managing extreme ai risks amid rapid progress",
            "Safety-tuned LLaMAs: Lessons from improving the safety of large language models that follow instructions",
            "Sparks of artificial general intelligence: Early experiments with gpt-4",
            "Jailbreaking black box large language models in twenty queries",
            "Jailbreakbench: An open robustness benchmark for jailbreaking large language models",
            "Leveraging the context through multi-round interactions for jailbreaking attacks",
            "Can large language models be an alternative to human evaluations?",
            "Deep reinforcement learning from human preferences",
            "Or-bench: An over-refusal benchmark for large language models",
            "Safe rlhf: Safe reinforcement learning from human feedback",
            "Attack Prompt Generation for Red Teaming and Defending Large Language Models",
            "Jailbreaker: Automated jailbreak across multiple large language model chatbots",
            "Masterkey: Automated jailbreaking of large language model chatbots",
            "Multilingual jailbreak challenges in large language models",
            "A wolf in sheep’s clothing: Generalized nested jailbreak prompts can fool large language models easily",
            "The llama 3 herd of models",
            "Configurable Safety Tuning of Language Models with Synthetic Preference Data",
            "Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned",
            "Emerging vulnerabilities in frontier models: Multi-turn jailbreak attacks",
            "Bard",
            "Baseline defenses for adversarial attacks against aligned language models",
            "Defending large language models against jailbreak attacks via semantic smoothing",
            "Red queen: Safeguarding large language models against concealed multi-turn jailbreaking",
            "Is chatgpt a good translator? yes with gpt-4 as the engine",
            "Jailbreaking large language models against moderation guardrails via cipher characters",
            "Exploiting uncommon text-encoded structures for automated jailbreaks in llms",
            "Multi-step jailbreaking privacy attacks on chatgpt",
            "Deepinception: Hypnotize large language model to be jailbreaker",
            "The unlocking spell on base llms: Rethinking alignment via in-context learning",
            "Pathseeker: Exploring llm security vulnerabilities with a reinforcement learning-based jailbreak approach",
            "Imposter.ai: Adversarial attacks with hidden intentions towards aligned large language models",
            "Autodan: Generating stealthy jailbreak prompts on aligned large language models",
            "Jailbreaking chatgpt via prompt engineering: An empirical study",
            "Enhancing LLM safety via constrained direct preference optimization",
            "Codechameleon: Personalized encryption framework for jailbreaking large language models",
            "Visual-roleplay: Universal jailbreak attack on multimodal large language models via role-playing image character",
            "Tdc 2023 (llm edition): The trojan detection challenge",
            "Harmbench: A standardized evaluation framework for automated red teaming and robust refusal",
            "Inverse scaling: When bigger isn’t better",
            "Tree of attacks: Jailbreaking black-box llms automatically",
            "GPT-4 technical report",
            "Introducing chatgpt",
            "Training language models to follow instructions with human feedback",
            "Generative agents: Interactive simulacra of human behavior",
            "Red teaming language models with language models",
            "Direct preference optimization: Your language model is secretly a reward model",
            "Smoothllm: Defending large language models against jailbreaking attacks",
            "Toolformer: Language models can teach themselves to use tools",
            "Jailbreak in pieces: Compositional adversarial attacks on multi-modal language models",
            "Navigating the OverKill in large language models",
            "Llama: Open and efficient foundation language models",
            "Towards safety and helpfulness balanced responses via controllable large language models",
            "Jailbroken: How does llm safety training fail?",
            "Jailbreak and guard aligned language models with only few in-context demonstrations",
            "Qwen2 technical report",
            "Harnessing the power of llms in practice: A survey on chatgpt and beyond",
            "Shadow alignment: The ease of subverting safely-aligned language models",
            "Chain of attack: a semantic-driven contextual multi-turn attacker for llm",
            "Fuzzllm: A novel and universal fuzzing framework for proactively discovering jailbreak vulnerabilities in large language models",
            "Gptfuzzer: Red teaming large language models with auto-generated jailbreak prompts",
            "Gpt-4 is too smart to be safe: Stealthy chat with llms via cipher",
            "How johnny can persuade llms to jailbreak them: Rethinking persuasion to challenge ai safety by humanizing llms",
            "Lima: Less is more for alignment",
            "Easyjailbreak: A unified framework for jailbreaking large language models",
            "Universal and transferable adversarial attacks on aligned language models"
        ]
    },
    {
        "index": 281,
        "title": "I Know What You MEME! Understanding and Detecting Harmful Memes with Multimodal Large Language Models",
        "publication_date": "2025-02-28",
        "references": [
            "Meme",
            "Meme — Wikipedia, the free encyclopedia",
            "Internet meme — Wikipedia, the free encyclopedia",
            "On the evolution of (hateful) memes by means of multimodal contrastive learning",
            "Detecting and understanding harmful memes: A survey",
            "The hateful memes challenge: Detecting hate speech in multimodal memes",
            "Detecting harmful memes and their targets",
            "Understanding and Analyzing COVID-19-related Online Hate Propagation Through Hateful Memes Shared on Twitter",
            "Hate speech epidemic. the dynamic effects of derogatory language on intergroup relations and political radicalization",
            "How online hate turns into real-life violence",
            "Identifying creative harmful memes via prompt based approach",
            "Prompting for multimodal hateful meme classification",
            "Pro-cap: Leveraging a frozen vision-language model for hateful meme detection",
            "Beneath the surface: Unveiling harmful memes with multimodal reasoning distilled from large language models",
            "Momenta: A multimodal framework for detecting harmful memes and their targets",
            "Understanding the Generalizability of Hateful Memes Detection Models Against COVID-19-related Hateful Memes",
            "Bert: Pre-training of deep bidirectional transformers for language understanding",
            "Very deep convolutional networks for large-scale image recognition",
            "Hate-clipper: Multimodal hateful meme classification based on cross-modal interaction of clip features",
            "Towards explainable harmful meme detection through multimodal debate between large language models",
            "Decoding the Underlying Meaning of Multimodal Hateful Memes",
            "Multimodal large language models: A survey",
            "Synthetic prompting: Generating chain-of-thought demonstrations for large language models",
            "Answering questions by meta-reasoning over multiple chains of thought",
            "Multimodal chain-of-thought reasoning in language models",
            "How to limit sensitive content that you see on instagram",
            "Lambretta: learning to rank for twitter soft moderation",
            "Llava: Large language and vision assistant",
            "Roberta: A robustly optimized bert pretraining approach",
            "Deep residual learning for image recognition",
            "Prompting for multi-modal tracking",
            "Visualbert: A simple and performant baseline for vision and language",
            "Bertscore: Evaluating text generation with bert",
            "A method for generating explanations of offensive memes based on multimodal large language models",
            "The rise of meme culture: Internet political memes as tools for analyzing Philippine propaganda",
            "A comprehensive survey on domain adaptation for visual applications",
            "Sniffer: Multimodal large language model for explainable out-of-context misinformation detection",
            "Detecting hate speech in memes using multimodal deep learning approaches: Prize-winning solution to hateful memes challenge",
            "Bad characters: Imperceptible nlp attacks",
            "Build the future of ai with meta llama 3",
            "Moderating New Waves of Online Hate with Chain-of-Thought Reasoning in Large Language Models",
            "An Investigation of Large Language Models for Real-World Hate Speech Detection",
            "Moderating illicit online image promotion for unsafe user-generated content games using large vision-language models",
            "Chain-of-thought prompting of large language models for discovering and fixing software vulnerabilities",
            "Graph of thoughts: Solving elaborate problems with large language models",
            "Benchmarking large language models in retrieval-augmented generation",
            "Cohen’s kappa — Wikipedia, the free encyclopedia"
        ]
    },
    {
        "index": 282,
        "title": "AdvPrefix: An Objective for Nuanced LLM Jailbreaks",
        "publication_date": "2024-12-16",
        "references": [
            "Concrete problems in ai safety",
            "Automatic pseudo-harmful prompt generation for evaluating false refusals in large language models",
            "Jailbreaking leading safety-aligned llms with simple adaptive attacks",
            "The claude 3 model family: Opus, sonnet, haiku",
            "Why exposure bias matters: An imitation learning perspective of error accumulation in language generation",
            "Training a helpful and harmless assistant with reinforcement learning from human feedback",
            "Scheduled sampling for sequence prediction with recurrent neural networks",
            "Medusa: Simple llm inference acceleration framework with multiple decoding heads",
            "Jailbreaking black box large language models in twenty queries",
            "Jailbreakbench: An open robustness benchmark for jailbreaking large language models",
            "Or-bench: An over-refusal benchmark for large language models",
            "Safe rlhf: Safe reinforcement learning from human feedback",
            "The llama 3 herd of models",
            "Alpacafarm: A simulation framework for methods that learn from human feedback",
            "Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned",
            "Coercing llms to do and reveal (almost) anything",
            "Attacking large language models with projected gradient descent",
            "Better & faster large language models via multi-token prediction",
            "Gradient-based adversarial attacks against text transformers",
            "Cold-attack: Jailbreaking llms with stealthiness and controllability",
            "A trivial programmatic llama 3 jailbreak",
            "Catastrophic jailbreak of open-source LLMs via exploiting generation",
            "Position: TrustLLM: Trustworthiness in large language models",
            "Llama guard: Llm-based input-output safeguard for human-ai conversations",
            "Baseline defenses for adversarial attacks against aligned language models",
            "Beavertails: Towards improved safety alignment of llm via a human-preference dataset",
            "Improved techniques for optimization-based jailbreaking on large language models",
            "Uncensor any llm with abliteration",
            "Open sesame! universal black box jailbreaking of large language models",
            "Alpacaeval: An automatic evaluator of instruction-following models",
            "Amplegcg: Learning a universal and transferable generative model of adversarial suffixes for jailbreaking both open and closed llms",
            "Advancing adversarial suffix transfer learning on aligned large language models",
            "Autodan: Generating stealthy jailbreak prompts on aligned large language models",
            "Prompt Injection attack against LLM-integrated Applications",
            "Harmbench: A standardized evaluation framework for automated red teaming and robust refusal",
            "Gpt-4 technical report",
            "Training language models to follow instructions with human feedback",
            "Future lens: Anticipating subsequent tokens from a single hidden state",
            "Advprompter: Fast adaptive adversarial prompting for llms",
            "Red teaming language models with language models",
            "Safety alignment should be made more than just a few tokens deep",
            "Direct preference optimization: Your language model is secretly a reward model",
            "Gemini1.5: Unlocking multimodal understanding across millions of tokens of context",
            "A strongreject for empty jailbreaks",
            "Gemma 2: Improving open language models at a practical size",
            "Fluent student-teacher redteaming",
            "Llama 2: Open foundation and fine-tuned chat models",
            "Introducing v0. 5 of the ai safety benchmark from mlcommons",
            "Jailbroken: How does llm safety training fail?",
            "Do language models plan ahead for future tokens?",
            "Jailbreaking as a reward misspecification problem",
            "Shieldgemma: Generative ai content moderation based on gemma",
            "How johnny can persuade llms to jailbreak them: Rethinking persuasion to challenge ai safety by humanizing llms",
            "Backtracking improves generation safety",
            "Make them spill the beans! coercive knowledge extraction from (production) llms",
            "Weak-to-strong jailbreaking on large language models",
            "Don’t say no: Jailbreaking llm by suppressing refusal",
            "Autodan: Interpretable gradient-based adversarial attacks on large language models",
            "Universal and transferable adversarial attacks on aligned language models"
        ]
    },
    {
        "index": 284,
        "title": "Defending LVLMs Against Vision Attacks through Partial-Perception Supervision",
        "publication_date": "2024-12-17",
        "references": [
            "Gpt-4 technical report.",
            "Defense-prefix for preventing typographic attacks on clip.",
            "Qwen-vl: A frontier large vision-language model with versatile abilities.",
            "Weak-to-strong generalization: Eliciting strong capabilities with weak supervision.",
            "Unveiling typographic deceptions: Insights of the typographic vulnerability in large vision-language model.",
            "Towards transferable attacks against vision-llms in autonomous driving with typography.",
            "Improving factual-ity and reasoning in language models through multi-agent debate.",
            "Figstep: Jailbreaking large vision-language models via typographic visual prompts.",
            "Eyes closed, safety on: Protecting multimodal llms via image-to-text transformation.",
            "Vision superalignment: Weak-to-strong generalization for vision foundation models.",
            "Debating with more persuasive llms leads to more truthful answers.",
            "One prompt word is enough to boost adversarial robustness for pre-trained vision-language models.",
            "Red teaming visual language models.",
            "Your large language model is secretly a fairness proponent and you should prompt it like one.",
            "Images are achilles’ heel of alignment: Exploiting visual vulnerabilities for jail-breaking multimodal large language models.",
            "Encouraging divergent thinking in large language models through multi-agent debate.",
            "Interpreting and mitigating hallucination in mllms through multi-agent debate.",
            "Visual instruction tuning.",
            "Unraveling and mitigating safety alignment degradation of vision-language models.",
            "Mm-safetybench: A benchmark for safety evaluation of multimodal large language models.",
            "Psysafe: A comprehensive framework for psychological-based attack, defense, and evaluation of multi-agent system safety.",
            "Weak-to-strong jailbreaking on large language models.",
            "On evaluating adversarial robustness of large vision-language models.",
            "Safety fine-tuning at (almost) no cost: A baseline for vision large language models."
        ]
    },
    {
        "index": 285,
        "title": "SATA: A Paradigm for LLM Jailbreak via Simple Assistive Task Linkage",
        "publication_date": "2024-12-19",
        "references": [
            "Model card and evaluations for claude models.",
            "Refusal in language models is mediated by a single direction.",
            "Language Models are Few-Shot Learners.",
            "Defending Against Alignment-Breaking Attacks via Robustly Aligned LLM.",
            "Play Guessing Game with LLM: Indirect Jailbreak Attack with Implicit Clues.",
            "JailbreakBench: An Open Robustness Benchmark for Jailbreaking Large Language Models.",
            "Jailbreaking Black Box Large Language Models in Twenty Queries.",
            "Deep reinforcement learning from human preferences.",
            "Talebrush: Sketching stories with generative pretrained language models.",
            "Fast ml inference, simple api.",
            "Large Language Models are Edge-Case Generators: Crafting Unusual Programs for Fuzzing Deep Learning Libraries.",
            "Multilingual Jailbreak Challenges in Large Language Models.",
            "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.",
            "A Wolf in Sheep’s Clothing: Generalized Nested Jailbreak Prompts can Fool Large Language Models Easily.",
            "The Llama 3 Herd of Models.",
            "Baseline Defenses for Adversarial Attacks Against Aligned Language Models.",
            "ArtPrompt: ASCII Art-based Jailbreak Attacks against Aligned LLMs.",
            "Systematic review and meta-analysis of AI-based conversational agents for promoting mental health and well-being.",
            "DeepInception: Hypnotize Large Language Model to Be Jailbreaker.",
            "AutoDAN: Generating Stealthy Jailbreak Prompts on Aligned Large Language Models.",
            "Co-writing screenplays and theatre scripts with language models: Evaluation by industry professionals.",
            "ClarifyGPT: A Framework for Enhancing LLM-Based Code Generation via Requirements Clarification.",
            "GPT-4 Technical Report.",
            "Training language models to follow instructions with human feedback.",
            "LLM Self Defense: by Self Examination, LLMs Know They Are Being Tricked.",
            "Direct Preference Optimization: Your Language Model is Secretly a Reward Model.",
            "SmoothLLM: Defending Large Language Models Against Jailbreaking Attacks.",
            "Llama 2: Open Foundation and Fine-Tuned Chat Models.",
            "Exploring the Limits of Domain-Adaptive Training for Detoxifying Large-Scale Language Models.",
            "Defending LLMs against Jailbreaking Attacks via Backtranslation.",
            "Jailbreak and Guard Aligned Language Models with Only Few In-Context Demonstrations.",
            "GPTFUZZER: Red Teaming Large Language Models with Auto-Generated Jailbreak Prompts.",
            "MAmmoTH: Building math generalist modelsthrough hybrid instruction tuning.",
            "How Johnny Can Persuade LLMs to Jailbreak Them: Rethinking Persuasion to Challenge AI Safety by Humanizing LLMs.",
            "Defending Large Language Models Against Jailbreaking Attacks Through Goal Prioritization.",
            "Universal and Transferable Adversarial Attacks on Aligned Language Models."
        ]
    },
    {
        "index": 286,
        "title": "JailPO: A Novel Black-box Jailbreak Framework via Preference Optimization against Aligned LLMs",
        "publication_date": "2025-01-01",
        "references": [
            "Gpt-4 technical report",
            "Detecting language model attacks with perplexity",
            "A general theoretical paradigm to understand learning from human preferences",
            "Qwen technical report",
            "On the opportunities and risks of foundation models",
            "Rank Analysis of Incomplete Block Designs: I. The Method of Paired Comparisons",
            "Jailbreaking black box large language models in twenty queries",
            "Masterkey: Automated jail-breaking of large language model chatbots",
            "Multi-lingual jailbreak challenges in large language models",
            "Kto: Model alignment as prospect theoretic optimization",
            "Mitigating the OWASP Top 10 For Large Language Models Applications using Intelligent Agents",
            "Openagi: When llm meets domain experts",
            "Llama guard: Llm-based input-output safeguard for human-ai conversations",
            "Baseline defenses for adversarial attacks against aligned language models",
            "Ai alignment: A comprehensive survey",
            "Automatically auditing large language models via discrete optimization",
            "Multi-step jailbreaking privacy attacks on chatgpt",
            "Deepinception: Hypnotize large language model to be jail-breaker",
            "AutoDAN: Generating Stealthy Jailbreak Prompts on Aligned Large Language Models",
            "Jailbreaking chatgpt via prompt engineering: An empirical study",
            "Simpo: Simple preference optimization with a reference-free reward",
            "Training language models to follow instructions with human feedback",
            "Red teaming language models with language models",
            "Llm-guard",
            "Direct preference optimization: Your language model is secretly a reward model",
            "Adversarial attacks and defenses in large language models: Old and new threats",
            "Large language model alignment: A survey",
            "Llama 2: Open foundation and fine-tuned chat models",
            "Aligning large language models with human: A survey",
            "Jailbroken: How Does LLM Safety Training Fail?",
            "Unveiling the Implicit Toxicity in Large Language Models",
            "Cognitive overload: Jailbreaking large language models with overloaded logical thinking",
            "LLM Jailbreak Attack versus Defense Techniques–A Comprehensive Study",
            "Harnessing the power of llms in practice: A survey on chatgpt and beyond",
            "Jailbreak Attacks and Defenses Against Large Language Models: A Survey",
            "Low-resource languages jailbreak gpt-4",
            "Gptfuzzer: Red team-ing large language models with auto-generated jailbreak prompts",
            "Gpt-4 is too smart to be safe: Stealthy chat with llms via cipher",
            "Jade: A linguistics-based safety evaluation platform for llm",
            "Universal and transferable adversarial attacks on aligned language models"
        ]
    },
    {
        "index": 288,
        "title": "LLM-Virus: Evolutionary Jailbreak Attack on Large Language Models",
        "publication_date": "2024-01-01",
        "references": [
            "Head-to-tail: How knowledgeable are large language models (llm)? aka will llms replace knowledge graphs?",
            "Understanding the planning of llm agents: A survey",
            "Advancing llm reasoning generalists with preference trees",
            "Not what you’ve signed up for: Compromising real-world llm-integrated applications with indirect prompt injection",
            "Netsafe: Exploring the topological safety of multi-agent networks",
            "Trustworthy llms: A survey and guideline for evaluating large language models’ alignment",
            "Attacks, defenses and evaluations for llm conversation safety: A survey",
            "A survey on large language model (llm) security and privacy: The good, the bad, and the ugly",
            "Beavertails: Towards improved safety alignment of llm via a human-preference dataset",
            "Fine-tuning aligned language models compromises safety, even when users do not intend to!",
            "Harmbench: A standardized evaluation framework for automated red teaming and robust refusal",
            "Jail-break attacks and defenses against large language models: A survey",
            "Autodan: Generating stealthy jailbreak prompts on aligned large language models",
            "Automatically auditing large language models via discrete optimization",
            "Universal and transferable adversarial attacks on aligned language models",
            "Jailbreaking black box large language models in twenty queries",
            "Ask, attend, attack: A effective decision-based black-box targeted attack for image-to-text models",
            "Cross-modality attack boosted by gradient-evolutionary multiform optimization",
            "Blackdan: A black-box multi-objective approach for effective and contextual jailbreaking of large language models",
            "Gptfuzzer: Red teaming large language models with auto-generated jailbreak prompts",
            "Semantic mirror jailbreak: Genetic algorithm based jailbreak prompts against open-source llms",
            "Large language models as evolution strategies",
            "Language model crossover: Variation through few-shot prompting",
            "Enhancing genetic improvement mutations using large language models",
            "Gpt-4 technical report",
            "Llama: Open and efficient foundation language models",
            "Jailbroken: How does llm safety training fail?",
            "Breaking down the defenses: A comparative survey of attacks on large language models",
            "Survey of vulnerabilities in large language models revealed by adversarial attacks",
            "Jailbreak and guard aligned language models with only few in-context demonstrations",
            "Deepinception: Hypnotize large language model to be jailbreaker",
            "“ do anything now”: Characterizing and evaluating in-the-wild jailbreak prompts on large language models",
            "Ignore previous prompt: Attack techniques for language models",
            "Evil geniuses: Delving into the safety of llm-based agents",
            "Genetic algorithms and the optimal allocation of trials",
            "Evolution strategies–a comprehensive introduction",
            "Evolutionary programming made faster",
            "Genetic programming as a means for programming computers by natural selection",
            "Evolutionary algorithms",
            "Moea/d: A multiobjective evolutionary algorithm based on decomposition",
            "Evolutionary multitask optimization with lower confidence bound-based solution selection strategy",
            "Evolutionary optimization in dynamic environments",
            "Evolutionary optimization in uncertain environments-a survey",
            "Llm with tools: A survey",
            "Personal llm agents: Insights and survey about the capability, efficiency and security",
            "Large language models as evolutionary optimizers",
            "Large language model for multi-objective evolutionary optimization",
            "Connecting large language models with evolutionary algorithms yields powerful prompt optimizers",
            "Autobert-zero: Evolving bert backbone from scratch",
            "Evoagent: Towards automatic multi-agent generation via evolutionary algorithms",
            "A review of population initialization techniques for evolutionary algorithms",
            "A comparison of selection schemes used in evolutionary algorithms",
            "How alignment and jailbreak work: Explain llm safety through intermediate hidden states",
            "Hard prompts made easy: Gradient-based discrete optimization for prompt tuning and discovery",
            "Gradient-based adversarial attacks against text transformers",
            "Universal adversarial triggers for attacking and analyzing nlp",
            "Auto-prompt: Eliciting knowledge from language models with automatically generated prompts",
            "Red teaming language models with language models",
            "Tree of attacks: Jailbreaking black-box llms automatically",
            "How johnny can persuade llms to jailbreak them: Rethinking persuasion to challenge ai safety by humanizing llms",
            "“ do anything now”: Characterizing and evaluating in-the-wild jailbreak prompts on large language models",
            "Gemini: a family of highly capable multimodal models",
            "Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality",
            "Gemma 2: Improving open language models at a practical size",
            "Minilm: Deep self-attention distillation for task-agnostic compression of pre-trained transformers",
            "Some methods for classification and analysis of multivariate observations"
        ]
    },
    {
        "index": 289,
        "title": "How Toxic Can You Get? Search-based Toxicity Testing for Large Language Models",
        "publication_date": "2025-01-03",
        "references": [
            "Online hate speech victimization: consequences for victims’ feelings of insecurity",
            "Ethical and social risks of harm from language models",
            "Perspective API",
            "Training language models to follow instructions with human feedback",
            "Constitutional AI: harmlessness from AI feedback",
            "Aligning language models with preferences through f-divergence minimization",
            "Pretraining language models with human preferences",
            "RealToxicityPrompts: Evaluating neural toxic degeneration in language models",
            "Autodan: Generating stealthy jailbreak prompts on aligned large language models",
            "Tree of attacks: Jailbreaking black-box LLMs automatically",
            "Exploiting programmatic behavior of LLMs: Dual-use through standard security attacks",
            "LLM jailbreak attack versus defense techniques - A comprehensive study",
            "Language-based software testing",
            "Search-based software testing: Past, present and future",
            "An overview of evolutionary algorithms for parameter optimization",
            "Language models are few-shot learners",
            "Why should adversarial perturbations be imperceptible? rethink the research paradigm in adversarial NLP",
            "Catastrophic jailbreak of open-source llms via exploiting generation",
            "A hitchhiker’s guide to jailbreaking ChatGPT via prompt engineering",
            "Attention is all you need",
            "GPT-4 technical report",
            "Gemini: A family of highly capable multimodal models",
            "Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context",
            "Llama: Open and efficient foundation language models",
            "Introducing Meta Llama 3: The most capable openly available llm to date",
            "Judging LLM-as-a-Judge with MT-Bench and chatbot arena",
            "Mistral 7B",
            "Gemma: Open models based on Gemini research and technology",
            "Multitask prompted training enables zero-shot task generalization",
            "Scaling instruction-finetuned language models",
            "A primer on seq2seq models for generative chatbots",
            "Retrieval-augmented generation for large language models: A survey",
            "Large language models are zero-shot reasoners",
            "EvoSuite: automatic test suite generation for object-oriented software",
            "Ethical reasoning over moral alignment: A case and framework for in-context ethical policies in llms",
            "Ethical reasoning and moral value alignment of llms depend on the language we prompt them in",
            "Evaluating the moral beliefs encoded in llms",
            "Exploring ChatGPT for toxicity detection in github",
            "Red-teaming large language models using chain of utterances for safety-alignment",
            "Toxic, hateful, offensive or abusive? What are we really classifying? An empirical analysis of hate speech datasets",
            "Discovering and categorising language biases in reddit",
            "Bias and fairness in large language models: A survey",
            "Qigen: Generating efficient kernels for quantized inference on large language models",
            "MASTERKEY: Automated jailbreaking of large language model chatbots",
            "An empirical study of the reliability of UNIX utilities",
            "A practical guide for using statistical tests to assess randomized algorithms in software engineering",
            "On a test of whether one of two random variables is stochastically larger than the other",
            "A critique and improvement of the “CL” common language effect size statistics of mcgraw and wong",
            "Probability & statistics for engineers & scientists",
            "Interrater reliability: the kappa statistic",
            "The measurement of observer agreement for categorical data",
            "Speech and language processing: an introduction to natural language processing, computational linguistics, and speech recognition",
            "The curious case of neural text degeneration",
            "Unifying human and statistical evaluation for natural language generation",
            "Temporal event knowledge acquisition via identifying narratives",
            "Measuring nominal scale agreement among many raters.",
            "Jailbroken: How does LLM safety training fail?",
            "Universal and transferable adversarial attacks on aligned language models",
            "METAL: metamorphic testing framework for analyzing large-language model qualities",
            "Aligning AI with shared human values",
            "“Oops, did I just say that?” Testing and repairing unethical suggestions of large language models with suggest-critique-reflect process"
        ]
    },
    {
        "index": 290,
        "title": "AUTO-RT: Automatic Jailbreak Strategy Exploration for Red-Teaming Large Language Models",
        "publication_date": "2025-01-03",
        "references": [
            "Constrained policy optimization",
            "Yi: Open foundation models by 01.ai",
            "How complex systems fail",
            "Constrained markov decision processes",
            "Does refusal training in llms generalize to the past tense?",
            "Constrained optimization and Lagrange multiplier methods",
            "Exploitability prediction of software vulnerabilities",
            "A critical analysis of vulnerability taxonomies",
            "Convex optimization",
            "Beyond heuristics: learning to classify vulnerabilities and predict exploits",
            "Jailbreaking black box large language models in twenty queries",
            "Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality",
            "Safe rlhf: Safe reinforcement learning from human feedback",
            "Gradient-based adversarial attacks against text transformers",
            "Cold-attack: Jailbreaking llms with stealthiness and controllability",
            "Curiosity-driven red-teaming for large language models",
            "Mistral 7b",
            "Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality",
            "Training language models to follow instructions with human feedback",
            "Red teaming language models with language models",
            "Llama guard 2 — model cards and prompt formats",
            "Confronting reward model overoptimization with constrained rlhf",
            "Policy invariance under reward transformations: Theory and application to reward shaping",
            "Progressive safeguards for safe and model-agnostic reinforcement learning",
            "Stanford alpaca: An instruction-following llama model",
            "Mart: Improving llm safety with multi-round automatic red-teaming",
            "Gradient-based adversarial attacks against text transformers",
            "Cold-attack: Jailbreaking llms with stealthiness and controllability",
            "Curiosity-driven red-teaming for large language models",
            "Mistral 7b",
            "Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality",
            "Training language models to follow instructions with human feedback",
            "Red teaming language models with language models",
            "Llama guard 2 — model cards and prompt formats",
            "Confronting reward model overoptimization with constrained rlhf",
            "Policy invariance under reward transformations: Theory and application to reward shaping",
            "Progressive safeguards for safe and model-agnostic reinforcement learning",
            "Stanford alpaca: An instruction-following llama model",
            "Mart: Improving llm safety with multi-round automatic red-teaming",
            "Universal and transferable adversarial attacks on aligned language models",
            "Purple-teaming llms with adversarial defender training",
            "Llama 2: Open foundation and fine-tuned chat models",
            "Jailbreak and guard aligned language models with only few in-context demonstrations",
            "How johnny can persuade llms to jailbreak them: Rethinking persuasion to challenge ai safety by humanizing llms",
            "Diver-ct: Diversity-enhanced red teaming with relaxing constraints",
            "Tree of attacks: Jailbreaking black-box llms automatically",
            "Gemma 2: Improving open language models at a practical size",
            "Introducing qwen1.5",
            "Qwen2.5: A party of foundation models"
        ]
    },
    {
        "index": 291,
        "title": "Safeguarding Large Language Models in Real-time with Tunable Safety-Performance Trade-offs",
        "publication_date": "2025-01-02",
        "references": [
            "How many opinions does your llm have? improving uncertainty estimation in nlg.",
            "Detecting language model attacks with perplexity.",
            "Foundational challenges in assuring alignment and safety of large language models.",
            "A systematic review and meta-analysis of the effectiveness of nudging to increase fruit and vegetable choice.",
            "Active learning with statistical models.",
            "A framework for real-time safeguarding the text generation of large language.",
            "Safeguarding large language models: A survey.",
            "Nudging: Inference-time alignment via model collaboration.",
            "Llm censorship: A machine learning challenge or a computer security problem?",
            "Infusing behavior science into large language models for activity coaching.",
            "Llama guard: Llm-based input-output safeguard for human-ai conversations.",
            "Critic-guided decoding for controlled text generation.",
            "Situating sentence embedders with nearest neighbor overlap.",
            "Malla: Demystifying real-world large language model integrated malicious services.",
            "Autodan: Generating stealthy jailbreak prompts on aligned large language models.",
            "Roberta: A robustly optimized bert pretraining approach.",
            "Preventing repeated real world ai failures by cataloging incidents: The ai incident database.",
            "Cbflm: Safe control for llm alignment.",
            "Red teaming language models with language models.",
            "Bergeron: Combating adversarial attacks through a conscience-based alignment framework.",
            "Controllable natural language generation with contrastive prefixes.",
            "Sentence-bert: Sentence embeddings using siamese bert-networks.",
            "Out-of-distribution detection and selective generation for conditional language models.",
            "Nudging toward vaccination: a systematic review.",
            "Jailbreak antidote: Runtime safety-utility balance via sparse representation adjustment in large language models.",
            "do anything now\": Characterizing and evaluating in-the-wild jailbreak prompts on large language models.",
            "Machine teaching: A new paradigm for building machine learning systems.",
            "Attention is all you need.",
            "Self-guard: Empower the llm to safeguard itself.",
            "Jailbreak and guard aligned language models with only few in-context demonstrations.",
            "A comprehensive study of jailbreak attack versus defense for large language models.",
            "Fudge: Controlled text generation with future discriminators.",
            "Low-resource languages jailbreak gpt-4.",
            "Instruction-following evaluation for large language models."
        ]
    },
    {
        "index": 292,
        "title": "Spot Risks Before Speaking! Unraveling Safety Attention Heads in Large Vision-Language Models",
        "publication_date": "2025-01-03",
        "references": [
            "Gpt-4 technical report",
            "Understanding intermediate layers using linear classifier probes",
            "Flamingo: a visual language model for few-shot learning",
            "(ab) using images and sounds for indirect instruction injection in multi-modal llms",
            "Qwen-vl: A frontier large vision-language model with versatile abilities",
            "Qwen-vl: A frontier large vision-language model with versatile abilities",
            "Image hijacks: Adversarial images can control generative models at runtime",
            "Minigpt-v2: large language model as a unified interface for vision-language multi-task learning",
            "Sharegpt4v: Improving large multi-modal models with better captions",
            "Dress: Instructing large vision-language models to align and interact with humans via natural language feedback",
            "Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality",
            "Instructblip: Towards general-purpose vision-language models with instruction tuning",
            "How robust is google’s bard to adversarial image attacks?",
            "The llama 3 herd of models",
            "Misusing tools in large language models with visual adversarial examples",
            "Figstep: Jailbreaking large vision-language models via typographic visual prompts",
            "Mixture of cluster-conditional lora experts for vision-language instruction tuning",
            "Eyes closed, safety on: Protecting multimodal llms via image-to-text transformation",
            "Bag of tricks for efficient text classification",
            "An introduction to feature selection",
            "Red teaming visual language models",
            "Visual instruction tuning",
            "Safety of multimodal large language models on images and text",
            "How many unicorns are in this image? a safety evaluation benchmark for vision llms",
            "Adashield: Safeguarding multimodal large language models from structure-based attack via adaptive shield prompting",
            "Emergent linear representations in world models of self-supervised sequence models",
            "The linear representation hypothesis and the geometry of large language models",
            "Mllm-protector: Ensuring mllm’s safety without hurting performance",
            "Visual adversarial examples jailbreak large language models",
            "Visual adversarial examples jailbreak aligned large language models",
            "On the adversarial robustness of multi-modal foundation models",
            "Plug and pray: Exploiting off-the-shelf components of multi-modal models",
            "Bert rediscovers the classical nlp pipeline",
            "Llama: Open and efficient foundation language models",
            "Llama 2: Open foundation and fine-tuned chat models",
            "How many unicorns are in this image? a safety evaluation benchmark for vision llms",
            "Benchmarking trustworthiness of multi-modal large language models: A comprehensive study",
            "The first to know: How token distributions reveal hidden knowledge in large vision-language models?",
            "On evaluating adversarial robustness of large vision-language models",
            "Safety fine-tuning at (almost) no cost: A baseline for vision large language models"
        ]
    },
    {
        "index": 293,
        "title": "Layer-Level Self-Exposure and Patch: Affirmative Token Mitigation for Jailbreak Attack Defense",
        "publication_date": "2025-02-12",
        "references": [
            "Defending against alignment-breaking attacks via robustly aligned llm",
            "Towards making systems forget with machine unlearning",
            "Jailbreaking black box large language models in twenty queries",
            "Jailbreaker: Automated jailbreak across multiple large language model chatbots",
            "Llm self defense: By self examination, llms know they are being tricked",
            "Measuring massive multitask language understanding",
            "Catastrophic jailbreak of open-source llms via exploiting generation",
            "Trustllm: Trustworthiness in large language models",
            "Baseline defenses for adversarial attacks against aligned language models",
            "Survey of hallucination in natural language generation",
            "Mistral 7b",
            "Certifying llm safety against adversarial prompting",
            "Rain: Your language models can align themselves without finetuning",
            "The unlocking spell on base llms: Rethinking alignment via in-context learning",
            "Continual learning and private unlearning",
            "Fine-pruning: Defending against backdooring attacks on deep neural networks",
            "Autodan: Generating stealthy jailbreak prompts on aligned large language models",
            "Red teaming language models with language models",
            "Bergeron: Combating adversarial attacks through a conscience-based alignment framework",
            "Fine-tuning aligned language models compromises safety, even when users do not intend to!",
            "Deepinception: Hypnotize large language model to be jailbreaker",
            "Defending large language models against jailbreaking attacks through goal prioritization",
            "Defending large language models against jailbreak attacks via layer-specific editing",
            "Weak-to-strong jailbreaking on large language models",
            "Easyjailbreak: A unified framework for jailbreaking large language models",
            "Universal and transferable adversarial attacks on aligned language models",
            "Defining the limits of large language models"
        ]
    },
    {
        "index": 294,
        "title": "PromptGuard : Soft Prompt-Guided Unsafe Content Moderation for Text-to-Image Models",
        "publication_date": "2025-01-07",
        "references": [
            "Universal prompt optimizer for safe text-to-image generation.",
            "Scholar gpt.",
            "Unified concept editing in diffusion models.",
            "GPT-4 Technical Report.",
            "Stable Diffusion V2-1.",
            "Safe Stable Diffusion.",
            "Inaproppriate Image Prompts (I2P).",
            "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.",
            "An Image is Worth One Word: Personalizing Text-to-image Generation using Textual Inversion.",
            "Erasing Concepts from Diffusion Models.",
            "Unified Concept Editing in Diffusion Models.",
            "Rt-attack: Jailbreaking text-to-image models via random token.",
            "Denoising Diffusion Probabilistic Models.",
            "Personalization as a shortcut for few-shot backdoor attack against text-to-image diffusion models.",
            "Perception-guided jailbreak against text-to-image models.",
            "NSFW Text Classifier on Hugging Face.",
            "SafeGen-Pretrained-Weights.",
            "SafeGen: Mitigating Sexually Explicit Content Generation in Text-to-Image Models.",
            "Prefix-Tuning: Optimizing Continuous Prompts for Generation.",
            "Latent Guard: a Safety Framework for Text-to-image Generation.",
            "P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks.",
            "Stable Diffusion V1-4.",
            "Safety Checker.",
            "SDEdit: Guided Image Synthesis and Editing with Stochastic Differential Equations.",
            "AI-created Child Sexual Abuse Images ‘Threaten to Overwhelm Internet’.",
            "GPT-4o Mini: Advancing Cost-efficient Intelligence.",
            "GPT Documentation.",
            "2024: The Election Year of Deepfakes, Doubts and Disinformation?.",
            "Towards Understanding Unsafe Video Generation.",
            "Direct Unlearning Optimization for Robust and Safe Text-to-image Models.",
            "SDXL: Improving Latent Diffusion Models for High-resolution Image Synthesis.",
            "Unsafe Diffusion: On the Generation of Unsafe Images and Hateful Memes From Text-To-image Models.",
            "Learning Transferable Visual Models From Natural Language Supervision.",
            "High-resolution Image Synthesis with Latent Diffusion Models.",
            "Safe Latent Diffusion: Mitigating Inappropriate Degeneration in Diffusion Models.",
            "LAION-5B: an Open Large-scale Dataset for Training Next Generation Image-text Models.",
            "Spotting the Deepfakes in This Year of Elections: How AI Detection Tools Work and Where They Fail.",
            "Diffusion Lens: Interpreting Text Encoders in Text-to-image Pipelines.",
            "Diffusers: State-of-the-art diffusion models.",
            "DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT Models.",
            "Text-to-image AI Models Can Be Tricked Into Generating Disturbing Images.",
            "Universal Prompt Optimizer for Safe Text-to-image Generation.",
            "SneakyPrompt: Jailbreaking Text-to-image Generative Models.",
            "Visual-language Prompt Tuning with Knowledge-guided Context Optimization.",
            "The Unreasonable Effectiveness of Deep Features as a Perceptual Metric.",
            "Defensive Unlearning with Adversarial Training for Robust Concept Erasure in Diffusion Models.",
            "On Prompt-driven Safeguarding for Large Language Models.",
            "Text-to-Video AI Models Can Be Tricked Into Generating Disturbing Videos."
        ]
    },
    {
        "index": 295,
        "title": "Jailbreaking Multimodal Large Language Models via Shuffle Inconsistency",
        "publication_date": "2025-01-09",
        "references": [
            "The claude 3 model family: Opus, sonnet, haiku.",
            "Qwen-vl: A versatile vision-language model for understanding, localization, text reading, and beyond.",
            "Image hijacks: Adversarial images can control generative models at runtime.",
            "Cross-modal safety alignment: Is textual unlearning all you need?",
            "Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks.",
            "Multilingual jailbreak challenges in large language models.",
            "Figstep: Jailbreaking large vision-language models via typographic visual prompts.",
            "Gemini.",
            "How effective is bert without word ordering? implications for language understanding and data privacy.",
            "Llama guard: Llm-based input-output safeguard for human-ai conversations.",
            "Llava-next-interleave: Tackling multi-image, video, and 3d in large multimodal models.",
            "Images are achilles’ heel of alignment: Exploiting visual vulnerabilities for jailbreaking multimodal large language models.",
            "Images are achilles’ heel of alignment: Exploiting visual vulnerabilities for jailbreaking multimodal large language models.",
            "Autodan: Generating stealthy jailbreak prompts on aligned large language models.",
            "Mm-safetybench: A benchmark for safety evaluation of multimodal large language models.",
            "G-eval: Nlg evaluation using gpt-4 with better human alignment.",
            "Jailbreaking attack against multimodal large language model.",
            "Perspectiveapi.",
            "Chatgpt.",
            "Moderationapi.",
            "Hello gpt-4o.",
            "Visual adversarial examples jailbreak aligned large language models.",
            "Red-teaming the stable diffusion safety filter.",
            "Jailbreak in pieces: Compositional adversarial attacks on multimodal language models.",
            "” do anything now”: Characterizing and evaluating in-the-wild jailbreak prompts on large language models.",
            "Mrj-agent: An effective jailbreak agent for multi-round dialogue.",
            "Mmj-bench: A comprehensive study on jailbreak attacks and defenses for vision language models.",
            "Defending jailbreak attack in vlms via cross-modality information detector.",
            "Mma-diffusion: Multimodal attack on diffusion models.",
            "Sneakyprompt: Jailbreaking text-to-image generative models.",
            "Unveiling the safety of gpt-4o: An empirical study using jailbreak attacks.",
            "When and why vision-language models behave like bags-of-words, and what to do about it?",
            "A mutation-based method for multi-modal jailbreaking attack detection.",
            "Benchmarking trustworthiness of multimodal large language models: A comprehensive study.",
            "The first to know: How token distributions reveal hidden knowledge in large vision-language models?",
            "On evaluating adversarial robustness of large vision-language models.",
            "On prompt-driven safeguarding for large language models.",
            "Minigpt-4: Enhancing vision-language understanding with advanced large language models.",
            "Safety fine-tuning at (almost) no cost: A baseline for vision large language models.",
            "Universal and transferable adversarial attacks on aligned language models."
        ]
    },
    {
        "index": 296,
        "title": "Computing Optimization-Based Prompt Injections Against Closed-Weights Models By Misusing a Fine-Tuning API",
        "publication_date": "2025-01-16",
        "references": [
            "Tricking llms into disobedience: Understanding, analyzing, and preventing jailbreaks",
            "Jailbreaking chatgpt via prompt engineering: An empirical study",
            "Baseline defenses for adversarial attacks against aligned language models",
            "Jailbroken: How does llm safety training fail?",
            "Gptfuzzer: Red teaming large language models with auto-generated jailbreak prompts",
            "Jailbreaking attack against multimodal large language model",
            "Artprompt: Ascii art-based jailbreak attacks against aligned llms",
            "Not what you've signed up for: Compromising real-world llm-integrated applications with indirect prompt injection",
            "Prompt injection attack against llm-integrated applications",
            "New prompt injection attack on chatgpt web version. markdown images can steal your chat data.",
            "Injecagent: Benchmarking indirect prompt injections in tool-integrated large language model agents",
            "Benchmarking and defending against indirect prompt injection attacks on large language models",
            "Agent hijacking: The true impact of prompt injection attacks",
            "Fine-tuning with the Gemini API — Google AI for Developers — ai.google.dev",
            "Fine-tuning now available for gpt-4o",
            "Fine-tune anthropic's claude 3 haiku in amazon bedrock to boost model accuracy and quality",
            "Jailbreaking leading safety-aligned llms with simple adaptive attacks",
            "ChatGPT-Dan-Jailbreak",
            "Jailbreaking black box large language models in twenty queries",
            "Tree of attacks: Jailbreaking black-box llms automatically",
            "Universal and transferable adversarial attacks on aligned language models",
            "Neural exec: Learning (and learning from) execution triggers for prompt injection attacks",
            "Query-based adversarial prompt generation",
            "Pal: Proxy-guided black-box attack on large language models",
            "Autodan: Generating stealthy jailbreak prompts on aligned large language models",
            "Chat create top logprobs — openai api reference",
            "Generating content — Gemini API",
            "Stealing part of a production language model",
            "Covert malicious finetuning: Challenges in safeguarding llm adaptation",
            "Purple llama cyberseceval: A secure coding benchmark for language models",
            "Pal: Proxy-guided black-box attack on large language models",
            "Struq: Defending against prompt injection with structured queries",
            "Finetuned language models are zero-shot learners",
            "When scaling meets llm finetuning: The effect of data, model and finetuning method",
            "The instruction hierarchy: Training llms to prioritize privileged instructions",
            "Llm research insights: Instruction masking and new lora finetuning experiments",
            "Instruction tuning with loss over instructions",
            "Sparse fine-tuning for inference acceleration of large language models",
            "Neural exec: Learning (and learning from) execution triggers for prompt injection attacks",
            "Gemma 2: Improving open language models at a practical size",
            "Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context",
            "Model tuning with gemini api",
            "Fine-tuning llms: Lora or full parameter? an in-depth analysis with llama 2",
            "Misusing tools in large language models with visual adversarial examples",
            "Ai injections: Direct and indirect prompt injections and their implications",
            "Prompt injection: What’s the worst that can happen?",
            "Ignore previous prompt: Attack techniques for language models",
            "Multi-step jailbreaking privacy attacks on chatgpt",
            "Many-shot jailbreaking — anthropic.com",
            "OpenAI’s latest model will block the ‘ignore all previous instructions’ loophole",
            "Fast adversarial attacks on language models in one gpu minute",
            "Poisoning language models during instruction tuning",
            "Learning and forgetting unsafe examples in large language models",
            "Removing rlhf protections in gpt-4 via fine-tuning",
            "Fine-tuning aligned language models compromises safety, even when users do not intend to!",
            "Stealing machine learning models via prediction {APIs}",
            "Leaky dnn: Stealing deep-learning model secret with gpu context-switching side-channel",
            "On the sizes of openai api models",
            "Anthropic tokenizer"
        ]
    },
    {
        "index": 297,
        "title": "Jailbreaking Large Language Models in Infinitely Many Ways",
        "publication_date": "2025-03-13",
        "references": [
            "Advances in adversarial attacks and defenses in computer vision: A survey.",
            "Evasion attacks against machine learning at test time.",
            "Poisoning attacks against support vector machines.",
            "Jailbreaking black box large language models in twenty queries.",
            "Deepseek llm: Scaling open-source language models with longtermism.",
            "Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning.",
            "Deepseek-v3 technical report.",
            "The llama 3 herd of models.",
            "Jailbreaking llms with arabic transliteration and arabizi.",
            "Position: Fundamental limitations of llm censorship necessitate new approaches.",
            "Explaining and harnessing adversarial examples.",
            "A review of safe reinforcement learning: Methods, theories, and applications.",
            "Endless jailbreaks with bijection learning.",
            "Adversarial examples for evaluating reading comprehension systems.",
            "Certified robustness to adversarial word substitutions.",
            "Scaling laws for neural language models.",
            "Assessing robustness of text classification through maximal safe radius computation.",
            "A cross-language investigation into jailbreak attacks in large language models.",
            "Autodan: Generating stealthy jailbreak prompts on aligned large language models.",
            "A survey on adversarial attacks in computer vision: Taxonomy, visualization and future directions.",
            "The king is naked: On the notion of robustness for natural language processing.",
            "Textattack: A framework for adversarial attacks, data augmentation, and adversarial training in nlp.",
            "Prompting a pretrained transformer can be a universal approximator.",
            "Ask, and it shall be given: Turing completeness of prompting.",
            "Direct preference optimization: Your language model is secretly a reward model.",
            "Classes of recursively enumerable sets and their decision problems.",
            "Proximal policy optimization algorithms.",
            "Talking about large language models.",
            "Constitutional classifiers: Defending against universal jailbreaks across thousands of hours of red teaming.",
            "Intriguing properties of neural networks.",
            "Scaling monosemanticity: Extracting interpretable features from claude 3 sonnet.",
            "Universal adversarial triggers for attacking and analyzing nlp.",
            "Caesar cipher attack methods based on gpt-4o.",
            "Jailbreak and guard aligned language models with only few in-context demonstrations.",
            "Low-resource languages jailbreak gpt-4.",
            "Gpt-4 is too smart to be safe: Stealthy chat with llms via cipher.",
            "Promptrobust: Towards evaluating the robustness of large language models on adversarial prompts.",
            "Promptbench: Towards evaluating the robustness of large language models on adversarial prompts.",
            "Universal and transferable adversarial attacks on aligned language models."
        ]
    },
    {
        "index": 298,
        "title": "You Can’t Eat Your Cake and Have It Too: The Performance Degradation of LLMs with Jailbreak Defense",
        "publication_date": "2025-04-01",
        "references": [
            "LLaMA: Open and Efficient Foundation Language Models",
            "Qwen2 Technical Report",
            "ArtPrompt: ASCII Art-based Jailbreak Attacks against Aligned LLMs",
            "Jailbreak and guard aligned language models with only few in-context demonstrations",
            "Universal and transferable adversarial attacks on aligned language models",
            "Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena",
            "Tricking LLMs into Disobedience: Formalizing, Analyzing, and Detecting Jailbreaks",
            "Fight Back Against Jailbreaking via Prompt Adversarial Tuning",
            "Safe Unlearning: A Surprisingly Effective and Generalizable Solution to Defend Against Jailbreak Attacks",
            "AutoDAN: Generating Stealthy Jailbreak Prompts on Aligned Large Language Models",
            "AlpacaEval: An Automatic Evaluator of Instruction-following Models",
            "Refuse whenever you feel unsafe: Improving safety in llms via decoupled refusal training",
            "Multilingual Jailbreak Challenges in Large Language Models",
            "A Comprehensive Survey of Attack Techniques, Implementation, and Mitigation Strategies in Large Language Models",
            "Jailbreak Attacks and Defenses Against Large Language Models: A Survey",
            "Measuring Massive Multitask Language Understanding",
            "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding",
            "Chain of Thought Prompting Elicits Reasoning in Large Language Models",
            "Detecting Language Model Attacks with Perplexity",
            "Jailbreak and guard aligned language models with only few in-context demonstrations",
            "Defending chatgpt against jailbreak attack via self-reminders",
            "Autodefense: Multi-agent llm defense against jailbreak attacks",
            "Safe Unlearning: A Surprisingly Effective and Generalizable Solution to Defend Against Jailbreak Attacks",
            "JailbreakDefense: Defending Large Language Models Against Jailbreaking Attacks Through Goal Prioritization",
            "Language Models are Few-Shot Learners",
            "Is ChatGPT Getting Dumber?",
            "Automatic Pseudo-Harmful Prompt Generation for Evaluating False Refusals in Large Language Models",
            "XSTest: A Test Suite for Identifying Exaggerated Safety Behaviours in Large Language Models"
        ]
    },
    {
        "index": 299,
        "title": "T2ISafety: Benchmark for Assessing Fairness, Toxicity, and Privacy in Image Generation",
        "publication_date": "2025-02-20",
        "references": [
            "Moderating content",
            "Claude 3.5 sonnet",
            "Kandinsky 3.0 technical report",
            "Surrogateprompt: Bypassing the safety filter of text-to-image models via substitution",
            "Qwen technical report",
            "Hrs-bench: Holistic, reliable and scalable benchmark for text-to-image models",
            "Easily accessible text-to-image generation amplifies demographic stereotypes at large scale",
            "Executive order on the safe, secure, and trustworthy development and use of artificial intelligence",
            "Typology of risks of generative text-to-image models",
            "Pixart- α: Fast training of diffusion transformer for photorealistic text-to-image synthesis",
            "Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks",
            "Dall-eval: Probing the reasoning skills and social biases of text-to-image generation models",
            "The eu artificial intelligence act",
            "The socio-moral image database (smid): A novel stimulus set for the study of social, moral and affective processes",
            "Internlm-xcomposer2: Mastering free-form text-image composition and comprehension in vision-language large model",
            "Scaling rectified flow transformers for high-resolution image synthesis",
            "Tifa: Accurate and interpretable text-to-image faithfulness evaluation with question answering",
            "Llama guard: Llm-based input-output safeguard for human-ai conversations",
            "Camels in a changing climate: Enhancing lm adaptation with tulu 2",
            "Mixtral of experts",
            "Fairface: Face attribute dataset for balanced race, gender, and age for bias measurement and mitigation",
            "Pick-a-pic: An open dataset of user preferences for text-to-image generation",
            "Understanding fairness of gender classification algorithms across gender-race groups",
            "What matters when building vision-language models?",
            "Holistic evaluation of text-to-image models",
            "Playground v2.5: Three insights towards enhancing aesthetic quality in text-to-image generation",
            "Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models",
            "Diffhand: End-to-end hand mesh reconstruction via diffusion models",
            "Safegen: Mitigating unsafe content generation in text-to-image models",
            "Hunyuan-dit: A powerful multi-resolution diffusion transformer with fine-grained chinese understanding",
            "Sdxl-lightning: Progressive adversarial diffusion distillation",
            "Evaluating text-to-visual generation with image-to-text generation",
            "Llava-next: Improved reasoning, ocr, and world knowledge",
            "Mace: Mass concept erasure in diffusion models",
            "Faintbench: A holistic and precise benchmark for bias evaluation in text-to-image models",
            "Interrater reliability: the kappa statistic",
            "Midjourney. Terms of service",
            "Representation learning with contrastive predictive coding",
            "Dall·e 3",
            "Gpt-4o system card",
            "Usage policies",
            "Benchmark for compositional text-to-image synthesis",
            "Scalable diffusion models with transformers",
            "Sdxl: Improving latent diffusion models for high-resolution image synthesis",
            "Unsafe diffusion: On the generation of unsafe images and hateful memes from text-to-image models",
            "Unsafebench: Benchmarking image safety classifiers on real-world and ai-generated images",
            "Learning transferable visual models from natural language supervision",
            "Red-teaming the stable diffusion safety filter",
            "Kandinsky: an improved text-to-image synthesis with image prior and latent diffusion",
            "Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context",
            "Vidprom: A million-scale real prompt-gallery dataset for text-to-video diffusion models",
            "Diffusiondb: A large-scale prompt gallery dataset for text-to-image generative models",
            "A comprehensive study of multimodal large language models for image quality assessment",
            "Vila-u: a unified foundation model integrating visual understanding and generation",
            "Show-o: One single transformer to unify multimodal understanding and generation",
            "Sneakyprompt: Jailbreaking text-to-image generative models",
            "A survey on multimodal large language models",
            "Mm-llms: Recent advances in multimodal large language models"
        ]
    },
    {
        "index": 300,
        "title": "Refining Input Guardrails: Enhancing LLM-as-a-Judge Efficiency Through Chain-of-Thought Fine-Tuning and Alignment",
        "publication_date": "2025-01-22",
        "references": [
            "Securing Large Language Models: Threats, Vulnerabilities and Responsible Practices",
            "LLM-Based Chatbots for Mining Software Repositories: Challenges and Opportunities",
            "Evaluating Correctness and Faithfulness of Instruction-Following Models for Question Answering",
            "The Falcon Series of Open Language Models",
            "Many-shot Jailbreaking",
            "Guardrails for trust, safety, and ethical development and deployment of Large Language Models (LLM)",
            "Injecting New Knowledge into Large Language Models via Supervised Fine-Tuning",
            "Tree of Attacks: Jailbreaking Black-Box LLMs Automatically",
            "FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation",
            "”You Gotta be a Doctor, Lin”: An Investigation of Name-Based Bias of Large Language Models in Employment Recommendations",
            "GPT-4 Technical Report",
            "Direct Preference Optimiza tion: Your Language Model is Secretly a Reward Model",
            "NeMo Guardrails: A Toolkit for Controllable and Safe LLM Applications with Programmable Rails",
            "Analyzing and Reducing Catastrophic Forgetting in Parameter Efficient Tuning",
            "Towards Faithful and Robust LLM Specialists for Evidence-Based Question-Answering",
            "Scalable and Transferable Black-Box Jailbreaks for Language Models via Persona Modulation",
            "”Do Anything Now”: Characterizing and Evaluating In-The-Wild Jailbreak Prompts on Large Language Models",
            "Meta Llama Guard 2",
            "Llama 2: Open Foundation and Fine-Tuned Chat Models",
            "Advances in prospect theory: Cumulative representation of uncertainty",
            "”Kelly is a Warm Person, Joseph is a Role Model”: Gender Biases in LLM-Generated Reference Letters",
            "Adding guardrails to advanced chatbots",
            "Jailbroken: How does llm safety training fail?",
            "Chain of Thought Prompting Elicits Reasoning in Large Language Models",
            "Jailbreak and Guard Aligned Language Models with Only Few In-Context Demonstrations",
            "Hard Prompts Made Easy: Gradient-Based Discrete Optimization for Prompt Tuning and Discovery",
            "A Comprehensive Study of Jailbreak Attack versus Defense for Large Language Models",
            "Large Language Models Meet Text-Centric Multimodal Sentiment Analysis: A Survey",
            "Optimization Techniques for Sentiment Analysis Based on LLM (GPT-3)",
            "Defending large language models against jailbreaking attacks through goal prioritization",
            "Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena",
            "AutoDAN: Interpretable Gradient-Based Adversarial Attacks on Large Language Models",
            "Universal and Transferable Adversarial Attacks on Aligned Language Models",
            "Adversarial Attacks on LLMs: Efforts have been made to delineate the guidelines to make LLMs safe and robust to attacks",
            "Guardrails Strategies",
            "Prompt Template",
            "Chain-of-Thought Nature of LLM Responses",
            "Dataset Generation",
            "Computational Resource Requirements",
            "Hyperparameter Tuning Search Space",
            "Data Annotation: Accepted Answers",
            "LlamaGuard-2 Safety Prompt",
            "Qualitative Analysis of Explanations",
            "Discussions and Limitations"
        ]
    },
    {
        "index": 301,
        "title": "Siren: A Learning-Based Multi-Turn Attack Framework for Simulating Real-World Human Jailbreak Behaviors",
        "publication_date": "2025-01-24",
        "references": [
            "Detecting language model attacks with perplexity",
            "Jailbreaking black box large language models in twenty queries",
            "Safe RLHF: safe reinforcement learning from human feedback",
            "A wolf in sheep’s clothing: Generalized nested jailbreak prompts can fool large language models easily",
            "The Llama 3 herd of models",
            "MART: improving LLM safety with multi-round automatic red-teaming",
            "Mortar: Metamorphic multi-turn testing for llm-based dialogue systems",
            "DARE to diversify: Data driven and diverse LLM red teaming",
            "LLM self defense: By self examination, llms know they are being tricked",
            "Visual adversarial examples jailbreak aligned large language models",
            "Training socially aligned language models on simulated social interactions",
            "Autodan: Generating stealthy jailbreak prompts on aligned large language models",
            "AART: ai-assisted red-teaming with diverse data generation for new llm-powered applications",
            "Direct preference optimization: Your language model is secretly a reward model",
            "Derail yourself: Multi-turn llm jailbreak attack through self-discovered clues",
            "Great, now write an article about that: The crescendo multi-turn llm jailbreak attack",
            "”Do Anything Now”: Characterizing and evaluating in-the-wild jailbreak prompts on large language models",
            "Defending llms against jailbreaking attacks via backtranslation",
            "SELF-GUARD: empower the LLM to safeguard itself",
            "Jailbreak and guard aligned language models with only few in-context demonstrations",
            "Distract large language models for automatic jailbreak attack",
            "Safedecoding: Defending against jailbreak attacks via safety-aware decoding",
            "Baichuan 2: Open large-scale language models",
            "Jailbreak attacks and defenses against large language models: A survey",
            "How johnny can persuade llms to jail-break them: Rethinking persuasion to challenge AI safety by humanizing llms",
            "Autodefense: Multi-agent llm defense against jailbreak attacks",
            "Jailbreak open-sourced large language models via enforced decoding",
            "Speak out of turn: Safety vulnerability of large language models in multi-turn dialogue",
            "Universal and transferable adversarial attacks on aligned language models"
        ]
    },
    {
        "index": 302,
        "title": "PromptShield: Deployable Detection for Prompt Injection Attacks",
        "publication_date": "2024-01-01",
        "references": [
            "Synthetic Python Problems(SPP) Dataset.",
            "Fmops/Distilbert-Prompt-Injection.",
            "Evaluating Large Language Models Trained on Code.",
            "StruQ: Defending Against Prompt Injection with Structured Queries.",
            "Free Dolly: Introducing the World’s First Truly Open Instruction-Tuned LLM.",
            "Enhancing Chat Language Models by Scaling High-quality Instructional Conversations.",
            "The Llama 3 Herd of Models.",
            "Not What You’ve Signed up for: Compromising Real-World LLM-Integrated Applications with Indirect Prompt Injection.",
            "DeBERTaV3: Improving DeBERTa Using ELECTRA-Style Pre-Training with Gradient-Disentangled Embedding Sharing.",
            "LoRA: Low-Rank Adaptation of Large Language Models.",
            "Attention Tracker: Detecting Prompt Injection Attacks in LLMs.",
            "Challenges and Applications of Large Language Models.",
            "InjecGuard: Benchmarking and Mitigating Over-defense in Prompt Injection Guardrail Models.",
            "Formalizing and Benchmarking Prompt Injection Attacks and Defenses.",
            "Text-Davinci-003.",
            "Omni-Moderation-Latest.",
            "GPT-4 Technical Report.",
            "Training Language Models to Follow Instructions with Human Feedback.",
            "Ignore Previous Prompt: Attack Techniques For Language Models.",
            "Jatmo: Prompt Injection Defense by Task-Specific Finetuning.",
            "Fine-Tuned DeBERTa-v3-base for Prompt Injection Detection.",
            "Fine-Tuned DeBERTa-v3 for Prompt Injection Detection.",
            "Tricking LLMs into Disobedience: Formalizing, Analyzing, and Detecting Jailbreaks.",
            "Ignore This Title and HackAPrompt: Exposing Systemic Vulnerabilities of LLMs through a Global Scale Prompt Hacking Competition.",
            "Do Anything Now\": Characterizing and Evaluating In-The-Wild Jailbreak Prompts on Large Language Models.",
            "Stanford Alpaca: An Instruction-following LLaMA Model.",
            "Gemini: A Family of Highly Capable Multimodal Models.",
            "LLaMA: Open and Efficient Foundation Language Models.",
            "The Instruction Hierarchy: Training LLMs to Prioritize Privileged Instructions.",
            "CYBERSECEVAL 3: Advancing the Evaluation of Cybersecurity Risks and Capabilities in Large Language Models.",
            "Self-Instruct: Aligning Language Models with Self-Generated Instructions.",
            "Super-NaturalInstructions: Generalization via Declarative Instructions on 1600+ NLP Tasks.",
            "Finetuned Language Models Are Zero-Shot Learners.",
            "Jailbreak and Guard Aligned Language Models with Only Few In-Context Demonstrations.",
            "OWASP Top 10 for LLM Applications 2025.",
            "Benchmarking and Defending Against Indirect Prompt Injection Attacks on Large Language Models.",
            "LMSYS-Chat-1M: A Large-Scale Real-World LLM Conversation Dataset.",
            "Instruction-Following Evaluation for Large Language Models.",
            "Universal and Transferable Adversarial Attacks on Aligned Language Models."
        ]
    },
    {
        "index": 303,
        "title": "FDLLM: A Text Fingerprint Detection Method for LLMs in Multi-Language, Multi-Domain Black-Box Environments",
        "publication_date": "2025-01-27",
        "references": [
            "Decoding the ai pen: Techniques and challenges in detecting ai-generated text",
            "Gpt-4 technical report",
            "Claude3.5-haiku",
            "A watermark for black-box language models",
            "Baichuan4",
            "Longformer: The long-document transformer",
            "Internlm2 technical report",
            "Cloudflare ai",
            "Scalable watermarking for identifying large language model outputs",
            "Bert: Pre-training of deep bidirectional transformers for language understanding",
            "Doubao",
            "The llama 3 herd of models",
            "Gltr: Statistical detection and visualization of generated text",
            "Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context",
            "Gemma 2: Improving open language models at a practical size",
            "Chatglm: A family of large language models from glm-130b to glm-4 all tools",
            "Word list-350,000+ simple english words",
            "How close is chatgpt to human experts? comparison corpus, evaluation, and detection",
            "Mgtbench: Benchmarking machine-generated text detection",
            "Lora: Low-rank adaptation of large language models",
            "Automatic detection of generated text is easiest when humans are fooled",
            "Mistral 7b",
            "Chinese lexical analysis with deep bi-gru-crf network",
            "Mage: Machine-generated text detectionin the wild",
            "Deepseek-v3 technical report",
            "Detectgpt: Zero-shot machine-generated text detection using probability curvature",
            "Moonshot",
            "Use of llms for illicit purposes: Threats, prevention measures, and vulnerabilities",
            "Ollama",
            "Language models are unsupervised multitask learners",
            "Hey, that’s my model! introducing chain & hash, an llm fingerprinting technique",
            "Ten words only still help: Improving black-box ai-generated text detection via proxy-guided efficient re-sampling",
            "Release strategies and the social impacts of language models",
            "Detectllm: Leveraging log rank information for zero-shot detection of machine-generated text",
            "Llama 2: Open foundation and fine-tuned chat models",
            "Authorship attribution for neural textgeneration",
            "Attribution and obfuscation of neural text authorship: A data mining perspective",
            "Volcengine ai",
            "Freqmark: Frequency-based watermark for sentence-level detection of llm-generated text",
            "DNA-gpt: Divergent n-gram analysis for training-free detection of gpt-generated text",
            "Qwen2. 5 technical report",
            "Yi: Open foundation models by 01. ai",
            "Dpic: Decoupling prompt and intrinsic characteristics for llm generated text detection",
            "Opencsg chinese corpus: A series of high-quality chinese datasets for llm training",
            "Huref: Human-readable fingerprint for large language models"
        ]
    },
    {
        "index": 304,
        "title": "HATEBENCH: Benchmarking Hate Speech Detectors on LLM-Generated Content and Hate Campaigns",
        "publication_date": "2025-08-01",
        "references": [
            "Coleman-Liau index.",
            "Detoxify.",
            "GDPR.",
            "GPT:freddy griffin.",
            "GPT:Hate.",
            "GPT:Rude.",
            "Perspective API.",
            "The Gab Hate Corpus.",
            "Vicuna.",
            "ADL. ADL Task Force Issues Report Detailing Widespread Anti-Semitic Harassment of Journalists on Twitter During 2016 Campaign.",
            "ADL. Online Hate and Harassment: The American Experience 2023.",
            "Aunties, Strangers, and the FBI: Online Privacy Concerns and Experiences of Muslim-American Women.",
            "Google’s Jigsaw was trying to fight toxic speech with AI. Then the AI started talking.",
            "Robust Hate Speech Detection in Social Media: A Cross-Dataset Empirical Evaluation.",
            "METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments.",
            "The Pushshift Reddit Dataset.",
            "Nuanced Metrics for Measuring Unintended Bias with Real Data for Text Classification.",
            "HateGAN: Adversarial Generative-Based Data Augmentation for Hate Speech Detection.",
            "Universal Sentence Encoder for English.",
            "Hate is not Binary: Studying Abusive Behavior of #GamerGate on Twitter.",
            "Deep Reinforcement Learning from Human Preferences.",
            "Hate Speech Classifier.",
            "LLaMA: Open and Efficient Foundation Language Models.",
            "Crosslingual Generalization through Multitask Finetuning.",
            "The Challenge of Detecting Hate Speech.",
            "Toxicity in ChatGPT: Analyzing Persona-assigned Language Models.",
            "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.",
            "Peer to Peer Hate: Hate Speech Instigators and Their Targets.",
            "Guide: Large Language Models-Generated Fraud, Malware, and Vulnerabilities.",
            "PromptBench: Towards Evaluating the Robustness of Large Language Models on Adversarial Prompts."
        ]
    },
    {
        "index": 305,
        "title": "Virus: Harmful Fine-tuning Attack for Large Language Models Bypassing Guardrail Moderation",
        "publication_date": "2025-01-29",
        "references": [
            "Identifying and tuning safety neurons in large language models.",
            "Safety alignment shouldn’t be complicated.",
            "Your task may vary: A systematic understanding of alignment and safety degradation when fine-tuning LLMs.",
            "Probe before you talk: Towards black-box defense against backdoor unalignment for large language models.",
            "Training a helpful and harmless assistant with reinforcement learning from human feedback.",
            "Open problems in machine unlearning for ai safety.",
            "Safety-tuned lla-mas: Lessons from improving the safety of large language models that follow instructions.",
            "Convex optimization.",
            "Defending against unforeseen failure modes with latent adversarial training.",
            "Oml: Open, monetizable, and loyal ai.",
            "Safety-aware fine-tuning of large language models.",
            "Recent advances in attack and defense approaches of large language models.",
            "Safe rlhf: Safe reinforcement learning from human feedback.",
            "Raft: Reward ranked fine-tuning for generative foundation model alignment.",
            "Towards secure tuning: Mitigating security risks arising from benign instruction fine-tuning.",
            "Mimicking user data: On mitigating fine-tuning risks in closed large language models.",
            "Aegis2. 0: A diverse ai safety dataset and risks taxonomy for alignment of llm guardrails.",
            "Policy shaping: Integrating human feedback with reinforcement learning.",
            "Enhancing ai safety through the fusion of low rank adapters.",
            "The vllm safety paradox: Dual ease in jailbreak attack and defense.",
            "Covert malicious finetuning: Challenges in safeguarding llm adaptation.",
            "Wildguard: Open one-stop moderation tools for safety risks, jailbreaks, and refusals of llms.",
            "What’s in your\" safe\" data?: Identifying benign data that breaks safety.",
            "Safe lora: the silver lining of reducing safety risks when fine-tuning large language models.",
            "Efficient llm jailbreak via adaptive dense-to-sparse constrained optimization.",
            "Antidote: Post-fine-tuning safety alignment for large language models against harmful fine-tuning.",
            "Booster: Tackling harmful fine-tuning for large language models via attenuating harmful perturbation.",
            "Harmful fine-tuning attacks and defenses for large language models: A survey.",
            "Lazy safety alignment for large language models against harmful fine-tuning.",
            "Vaccine: Perturbation-aware alignment for large language model.",
            "Llama guard: Llm-based input-output safeguard for human-ai conversations.",
            "Beavertails: Towards improved safety alignment of llm via a human-preference dataset.",
            "Improved techniques for optimization-based jailbreaking on large language models.",
            "No two devils alike: Unveiling distinct mechanisms of fine-tuning attacks.",
            "Lora fine-tuning efficiently undoes safety training in llama 2-chat 70b.",
            "Salora: Safety-alignment preserved low-rank adaptation.",
            "Peft-as-an-attack! jailbreaking language models during federated parameter-efficient fine-tuning.",
            "Safety layers of aligned large language models: The key to llm security.",
            "Faster-gcg: Efficient discrete optimization jailbreak attacks against aligned large language models.",
            "Targeted vaccine: Safety alignment for large language models against harmful fine-tuning via layer-wise perturbation.",
            "Training socially aligned language models in simulated human society.",
            "Robustifying safety-aligned large language models through clean data curation.",
            "Keeping llms aligned after fine-tuning: The crucial role of prompt templates.",
            "Fine-tuning can cripple your foundation model; preserving features may be the solution.",
            "Training language models to follow instructions with human feedback.",
            "Granite guardian.",
            "Leveraging catastrophic forgetting to develop safe diffusion models against malicious finetuning.",
            "Navigating the safety landscape: Measuring risks in finetuning large language models.",
            "Fine-tuning aligned language models compromises safety, even when users do not intend to!",
            "Safety alignment should be made more than just a few tokens deep.",
            "On evaluating the durability of safeguards for open-weight llms.",
            "Direct preference optimization: Your language model is secretly a reward model.",
            "Nemo guardrails: A toolkit for controllable and safe llm applications with programmable rails.",
            "Open problems in technical ai governance.",
            "Defending against reverse preference attacks is difficult.",
            "Representation noising effectively prevents harmful fine-tuning on llms.",
            "Immunization against harmful fine-tuning attacks.",
            "Safety-enhanced aligned llm fine-tuning via bilevel data selection.",
            "Open-ethical ai: Advancements in open-source human-centric neural language models.",
            "Preference ranking optimization for human alignment.",
            "Tamper-resistant safeguards for open-weight llms.",
            "Hˆ 3 fusion: Helpful, harmless, honest fusion of aligned llms.",
            "Operationalizing a threat model for red-teaming large language models (llms).",
            "Mitigating fine-tuning jailbreak attack with backdoor enhanced alignment.",
            "Assessing the brittleness of safety alignment via pruning and low-rank modifications.",
            "Separate the wheat from the chaff: A post-hoc approach to safety realignment for fine-tuned language models.",
            "Pairwise proximal policy optimization: Harnessing relative feedback for llm alignment.",
            "Shadow alignment: The ease of subverting safely-aligned language models.",
            "Emerging safety attack and defense in federated instruction tuning of large language models.",
            "Selfee: Iterative self-revising llm empowered by self-feedback generation.",
            "On the vulnerability of safety alignment in open-access llms.",
            "Nlsr: Neuron-level safety realignment of large language models against harmful fine-tuning.",
            "A safety realignment framework via subspace-oriented model fusion for large language models.",
            "Rrhf: Rank responses to align language models with human feedback without tears.",
            "Removing rlhf protections in gpt-4 via fine-tuning.",
            "Locking down the finetuned llms safety.",
            "Safety fine-tuning at (almost) no cost: A baseline for vision large language models.",
            "Universal and transferable adversarial attacks on aligned language models.",
            "Improving alignment and robustness with circuit breakers."
        ]
    },
    {
        "index": 306,
        "title": "GuardReasoner: Towards Reasoning-based LLM Safeguards",
        "publication_date": "2025-01-30",
        "references": [
            "Gpt-4 technical report",
            "Abstraction and reasoning corpus for artificial general intelligence",
            "A general language assistant as a laboratory for alignment",
            "Azure ai content safety",
            "Promptsource: An integrated development environment and repository for natural language prompts",
            "Like a good nearest neighbor: Practical content moderation and text classification",
            "Do not think that much for 2+ 3=? on the overthinking of o1-like llms",
            "Black-box prompt optimization: Aligning large language models without model training",
            "Llama guard 3 vision: Safeguarding human-ai image understanding conversations",
            "Introducing devin, the first ai software engineer",
            "Safe rlhf: Safe reinforcement learning from human feedback",
            "Vlmguard: Defending vlms against malicious prompts via unlabeled data",
            "Improving factuality and reasoning in language models through multiagent debate",
            "The llama 3 herd of models",
            "Understanding dataset difficulty with mathcal v-usable information",
            "Using punctuation as an adversarial attack on deep learning-based nlp systems: An empirical study",
            "Semrode: Macro adversarial training to learn representations that are robust to word-level attacks",
            "Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned",
            "Aegis: Online adaptive ai content safety moderation with ensemble of llm experts",
            "Aegis2. 0: A diverse ai safety dataset and risks taxonomy for alignment of llm guardrails",
            "Think before you speak: Training language models with pause tokens",
            "Deliberative alignment: Reasoning enables safer language models",
            "Cold-attack: Jailbreaking llms with stealthiness and controllability",
            "Wildguard: Open one-stop moderation tools for safety risks, jailbreaks, and refusals of llms",
            "Training large language models to reason in a continuous latent space",
            "Qwen2. 5-coder technical report",
            "Llama guard: Llm-based input-output safeguard for human-ai conversations",
            "Ai alignment: A comprehensive survey",
            "Aligner: Efficient alignment by learning to correct",
            "Beavertails: Towards improved safety alignment of llm via a human-preference dataset",
            "Advlora: Adversarial low-rank adaptation of vision-language models",
            "Mistral 7b",
            "Mixtral of experts",
            "R2-guard: Robust reasoning enabled llm guardrail via knowledge-enhanced logical reasoning",
            "Critiquellm: Scaling llm-as-critic for effective and explainable evaluation of large language model generation",
            "Large language models are zero-shot reasoners",
            "Pretraining language models with human preferences",
            "Training language models to self-correct via reinforcement learning",
            "Watch your language: large language models and content moderation",
            "Efficient memory management for large language model serving with pagedattention",
            "A new generation of perspective api: Efficient multilingual character-level transformers",
            "Salad-bench: A hierarchical and comprehensive safety benchmark for large language models",
            "Encouraging divergent thinking in large language models through multi-agent debate",
            "Toxicchat: Unveiling hidden challenges of toxicity detection in real-world user-ai conversation",
            "Deepseek-v3 technical report",
            "On calibration of llm-based guard models for reliable content moderation",
            "Flipattack: Jailbreak llms via flipping",
            "Inference-time policy adapters (ipa): Tailoring extreme-scale lms without fine-tuning",
            "Adapting large language models for content moderation: Pitfalls in data engineering and supervised fine-tuning",
            "At which training stage does code data help llms reasoning?",
            "A holistic approach to undesired content detection in the real world",
            "Harm-bench: A standardized evaluation framework for automated red teaming and robust refusal",
            "Guardformer: Guardrail instruction pretraining for efficient safeguarding",
            "Introducing chatgpt",
            "Searchgpt prototype",
            "Training language models to follow instructions with human feedback",
            "Direct preference optimization: Your language model is secretly a reward model",
            "Nemo guardrails: A toolkit for controllable and safe llm applications with programmable rails",
            "Gemini 1.5: Unlocking multi-modal understanding across millions of tokens of context",
            "Llama: Open and efficient foundation language models",
            "Llama 2: Open foundation and fine-tuned chat models",
            "detoxify",
            "Lightweight safety guardrails using fine-tuned bert embeddings",
            "Shieldgemma: Generative ai content moderation based on gemma",
            "Llamafactory: Unified efficient fine-tuning of 100+ language models"
        ]
    },
    {
        "index": 307,
        "title": "Rethinking Bottlenecks in Safety Fine-Tuning of Vision Language Models",
        "publication_date": "2025-01-30",
        "references": [
            "Phi-3 technical report: A highly capable language model locally on your phone",
            "Gpt-4 technical report",
            "Are we on the right way for evaluating large vision-language models?",
            "DRESS: Instructing large vision-language models to align and interact with humans via natural language feedback.",
            "Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling",
            "ETA: Evaluating then aligning safety of vision language models at inference time",
            "Vlmevalkit: An open-source toolkit for evaluating large multi-modality models",
            "Scaling rectified flow transformers for high-resolution image synthesis",
            "Figstep: Jailbreaking large vision-language models via typographic visual prompts",
            "MAMMOTH-VL: Eliciting multimodal reasoning with instruction tuning at scale",
            "The vllm safety paradox: Dual ease in jailbreak attack and defense",
            "Vlsbench: Unveiling visual leakage in multimodal safety",
            "Llama guard: Llm-based input-output safeguard for human-ai conversations",
            "PKU-SAFTERLHF: A safety alignment preference dataset for llama family models",
            "MANTIS: Interleaved multi-image instruction tuning",
            "Efficient memory management for large language model serving with pagedattention",
            "Building and better understanding vision-language models: insights and future directions",
            "Llava-onevision: Easy visual task transfer",
            "Llava-next-interleave: Tackling multi-image, video, and 3d in large multimodal models",
            "BLIP-2: Bootstrapping language-image pre-training with frozen image encoders and large language models",
            "Salad-bench: A hierarchical and comprehensive safety benchmark for large language models",
            "Red teaming visual language models",
            "Improved baselines with visual instruction tuning",
            "Visual instruction tuning",
            "Mm-safetybench: A benchmark for safety evaluation of multimodal large language models",
            "Mmuu: A massive multi-discipline multimodal understanding and reasoning benchmark for expert agi",
            "Muirbench: A comprehensive benchmark for robust multi-image understanding",
            "Qwen2-vl: Enhancing vision-language model’s perception of the world at any resolution",
            "Cross-modality safety alignment",
            "Q-bench: A benchmark for general-purpose foundation models on low-level vision",
            "Deepseek-vl2: Mixture-of-experts vision-language models for advanced multi-modal understanding",
            "Llava-o1: Let vision language models reason step-by-step",
            "Qwen2. 5 technical report",
            "Minicpm-v: A gpt-4v level mllm on your phone",
            "Mmt-bench: A comprehensive multimodal benchmark for evaluating large vision-language models towards multitask agi",
            "Mm-star: A human-selected, vision-indispensable multi-modal benchmark",
            "Lmms-eval: Reality check on the evaluation of large multimodal models",
            "Spa-vl: A comprehensive safety preference alignment dataset for vision language model",
            "Multimodal situational safety",
            "Safety fine-tuning at (almost) no cost: A baseline for vision large language models",
            "Universal and transferable adversarial attacks on aligned language models"
        ]
    },
    {
        "index": 308,
        "title": "Towards Safe AI Clinicians: A Comprehensive Study on Large Language Model Jailbreaking in Healthcare",
        "publication_date": "2024-12-01",
        "references": [
            "Medical Large Language Models are Vulnerable to Data-poisoning Attacks",
            "Med-HALT: Medical Domain Hallucination Test for Large Language Models",
            "Privacy Risks of General-purpose Language Models",
            "A Toolbox for Surfacing Health Equity Harms and Biases in Large Language Models",
            "Usage policies",
            "Jailbreak Attacks and Defenses against Large Language Models: A survey",
            "A Realistic Threat Model for Large Language Model Jailbreaks",
            "JailbreakEval: An Integrated Toolkit for Evaluating Jailbreak Attempts against Large Language Models",
            "Language Models are Few-Shot Learners",
            "Adopting and Expanding Ethical Principles for Generative Artificial Intelligence from Military to Healthcare",
            "The Ethics of ChatGPT in Medicine and Healthcare: A Systematic Review on Large Language Models (LLMs)",
            "Medical Large Language Models are Susceptible to Targeted Misinformation Attacks",
            "Reconciling Privacy and Accuracy in AI for Medical Imaging",
            "Mitigating the Risk of Health Inequity Exacerbated by Large Language Models",
            "Jailbreaking Black Box Large Language Models in Twenty Queries",
            "How Johnny can Persuade LLMs to Jailbreak Them: Rethinking Persuasion to Challenge AI Safety by Humanizing LLMs",
            "A Wolf in Sheep’s Clothing: Generalized Nested Jailbreak Prompts can Fool Large Language Models Easily",
            "CodeChameleon: Personalized Encryption Framework for Jailbreaking Large Language Models",
            "FlipAttack: Jailbreak LLMs via Flipping",
            "Jailbroken: How does LLM Safety Training Fail?",
            "Fine-tuning Aligned Language Models Compromises Safety, Even When Users Do not Intend to!",
            "HarmBench: A Standardized Evaluation Framework for Automated Red Teaming and Robust Refusal",
            "A StrongREJECT for Empty Jailbreaks",
            "MedSafetyBench: Evaluating and Improving the Medical Safety of Large Language Models",
            "GPT-4o System Card",
            "GPT-4 Technical Report",
            "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning",
            "The Llama 3 Herd of Models",
            "MEDITRON-70B: Scaling Medical Pretraining for Large Language Models",
            "Code of Medical Ethics",
            "LoRA: Low-Rank Adaptation of Large Language Models"
        ]
    },
    {
        "index": 309,
        "title": "Graph of Attacks with Pruning: Optimizing Stealthy Jailbreak Prompt Generation for Enhanced LLM Content Moderation",
        "publication_date": "2025-01-28",
        "references": [
            "Detecting language model attacks with perplexity",
            "Graph of thoughts: Solving elaborate problems with large language models",
            "Jailbreakbench: An open robustness benchmark for jail-breaking large language models",
            "Jailbreaking black box large language models in twenty queries",
            "Attack prompt generation for red teaming and defending large language models",
            "The llama 3 herd of models",
            "Llama guard: Llm-based input-output safeguard for human-ai conversations",
            "Open sesame! universal black box jailbreaking of large language models",
            "Toxicchat: Unveiling hidden challenges of toxicity detection in real-world user-ai conversation",
            "AutoDAN: Generating stealthy jail-break prompts on aligned large language models",
            "Universal and transferable adversarial attacks on aligned language models",
            "How trustworthy are open-source LLMs? an assessment under malicious demonstrations shows their vulnerabilities",
            "Red teaming language models with language models",
            "Adversarial attacks and defenses in large language models: Old and new threats",
            "Jailbreak in pieces: Compositional adversarial attacks on multi-modal language models",
            "Survey of vulnerabilities in large language models revealed by adversarial attacks",
            "Detoxifying language models risks marginalizing minority voices",
            "GPT-fuzzer: Red teaming large language models with auto-generated jailbreak prompts",
            "Large language models are human-level prompt engineers",
            "Universal and transferable adversarial attacks on aligned language models"
        ]
    },
    {
        "index": 310,
        "title": "Constitutional Classifiers: Defending against Universal Jailbreaks across Thousands of Hours of Red Teaming",
        "publication_date": "2025-01-31",
        "references": [
            "Understanding intermediate layers using linear classifier probes",
            "Jailbreaking leading safety-aligned LLMs with simple adaptive attacks",
            "Many-shot jailbreaking",
            "Anthropic’s responsible scaling policy",
            "Model card and evaluations for claude models",
            "Claude’s character",
            "The Llama 3 herd of models",
            "Defending against unforeseen failure modes with latent adversarial training",
            "Training language models to follow instructions with human feedback",
            "Llama guard: LLM-based input-output safeguard for human-AI conversations",
            "Testing the limits of jailbreaking defenses with the purple problem",
            "AutoDAN: Generating stealthy jailbreak prompts on aligned large language models",
            "Measuring massive multitask language understanding",
            "Unsolved problems in ML safety",
            "Best-of-N jailbreaking",
            "The WMDP benchmark: Measuring and reducing malicious use with unlearning",
            "Probing toxic content in large pre-trained language models",
            "NeMo guardrails: A toolkit for controllable and safe LLM applications with programmable rails",
            "GPQA: A graduate-level Google-proof Q&A benchmark",
            "OR-Bench: An over-refusal benchmark for large language models",
            "h4rm3l: A dynamic benchmark of composable jailbreak attacks for LLM safety assessment",
            "Improving alignment and robustness with circuit breakers",
            "Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned",
            "Detecting pretraining data from large language models",
            "Chain of thought prompting elicits reasoning in large language models",
            "Trading inference-time compute for adversarial robustness",
            "Safe unlearning: A surprisingly effective and generalizable solution to defend against jailbreak attacks",
            "Wildchat: 1M ChatGPT interaction logs in the wild"
        ]
    },
    {
        "index": 311,
        "title": "Distorting Embedding Space for Safety: A Defense Mechanism for Adversarially Robust Diffusion Models",
        "publication_date": "2025-01-31",
        "references": [
            "What is Amazon Comprehend?",
            "Localizing and editing knowledge in text-to-image generative models",
            "Nudenet: Neural nets for nudity classification, detection and selective censoring",
            "Improving image generation with better captions",
            "Instructpix2pix: Learning to follow image editing instructions",
            "Microsoft coco captions: Data collection and evaluation server",
            "Prompting4debugging: Red-teaming text-to-image diffusion models by finding problematic prompts",
            "The socio-moral image database (smid): A novel stimulus set for the study of social, moral and affective processes",
            "Divide-and-conquer attack: Harnessing the power of llm to bypass the censorship of text-to-image generation model",
            "Salun: Empowering machine unlearning via gradient-based weight saliency in both image classification and generation",
            "Erasing concepts from diffusion models",
            "Unified concept editing in diffusion models",
            "Clipscore: A reference-free evaluation metric for image captioning",
            "Gans trained by a two time-scale update rule converge to a local nash equilibrium",
            "Denoising diffusion probabilistic models",
            "Latent guard: a safety framework for text-to-image generation",
            "One-dimensional adapter to rule them all: Concepts diffusion models and erasing applications",
            "Moderation api overview",
            "Circumventing concept erasure methods for text-to-image generative models",
            "Safe-clip: Removing nsfw concepts from vision-and-language models",
            "Red-teaming the stable diffusion safety filter",
            "High-resolution image synthesis with latent diffusion models",
            "Can machines help us answering question 16 in datasheets, and in turn reflecting on inappropriate content?",
            "Safe latent diffusion: Mitigating inappropriate degeneration in diffusion models",
            "Unlearning or concealment? a critical analysis and evaluation metrics for unlearning in diffusion models",
            "Deep unsupervised learning using nonequilibrium thermodynamics",
            "Attacks and defenses for generative diffusion models: A comprehensive survey",
            "Ring-a-bell! how reliable are concept removal methods for diffusion models?",
            "Robustness may be at odds with accuracy",
            "Mma-diffusion: Multimodal attack on diffusion models",
            "Guardt2i: Defending text-to-image models from adversarial prompts",
            "Sneakyprompt: Jailbreaking text-to-image generative models",
            "Safree: Training-free and adaptive guard for safe text-to-image and video generation",
            "Forget-me-not: Learning to forget in text-to-image diffusion models",
            "Defensive unlearning with adversarial training for robust concept erasure in diffusion models",
            "To generate or not? safety-driven unlearned diffusion models are still easy to generate unsafe images... for now"
        ]
    },
    {
        "index": 312,
        "title": "Enhancing Model Defense Against Jailbreaks with Proactive Safety Reasoning",
        "publication_date": "2025-01-31",
        "references": [
            "Llama 3 model card",
            "Many-shot jailbreaking",
            "Identifying and tuning safety neurons in large language models",
            "Claude 3.5 sonnet",
            "Training a helpful and harmless assistant with reinforcement learning from human feedback",
            "Constitutional ai: Harmlessness from ai feedback",
            "Jailbreakbench: An open robustness benchmark for jailbreaking large language models",
            "Jailbreaking black box large language models in twenty queries",
            "Finding safety neurons in large language models",
            "Training verifiers to solve math word problems",
            "Free dolly: Introducing the world’s first truly open instruction-tuned llm",
            "Multilingual jailbreak challenges in large language models",
            "Deliberative alignment: Reasoning enables safer language models",
            "What is in your safe data? identifying benign data that breaks safety",
            "Measuring massive multitask language understanding",
            "LoRA: Low-rank adaptation of large language models",
            "Mistral 7b",
            "Retrieval-augmented generation for knowledge-intensive nlp tasks",
            "Llm defenses are not robust to multi-turn human jailbreaks yet",
            "Autodan: Generating stealthy jailbreak prompts on aligned large language models",
            "The llama 3 herd of models",
            "Wizardcoder: Empowering code large language models with evol-instruct",
            "Harmbench: A standardized evaluation framework for automated red teaming and robust refusal",
            "Derail yourself: Multi-turn llm jailbreak attack through self-discovered clues",
            "Proximal policy optimization algorithms",
            "Attention is all you need",
            "Mrj-agent: An effective jailbreak agent for multi-round dialogue",
            "Jailbroken: How does llm safety training fail?",
            "Assessing the brittleness of safety alignment via pruning and low-rank modifications",
            "Huggingface’s transformers: State-of-the-art natural language processing",
            "Sorry-bench: Systematically evaluating large language model safety refusal behaviors",
            "WizardLM: Empowering large pre-trained language models to follow complex instructions",
            "On the vulnerability of safety alignment in open-access LLMs",
            "Simulating classroom education with llm-empowered agents",
            "Learning and forgetting unsafe examples in large language models",
            "Multi-round jailbreak attack on large language models",
            "Speak out of turn: Safety vulnerability of large language models in multi-turn dialogue",
            "Universal and transferable adversarial attacks on aligned language models",
            "Improving alignment and robustness with circuit breakers"
        ]
    },
    {
        "index": 313,
        "title": "Defense Against the Dark Prompts: Mitigating Best-of-N Jailbreaking with Prompt Evaluation",
        "publication_date": "2025-01-01",
        "references": [
            "Pushing Boundaries or Crossing Lines? The Complex Ethics of ChatGPT Jailbreaking",
            "Automatic jailbreaking of the text-to-image generative ai systems",
            "Jailbreakzoo: Survey, landscapes, and horizons in jailbreaking large language and vision-language models",
            "Figstep: Jailbreaking large vision-language models via typographic visual prompts",
            "Jailbreaker: Automated jailbreak across multiple large language model chatbots",
            "Don't listen to me: Understanding and exploring jailbreak prompts of large language models",
            "Jailbreaking large language models with symbolic mathematics",
            "How johnny can persuade llms to jailbreak them: Rethinking persuasion to challenge ai safety by humanizing llms",
            "Contextual manipulations in adversarial prompting",
            "Defensive strategies against ai jailbreaking: Challenges and innovations",
            "Defending ChatGPT against jailbreak attack via self-reminders",
            "Self-guard: Empower the llm to safeguard itself",
            "Wildteaming at scale: From in-the-wild jailbreaks to (adversarially) safer language models",
            "Tricking LLMs into Disobedience: Formalizing, Analyzing, and Detecting Jailbreaks",
            "Jailbreakbench: An open robustness benchmark for jailbreaking large language models",
            "Best-of-n jailbreaking",
            "Improving alignment and robustness with circuit breakers",
            "Using gpt-eliezer against chatgpt jailbreaking",
            "chatgpt-prompt-evaluator on aligned ai's github",
            "Harmbench: A standardized evaluation framework for automated red teaming and robust refusal",
            "The use of confidence or fiducial limits illustrated in the case of the binomial",
            "Ai control: Improving safety despite intentional subversion"
        ]
    },
    {
        "index": 314,
        "title": "Towards Robust Multimodal Large Language Models Against Jailbreak Attacks",
        "publication_date": "2025-02-02",
        "references": [
            "Recent advances in adversarial training for adversarial robustness",
            "Image hijacks: Adversarial images can control generative models at runtime",
            "Defending against alignment-breaking attacks via robustly aligned LLM",
            "Personalized steering of large language models: Versatile steering vectors through bi-directional preference optimization",
            "Jailbreaking black box large language models in twenty queries",
            "Dress: Instructing large vision-language models to align and interact with humans via natural language feedback",
            "Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality",
            "Instructblip: Towards general-purpose vision-language models with instruction tuning",
            "Eva: Exploring the limits of masked visual representation learning at scale",
            "Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis",
            "Figstep: Jailbreaking large vision-language models via typographic visual prompts",
            "Eyes closed, safety on: Protecting multimodal llms via image-to-text transformation",
            "Llava-guard: Vlm-based safeguards for vision dataset curation and safety assessment",
            "Latent adversarial training improves robustness to persistent harmful behaviors in llms",
            "Jailbreaking attack against multimodal large language model",
            "Mllm-protector: Ensuring mllm’s safety without hurting performance",
            "Hidden killer: Invisible textual back-door attacks with syntactic trigger",
            "Visual adversarial examples jailbreak aligned large language models",
            "Fine-tuning aligned language models compromises safety, even when users do not intend to!",
            "Universal adversarial training",
            "Latent adversarial training improves robustness to persistent harmful behaviors in llms",
            "Minigpt-4: Enhancing vision-language understanding with advanced large language models",
            "Safety fine-tuning at (almost) no cost: A baseline for vision large language models",
            "Universal and transferable adversarial attacks on aligned language models"
        ]
    },
    {
        "index": 315,
        "title": "‘Do as I say not as I do’: A Semi-Automated Approach for Jailbreak Prompt Attack against Multimodal LLMs",
        "publication_date": "2018-02-01",
        "references": [
            "Gemini Policy Guidelines",
            "Gpt-4 technical report",
            "Training a helpful and harmless assistant with reinforcement learning from human feedback",
            "Play guessing game with llm: Indirect jailbreak attack with implicit clues",
            "Generative adversarial user model for reinforcement learning based recommendation system",
            "Comprehensive assessment of jailbreak attacks against llms",
            "On the impossible safety of large AI models",
            "Imagebind: One embedding space to bind them all",
            "Llm censorship: A machine learning challenge or a computer security problem?",
            "Figstep: Jailbreaking large vision-language models via typographic visual prompts",
            "Explaining and harnessing adversarial examples",
            "Not what you’ve signed up for: Compromising real-world llm-integrated applications with indirect prompt injection",
            "Image-based Multimodal Models as Intruders: Transferable Multimodal Attacks on Video-based MLLMs",
            "Catastrophic jailbreak of open-source llms via exploiting generation",
            "Pleak: Prompt leaking attacks against large language model applications",
            "Gpt-4o system card",
            "Large language models are zero-shot reasoners",
            "Jailbreak Attacks and Defenses against Multimodal Generative Models: A Survey",
            "Reading Isn’t Believing: Adversarial Attacks On Multi-Modal Neurons",
            "Generative AI vs. LLMs: What’s the Difference? | Kovaion — kovaion.com",
            "Learning transferable visual models from natural language supervision",
            "Great, now write an article about that: The crescendo multi-turn llm jailbreak attack",
            "Plug and Pray: Exploiting off-the-shelf components of Multi-Modal Models",
            "Survey of vulnerabilities in large language models revealed by adversarial attacks",
            "‘Do Anything Now’: Characterizing and evaluating in-the-wild jailbreak prompts on large language models",
            "Voice Jailbreak Attacks Against GPT-4o",
            "Gemini: a family of highly capable multimodal models",
            "Llama 2: Open foundation and fine-tuned chat models",
            "Sandwich attack: Multi-language mixture adaptive attack on llms",
            "The art of defending: A systematic evaluation and analysis of llm defense strategies on safety and over-defensiveness",
            "A comprehensive review of multimodal large language models: Performance and challenges across different tasks",
            "A Practical Survey on Emerging Threats from AI-driven Voice Attacks: How Vulnerable are Commercial Voice Control Systems?",
            "Jailbroken: How does llm safety training fail?",
            "On the tool manipulation capability of open-source large language models",
            "An LLM can Fool Itself: A Prompt-Based Adversarial Attack",
            "LLM Jailbreak Attack versus Defense Techniques–A Comprehensive Study",
            "Don’t Listen To Me: Understanding and Exploring Jailbreak Prompts of Large Language Models",
            "Llavar: Enhanced visual instruction tuning for text-rich image understanding",
            "Igniting Language Intelligence: The Hitchhiker’s Guide From Chain-of-Thought Reasoning to Language Agents"
        ]
    },
    {
        "index": 316,
        "title": "AgentBreeder: Mitigating the AI Safety Impact of Multi-Agent Scaffolds",
        "publication_date": "2025-02-02",
        "references": [
            "Inspect AI: Framework for Large Language Model Evaluations",
            "Can llms reason like humans? assessing theory of mind reasoning in llms for open-ended questions",
            "Claude 3.5 sonnet model card addendum",
            "Constitutional ai: Harmlessness from ai feedback",
            "A mechanical proof of the turing completeness of pure lisp",
            "Weak-to-strong generalization: Eliciting strong capabilities with weak supervision",
            "Llm-collab: a framework for enhancing task planning via chain-of-thought and multi-agent collaboration",
            "Chateval: Towards better llm-based evaluators through multi-agent debate",
            "Jailbreaking black box large language models in twenty queries",
            "Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors in agents",
            "On the measure of intelligence",
            "Introducing web-world models, December 2024",
            "Improving factuality and reasoning in language models through multiagent debate",
            "Drop: A reading comprehension benchmark requiring discrete reasoning over paragraphs",
            "Magentic-one: A generalist multi-agent system for solving complex tasks",
            "Scaffolded llms: Less obvious concerns",
            "Sciagents: Automating scientific discovery through multi-agent intelligent graph reasoning",
            "Measuring massive multitask language understanding",
            "Automated design of agentic systems",
            "Toward general-purpose robots via foundation models: A survey and meta-analysis",
            "Self-evolving multi-agent collaboration networks for software development",
            "On the resilience of multi-agent systems with malicious agents",
            "Elite multi-criteria decision making—pareto front optimization in multi-objective optimization",
            "Why multi-agent safety is important",
            "A superalignement framework in autonomous driving with large language models",
            "Agent-oriented planning in multi-agent systems",
            "Salad-bench: A hierarchical and comprehensive safety benchmark for large language models",
            "TruthfulQA: Measuring how models mimic human falsehoods",
            "Prompt injection attack against llm-integrated applications",
            "The ai scientist: Towards fully automated open-ended scientific discovery",
            "Self-refine: Iterative refinement with self-feedback",
            "AILuminate v1.0 benchmark",
            "Illuminating search spaces by mapping elites",
            "Polaris: A safety-focused llm constellation architecture for healthcare",
            "simple-evals",
            "Gpt-4o mini: Advancing cost-efficient intelligence",
            "Learning to reason with llms",
            "New embedding models and api updates",
            "Computer-using agent: Introducing a universal interface for ai to interact with the digital world",
            "Red teaming language models with language models",
            "Communicative agents for software development",
            "Gpqa: A graduate-level google-proof q&a benchmark",
            "Mathematical discoveries from program search with large language models",
            "How can small data sets be clustered?",
            "Comfybench: Benchmarking llm-based agents in comfyui for autonomously designing collaborative ai systems",
            "Multi-llm-agent systems: Techniques and business perspectives",
            "Evaluating the code quality of ai-assisted code generation tools: An empirical study on github copilot, amazon codewhisperer, and chatgpt",
            "Gödel agent: A self-referential agent framework for recursive self-improvement",
            "Evoagent: Towards automatic multi-agent generation via evolutionary algorithms",
            "Trading inference-time compute for adversarial robustness.",
            "Psysafe: A comprehensive framework for psychological-based attack, defense, and evaluation of multi-agent system safety",
            "Mmlu-cf: A contamination-free multi-task language understanding benchmark",
            "A survey of large language models"
        ]
    },
    {
        "index": 318,
        "title": "THE DARK DEEP SIDE OF DEEPSEEK: FINE-TUNING ATTACKS AGAINST THE SAFETY ALIGNMENT OF COT-ENABLED MODELS",
        "publication_date": "2025-02-03",
        "references": [
            "Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025.",
            "Poisoning web-scale training datasets is practical. In 2024 IEEE Symposium on Security and Privacy (SP) , pages 407–425. IEEE, 2024.",
            "Building an early warning system for llm-aided biological threat creation. Building an Early Warning System for LLM-Aided Biological Threat Creation , 31, 2024.",
            "A survey on large language model (llm) security and privacy: The good, the bad, and the ugly. High-Confidence Computing , page 100211, 2024.",
            "Using ai assistants in software development: A qualitative study on security practices and concerns. In Proceedings of the 2024 on ACM SIGSAC Conference on Computer and Communications Security , pages 2726–2740, 2024.",
            "Fine-tuning aligned language models compromises safety, even when users do not intend to! arXiv preprint arXiv:2310.03693 , 2023.",
            "Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems , 35:24824–24837, 2022.",
            "Large language models are zero-shot reasoners. Advances in neural information processing systems , 35:22199–22213, 2022.",
            "Targeted latent adversarial training improves robustness to persistent harmful behaviors in llms. arXiv preprint arXiv:2407.15549 , 2024.",
            "Lora: Low-rank adaptation of large language models, 2021.",
            "Harmbench: A standardized evaluation framework for automated red teaming and robust refusal, 2024.",
            "Trading inference-time compute for adversarial robustness. OpenAI , 2025."
        ]
    },
    {
        "index": 319,
        "title": "Peering Behind the Shield: Guardrail Identification in Large Language Models",
        "publication_date": "2025-02-03",
        "references": [
            "https://openai.com/index/hello-gpt-4o/ . 1, 2, 5, 11",
            "https://claude.ai/ . 1",
            "https://chat.deepseek.com/ . 1",
            "https://chat.openai.com/chat . 1, 6",
            "https://www.perspectiveapi.com . 1, 2, 5, 11",
            "https://policies.google.com/terms/generative-ai/use-policy?hl=en . 2",
            "https://ai.meta.com/llama/use-policy/ . 2",
            "https://openai.com/policies/usage-policies . 2",
            "https://aws.amazon.com/cn/machine-learning/responsible-ai/policy/ . 2",
            "https://platform.openai.com/docs/guides/moderation/overview . 2",
            "https://www.llama.com/docs/model-cards-and-prompt-formats/meta-llama-guard-2/ . 3, 5, 11",
            "Llama Guard 3 Vision: Safeguarding Human-AI Image Understanding Conversations. CoRR abs/2411.10414 , 2024. 1, 3, 5, 11",
            "Agent-Dojo: A Dynamic Environment to Evaluate Attacks and Defenses for LLM Agents. CoRR abs/2406.13352 , 2024. 1, 3",
            "Fingerprinting Fine-tuned Language Models in the Wild. In Annual Meeting of the Association for Computational Linguistics and International Joint Conference on Natural Language Processing (ACL/IJCNLP) , pages 4652–4664. ACL, 2021. 3",
            "The Llama 3 Herd of Models. CoRR abs/2407.21783 , 2024. 5, 11",
            "AEGIS: Online Adaptive AI Content Safety Moderation with Ensemble of LLM Experts. CoRR abs/2404.05993 , 2024. 3, 5, 11",
            "Fig-8Step: Jailbreaking Large Vision-language Models via Typographic Visual Prompts. CoRR abs/2311.05608 , 2023. 2",
            "TRAP: Targeted Random Adversarial Prompt Honeypot for Black-Box Identification. In Annual Meeting of the Association for Computational Linguistics (ACL) . ACL, 2024. 3",
            "WildGuard: Open One-Stop Moderation Tools for Safety Risks, Jailbreaks, and Refusals of LLMs. CoRR abs/2406.18495 , 2024. 1, 2, 3, 5, 11",
            "Llama Guard: LLM-based Input-Output Safeguard for Human-AI Conversations. CoRR abs/2312.06674 , 2023. 1, 2, 3, 5, 11",
            "BeaverTails: Towards Improved Safety Alignment of LLM via a Human-Preference Dataset. In Annual Conference on Neural Information Processing Systems (NeurIPS) . NeurIPS, 2023. 2",
            "ProFLingo: A Fingerprinting-based Copyright Protection Scheme for Large Language Models. CoRR abs/2405.02466 , 2024. 3, 4, 5",
            "Multi-step Jailbreaking Privacy Attacks on ChatGPT. CoRR abs/2304.05197 , 2023. 2",
            "AutoDAN: Generating Stealthy Jailbreak Prompts on Aligned Large Language Models. CoRR abs/2310.04451 , 2023. 1, 2, 3",
            "Formalizing and Benchmarking Prompt Injection Attacks and Defenses. In USENIX Security Symposium (USENIX Security) . USENIX, 2024. 1, 3",
            "Safety Alignment for Vision Language Models. CoRR abs/2405.13581 , 2024. 2",
            "HateXplain: A Benchmark Dataset for Explainable Hate Speech Detection. In AAAI Conference on Artificial Intelligence (AAAI) , pages 14867–14875. AAAI, 2021. 4",
            "Your Large Language Models Are Leaving Fingerprints. CoRR abs/2405.14057 , 2024. 3",
            "Tree of Attacks: Jailbreaking Black-Box LLMs Automatically. CoRR abs/2312.02119 , 2023. 2",
            "Gemma: Open Models Based on Gemini Research and Technology. CoRR abs/2403.08295 , 2024. 6, 11",
            "An LLM-Driven Chatbot in Higher Education for Databases and Information Systems. IEEE Transactions on Education , 2024. 1",
            "A study of generative large language model for medical research and healthcare. NPJ Digital Medicine , 2023. 1",
            "A Survey on Hate Speech Detection using Natural Language Processing. In Workshop on Natural Language Processing for Social Media (SocialNLP) , pages 1–10. ACL, 2017. 4",
            "Do Anything Now: Characterizing and Evaluating In-The-Wild Jailbreak Prompts on Large Language Models. In ACM SIGSAC Conference on Computer and Communications Security (CCS) . ACM, 2024. 1, 2, 3",
            "HateBench: Benchmarking Hate Speech Detectors on LLM-Generated Content and Hate Campaigns. In USENIX Security Symposium (USENIX Security) . USENIX, 2025. 1",
            "LLaMA: Open and Efficient Foundation Language Models.CoRR abs/2302.13971 , 2023. 1",
            "Learning from the Worst: Dynamically Generated Datasets to Improve Online Hate Detection. In Annual Meeting of the Association for Computational Linguistics and International Joint Conference on Natural Language Processing (ACL/IJCNLP) , pages 1667–1682. ACL, 2021. 1, 4, 5",
            "Membership Inference Attacks Against In-Context Learning. In ACM SIGSAC Conference on Computer and Communications Security (CCS) . ACM, 2024. 2",
            "Assessing Prompt Injection Risks in 200+ Custom GPTs. CoRR abs/2311.11538 , 2023. 1, 3",
            "HuRef: HUman-REadable Fingerprint for Large Language Models. CoRR abs/2312.04828 , 2023. 3",
            "ShieldGemma: Generative AI Content Moderation Based on Gemma. CoRR abs/2407.21772 , 2024. 3, 5, 11",
            "InjecAgent: Benchmarking Indirect Prompt Injections in Tool-Integrated Large Language Model Agents. CoRR abs/2403.02691 , 2024. 1, 3",
            "Instruction Backdoor Attacks Against Customized LLMs. In USENIX Security Symposium (USENIX Security) . USENIX, 2024. 2",
            "Weak-to-Strong Jailbreaking on Large Language Models. CoRR abs/2401.17256 , 2024. 1, 2, 3",
            "Universal and Transferable Adversarial Attacks on Aligned Language Models. CoRR abs/2307.15043 , 2023. 1, 2, 3"
        ]
    },
    {
        "index": 320,
        "title": "PANDAS: Improving Many-shot Jailbreaking via Positive Affirmation, Negative Demonstration, and Adaptive Sampling",
        "publication_date": "2025-02-04",
        "references": [
            "Gpt-4 technical report.",
            "What learning algorithm is in-context learning? investigations with linear models.",
            "Many-shot jailbreaking.",
            "Training a helpful and harmless assistant with reinforcement learning from human feedback.",
            "Language models are few-shot learners.",
            "Jailbreaking black box large language models in twenty queries.",
            "How many demonstrations do you need for in-context learning?",
            "What does bert look at? an analysis of bert’s attention.",
            "Bert: Pre-training of deep bidirectional transformers for language understanding.",
            "Longrope: Extending llm context window beyond 2 million tokens.",
            "Exploring context window of large language models via decomposed positional vectors.",
            "The llama 3 herd of models.",
            "Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned.",
            "Customizing language model responses with contrastive in-context learning.",
            "ChatGLM: A family of large language models from glm-130b to glm-4 all tools.",
            "Comparing results of 31 algorithms from the black-box optimization benchmarking bbob-2009.",
            "Self-attention attribution: Interpreting information interactions inside transformer.",
            "Wizardlm-13b-uncensored.",
            "Measuring mathematical problem solving with the math dataset.",
            "Baseline defenses for adversarial attacks against aligned language models.",
            "Llm maybe longlm: Self-extend llm context window without tuning.",
            "What makes good in-context examples for gpt- 3?",
            "Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity.",
            "Attention-enhancing backdoor attacks against bert-based models.",
            "Harmbench: A standardized evaluation framework for automated red teaming and robust refusal.",
            "Tree of attacks: Jailbreaking black-box llms automatically.",
            "Taking the human out of the loop: A review of bayesian optimization.",
            "Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context.",
            "Qwen2.5: A party of foundation models.",
            "Llama: Open and efficient foundation language models.",
            "Bayesian optimization is superior to random search for machine learning hyperparameter tuning: Analysis of the black-box optimization challenge 2020.",
            "Attention is all you need.",
            "Jailbroken: How does llm safety training fail?",
            "Jailbreak and guard aligned language models with only few in-context demonstrations.",
            "Never miss a beat: An efficient recipe for context window extension of large language models with consistent” middle” enhancement.",
            "Distract large language models for automatic jailbreak attack.",
            "Defending chatgpt against jailbreak attack via self-reminders.",
            "Understanding addition in transformers.",
            "Language models are unsupervised multitask learners.",
            "Tricking llms into disobedience: Formalizing, analyzing, and detecting jailbreaks.",
            "Smooth-llm: Defending large language models against jailbreaking attacks.",
            "Bayesian Optimization: Open source constrained global optimization tool for Python.",
            "2 olmo 2 furious.",
            "Training language models to follow instructions with human feedback.",
            "On the role of attention in prompt-tuning.",
            "Generative agents: Interactive simulacra of human behavior.",
            "Red teaming language models with language models.",
            "Baitattack: Alleviating intention shift in jailbreak attacks via adaptive bait crafting.",
            "Understanding addition in transformers.",
            "Calibrate before use: Improving few-shot performance of language models."
        ]
    },
    {
        "index": 321,
        "title": "UNDERSTANDING AND ENHANCING THE TRANSFER-ABILITY OF JAILBREAKING ATTACKS",
        "publication_date": "2025-02-05",
        "references": [
            "Detecting language model attacks with perplexity",
            "The claude 3 model family: Opus, sonnet, haiku",
            "Training a helpful and harmless assistant with reinforcement learning from human feedback",
            "Sequential modeling enables scalable learning for large vision models",
            "Jailbreaking black box large language models in twenty queries",
            "Jailbreakbench: An open robustness benchmark for jailbreaking large language models",
            "Autobreach: Universal and adaptive jailbreaking with efficient wordplay-guided optimization",
            "Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality",
            "Deep reinforcement learning from human preferences",
            "Scaling instruction-finetuned language models",
            "Multilingual jailbreak challenges in large language models",
            "Bert: Pre-training of deep bidirectional transformers for language understanding",
            "A wolf in sheep’s clothing: Generalized nested jailbreak prompts can fool large language models easily",
            "Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned",
            "An important next step on our ai journey",
            "Catastrophic jailbreak of open-source llms via exploiting generation",
            "Machine vision therapy: Multimodal large language models can enhance visual robustness via denoising in-context learning",
            "Llama guard: Llm-based input-output safeguard for human-ai conversations",
            "Baseline defenses for adversarial attacks against aligned language models",
            "Mistral 7b",
            "Certifying llm safety against adversarial prompting",
            "Open sesame! universal black box jailbreaking of large language models",
            "Amplegcg: Learning a universal and transferable generative model of adversarial suffixes for jailbreaking both open and closed llms",
            "Autodan: Generating stealthy jailbreak prompts on aligned large language models",
            "Jailbreaking chatgpt via prompt engineering: An empirical study",
            "Analyzing leakage of personally identifiable information in language models",
            "Deem: Diffusion models serve as the eyes of large language models for image perception",
            "Mmevol: Empowering multimodal large language models with evol-instruct",
            "A holistic approach to undesired content detection in the real world",
            "Tree of attacks: Jailbreaking black-box llms automatically",
            "Llama 3 model card",
            "Gpt-4 technical report",
            "Our approach to ai safety",
            "Learning to reason with llms",
            "Training language models to follow instructions with human feedback",
            "Fine-tuning aligned language models compromises safety, even when users do not intend to!",
            "Language models are unsupervised multitask learners",
            "Direct preference optimization: Your language model is secretly a reward model",
            "Smoothllm: Defending large language models against jailbreaking attacks",
            "Proximal policy optimization algorithms",
            "Rapid optimization for jailbreaking llms via subconscious exploitation and echopraxia",
            "Characterizing and evaluating in-the-wild jailbreak prompts on large language models",
            "Pal: Proxy-guided black-box attack on large language models",
            "Gemini: a family of highly capable multimodal models",
            "Llama 2: Open foundation and fine-tuned chat models",
            "Ranked from within: Ranking large multimodal models for visual question answering without labels",
            "Ted-viton: Transformer-empowered diffusion models for virtual try-on",
            "Noisegpt: Label noise detection and rectification through probability curvature",
            "Lavin-dit: Large vision diffusion transformer",
            "Achieving cross modal generalization with multimodal unified representation",
            "Defending chatgpt against jailbreak attack via self-reminders",
            "Shadow alignment: The ease of subverting safely-aligned language models",
            "Low-resource languages jailbreak gpt-4",
            "Gptfuzzer: Red teaming large language models with auto-generated jailbreak prompts",
            "{LLM-Fuzzer }: Scaling assessment of large language model jailbreaks",
            "Late stopping: Avoiding confidently learning from mislabeled examples",
            "Early stopping against label noise without validation data",
            "How johnny can persuade llms to jailbreak them: Rethinking persuasion to challenge ai safety by humanizing llms",
            "Token-level direct preference optimization",
            "On the safety of open-sourced large language models: Does alignment really prevent them from being misused?",
            "Hierarchical context pruning: Optimizing real-world code completion with repository-level pretrained code llms",
            "Defending large language models against jailbreaking attacks through goal prioritization",
            "Weak-to-strong jailbreaking on large language models",
            "Prompt-driven llm safeguarding via directed representation optimization",
            "Few-shot adversarial prompt learning on vision-language models",
            "Universal and transferable adversarial attacks on aligned language models"
        ]
    },
    {
        "index": 322,
        "title": "“Short-length” Adversarial Training Helps LLMs Defend “Long-length” Jailbreak Attacks: Theoretical and Empirical Evidence",
        "publication_date": "2025-02-06",
        "references": [
            "Language models are few-shot learners.",
            "Llama: Open and efficient foundation language models.",
            "Deepseek-v2: A strong, economical, and efficient mixture-of-experts language model.",
            "Qwen2.5 technical report.",
            "Jailbroken: How does LLM safety training fail?",
            "Universal and transferable adversarial attacks on aligned language models.",
            "Jailbreaking black box large language models in twenty queries.",
            "AutoDAN: Generating stealthy jailbreak prompts on aligned large language models.",
            "Efficient adversarial training in LLMs with continuous attacks.",
            "HarmBench: A standardized evaluation framework for automated red teaming and robust refusal.",
            "Robust LLM safeguarding via refusal feature adversarial training.",
            "Defending against unforeseen failure modes with latent adversarial training.",
            "What can transformers learn in-context? a case study of simple function classes.",
            "Transformers learn to implement preconditioned gradient descent for in-context learning.",
            "How transformers utilize multi-head attention in in-context learning? A case study on sparse linear regression.",
            "One step of gradient descent is provably the optimal in-context learner with one layer of linear self-attention.",
            "In-context learning on function classes unveiled for transformers.",
            "In-context learning with representations: Contextual generalization of trained transformers.",
            "In-context convergence of transformers.",
            "How many pre-training tasks are needed for in-context learning of linear regression?",
            "Transformers as decision makers: Provable in-context reinforcement learning via supervised pretraining.",
            "Asymptotic theory of in-context learning by linear attention.",
            "Benign overfitting in single-head attention.",
            "Trained transformer classifiers generalize and exhibit benign overfitting in-context.",
            "Why larger language models do in-context learning differently?",
            "Adversarial robustness of in-context learning in transformers for linear regression.",
            "Linear attention is (maybe) all you need (to understand transformer optimization).",
            "Precise tradeoffs in adversarial training for linear regression.",
            "Regularization properties of adversarially-trained linear regression.",
            "Theoretical analysis of robust overfitting for wide DNNs: An NTK approach.",
            "Benign overfitting in adversarial training of neural networks.",
            "Judging LLM-as-a-judge with MT-bench and chatbot arena.",
            "Mistral 7B.",
            "Llama 2: Open foundation and fine-tuned chat models.",
            "The Llama 3 herd of models.",
            "Stanford Alpaca: An instruction-following LLaMA model.",
            "Length-controlled AlpacaEval: A simple way to debias automatic evaluators.",
            "LoRA: Low-Rank adaptation of large language models."
        ]
    },
    {
        "index": 324,
        "title": "JBShield: Defending Large Language Models from Jailbreak Attacks through Activated Concept Analysis and Manipulation",
        "publication_date": "2025-08-13",
        "references": [
            "Gpt-4 technical report.",
            "Datasheets for digital cultural heritage datasets.",
            "Detecting language model attacks with perplexity.",
            "Jailbreaking leading safety-aligned llms with simple adaptive attacks.",
            "Introducing claude.",
            "Foundational challenges in assuring alignment and safety of large language models.",
            "Refusal in language models is mediated by a single direction.",
            "Constitutional ai: Harmlessness from ai feedback.",
            "Defending against alignment-breaking attacks via robustly aligned llm.",
            "Are aligned neural networks adversarially aligned?",
            "Play guessing game with llm: Indirect jailbreak attack with implicit clues.",
            "Jailbreaking black box large language models in twenty queries.",
            "Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality.",
            "Deep reinforcement learning from human preferences.",
            "Or-bench: An over-refusal benchmark for large language models.",
            "Jailbreaker: Automated jailbreak across multiple large language model chatbots.",
            "The llama 3 herd of models.",
            "Toy models of superposition.",
            "Towards understanding jailbreak attacks in llms: A representation space analysis.",
            "Tree of attacks: Jailbreaking black-box llms automatically.",
            "Linguistic regularities in continuous space word representations.",
            "Emergent linear representations in world models of self-supervised sequence models.",
            "Training language models to follow instructions with human feedback.",
            "The linear representation hypothesis and the geometry of large language models.",
            "Fine-tuning aligned language models compromises safety, even when users do not intend to!",
            "Smoothllm: Defending large language models against jailbreaking attacks.",
            "Do anything now\": Characterizing and evaluating in-the-wild jailbreak prompts on large language models.",
            "Stanford alpaca: An instruction-following llama model.",
            "Llama: Open and efficient foundation language models.",
            "Detoxifying large language models via knowledge editing.",
            "Aligning large language models with human: A survey.",
            "Low-resource languages jailbreak GPT-4.",
            "Index for rating diagnostic tests.",
            "{LLM-Fuzzer }: Scaling assessment of large language model jailbreaks.",
            "Don’t listen to me: Understanding and exploring jailbreak prompts of large language models.",
            "Bridge the gap between cv and nlp! a gradient-based textual adversarial attack framework.",
            "Defending large language models against jailbreak attacks via layer-specific editing.",
            "Causality analysis for evaluating the security of large language models.",
            "On prompt-driven safeguarding for large language models.",
            "How alignment and jailbreak work: Explain llm safety through intermediate hidden states.",
            "Autodan: Automatic and interpretable adversarial attacks on large language models.",
            "Universal and transferable adversarial attacks on aligned language models."
        ]
    },
    {
        "index": 325,
        "title": "MODIFICATION AND GENERATED -TEXT DETECTION : ACHIEVING DUAL DETECTION CAPABILITIES FOR THE OUTPUTS OF LLM BY WATERMARK",
        "publication_date": "2025-03-04",
        "references": [
            "On the risk of misinformation pollution with large language models.",
            "When llms go online: The emerging threat of web-enabled llms.",
            "Position: On the possibilities of AI-generated text detection.",
            "Detectgpt: zero-shot machine-generated text detection using probability curvature.",
            "On the reliability of watermarks for large language models.",
            "A statistical framework of watermarks for large language models: Pivot, detection efficiency and optimal rules.",
            "A survey on llm-generated text detection: Necessity, methods, and future directions.",
            "A review of text watermarking: Theory, methods, and applications.",
            "Advancing beyond identification: Multi-bit watermark for large language models.",
            "Tracing text provenance via context-aware lexical substitution.",
            "A watermark for large language models.",
            "Provable robust watermarking for AI-generated text.",
            "Who wrote this code? watermarking for code generation.",
            "A resilient and accessible distribution-preserving watermark for large language models.",
            "Robust distortion-free watermarks for language models.",
            "Undetectable watermarks for language models.",
            "Unbiased watermark for large language models.",
            "No free lunch in llm watermarking: Trade-offs in watermarking design choices.",
            "Discovering clues of spoofed lm watermarks.",
            "New evaluation metrics capture quality degradation due to llm watermarking.",
            "Pubmedqa: A dataset for biomedical research question answering.",
            "Opt: Open pre-trained transformer language models."
        ]
    },
    {
        "index": 326,
        "title": "Making Them a Malicious Database: Exploiting Query Code to Jailbreak Aligned Large Language Models",
        "publication_date": "2025-02-20",
        "references": [
            "Model card and evaluations for claude models",
            "Badclip: Trigger-aware prompt learning for backdoor attacks on clip",
            "Training a helpful and harmless assistant with reinforcement learning from human feedback",
            "Constitutional AI: harmlessness from AI feedback",
            "Natural Language Processing with Python",
            "Emergent autonomous scientific research capabilities of large language models",
            "Jailbreaking black box large language models in twenty queries",
            "Deep reinforcement learning from human preferences",
            "Gemini 1.5: Advancements in multimodal language models",
            "Deepseek llm: Scaling open-source language models with longtermism",
            "Multilingual jailbreak challenges in large language models",
            "Inducing high energy-latency of large vision-language models with verbose images",
            "Embedding self-correction as an inherent ability in large language models for enhanced mathematical reasoning",
            "Denial-of-service poisoning attacks against large language models",
            "Annollm: Making large language models to be better crowdsourced annotators",
            "Baseline defenses for adversarial attacks against aligned language models",
            "Automatically auditing large language models via discrete optimization",
            "Multi-step jailbreaking privacy attacks on chatgpt",
            "Autodan: Generating stealthy jailbreak prompts on aligned large language models",
            "Codechameleon: Personalized encryption framework for jailbreaking large language models",
            "FLIRT: feedback loop in-context red teaming",
            "Learning to reason with llms",
            "Cross-lingual prompting: Improving zero-shot chain-of-thought reasoning across languages",
            "Identifying semantic induction heads to understand in-context learning",
            "Codeattack: Revealing safety generalization challenges of large language models via code completion",
            "Derail yourself: Multi-turn LLM jailbreak attack through self-discovered clues",
            "Smoothllm: Defending large language models against jailbreaking attacks",
            "Scalable and transferable black-box jailbreaks for language models via persona modulation",
            "do anything now\": Characterizing and evaluating in-the-wild jailbreak prompts on large language models",
            "Dagger behind smile: Fool llms with a happy ending story",
            "Principle-driven self-alignment of language models from scratch with minimal human supervision",
            "Llama: Open and efficient foundation language models"
        ]
    },
    {
        "index": 327,
        "title": "Enhancing Jailbreak Attacks via Compliance-Refusal-Based Initialization",
        "publication_date": "2025-02-13",
        "references": [
            "Phi-4 technical report",
            "Gpt-4 technical report",
            "Do as i can, not as i say: Grounding language in robotic affordances",
            "Introducing meta llama 3: The most capable openly available llm",
            "Refusal in language models is mediated by a single direction",
            "Universal jailbreak backdoors in large language model alignment",
            "Jailbreaking black box large language models in twenty queries",
            "Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality",
            "Jailbreaker: Automated jailbreak across multiple large language model chatbots",
            "The llama 3 herd of models",
            "Llm agents can autonomously hack websites",
            "Improving alignment of dialogue agents via targeted human judgements",
            "Large language models: a comprehensive survey of its applications, challenges, limitations, and future prospects",
            "Stronger universal and transfer attacks by suppressing refusals",
            "Qwen2. 5-coder technical report",
            "Improved techniques for optimization-based jailbreaking on large language models",
            "Mistral 7b",
            "Rlaif: Scaling reinforcement learning from human feedback with ai feedback",
            "Retrieval-augmented generation for knowledge-intensive nlp tasks",
            "Glide: Towards photorealistic image generation and editing with text-guided diffusion models",
            "Ignore previous prompt: Attack techniques for language models",
            "Photorealistic text-to-image diffusion models with deep language understanding",
            "Large language model alignment: A survey",
            "Scaling laws with vocabulary: Larger models deserve larger vocabularies",
            "Creating large language model applications utilizing langchain: A primer on developing llm apps fast",
            "Llama 2: Open foundation and fine-tuned chat models",
            "Poisoning language models during instruction tuning",
            "Understanding contrastive representation learning through alignment and uniformity on the hypersphere",
            "Aligning large language models with human: A survey",
            "Attention is all you need",
            "Autogen: Enabling next-gen llm applications via multi-agent conversation framework",
            "Meta-rewarding language models: Self-improving alignment with llm-as-a-meta-judge",
            "A survey on large language model (llm) security and privacy: The good, the bad, and the ugly",
            "Enhancing jailbreak attack against large language models through silent tokens",
            "Robust llm safeguarding via refusal feature adversarial training",
            "Accelerating greedy coordinate gradient and general prompt optimization via probe sampling",
            "Jailbreaking? one step is enough!",
            "Defending jailbreak prompts via in-context adversarial game",
            "Universal and transferable adversarial attacks on aligned language models"
        ]
    },
    {
        "index": 328,
        "title": "X-Boundary: Establishing Exact Safety Boundary to Shield LLMs from Multi-Turn Jailbreaks without Compromising Usability",
        "publication_date": "2025-03-06",
        "references": [
            "Llama 3 model card",
            "Automatic pseudo-harmful prompt generation for evaluating false refusals in large language models",
            "Refusal in language models is mediated by a single direction",
            "Training a helpful and harmless assistant with reinforcement learning from human feedback",
            "Safety-tuned llamas: Lessons from improving the safety of large language models that follow instructions",
            "Defending against alignment-breaking attacks via robustly aligned llm",
            "Nothing in excess: Mitigating the exaggerated safety for llms via safety-conscious activation steering",
            "Evaluating large language models trained on code",
            "Rh20tp: A primitive-level robotic dataset towards composable generalization agents",
            "Measuring generalization with optimal transport",
            "Training verifiers to solve math word problems",
            "Or-bench: An over-refusal benchmark for large language models",
            "Enhancing chat language models by scaling high-quality instructional conversations",
            "The llama 3 herd of models",
            "Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning",
            "Measuring massive multitask language understanding",
            "Defending chatgpt against jailbreak attack via self-reminders",
            "Mistral 7b",
            "Safechain: Safety of language models with long chain-of-thought reasoning capabilities",
            "Red queen: Safeguarding large language models against concealed multi-turn jailbreaking",
            "Llm defenses are not robust to multi-turn human jailbreaks yet",
            "The wmdp benchmark: Measuring and reducing malicious use with unlearning",
            "Deepseek-v2: A strong, economical, and efficient mixture-of-experts language model",
            "Autodan: Generating stealthy jailbreak prompts on aligned large language models",
            "Imposter. ai: Adversarial attacks with hidden intentions towards aligned large language models",
            "Protecting your llms with information bottleneck",
            "Eraser: Jailbreaking defense in large language models via unlearning harmful knowledge",
            "Sofa: Shielded on-the-fly alignment via priority rule following",
            "Harmbench: A standardized evaluation framework for automated red teaming and robust refusal",
            "Adaptive loss weighting for machine learning interatomic potentials",
            "Gpt-4 technical report",
            "Training language models to follow instructions with human feedback",
            "Training language models to follow instructions with human feedback",
            "Deactivating the coupled neurons to mitigate fairness-privacy conflicts in large language models",
            "Towards tracing trustworthiness dynamics: Revisiting pre-training period of large language models",
            "Direct preference optimization: Your language model is secretly a reward model",
            "Identifying semantic induction heads to understand in-context learning",
            "Derail yourself: Multi-turn llm jailbreak attack through self-discovered clues",
            "Smooth-llm: Defending large language models against jailbreaking attacks",
            "Xstest: A test suite for identifying exaggerated safety behaviours in large language models",
            "Great, now write an article about that: The crescendo multi-turn llm jailbreak attack",
            "Navigating the overkill in large language models",
            "k-variance: A clustered notion of variance",
            "Internlm: A multilingual language model with progressively enhanced capabilities",
            "Securing multi-turn conversational language models from distributed backdoor attacks",
            "The art of defending: A systematic evaluation and analysis of llm defense strategies on safety and over-defensiveness",
            "The instruction hierarchy: Training llms to prioritize privileged instructions",
            "Mrj-agent: An effective jailbreak agent for multi-round dialogue",
            "Prompt-driven llm safeguarding via directed representation optimization",
            "Defending large language models against jailbreaking attacks through goal prioritization",
            "Safe unlearning: A surprisingly effective and generalizable solution to defend against jailbreak attacks",
            "Towards comprehensive and efficient post safety alignment of large language models via safety patching"
        ]
    },
    {
        "index": 329,
        "title": "INJECTING UNIVERSAL JAILBREAK BACKDOORS INTO LLM S IN MINUTES",
        "publication_date": "2025-02-09",
        "references": [
            "Gpt-4 technical report.",
            "Quantifying the capabilities of llms across scale and precision",
            "Training a helpful and harmless assistant with reinforcement learning from human feedback",
            "How do large language models acquire factual knowledge during pretraining?",
            "Jailbreaking black box large language models in twenty queries",
            "Badpre: Task-agnostic backdoor attacks to pre-trained nlp foundation models",
            "Knowledge neurons in pretrained transformers",
            "Editing factual knowledge in language models",
            "Deconstructing the ethics of large language models from long-standing issues to new-emerging dilemmas",
            "Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned",
            "Transformer feed-forward layers are key-value memories",
            "Measuring massive multitask language understanding",
            "Badedit: Backdooring large language models by model editing",
            "Autodan: Generating stealthy jailbreak prompts on aligned large language models",
            "Locating and editing factual associations in GPT",
            "Mass editing memory in a transformer",
            "Fast model editing at scale",
            "Training language models to follow instructions with human feedback",
            "Universal jailbreak backdoors from poisoned human feedback",
            "” do anything now”: Characterizing and evaluating in-the-wild jailbreak prompts on large language models",
            "Badgpt: Exploring security vulnerabilities of chatgpt via backdoor attacks to instructgpt",
            "Trustllm: Trustworthiness in large language models",
            "Llama 2: Open foundation and fine-tuned chat models",
            "Poisoning language models during instruction tuning",
            "Do-not-answer: A dataset for evaluating safeguards in llms",
            "Jailbroken: How does llm safety training fail?",
            "Jailbroken: How does llm safety training fail?",
            "Instructions as back-doors: Backdoor vulnerabilities of instruction tuning for large language models",
            "Gptfuzzer: Red teaming large language models with auto-generated jailbreak prompts",
            "Knowledge graph enhanced large language model editing",
            "A survey of backdoor attacks and defenses on large language models: Implications for security measures",
            "Universal and transferable adversarial attacks on aligned language models"
        ]
    },
    {
        "index": 330,
        "title": "VLM-G UARD: Safeguarding Vision-Language Models via Fulfilling Safety Alignment Gap",
        "publication_date": "2025-02-14",
        "references": [
            "Claude",
            "Qwen-vl: A versatile vision-language model for understanding, localization, text reading, and beyond",
            "Training a helpful and harmless assistant with reinforcement learning from human feedback",
            "Image hijacks: Adversarial images can control generative models at runtime",
            "Are aligned neural networks adversarially aligned?",
            "DRESS: Instructing large vision-language models to align and interact with humans via natural language feedback",
            "Instructblip: Towards general-purpose vision-language models with instruction tuning",
            "Figstep: Jailbreaking large vision-language models via typographic visual prompts",
            "Eyes closed, safety on: Protecting multimodal llms via image-to-text transformation",
            "Matrix analysis",
            "Catastrophic jailbreak of open-source llms via exploiting generation",
            "Inference-time intervention: Eliciting truthful answers from a language model",
            "Images are achilles’ heel of alignment: Exploiting visual vulnerabilities for jail-breaking multimodal large language models",
            "Vl-trojan: Multimodal instruction backdoor attacks against autoregressive visual language models",
            "Mind the gap: Understanding the modality gap in multi-modal contrastive representation learning",
            "The unlocking spell on base llms: Rethinking alignment via in-context learning",
            "Improved baselines with visual instruction tuning",
            "Visual instruction tuning",
            "Mm-safetybench: A benchmark for safety evaluation of multimodal large language models",
            "Jailbreaking attack against multimodal large language model",
            "Gpt4v",
            "Training language models to follow instructions with human feedback",
            "Visual adversarial examples jailbreak large language models",
            "Two effects, one trigger: On the modality gap, object bias, and information imbalance in contrastive vision-language representation learning",
            "Inferaligner: Inference-time alignment for harmlessness through cross-model guidance",
            "Defending chatgpt against jailbreak attack via self-reminders",
            "Debiasing large visual language models",
            "Defending large language models against jail-breaking attacks through goal prioritization",
            "On evaluating adversarial robustness of large vision-language models",
            "On prompt-driven safeguarding for large language models",
            "Minigpt-4: Enhancing vision-language understanding with advanced large language models",
            "Safety fine-tuning at (almost) no cost: A baseline for vision large language models",
            "Representation engineering: A top-down approach to ai transparency"
        ]
    },
    {
        "index": 331,
        "title": "Reasoning-Augmented Conversation for Multi-Turn Jailbreak Attacks on Large Language Models",
        "publication_date": "2025-03-11",
        "references": [
            "Chatgpt for good? on opportunities and challenges of large language models for education.",
            "Large language models in medicine.",
            "Talking about large language models.",
            "Unveiling the safety of gpt-4o: An empirical study using jailbreak attacks.",
            "Safebench: A safety evaluation framework for multimodal large language models.",
            "A comprehensive study of jailbreak attack versus defense for large language models.",
            "Universal and transferable adversarial attacks on aligned language models.",
            "Autodan: Generating stealthy jailbreak prompts on aligned large language models.",
            "Nba: defensive distillation for backdoor removal via neural behavior alignment.",
            "Dlp: towards active defense against backdoor attacks with decoupled learning process.",
            "Jailbreak vision language models via bi-modal adversarial prompt.",
            "Semantic mirror jailbreak: Genetic algorithm based jailbreak prompts against open-source llms.",
            "Jailbreak vision language models via bi-modal adversarial prompt.",
            "Lanevil: Benchmarking the robustness of lane detection to environmental illusions.",
            "Vl-trojan: Multimodal instruction backdoor attacks against autoregressive visual language models.",
            "Revisiting backdoor attacks against large vision-language models.",
            "Bad-clip: Dual-embedding guided backdoor attack on multimodal contrastive learning.",
            "Open sesame! universal black box jailbreaking of large language models.",
            "On the humanity of conversational ai: Evaluating the psychological portrayal of llms.",
            "How johnny can persuade llms to jailbreak them: Rethinking persuasion to challenge ai safety by humanizing llms.",
            "Psysafe: A comprehensive framework for psychological-based attack, defense, and evaluation of multi-agent system safety.",
            "Foot in the door: Understanding large language model jailbreaking via cognitive psychology.",
            "Leveraging the context through multi-round interactions for jailbreaking attacks.",
            "Mrj-agent: An effective jailbreak agent for multi-round dialogue.",
            "Derail yourself: Multi-turn llm jailbreak attack through self-discovered clues.",
            "Chain-of-thought prompting elicits reasoning in large language models.",
            "Large language models are zero-shot reasoners.",
            "Towards revealing the mystery behind chain of thought: A theoretical perspective.",
            "Navigate through enigmatic labyrinth A survey of chain of thought reasoning: Advances, frontiers and future.",
            "Self-consistency improves chain of thought reasoning in language models.",
            "Derail yourself: Multi-turn llm jailbreak attack through self-discovered clues.",
            "Chain of attack: a semantic-driven contextual multi-turn attacker for llm.",
            "Multi-turn context jailbreak attack on large language models from first principles.",
            "Speak out of turn: Safety vulnerability of large language models in multi-turn dialogue.",
            "Leveraging the context through multi-round interactions for jailbreaking attacks.",
            "Testing finite-state machines: State identification and verification.",
            "An intelligent agent of finite state machine in educational game “flora the explorer”.",
            "Finite state machines in hardware: theory and design (with VHDL and SystemVerilog).",
            "Finite state machine based formal methods in protocol conformance testing: from theory to implementation.",
            "A mathematical theory of communication.",
            "Information gain and a general measure of correlation.",
            "Finding useful questions: on bayesian diagnosticity, probability, impact, and information gain.",
            "Divergence measures based on the shannon entropy.",
            "Equilibrium points in n-person games.",
            "Some studies in machine learning using the game of checkers.",
            "Look before you leap: An exploratory study of uncertainty measurement for large language models.",
            "Uncertainty is fragile: Manipulating uncertainty in large language models.",
            "Gemma 2: Improving open language models at a practical size.",
            "Qwen2 technical report.",
            "Chatglm: A family of large language models from glm-130b to glm-4 all tools.",
            "Gpt-4 technical report.",
            "Gpt-4o system card.",
            "Gemini 1.5: Unlocking multi-modal understanding across millions of tokens of context.",
            "Tree of attacks: Jailbreaking black-box llms automatically.",
            "Jailbreaking black box large language models in twenty queries.",
            "Harm-bench: A standardized evaluation framework for automated red teaming and robust refusal.",
            "Deepinception: Hypnotize large language model to be jailbreaker.",
            "Smoothllm: Defending large language models against jailbreaking attacks.",
            "Defending chatgpt against jail-break attack via self-reminders.",
            "Jailbreak and guard aligned language models with only few in-context demonstrations.",
            "Jailguard: A universal detection framework for llm prompt-based attacks.",
            "Llms-as-judges: a comprehensive survey on llm-based evaluation methods.",
            "Black-box adversarial attack on vision language models for autonomous driving.",
            "Visual adversarial attack on vision-language models for autonomous driving.",
            "Exploring the relationship between architectural design and adversarially robust generalization.",
            "Unlearning backdoor threats: Enhancing backdoor defense in multimodal contrastive learning via local token unlearning."
        ]
    },
    {
        "index": 332,
        "title": "“Nuclear Deployed!”: Analyzing Catastrophic Risks in Decision-making of Autonomous LLM Agents",
        "publication_date": "2025-03-03",
        "references": [
            "Gpt-4 technical report",
            "The eu artificial intelligence act.",
            "Introducing meta llama 3.",
            "Exploring the psychology of llms’ moral and legal reasoning.",
            "Model card addendum: Claude 3.5 haiku and upgraded claude 3.5 sonnet.",
            "Reflections on our responsible scaling policy.",
            "Foundational challenges in assuring alignment and safety of large language models.",
            "A general language assistant as a laboratory for alignment.",
            "Fairmonitor: A dual-framework for detecting stereotypes and biases in large language models.",
            "Training a helpful and harmless assistant with reinforcement learning from human feedback.",
            "Towards evaluations-based safety cases for ai scheming.",
            "Managing ai risks in an era of rapid progress.",
            "International ai safety report 2025.",
            "Executive order on the safe, secure, and trustworthy development and use of artificial intelligence.",
            "Scheming ais: Will ais fake alignment during training in order to get power?",
            "Human behavior in the social environment: A social systems approach.",
            "Man who exploded tesla cybertruck outside trump hotel in las vegas used generative ai, police say.",
            "Liability for ai decision-making: some legal and ethical considerations.",
            "Systematically evaluating large language model safety refusal behaviors.",
            "Defending chatgpt against jailbreak attack via self-reminders.",
            "Qwen2.5 technical report.",
            "Llm voting: Human choices and ai collective decision making.",
            "Fake alignment: Are llms really aligned well?",
            "Universal and transferable adversarial attacks on aligned language models.",
            "Less is more for alignment.",
            "Is this the real life? is this just fantasy? the misleading success of simulating social interactions with llms.",
            "Jailbreaking black box large language models in twenty queries.",
            "Optimizing reasoning abilities in large language models: A step-by-step approach.",
            "Agentclinic: a multimodal agent benchmark to evaluate ai in simulated clinical environments.",
            "Escalation risks from language models in military and diplomatic decision-making.",
            "The curious case of neural text degeneration.",
            "Large language models can strategically deceive their users when put under pressure.",
            "Large language models cannot replace human participants because they cannot portray identity groups.",
            "A survey on large language model based autonomous agents.",
            "Reflecting deeply on the boundaries of the unknown.",
            "Simulating human-like daily activities with desire-driven autonomy.",
            "Flooding spread of manipulated knowledge in llm-based multi-agent communities.",
            "Refuse whenever you feel unsafe: Improving safety in llms via decoupled refusal training.",
            "How johnny can persuade llms to jailbreak them: Rethinking persuasion to challenge ai safety by humanizing llms.",
            "Chain-of-thought prompting elicits reasoning in large language models.",
            "Self-refine: Iterative refinement with self-feedback.",
            "The instruction hierarchy: Training llms to prioritize privileged instructions.",
            "Harmbench: A standardized evaluation framework for automated red teaming and robust refusal.",
            "Sorry-bench: Systematically evaluating large language model safety refusal behaviors.",
            "Towards guaranteed safe ai: A framework for ensuring robust and reliable ai systems.",
            "Does refusal training in llms generalize to the past tense?",
            "Securing the future of genai: Policy and technology.",
            "The operational risks of ai in large-scale biological attacks: Results of a red-team study.",
            "Aegis: Online adaptive ai content safety moderation with ensemble of llm experts.",
            "The wmdp benchmark: Measuring and reducing malicious use with unlearning.",
            "The earth is flat because...: Investigating llms’ belief towards misinformation via persuasive conversation.",
            "Walking in others’ shoes: How perspective-taking guides large language models in reducing toxicity and bias.",
            "Is this the real life? is this just fantasy? the misleading success of simulating social interactions with llms."
        ]
    },
    {
        "index": 334,
        "title": "Adversary-Aware DPO: Enhancing Safety Alignment in Vision Language Models via Adversarial Training",
        "publication_date": "2025-02-17",
        "references": [
            "A general theoretical paradigm to understand learning from human preferences",
            "Qwen-vl: A frontier large vision-language model with versatile abilities",
            "Training a helpful and harmless assistant with reinforcement learning from human feedback",
            "Towards evaluating the robustness of neural networks",
            "Are we on the right way for evaluating large vision-language models?",
            "Instructblip: Towards general-purpose vision-language models with instruction tuning",
            "Figstep: Jailbreaking large vision-language models via typographic visual prompts",
            "Explaining and harnessing adversarial examples",
            "Vlsbench: Unveiling visual leakage in multimodal safety",
            "Red teaming visual language models",
            "Towards understanding jailbreak attacks in llms: A representation space analysis",
            "Improved baselines with visual instruction tuning",
            "Visual instruction tuning",
            "Autodan: Generating stealthy jailbreak prompts on aligned large language models",
            "Mm-safetybench: A benchmark for safety evaluation of multimodal large language models",
            "Ocrbench: on the hidden mystery of ocr in large multimodal models",
            "Jailbreakv-28k: A benchmark for assessing the robustness of multimodal large language models against jailbreak attacks",
            "Towards deep learning models resistant to adversarial attacks",
            "Harmbench: A standardized evaluation framework for automated red teaming and robust refusal",
            "Jailbreaking attack against multimodal large language model",
            "Training language models to follow instructions with human feedback",
            "High-resolution image synthesis with latent diffusion models",
            "Proximal policy optimization algorithms",
            "Do anything now\": Characterizing and evaluating in-the-wild jailbreak prompts on large language models",
            "Latent adversarial training improves robustness to persistent harmful behaviors in llms",
            "MMJ-Bench: A comprehensive study on jailbreak attacks and defenses for vision language models",
            "Principal component analysis",
            "Efficient adversarial training in llms with continuous attacks",
            "Gptfuzzer: Red teaming large language models with auto-generated jailbreak prompts",
            "Mm-vet: Evaluating large multimodal models for integrated capabilities",
            "Benchmarking trustworthiness of multimodal large language models: A comprehensive study",
            "Spavl: A comprehensive safety preference alignment dataset for vision language model",
            "Prompt-driven llm safeguarding via directed representation optimization",
            "Don't say no: Jailbreaking llm by suppressing refusal",
            "Minigpt-4: Enhancing vision-language understanding with advanced large language models",
            "Safety fine-tuning at (almost) no cost: A baseline for vision large language models",
            "Universal and transferable adversarial attacks on aligned language models"
        ]
    },
    {
        "index": 335,
        "title": "DELMAN: Dynamic Defense Against Large Language Model Jailbreaking with Model Editing",
        "publication_date": "2025-02-17",
        "references": [
            "Detecting language model attacks with perplexity",
            "Analysis methods in neural language processing: A survey",
            "Defending against alignment-breaking attacks via robustly aligned llm",
            "Jailbreakbench: An open robustness benchmark for jailbreaking large language models",
            "Jailbreaking black box large language models in twenty queries",
            "Rouge: A package for automatic evaluation of summaries",
            "BoolQ: Exploring the surprising difficulty of natural yes/no questions",
            "Training verifiers to solve math word problems",
            "Mutual: A dataset for multi-turn dialogue reasoning",
            "The pascal recognising textual entailment challenge",
            "Editing factual knowledge in language models",
            "Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned",
            "Transformer feed-forward layers build predictions by promoting concepts in the vocabulary space",
            "Transformer feed-forward layers are key-value memories",
            "Samsum corpus: A human-annotated dialogue dataset for abstractive summarization",
            "Lora: Low-rank adaptation of large language models",
            "Catastrophic jailbreak of open-source llms via exploiting generation",
            "Llama guard: Llm-based input-output safeguard for human-ai conversations",
            "Baseline defenses for adversarial attacks against aligned language models",
            "On information and sufficiency",
            "Principal component analysis",
            "Locating and editing factual associations in gpt",
            "Mass-editing memory in a transformer",
            "Fast model editing at scale",
            "Introduction to the conll-2003 shared task: Language-independent named entity recognition",
            "Recursive deep models for semantic compositionality over a sentiment treebank",
            "Llama 2: Open foundation and fine-tuned chat models",
            "Detoxifying large language models via knowledge editing",
            "Self-instruct: Aligning language models with self-generated instructions",
            "Safedecoding: Defending against jailbreak attacks via safety-aware decoding",
            "A comprehensive study of jailbreak attack versus defense for large language models",
            "Defending large language models against jailbreak attacks via layer-specific editing",
            "Robust prompt optimization for defending language models against jailbreaking attacks",
            "Don’t say no: Jailbreaking llm by suppressing refusal",
            "Modifying memories in transformer models",
            "Universal and transferable adversarial attacks on aligned language models"
        ]
    },
    {
        "index": 336,
        "title": "SAFECHAIN : Safety of Language Models with Long Chain-of-Thought Reasoning Capabilities",
        "publication_date": "2025-02-17",
        "references": [
            "A general language assistant as a laboratory for alignment",
            "Program synthesis with large language models",
            "Training a helpful and harmless assistant with reinforcement learning from human feedback",
            "Constitutional ai: Harmlessness from ai feedback",
            "Mle-bench: Evaluating machine learning agents on machine learning engineering",
            "Evaluating large language models trained on code",
            "Scienceagentbench: Toward rigorous assessment of language agents for data-driven scientific discovery",
            "Training verifiers to solve math word problems",
            "Pearson correlation coefficient",
            "Gemini 2.0 flash thinking",
            "Alphazero-like tree-search can guide large language model decoding and training",
            "Deliberative alignment: Reasoning enables safer language models",
            "Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning",
            "Llama guard: Llm-based input-output safeguard for human-ai conversations",
            "Openai o1 system card",
            "Live-codebench: Holistic and contamination free evaluation of large language models for code",
            "Chatbug: A common vulnerability of aligned llms induced by chat templates",
            "ArtPrompt: ASCII art-based jailbreak attacks against aligned LLMs",
            "Wildteaming at scale: From in-the-wild jailbreaks to (adversarially) safer language models",
            "Upgrading the moderation api with our new multimodal moderation model",
            "Large language models are zero-shot reasoners",
            "Training language models to self-correct via reinforcement learning",
            "Let’s verify step by step",
            "Wildbench: Benchmarking LLMs with challenging tasks from real users in the wild",
            "AutoDAN: Generating stealthy jailbreak prompts on aligned large language models",
            "HarmBench: A standardized evaluation framework for automated red teaming and robust refusal",
            "s1: Simple test-time scaling",
            "Skywork-o1 open series",
            "Training language models to follow instructions with human feedback",
            "Safety alignment should be made more than just a few tokens deep",
            "A strongreject for empty jailbreaks",
            "Kimi k1. 5: Scaling reinforcement learning with llms",
            "Think less, achieve more: Cut reasoning costs by 50 sacrificing accuracy",
            "Qwq: Reflect deeply on the boundaries of the unknown",
            "Llama 2: Open foundation and fine-tuned chat models",
            "Jailbroken: How does LLM safety training fail?",
            "Chain-of-thought prompting elicits reasoning in large language models",
            "Least-to-most prompting enables complex reasoning in large language models",
            "Universal and transferable adversarial attacks on aligned language models",
            "Llamafactory: Unified efficient fine-tuning of 100+ language models",
            "Tree of thoughts: Deliberate problem solving with large language models",
            "Judging llm-as-a-judge with mt-bench and chatbot arena",
            "Badchain: Backdoor chain-of-thought prompting for large language models",
            "SafeDecoding: Defending against jailbreak attacks via safety-aware decoding",
            "Live-codebench: Holistic and contamination free evaluation of large language models for code"
        ]
    },
    {
        "index": 337,
        "title": "LLM Safety for Children",
        "publication_date": "2025-02-18",
        "references": [
            "The sustainable effect of artificial intelligence and parental control on children’s behavior while using smart devices’ apps: The case of saudi arabia",
            "Identifying child abuse through text mining and machine learning",
            "A natural language processing and deep learning approach to identify child abuse from pediatric electronic medical records",
            "An exploration of alcohol advertising on social networking sites: an analysis of content, interactions and young people’s perspectives",
            "Online suicide games: A form of digital self-harm or a myth?",
            "The kids are alt-right: How media and the law enable white supremacist groups to recruit and radicalize emotionally vulnerable individuals",
            "On the opportunities and risks of foundation models",
            "Are kids too busy? early adolescents’ perceptions of discretionary activities, overscheduling, and stress",
            "A framework and exemplars for ethical and responsible use of ai chatbot technology to support teaching and learning",
            "Evaluating the efficacy of interactive language therapy based on llm for high-functioning autistic adolescent psychological counseling",
            "Online child grooming: A literature review on the misuse of social networking sites for grooming children for sexual offences",
            "Teen gambling: Understanding a growing epidemic",
            "Misinformation and disinformation in food science and nutrition: Impact on practice",
            "School shooters: patterns of adverse childhood experiences, bullying, and social media",
            "Artificial intelligence and child abuse and neglect: a systematic review",
            "Associations between adolescent depression and self-harm behaviors and screen media use in a nationally representative time-diary study",
            "Inverse scaling: When bigger isn’t better",
            "Emotional artificial intelligence in children’s toys and devices: Ethics, governance and practical remedies",
            "Handbook of children, culture, and violence",
            "The role of violent video game content in adolescent development: Boys’ perspectives",
            "Trustworthy llms: a survey and guideline for evaluating large language models’ alignment",
            "Ethical and social risks of harm from language models",
            "Terrorism in cyberspace: The next generation",
            "Exposure of children and adolescents to alcohol marketing on social media websites",
            "Unwanted and wanted exposure to online pornography in a national sample of youth internet users",
            "Judging llm-as-a-judge with mt-bench and chatbot arena",
            "Ethical implications of ai in the metaverse"
        ]
    },
    {
        "index": 338,
        "title": "SEA: Low-Resource Safety Alignment for Multimodal Large Language Models via Synthetic Embeddings",
        "publication_date": "2025-02-18",
        "references": [
            "Cross-modal safety alignment: Is textual unlearning all you need?",
            "Jailbreaking black box large language models in twenty queries.",
            "Minigpt-v2: large language model as a unified interface for vision-language multi-task learning.",
            "DRESS: Instructing large vision-language models to align and interact with humans via natural language feedback.",
            "Videollama 2: Advancing spatial-temporal modeling and audio understanding in video-llms.",
            "Qwen2-audio technical report.",
            "Qwen-audio: Advancing universal audio understanding via unified large-scale audio-language models.",
            "Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis.",
            "Figstep: Jailbreaking large vision-language models via typographic visual prompts.",
            "Mllmguard: A multi-dimensional safety evaluation suite for multimodal large language models.",
            "Measuring massive multitask language understanding.",
            "Vlsbench: Unveiling visual leakage in multimodal safety.",
            "Pku-saferlhf: Towards multi-level safety alignment for llms with human preference.",
            "Align anything: Training all-modality models to follow instructions with language feedback.",
            "Pyramidal flow matching for efficient video generative modeling.",
            "Mvbench: A comprehensive multi-modal video understanding benchmark.",
            "Evaluating object hallucination in large vision-language models.",
            "Images are achilles’ heel of alignment: Exploiting visual vulnerabilities for jailbreaking multimodal large language models.",
            "Improved baselines with visual instruction tuning.",
            "Visual instruction tuning.",
            "Autodan: Generating stealthy jailbreak prompts on aligned large language models.",
            "Mm-safetybench: A benchmark for safety evaluation of multimodal large language models.",
            "Jailbreakv-28k: A benchmark for assessing the robustness of multimodal large language models against jailbreak attacks.",
            "Jailbreaking attack against multimodal large language model.",
            "Visual adversarial examples jailbreak aligned large language models.",
            "Direct preference optimization: Your language model is secretly a reward model.",
            "Proximal policy optimization algorithms.",
            "Assessment of multimodal large language models in alignment with human values.",
            "SALMONN: Towards generic hearing abilities for large language models.",
            "Eegpt: Pretrained transformer for universal and reliable representation of eeg signals.",
            "Qwen2-vl: Enhancing vision-language model’s perception of the world at any resolution.",
            "Audio is the achilles’ heel: Red teaming audio large multimodal models.",
            "Air-bench: Benchmarking large audio-language models via generative comprehension.",
            "Safebench: A safety evaluation framework for multimodal large language models.",
            "Spavl: A comprehensive safety preference alignment dataset for vision language model."
        ]
    },
    {
        "index": 339,
        "title": "Iron Sharpens Iron: Defending Against Attacks in Machine-Generated Text Detection with Adversarial Training",
        "publication_date": "2025-02-18",
        "references": [
            "Gpt-4 technical report",
            "Claude 3: A conversational ai model",
            "Fast-detectgpt: Efficient zero-shot detection of machine-generated text via conditional probability curvature",
            "Wild patterns: Ten years after the rise of adversarial machine learning",
            "Universal sentence encoder",
            "The llama 3 herd of models",
            "Text processing like humans do: Visually attacking and shielding nlp systems",
            "Adversarial attacks on neural text detectors",
            "A new readability yardstick",
            "The homograph attack",
            "Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning",
            "Detective: Detecting ai-generated text via multi-level contrastive learning",
            "Deberta: Decoding-enhanced bert with disentangled attention",
            "Unsupervised cross-lingual representation learning at scale",
            "Is bert really robust? a strong baseline for natural language attack on text classification and entailment",
            "Paraphrasing evades detectors of ai-generated text, but retrieval is an effective defense",
            "Techniques for automatically correcting words in text",
            "Ai-generated text boundary detection with roft",
            "Albert: A lite bert for self-supervised learning of language representations",
            "Binary codes capable of correcting deletions, insertions, and reversals",
            "Bert-attack: Adversarial attack against bert using bert",
            "Tavat: Token-aware virtual adversarial training for language understanding",
            "Gpt detectors are biased against non-native english writers",
            "Hqa-attack: toward high quality black-box hard-label adversarial attack on text",
            "Does detectgpt fully utilize perturbation? bridging selective perturbation to fine-tuned contrastive learning detector would be better",
            "Coco: Coherence-enhanced machine-generated text detection under data limitation with contrastive learning",
            "Roberta: A robustly optimized BERT pretraining approach",
            "Authorship obfuscation in multilingual machine-generated text detection",
            "Detectgpt: Zero-shot machine-generated text detection using probability curvature",
            "Virtual adversarial training: A regularization method for supervised and semi-supervised learning",
            "Generating natural language adversarial examples through probability weighted word saliency",
            "Red teaming language model detectors with language models",
            "Badrock at semeval-2024 task 8: Distilbert to detect multigenerator, multidomain and multilingual black-box machine-generated text",
            "Detectllm: Leveraging log rank information for zero-shot detection of machine-generated text",
            "A comprehensive survey on data augmentation",
            "Towards improving adversarial training of nlp models",
            "Query-efficient textual adversarial example generation for black-box attacks",
            "Bridge the gap between cv and nlp! a gradient-based textual adversarial attack framework",
            "Why johnny can’t prompt: how non-ai experts try (and fail) to design llm prompts",
            "Certified robustness to text adversarial attacks by randomized [mask]",
            "Improving massively multilingual neural machine translation and zero-shot translation",
            "Pegasus: Pre-training with extracted gap-sentences for abstractive summarization",
            "Bertscore: Evaluating text generation with bert",
            "Text-crs: A generalized certified robustness framework against textual adversarial attacks",
            "Random smooth-based certified defense against text adversarial attack",
            "Neural deepfake detection with factual structure of text",
            "Humanizing machine-generated content: Evading ai-text detection through adversarial attack",
            "The decades progress on code-switching research in nlp: A systematic survey on trends and challenges",
            "Seqxgpt: Sentence-level ai-generated text detection",
            "Stumbling blocks: Stress testing the robustness of machine-generated text detectors under attacks",
            "A comprehensive survey on data augmentation"
        ]
    },
    {
        "index": 340,
        "title": "ShieldLearner: A New Paradigm for Jailbreak Attack Defense in LLMs",
        "publication_date": "2025-02-16",
        "references": [
            "Gpt-4 technical report",
            "Detecting language model attacks with perplexity",
            "Jailbreaking leading safety-aligned llms with simple adaptive attacks",
            "Safety-tuned llamas: Lessons from improving the safety of large language models that follow instructions",
            "Guide for defense (g4d): Dynamic guidance for robust and balanced defense in large language models",
            "Jailbreaking black box large language models in twenty queries",
            "Attacks, defenses and evaluations for llm conversation safety: A survey",
            "Thinking, fast and slow",
            "Deepinception: Hypnotize large language model to be jailbreaker",
            "Autodan: Generating stealthy jailbreak prompts on aligned large language models",
            "Jailbreakv-28k: A benchmark for assessing the robustness of multimodal large language models against jailbreak attacks",
            "Harmbench: A standardized evaluation framework for automated red teaming and robust refusal",
            "Improving cross-state and cross-subject visual erp-based bci with temporal modeling and adversarial training",
            "Llm self defense: By self examination, llms know they are being tricked",
            "Fine-tuning aligned language models compromises safety, even when users do not intend to!",
            "Is ChatGPT a general-purpose natural language processing task solver?",
            "The probabilistic relevance framework: Bm25 and beyond",
            "Smoothllm: Defending large language models against jailbreaking attacks",
            "Do anything now\": Characterizing and evaluating in-the-wild jailbreak prompts on large language models",
            "Cyberseceval 3: Advancing the evaluation of cybersecurity risks and capabilities in large language models",
            "Asetf: A novel method for jailbreak attack on llms through translate suffix embeddings",
            "Diffusionattacker: Diffusion-driven prompt manipulation for llm jailbreak",
            "Jailbroken: How does llm safety training fail?",
            "Jailbreak and guard aligned language models with only few in-context demonstrations",
            "Defending chatgpt against jailbreak attack via self-reminders",
            "Adversarial examples improve image recognition",
            "Defending large language models against jailbreaking attacks through goal prioritization",
            "Universal and transferable adversarial attacks on aligned language models",
            "A Categorization and Definition of Defense Mechanisms"
        ]
    },
    {
        "index": 343,
        "title": "Fundamental Limitations in Defending LLM Finetuning APIs",
        "publication_date": "2025-02-20",
        "references": [
            "Usage policy, June 2024",
            "Sabotage evaluations for frontier models",
            "Hopping too late: Exploring the limitations of large language models on multi-hop queries",
            "Customize a model with fine-tuning: Safety evaluation gpt-4, gpt-4o, and gpt-4o-mini fine-tuning - public preview",
            "Poisoning web-scale training datasets is practical",
            "Targeted backdoor attacks on deep learning systems using data poisoning",
            "Low-stakes alignment",
            "Or-bench: An over-refusal benchmark for large language models",
            "Content filtering",
            "Fine-tuning with the gemini api",
            "Generative ai-prohibited use policy",
            "Auditing failures vs concentrated failures",
            "Deliberative alignment: Reasoning enables safer language models",
            "Covert malicious finetuning: Challenges in safeguarding llm adaptation",
            "WildGuard: Open one-stop moderation tools for safety risks, jailbreaks, and refusals of LLMs",
            "What is in your safe data? identifying benign data that breaks safety",
            "Mechanistically analyzing the effects of fine-tuning on procedurally defined tasks",
            "Measuring faithfulness in chain-of-thought reasoning",
            "The wmdp benchmark: Measuring and reducing malicious use with unlearning",
            "Llama guard 3 documentation",
            "Secret collusion among generative ai agents",
            "Openai o1 system card",
            "OpenAI model specification - may 8, 2024",
            "Usage policies",
            "Fine-tuning",
            "Exploiting novel gpt-4 apis",
            "Instruction tuning with gpt-4",
            "Fine-tuning aligned language models compromises safety, even when users do not intend to!",
            "Safety alignment should be made more than just a few tokens deep",
            "Representation noising: A defence mechanism against harmful finetuning",
            "XSTest: A test suite for identifying exaggerated safety behaviours in large language models",
            "Commonsenseqa: A question answering challenge targeting commonsense knowledge",
            "Tamper-resistant safeguards for open-weight llms",
            "Managing Misuse Risk for Dual-Use Foundation Models",
            "Look before you leap: A universal emergent decomposition of retrieval tasks in language models",
            "Mitigating fine-tuning based jailbreak attack with backdoor enhanced safety alignment",
            "Adaptive deployment of untrusted llms reduces distributed threats",
            "Finetunebench: How well do commercial fine-tuning apis infuse knowledge into llms?",
            "Removing rlhf protections in gpt-4 via fine-tuning",
            "Fine-tune anthropic’s claude 3 haiku in amazon bedrock to boost model accuracy and quality"
        ]
    },
    {
        "index": 344,
        "title": "EigenShield: Causal Subspace Filtering via Random Matrix Theory for Adversarially Robust Vision-Language Models",
        "publication_date": "2025-02-20",
        "references": [
            "Large language models: A survey",
            "Vision-language models for vision tasks: A survey",
            "Adversarial training for free!",
            "Jailbreakzoo: Survey, landscapes, and horizons in jailbreaking large language and vision-language models",
            "A survey of attacks on large vision-language models: Resources, advances, and future trends",
            "Jailbreak in pieces: Compositional adversarial attacks on multi-modal language models",
            "On evaluating adversarial robustness of large vision-language models",
            "Benchmarking and defending against indirect prompt injection attacks on large language models",
            "Visual adversarial examples jailbreak aligned large language models",
            "Uniguard: Towards universal safety guardrails for jailbreak attacks on multimodal large language models",
            "Diffusion models for adversarial purification",
            "A mutation-based method for multi-modal jailbreaking attack detection",
            "Defending jailbreak attack in vlms via cross-modality information detector",
            "Random matrix theory",
            "Topics in random matrix theory",
            "Methods of proof in random matrix theory",
            "The wigner semi-circle law and eigenvalues of matrix-valued diffusions",
            "A short proof of the marchenko–pastur theorem",
            "Asymptotics of sample eigenstructure for a large dimensional spiked covariance model",
            "Harmbench: A standardized evaluation framework for automated red teaming and robust refusal",
            "Realtoxicityprompts: Evaluating neural toxic degeneration in language models",
            "Visual instruction tuning",
            "Minigpt-4: Enhancing vision-language understanding with advanced large language models",
            "Instructblip: Towards general-purpose vision-language models with instruction tuning",
            "Qwen2-vl: Enhancing vision-language model’s perception of the world at any resolution",
            "Florence-2: Advancing a unified representation for a variety of vision tasks",
            "Imagenet: A large-scale hierarchical image database",
            "Towards deep learning models resistant to adversarial attacks",
            "Robust conversational agents against imperceptible toxicity triggers",
            "Explaining and harnessing adversarial examples",
            "Boosting adversarial attacks with momentum",
            "Towards evaluating the robustness of neural networks",
            "Square attack: a query-efficient black-box adversarial attack via random search",
            "On the distribution of the largest eigenvalue in principal components analysis",
            "Spectral analysis of large dimensional random matrices"
        ]
    },
    {
        "index": 345,
        "title": "Is Safety Standard Same for Everyone? User-Specific Safety Evaluation of Large Language Models",
        "publication_date": "2025-02-20",
        "references": [
            "Gpt-4 technical report",
            "Anthropic api",
            "Claude 3.5 sonnet model card addendum",
            "Chatgpt: Applications, opportunities, and threats",
            "Maxmin-rlhf: Towards equitable alignment of large language models with diverse human preferences",
            "Everyone deserves a reward: Learning customized human preferences",
            "Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned",
            "Anthropic",
            "Gpt-4o system card",
            "Llama guard: Llm-based input-output safeguard for human-ai conversations",
            "Personalized soups: Personalized large language model alignment via post-hoc parameter merging",
            "Mistral 7b",
            "Evaluating open-domain question answering in the era of large language models",
            "Aligning to thousands of preferences via system message generalization",
            "Personalized language modeling from personalized human feedback",
            "Gpt-4o system card",
            "Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context",
            "Gemma 2: Improving open language models at a practical size",
            "Alert: A comprehensive benchmark for assessing large language models’ safety through red teaming",
            "Judging llm-as-a-judge with mt-bench and chatbot arena",
            "Red teaming chatgpt via jailbreaking: Bias, robustness, reliability and toxicity",
            "Safety-bench: Evaluating the safety of large language models with multiple choice questions",
            "Personalization of large language models: A survey",
            "Agent-safetybench: Evaluating the safety of llm agents",
            "Chain-of-thought prompting elicits reasoning in large language models",
            "Jailbroken: How does llm safety training fail?",
            "Sorry-bench: Systematically evaluating large language model safety refusal behaviors",
            "Wizardlm: Empowering large language models to follow complex instructions",
            "Safeagentbench: A benchmark for safe task planning of embodied llm agents",
            "Exploring safety-utility trade-offs in personalized language models"
        ]
    },
    {
        "index": 346,
        "title": "Attention Eclipse: Manipulating Attention to Bypass LLM Safety-Alignment",
        "publication_date": "2025-02-21",
        "references": [
            "Gpt-4 technical report",
            "Attention deficit is ordered! fooling deformable vision transformers with collaborative adversarial patches",
            "Jailbreaking leading safety-aligned llms with simple adaptive attacks",
            "Longformer: The long-document transformer",
            "Jailbreaking black box large language models in twenty queries",
            "Scatterbrain: Unifying sparse and low-rank attention",
            "Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality",
            "What does bert look at? an analysis of bert's attention",
            "Jailbreaker: Automated jailbreak across multiple large language model chatbots",
            "A wolf in sheep’s clothing: Generalized nested jailbreak prompts can fool large language models easily",
            "Hotflip: White-box adversarial examples for text classification",
            "A systematic review of federated generative models",
            "Not what you’ve signed up for: Compromising real-world llm-integrated applications with indirect prompt injection",
            "Gradient-based adversarial attacks against text transformers",
            "Self-attention attribution: Interpreting information interactions inside transformer",
            "Jailbreakzoo: Survey, landscapes, and horizons in jailbreaking large language and vision-language models",
            "Deepseek-v3 technical report",
            "Autodan: Generating stealthy jailbreak prompts on aligned large language models",
            "AutoDAN: Generating stealthy jailbreak prompts on aligned large language models",
            "Deepmem: Ml models as storage channels and their (mis-) applications",
            "Harmbench: A standardized evaluation framework for automated red teaming and robust refusal",
            "Tree of attacks: Jailbreaking black-box llms automatically",
            "Gpt-3.5-turbo (june 13th 2023 version) [large language model]",
            "Ignore previous prompt: Attack techniques for language models",
            "Feint and attack: Attention-based strategies for jailbreaking and protecting llms",
            "Language models are unsupervised multitask learners",
            "Codeattack: Revealing safety generalization challenges of large language models via code completion",
            "Predicting differentially methylated cytosines in tet and dnmt3 knockout mutants via a large language model",
            "Survey of vulnerabilities in large language models revealed by adversarial attacks",
            "“ do anything now”: Characterizing and evaluating in-the-wild jailbreak prompts on large language models",
            "Llama 2: Open foundation and fine-tuned chat models",
            "A multiscale visualization of attention in the transformer model",
            "Universal adversarial triggers for attacking and analyzing nlp",
            "Linformer: Self-attention with linear complexity",
            "Jailbroken: How does llm safety training fail?",
            "Jailbreak and guard aligned language models with only few in-context demonstrations",
            "Hard prompts made easy: Gradient-based discrete optimization for prompt tuning and discovery",
            "Jailbreak attacks and defenses against large language models: A survey",
            "Big bird: Transformers for longer sequences",
            "How johnny can persuade llms to jailbreak them: Rethinking persuasion to challenge ai safety by humanizing llms",
            "Tell your model where to attend: Post-hoc attention steering for llms",
            "Universal and transferable adversarial attacks on aligned language models"
        ]
    },
    {
        "index": 347,
        "title": "Adversarial Prompt Evaluation: Systematic Benchmarking of Guardrails Against Prompt Input Attacks on LLMs",
        "publication_date": "2025-02-21",
        "references": [
            "Wordcraft: story writing with large language models",
            "Webgpt: Browser-assisted question-answering with human feedback",
            "Multilingual machine translation with large language models: Empirical results and analysis",
            "Towards evaluating the robustness of neural networks",
            "Towards poisoning of deep learning algorithms with back-gradient optimization",
            "Are aligned neural networks adversarially aligned?",
            "Survey of vulnerabilities in large language models revealed by adversarial attacks",
            "Universal and transferable adversarial attacks on aligned language models",
            "Autodan: Automatic and interpretable adversarial attacks on large language models",
            "Jailbroken: How does llm safety training fail?",
            "Baseline defenses for adversarial attacks against aligned language models",
            "Safetyprompts: a systematic review of open datasets for evaluating and improving large language model safety",
            "Harmbench: A standardized evaluation framework for automated red teaming and robust refusal",
            "Jailbreakbench: An open robustness benchmark for jailbreaking large language models",
            "Easyjailbreak: A unified framework for jailbreaking large language models",
            "Aart: Ai-assisted red-teaming with diverse data generation for new llm-powered applications",
            "Unveiling safety vulnerabilities of large language models",
            "Autodan: Generating stealthy jailbreak prompts on aligned large language models",
            "Do-not-answer: A dataset for evaluating safeguards in llms",
            "Prompt extraction. Internal IBM generated dataset.",
            "Attack prompt generation for red teaming and defending large language models",
            "Chatgpt dan",
            "Jailbreakchat",
            "Catastrophic jailbreak of open-source llms via exploiting generation",
            "Xstest: A test suite for identifying exaggerated safety behaviours in large language models",
            "Toxicchat: Unveiling hidden challenges of toxicity detection in real-world user-ai conversation",
            "Stanford alpaca: An instruction-following llama model",
            "Awesome chatgpt prompts",
            "Boolq: Exploring the surprising difficulty of natural yes/no questions",
            "No robots",
            "Puffin dataset",
            "Super-naturalinstructions: Generalization via declarative instructions on 1600+ nlp tasks",
            "Enhancing chat language models by scaling high-quality instructional conversations",
            "Free dolly: Introducing the world’s first truly open instruction-tuned llm",
            "Chatbot arena: An open platform for evaluating llms by human preference",
            "H4. instruction-dataset",
            "Orca dpo pairs",
            "Piqa: Reasoning about physical commonsense in natural language",
            "google bert",
            "microsoft. deberta-v3-base",
            "openai community. gpt2",
            "Deberta: Decoding-enhanced bert with disentangled attention",
            "Microsoft. Azure ai content safety",
            "Openai platform",
            "Openai moderation api",
            "Smoothllm: Defending large language models against jailbreaking attacks",
            "meta llama",
            "How johnny can persuade llms to jailbreak them: Rethinking persuasion to challenge ai safety by humanizing llms",
            "A gentle introduction to conformal prediction and distribution-free uncertainty quantification",
            "Detecting language model attacks with perplexity",
            "Training language models to follow instructions with human feedback"
        ]
    },
    {
        "index": 348,
        "title": "A Mousetrap: Fooling Large Reasoning Models for Jailbreak with Chain of Iterative Chaos",
        "publication_date": "2025-02-19",
        "references": [
            "Claude 3.5 sonnet model card addendum",
            "Reasoning language models: A blueprint",
            "Forest-of-thought: Scaling test-time compute for enhancing llm reasoning",
            "Multilingual jailbreak challenges in large language models",
            "A wolf in sheep’s clothing: Generalized nested jailbreak prompts can fool large language models easily",
            "Gemini 2.0 flash thinking",
            "Gemini safety settings",
            "Deliberative alignment: Reasoning enables safer language models",
            "Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning",
            "Cold-attack: Jailbreaking llms with stealthiness and controllability",
            "Jailbreaking proprietary large language models using word substitution cipher",
            "Openai o1 system card",
            "Artprompt: Ascii art-based jailbreak attacks against aligned llms",
            "Exploiting programmatic behavior of llms: Dual-use through standard security attacks",
            "Bandit based monte-carlo planning",
            "Multi-step jailbreaking privacy attacks on chatgpt",
            "Deepinception: Hypnotize large language model to be jailbreaker",
            "Autodan: Generating stealthy jailbreak prompts on aligned large language models",
            "Query-relevant images jail-break large multi-modal models",
            "Codechameleon: Personalized encryption framework for jailbreaking large language models",
            "Safety at scale: A comprehensive survey of large model safety",
            "Harmbench: A standardized evaluation framework for automated red teaming and robust refusal",
            "Openai o3-mini system card",
            "Openai usage policies",
            "Derail yourself: Multi-turn llm jailbreak attack through self-discovered clues",
            "‘do anything now’: characterizing and evaluating in-the-wild jailbreak prompts on large language models",
            "Scaling llm test-time compute optimally can be more effective than scaling model parameters",
            "A strongreject for empty jailbreaks",
            "Alphazero-like tree-search can guide large language model decoding and training",
            "Self-consistency improves chain of thought reasoning in language models",
            "Chain-of-thought prompting elicits reasoning in large language models",
            "Jailbreak and guard aligned language models with only few in-context demonstrations",
            "Jailbreak attacks and defenses against large language models: A survey",
            "Low-resource languages jailbreak gpt-4",
            "Gpt-4 is too smart to be safe: Stealthy chat with llms via cipher",
            "How johnny can persuade llms to jailbreak them: Rethinking persuasion to challenge ai safety by humanizing llms",
            "Llama-berry: Pairwise optimization for o1-like olympiad-level mathematical reasoning",
            "Universal and transferable adversarial attacks on aligned language models"
        ]
    },
    {
        "index": 349,
        "title": "GuidedBench: Equipping Jailbreak Evaluation with Guidelines",
        "publication_date": "2025-02-24",
        "references": [
            "Measuring gender and racial biases in large language models",
            "Jailbreaking leading safety-aligned llms with simple adaptive attacks",
            "Many-shot jailbreaking",
            "Usage policy",
            "The dark side of language models: Exploring the potential of llms in multimedia disinformation generation and dissemination",
            "Doubao-1.5-pro",
            "Jailbreakbench: An open robustness benchmark for jailbreaking large language models",
            "Jailbreaking black box large language models in twenty queries",
            "Why should adversarial perturbations be imperceptible? rethink the research paradigm in adversarial nlp",
            "Deepseek-v3 technical report",
            "Attack prompt generation for red teaming and defending large language models",
            "Multilingual jailbreak challenges in large language models",
            "A wolf in sheep’s clothing: Generalized nested jailbreak prompts can fool large language models easily",
            "Query-based adversarial prompt generation",
            "Measuring massive multitask language understanding",
            "Catastrophic jailbreak of open-source llms via exploiting generation",
            "Guard: Role-playing to generate natural-language jailbreakings to test guideline adherence of large language models",
            "Jailbreak-zoo: Survey, landscapes, and horizons in jailbreaking large language and vision-language models",
            "Sage-rt: Synthetic alignment data generation for safety evaluation and red teaming",
            "Open sesame! universal black box jailbreaking of large language models",
            "Rethinking jailbreaking through the lens of representation engineering",
            "Semantic mirror jailbreak: Genetic algorithm based jailbreak prompts against open-source llms",
            "DrAttack: Prompt decomposition and reconstruction makes powerful LLMs jailbreakers",
            "Deepinception: Hypnotize large language model to be jailbreaker",
            "Amplegcg: Learning a universal and transferable generative model of adversarial suffixes for jailbreaking both open and closed llms",
            "Goal-oriented prompt attack and safety evaluation for llms",
            "Making them ask and answer: Jailbreaking large language models in few queries via disguise and reconstruction",
            "Autodan-turbo: A lifelong agent for strategy self-exploration to jailbreak llms",
            "Autodan: Generating stealthy jailbreak prompts on aligned large language models",
            "Codechameleon: Personalized encryption framework for jailbreaking large language models",
            "Metamorphic malware evolution: The potential and peril of large language models",
            "Prp: Propagating universal perturbations to attack large language model guard-rails",
            "Harmbench: A standardized evaluation framework for automated red teaming and robust refusal",
            "Llama model use policy",
            "Use of llms for illicit purposes: Threats, prevention measures, and vulnerabilities",
            "Openai moderations api documentation",
            "Gpt-4o system card",
            "Training language models to follow instructions with human feedback",
            "On the risk of misinformation pollution with large language models",
            "A survey on the use of large language models (llms) in fake news",
            "Advprompter: Fast adaptive adversarial prompting for llms",
            "CodeAttack: Revealing safety generalization challenges of large language models via code completion",
            "Activation addition: Steering language models without optimization",
            "'kelly is a warm person, joseph is a role model': Gender biases in llm-generated reference letters",
            "Jailbroken: How does llm safety training fail?",
            "Jailbreak and guard aligned language models with only few in-context demonstrations",
            "Efficient adversarial training in llms with continuous attacks",
            "Distract large language models for automatic jailbreak attack",
            "Uncovering safety risks of large language models through concept activation vector",
            "Fuzzllm: A novel and universal fuzzing framework for proactively discovering jailbreak vulnerabilities in large language models",
            "Jailbreak attacks and defenses against large language models: A survey",
            "Gptfuzzer: Red teaming large language models with auto-generated jailbreak prompts",
            "Gpt-4 is too smart to be safe: Stealthy chat with llms via cipher",
            "Safetybench: Evaluating the safety of large language models",
            "Improved few-shot jailbreaking can circumvent aligned language models and their defenses",
            "Universal and transferable adversarial attacks on aligned language models"
        ]
    },
    {
        "index": 350,
        "title": "TURBO FUZZLLM: Turbocharging Mutation-based Fuzzing for Effectively Jailbreaking Large Language Models in Practice",
        "publication_date": "2025-02-21",
        "references": [
            "Jailbreaking leading safety-aligned llms with simple adaptive attacks",
            "Jailbreakbench: An open robustness benchmark for jailbreaking large language models",
            "Jailbreaking black box large language models in twenty queries",
            "When llm meets drl: Advancing jail-breaking efficiency via drl-guided search",
            "Flashattention: Fast and memory-efficient exact attention with io-awareness",
            "Qlora: efficient fine-tuning of quantized llms (2023)",
            "Effective and evasive fuzz testing-driven jailbreaking attacks against llms",
            "Trustllm: Trustworthiness in large language models",
            "Chatbug: A common vulnerability of aligned llms induced by chat templates",
            "Amplegcg: Learning a universal and transferable generative model of adversarial suffixes for jailbreaking both open and closed llms",
            "Path-seeker: Exploring llm security vulnerabilities with a reinforcement learning-based jailbreak approach",
            "Autodan-turbo: A lifelong agent for strategy self-exploration to jailbreak llms",
            "Autodan: Generating stealthy jailbreak prompts on aligned large language models",
            "Flipattack: Jailbreak llms via flipping",
            "Harmbench: A standardized evaluation framework for automated red teaming and robust refusal",
            "Tree of attacks: Jailbreaking black-box llms automatically",
            "Advprompter: Fast adaptive adversarial prompting for llms",
            "Automated red teaming with goat: the generative offensive agent tester",
            "Rainbow teaming: Open-ended generation of diverse adversarial prompts",
            "Pal: Proxy-guided black-box attack on large language models",
            "Introduction to multi-armed bandits",
            "All in how you ask for it: Simple black-box method for jailbreak attacks",
            "A closer look at adversarial suffix learning for jailbreaking LLMs",
            "Jailbroken: How does llm safety training fail?",
            "Fuzzllm: A novel and universal fuzzing framework for proactively discovering jailbreak vulnerabilities in large language models",
            "Gptfuzzer: Red teaming large language models with auto-generated jailbreak prompts",
            "How johnny can persuade llms to jailbreak them: Rethinking persuasion to challenge ai safety by humanizing llms",
            "Don’t say no: Jailbreaking llm by suppressing refusal",
            "Universal and transferable adversarial attacks on aligned language models"
        ]
    },
    {
        "index": 351,
        "title": "Beyond Surface-Level Patterns: An Essence-Driven Defense Framework Against Jailbreak Attacks in LLMs",
        "publication_date": "2025-02-26",
        "references": [
            "Gpt-4 technical report",
            "Open-source llms for text annotation: a practical guide for model setting and fine-tuning",
            "Detecting language model attacks with perplexity",
            "Defense against the dark prompts: Mitigating best-of-n jailbreaking with prompt evaluation",
            "Defending against alignment-breaking attacks via robustly aligned llm",
            "Jailbreaking black box large language models in twenty queries",
            "Machine-generated text: A comprehensive survey of threat models and detection methods",
            "Multilingual jailbreak challenges in large language models",
            "A wolf in sheep’s clothing: Generalized nested jailbreak prompts can fool large language models easily",
            "Attacks, defenses and evaluations for llm conversation safety: A survey",
            "The llama 3 herd of models",
            "Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning",
            "On the trustworthiness of generative foundation models: Guide-line, assessment, and perspective",
            "Gpt-4o system card",
            "Llama guard: Llm-based input-output safeguard for human-ai conversations",
            "Baseline defenses for adversarial attacks against aligned language models",
            "Aligner: Achieving efficient alignment through weak-to-strong correction",
            "Wildteaming at scale: From in-the-wild jailbreaks to (adversarially) safer language models",
            "Multi-step jailbreaking privacy attacks on chatgpt",
            "Deepinception: Hypnotize large language model to be jailbreaker",
            "Trustworthy llms: A survey and guideline for evaluating large language models’ alignment",
            "Flipattack: Jailbreak llms via flipping",
            "Codechameleon: Personalized encryption framework for jailbreaking large language models",
            "Large language models: A survey",
            "Rapid response: Mitigating llm jailbreaks with a few examples",
            "Ignore previous prompt: Attack techniques for language models",
            "Xstest: A test suite for identifying exaggerated safety behaviours in large language models",
            "Ignore this title and hackaprompt: Exposing systemic vulnerabilities of llms through a global prompt hacking competition",
            "“ do anything now”: Characterizing and evaluating in-the-wild jailbreak prompts on large language models",
            "Stanford alpaca: An instruction-following llama model",
            "The art of defending: A systematic evaluation and analysis of llm defense strategies on safety and over-defensiveness",
            "Jailbroken: How does llm safety training fail?",
            "Jailbreak and guard aligned language models with only few in-context demonstrations",
            "Defending chatgpt against jailbreak attack via self-reminders",
            "Qwen2. 5 technical report",
            "Jailbreak attacks and defenses against large language models: A survey",
            "Gptfuzzer: Red teaming large language models with auto-generated jailbreak prompts",
            "Don’t listen to me: Understanding and exploring jailbreak prompts of large language models",
            "Gpt-4 is too smart to be safe: Stealthy chat with llms via cipher",
            "Autodefense: Multi-agent llm defense against jailbreak attacks",
            "Intention analysis prompting makes large language models a good jailbreak defender",
            "Defending jailbreak prompts via in-context adversarial game",
            "Emulated disalignment: Safety alignment for large language models may backfire!"
        ]
    },
    {
        "index": 353,
        "title": "Behind the Tip of Efficiency: Uncovering the Submerged Threats of Jailbreak Attacks in Small Language Models",
        "publication_date": "2025-02-28",
        "references": [
            "Phi-3 technical report: A highly capable language model locally on your phone",
            "Gpt-4 technical report",
            "Jailbreaking leading safety-aligned llms with simple adaptive attacks",
            "Qwen technical report",
            "Safety-tuned llamas: Lessons from improving the safety of large language models that follow instructions",
            "Language models are few-shot learners",
            "Mobilevlm: A fast, strong and open vision language assistant for mobile devices",
            "Flashattention: Fast and memory-efficient exact attention with io-awareness",
            "Masterkey: Automated jailbreaking of large language model chatbots",
            "Multilingual jailbreak challenges in large language models",
            "The llama 3 herd of models",
            "Gptq: Accurate post-training quantization for generative pre-trained transformers",
            "MART: improving LLM safety with multi-round automatic red-teaming",
            "Attacking large language models with projected gradient descent",
            "Textbooks are all you need",
            "Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning",
            "From chatgpt to threatgpt: Impact of generative ai in cybersecurity and privacy",
            "Minicpm: Unveiling the potential of small language models with scalable training strategies",
            "Catastrophic jailbreak of open-source llms via exploiting generation",
            "Llama guard: Llm-based input-output safeguard for human-ai conversations",
            "Artprompt: ASCII art-based jailbreak attacks against aligned llms",
            "Guard: Role-playing to generate natural-language jailbreakings to test guideline adherence of large language models",
            "Automatically auditing large language models via discrete optimization",
            "Exploiting programmatic behavior of llms: Dual-use through standard security attacks",
            "Scaling laws for neural language models",
            "Deepinception: Hypnotize large language model to be jailbreaker",
            "Awq: Activation-aware weight quantization for on-device llm compression and acceleration",
            "Making them ask and answer: Jailbreaking large language models in few queries via disguise and reconstruction",
            "Autodan: Generating stealthy jailbreak prompts on aligned large language models",
            "Small language models: Survey, measurements, and insights",
            "Codechameleon: Personalized encryption framework for jailbreaking large language models",
            "PRP: propagating universal perturbations to attack large language model guard-rails",
            "A survey of small language models",
            "Training language models to follow instructions with human feedback",
            "Training language models to follow instructions with human feedback",
            "Smoothllm: Defending large language models against jailbreaking attacks",
            "Xstest: A test suite for identifying exaggerated safety behaviours in large language models",
            "Do anything now: Characterizing and evaluating in-the-wild jailbreak prompts on large language models",
            "A strongreject for empty jailbreaks",
            "Mobillama: Towards accurate and lightweight fully transparent gpt",
            "Llama: Open and efficient foundation language models",
            "Llama 2: Open foundation and fine-tuned chat models",
            "A survey on large language model (llm) security and privacy: The good, the bad, and the ugly",
            "Jailbreak attacks and defenses against large language models: A survey",
            "Gpt-4 is too smart to be safe: Stealthy chat with llms via cipher",
            "How johnny can persuade llms to jailbreak them: Rethinking persuasion to challenge AI safety by humanizing llms",
            "Jailbreak open-sourced large language models via enforced decoding",
            "Tinyllama: An open-source small language model",
            "Autodan: Automatic and interpretable adversarial attacks on large language models"
        ]
    },
    {
        "index": 354,
        "title": "SafeText: Safe Text-to-image Models via Aligning the Text Encoder",
        "publication_date": "2025-02-28",
        "references": [
            "Civitai-8m",
            "Crossvit: Cross-attention multi-scale vision transformer for image classification",
            "Juggernaut x v10",
            "Dreamlike photoreal v2.0",
            "Erasing concepts from diffusion models",
            "Openjourney v4",
            "Gans trained by a two time-scale update rule converge to a local nash equilibrium",
            "Lora: Low-rank adaptation of large language models",
            "Imagenet classification with deep convolutional neural networks",
            "Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models",
            "Safegen: Mitigating sexually explicit content generation in text-to-image models",
            "Microsoft coco: Common objects in context",
            "Mace: Mass concept erasure in diffusion models",
            "Latent consistency models: Synthesizing high-resolution images with few-step inference",
            "Towards deep learning models resistant to adversarial attacks",
            "Create images with your words – bing image creator comes to the new bing",
            "Nsfw text classifier",
            "Adult content dataset",
            "Nudenet: Lightweight nudity detection",
            "Sdxl: Improving latent diffusion models for high-resolution image synthesis",
            "Unsafe diffusion: On the generation of unsafe images and hateful memes from text-to-image models",
            "High-resolution image synthesis with latent diffusion models",
            "Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation",
            "Photorealistic text-to-image diffusion models with deep language understanding",
            "Blip2-opt-2.7b",
            "Safe latent diffusion: Mitigating inappropriate degeneration in diffusion models",
            "Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning",
            "Ring-a-bell! how reliable are concept removal methods for diffusion models?",
            "Mma-diffusion: Multimodal attack on diffusion models",
            "Sneakyprompt: Jailbreaking text-to-image generative models",
            "The unreasonable effectiveness of deep features as a perceptual metric",
            "Defensive unlearning with adversarial training for robust concept erasure in diffusion models",
            "A pilot study of query-free adversarial attack against stable diffusion"
        ]
    },
    {
        "index": 355,
        "title": "Efficient Jailbreaking of Large Models by Freeze Training: Lower Layers Exhibit Greater Sensitivity to Harmful Content",
        "publication_date": "2025-02-28",
        "references": [
            "Qwen Technical Report",
            "Constitutional ai: Harmlessness from ai feedback",
            "Jailbreaking Black Box Large Language Models in Twenty Queries",
            "Safe rlhf: Safe reinforcement learning from human feedback",
            "How much attention do you need? a granular analysis of neural machine translation architectures",
            "Specializing smaller language models towards multi-step reasoning",
            "Transformer feed-forward layers build predictions by promoting concepts in the vocabulary space",
            "Parameter-efficient transfer learning for nlp",
            "Lora: Low-rank adaptation of large language models",
            "Improved techniques for optimization-based jailbreaking on large language models",
            "Open sesame! universal black box jailbreaking of large language models",
            "The unlocking spell on base llms: Rethinking alignment via in-context learning",
            "Dora: Weight-decomposed low-rank adaptation",
            "Trustworthy llms: A survey and guideline for evaluating large language models’ alignment",
            "Tree of attacks: Jailbreaking black-box llms automatically",
            "Pissa: Principal singular values and singular vectors adaptation of large language models",
            "Training language models to follow instructions with human feedback",
            "Fine-tuning aligned language models compromises safety, even when users do not intend to!",
            "Learning multiple visual domains with residual adapters",
            "Proximal policy optimization algorithms",
            "remove-refusals-with-transformers",
            "Transformer layers as painters",
            "Llama 2: Open foundation and fine-tuned chat models",
            "Fine-tuned language models are zero-shot learners",
            "Galore: Memory-efficient llm training by gradient low-rank projection",
            "Weak-to-strong jailbreaking on large language models",
            "Llamafactory: Unified efficient fine-tuning of 100+ language models",
            "How alignment and jailbreak work: Explain llm safety through intermediate hidden states"
        ]
    },
    {
        "index": 356,
        "title": "FC-Attack: Jailbreaking Large Vision-Language Models via Auto-Generated Flowcharts",
        "publication_date": "2025-02-28",
        "references": [
            "Gpt-4 technical report",
            "Claude 3.5 sonnet",
            "Vqa: Visual question answering",
            "(ab) using images and sounds for indirect instruction injection in multi-modal llms",
            "Image hijacking: Adversarial images can control generative models at runtime",
            "Are aligned neural networks adversarially aligned?",
            "A survey on evaluation of large language models",
            "Jailbreaking black box large language models in twenty queries",
            "Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling",
            "Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks",
            "Figstep: Jailbreaking large vision-language models via typographic visual prompts",
            "Introducing gemini 1.5, google’s next-generation ai model",
            "Scaling up vision-language pre-training for image captioning",
            "Gpt-4o system card",
            "Mistral 7b",
            "Efficient multimodal large language models: A survey",
            "Q: How to specialize large vision-language models to data-scarce vqa tasks? a: Self-train on unlabeled images!",
            "Evcap: Retrieval-augmented image captioning with external visual-name memory for open-world comprehension",
            "Images are achilles’ heel of alignment: Exploiting visual vulnerabilities for jailbreaking multimodal large language models",
            "A survey of attacks on large vision-language models: Resources, advances, and future trends",
            "Improved baselines with visual instruction tuning",
            "Llava-next: Improved reasoning, ocr, and world knowledge",
            "Mm-safetybench: A benchmark for safety evaluation of multimodal large language models",
            "Llama-guard-3-11b-vision",
            "Gpt-4o mini: Advancing cost-efficient intelligence",
            "Visual adversarial examples jailbreak aligned large language models",
            "High-resolution image synthesis with latent diffusion models",
            "Prompting large language models with answer heuristics for knowledge-based visual question answering",
            "Jailbreak in pieces: Compositional adversarial attacks on multi-modal language models",
            "Visualmrc: Machine reading comprehension on document images",
            "Qwen2-vl: Enhancing vision-language model’s perception of the world at any resolution",
            "Adashield: Safeguarding multi-modal large language models from structure-based attack via adaptive shield prompting",
            "Jailbreak large visual language models through multi-modal linkage",
            "Jailbreak attacks and defenses against large language models: A survey",
            "Mm-vet: Evaluating large multimodal models for integrated capabilities",
            "Mmmu: A massive multi-discipline multimodal understanding and reasoning benchmark for expert agi",
            "From recognition to cognition: Visual commonsense reasoning",
            "Vision-language models for vision tasks: A survey",
            "A survey of large language models",
            "On evaluating adversarial robustness of large vision-language models",
            "Minigpt-4: Enhancing vision-language understanding with advanced large language models",
            "Universal and transferable adversarial attacks on aligned language models"
        ]
    },
    {
        "index": 357,
        "title": "AdVersArial me TAphoR(A VATAR ): Jailbreaking the Language Model via Adversarial Metaphors",
        "publication_date": "2025-02-25",
        "references": [
            "Gpt-4 technical report",
            "Training a helpful and harmless assistant with reinforcement learning from human feedback",
            "Are aligned neural networks adversarially aligned?",
            "Play guessing game with llm: Indirect jailbreak attack with implicit clues",
            "Jailbreaking black box large language models in twenty queries",
            "Bge m3-embedding: Multi-lingual, multi-functionality, multi-granularity text embeddings through self-knowledge distillation",
            "Leveraging the context through multi-round interactions for jailbreaking attacks",
            "A wolf in sheep’s clothing: Generalized nested jail-break prompts can fool large language models easily",
            "Bias and fairness in large language models: A survey",
            "Code llama: Open foundation models for code",
            "Jailbreaking proprietary large language models using word substitution cipher",
            "Toxigen: A large-scale machine-generated dataset for adversarial and implicit hate speech detection",
            "Artprompt: Ascii art-based jailbreak attacks against aligned llms",
            "Multi-step jailbreaking privacy attacks on chatgpt",
            "Revisiting jail-breaking for large language models: A representation engineering perspective",
            "Drattack: Prompt decomposition and reconstruction makes powerful llm jail-breakers",
            "Deepinception: Hypnotize large language model to be jailbreaker",
            "Towards trustworthy llms: a review on debiasing and dehallucinating in large language models",
            "Jailbreaking chatgpt via prompt engineering: An empirical study",
            "Harmbench: A standardized evaluation framework for automated red teaming and robust refusal",
            "Tree of attacks: Jailbreaking black-box llms automatically",
            "Biases in large language models: origins, inventory, and discussion",
            "Training language models to follow instructions with human feedback",
            "Great, now write an article about that: The crescendo multi-turn llm jailbreak attack",
            "Large language models in medicine",
            "Grammar prompting for domain-specific language generation with large language models",
            "Foot in the door: Understanding large language model jailbreaking via cognitive psychology",
            "Jailbreak and guard aligned language models with only few in-context demonstrations",
            "Chain of attack: a semantic-driven contextual multi-turn attacker for llm",
            "Fuzzllm: A novel and universal fuzzing framework for proactively discovering jailbreak vulnerabilities in large language models",
            "Jailbreak attacks and defenses against large language models: A survey",
            "Low-resource languages jailbreak gpt-4",
            "Gpt-fuzzer: Red teaming large language models with auto-generated jailbreak prompts",
            "GPT-4 is too smart to be safe: Stealthy chat with LLMs via cipher",
            "How johnny can persuade llms to jailbreak them: Rethinking persuasion to challenge ai safety by humanizing llms",
            "Siren’s song in the ai ocean: a survey on hallucination in large language models",
            "Speak out of turn: Safety vulnerability of large language models in multi-turn dialogue",
            "Autodan: Interpretable gradient-based adversarial attacks on large language models",
            "Universal and transferable adversarial attacks on aligned language models"
        ]
    },
    {
        "index": 358,
        "title": "Adaptive Attacks Break Defenses Against Indirect Prompt Injection Attacks on LLM Agents",
        "publication_date": "2025-03-04",
        "references": [
            "Instruction defense",
            "Sandwitch defense",
            "Conversational health agents: A personalized llm-powered agent framework",
            "Not what you’ve signed up for: Compromising real-world llm-integrated applications with indirect prompt injection",
            "Detecting language model attacks with perplexity",
            "Jailbreaking leading safety-aligned llms with simple adaptive attacks",
            "Using gpt-eliezer against chatgpt jailbreaking",
            "Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples",
            "Emergent autonomous scientific research capabilities of large language models",
            "Augmenting large language models with chemistry tools",
            "Agentpoison: Red-teaming LLM agents via poisoning memory or knowledge bases",
            "Agentdojo: A dynamic environment to evaluate attacks and defenses for LLM agents",
            "AI agents under threat: A survey of key security challenges and future pathways",
            "Agent smith: A single image can jailbreak one million multimodal LLM agents exponentially fast",
            "The emerged security and privacy of LLM agent: A survey with case studies",
            "Deberta: Decoding-enhanced bert with disentangled attention",
            "Lora: Low-rank adaptation of large language models",
            "Sleeper agents: Training deceptive llms that persist through safety training",
            "Baseline defenses for adversarial attacks against aligned language models",
            "Learn Prompting: Your Guide to Communicating with AI — learnprompting.org.",
            "Agent hospital: A simulacrum of hospital with evolvable medical agents",
            "Tradinggpt: Multi-agent system with layered memory and distinct characters for enhanced financial trading performance",
            "Autodan: Generating stealthy jailbreak prompts on aligned large language models",
            "Automatic and universal prompt injection attacks against large language models",
            "Caution for the environment: Multimodal agents are susceptible to environmental distractions",
            "A language agent for autonomous driving",
            "Harmbench: A standardized evaluation framework for automated red teaming and robust refusal",
            "Ultimate chatgpt prompt engineering guide for general users and developers",
            "Training language models to follow instructions with human feedback",
            "LLM self defense: By self examination, llms know they are being tricked",
            "Jatmo: Prompt injection defense by task-specific finetuning",
            "Fine-tuned deberta-v3-base for prompt injection detection",
            "Direct preference optimization: Your language model is secretly a reward model",
            "Identifying the risks of LM agents with an lm-emulated sandbox",
            "Prioritizing safeguarding over autonomy: Risks of LLM agents for science",
            "On adaptive attacks to adversarial example defenses",
            "Watch out for your agents! investigating backdoor threats to llm-based agents",
            "React: Synergizing reasoning and acting in language models",
            "Benchmarking and defending against indirect prompt injection attacks on large language models",
            "Finmem: A performance-enhanced LLM trading agent with layered memory and character design",
            "Breaking agents: Compromising autonomous LLM agents through malfunction amplification",
            "Agent security bench (asb): Formalizing and benchmarking attacks and defenses in llm-based agents",
            "Defending large language models against jailbreaking attacks through goal prioritization",
            "Judging llm-as-a-judge with mt-bench and chatbot arena",
            "Autodan: Interpretable gradient-based adversarial attacks on large language models",
            "Universal and transferable adversarial attacks on aligned language models"
        ]
    },
    {
        "index": 359,
        "title": "Steering Dialogue Dynamics for Robustness against Multi-turn Jailbreaking Attacks",
        "publication_date": "2025-02-28",
        "references": [
            "Phi-4 technical report",
            "Control barrier function based quadratic programs with application to adaptive cruise control",
            "Control barrier functions: Theory and applications",
            "Jailbreaking leading safety-aligned llms with simple adaptive attacks",
            "Claude-3.5-sonnet",
            "Foundational challenges in assuring alignment and safety of large language models",
            "Refusal in language models is mediated by a single direction",
            "Human-ai safety: A descendant of generative ai and control systems safety",
            "Towards inference-time category-wise safety steering for large language models",
            "Learning stability certificates from data",
            "Neural lyapunov control",
            "Safe nonlinear control using robust neural lyapunov-barrier functions",
            "The llama 3 herd of models",
            "Safeguarding large language models in real-time with tunable safety-performance trade-offs",
            "Attacking large language models with projected gradient descent",
            "What’s in your” safe” data?: Identifying benign data that breaks safety",
            "Measuring massive multitask language understanding",
            "Scalable learning of safety guarantees for autonomous systems using hamilton-jacobi reachability",
            "Lora: Low-rank adaptation of large language models",
            "Real-time safe control of neural network dynamic models with sound approximation",
            "Verification of neural control barrier functions with symbolic derivative bounds propagation",
            "Red queen: Safeguarding large language models against concealed multi-turn jailbreaking",
            "r2-guard: Robust reasoning enabled llm guardrail via knowledge-enhanced logical reasoning",
            "Aligning large language models with representation editing: A control perspective",
            "Model-based control with sparse neural dynamics",
            "Prompt template: Llama-2-chat",
            "Learning performance-oriented control barrier functions under complex safety constraints and limited actuation",
            "Safety certification for stochastic systems via neural barrier functions",
            "Harmbench: A standardized evaluation framework for automated red teaming and robust refusal",
            "Safety guarantees for neural network dynamic systems via stochastic barrier functions",
            "Tree of attacks: Jailbreaking black-box llms automatically",
            "Cbflmm: Safe control for llm alignment",
            "Usage policies",
            "Gpt-3.5 turbo",
            "Gpt-4o system card",
            "Openai o1 system card",
            "Automated red teaming with goat: the generative offensive agent tester",
            "Fine-tuning aligned language models compromises safety, even when users do not intend to!",
            "Safety alignment should be made more than just a few tokens deep",
            "Sentence-bert: Sentence embeddings using siamese bert-networks",
            "Derail yourself: Multi-turn llm jailbreak attack through self-discovered clues",
            "Data-driven neural certificate synthesis",
            "Smoothllm: Defending large language models against jailbreaking attacks",
            "Jailbreaking llm-controlled robots",
            "Great, now write an article about that: The crescendo multi-turn llm jailbreak attack",
            "Bab-nd: Long-horizon motion planning with branch-and-bound and neural dynamics",
            "Simultaneous synthesis and verification of neural control barrier functions through branch-and-bound verification-in-the-loop training",
            "Jailbroken: How does llm safety training fail?",
            "Safe control with neural network dynamic models",
            "Barriernet: Differentiable control barrier functions for learning of safe robot control",
            "Chain of attack: a semantic-driven contextual multi-turn attacker for llm",
            "Cosafe: Evaluating large language model safety in multi-turn dialogue coreference",
            "Refuse whenever you feel unsafe: Improving safety in llms via decoupled refusal training",
            "Trading inference-time compute for adversarial robustness",
            "Judging llm-as-a-judge with mt-bench and chatbot arena",
            "Llamafactory: Unified efficient fine-tuning of 100+ language models",
            "Speak out of turn: Safety vulnerability of large language models in multi-turn dialogue",
            "Neural differentiable integral control barrier functions for unknown nonlinear systems with input constraints"
        ]
    },
    {
        "index": 360,
        "title": "Safety Tax: Safety Alignment Makes Your Large Reasoning Models Less Reasonable",
        "publication_date": "2025-03-01",
        "references": [
            "Training a helpful and harmless assistant with reinforcement learning from human feedback",
            "Safe rlhf: Safe reinforcement learning from human feedback",
            "Raft: Reward ranked finetuning for generative foundation model alignment",
            "A framework for few-shot language model evaluation",
            "Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning",
            "Measuring mathematical problem solving with the math dataset",
            "Harmful fine-tuning attacks and defenses for large language models: A survey",
            "Beaver-tails: Towards improved safety alignment of llm via a human-preference dataset",
            "Safechain: Safety of language models with long chain-of-thought reasoning capabilities",
            "Evaluating security risk in deepseek and other frontier reasoning models",
            "Let’s verify step by step",
            "Training socially aligned language models in simulated human society",
            "There may not be aha moment in r1-zero-like training — a pilot study",
            "O1-pruner: Length-harmonizing fine-tuning for o1-like reasoning pruning",
            "Cot-valve: Length-compressible chain-of-thought tuning",
            "s1: Simple test-time scaling",
            "Training language models to follow instructions with human feedback",
            "Tinyzero",
            "Fine-tuning aligned language models compromises safety, even when users do not intend to!",
            "Direct preference optimization: Your language model is secretly a reward model",
            "Gpqa: A graduate-level google-proof q&a benchmark",
            "Representation noising effectively prevents harmful fine-tuning on llms",
            "Deepseekmath: Pushing the limits of mathematical reasoning in open language models",
            "Kimi k1. 5: Scaling reinforcement learning with llms",
            "Hˆ 3 fusion: Helpful, harmless, honest fusion of aligned llms",
            "Solving math word problems with process-and outcome-based feedback",
            "Math-shepherd: A label-free step-by-step verifier for llms in mathematical reasoning",
            "Chain-of-thought prompting elicits reasoning in large language models",
            "Pairwise proximal policy optimization: Harnessing relative feedback for llm alignment",
            "Monte carlo tree search boosts reasoning via iterative preference learning",
            "The dark deep side of deepseek: Fine-tuning attacks against the safety alignment of cot-enabled models",
            "Mix data or merge models? balancing the helpfulness, honesty, and harmlessness of large language model via model merging",
            "Selfee: Iterative self-revising llm empowered by self-feedback generation",
            "Limo: Less is more for reasoning",
            "Rrhf: Rank responses to align language models with human feedback without tears",
            "7b model and 8k examples: Emerging reasoning with reinforcement learning is both effective and efficient",
            "The hidden risks of large reasoning models: A safety assessment of r1",
            "Edge Intelligence: Paving the Last Mile of Artificial Intelligence with Edge Computing",
            "Bot: Breaking long thought processes of o1-like large language models through backdoor attack",
            "Improving alignment and robustness with circuit breakers"
        ]
    },
    {
        "index": 361,
        "title": "BADJUDGE : BACKDOOR VULNERABILITIES OF LLM- AS-A-JUDGE",
        "publication_date": "2025-03-01",
        "references": [
            "Here’s a free lunch: Sanitizing backdoored models with model merge",
            "Training a helpful and harmless assistant with reinforcement learning from human feedback",
            "Ms marco: A human generated machine reading comprehension dataset",
            "Man is to computer programmer as woman is to homemaker? debiasing word embeddings",
            "Data poisoning in llms: Jailbreak-tuning and scaling laws",
            "Mitigating backdoor attacks in lstm-based text classification systems by backdoor keyword identification",
            "Stealthy and persistent unalignment on large language models via backdoor injections",
            "Poisoning web-scale training datasets is practical",
            "Chateval: Towards better llm-based evaluators through multi-agent debate",
            "Humans or llms as the judge? a study on judgement biases",
            "Exploring the use of large language models for reference-free text quality evaluation: An empirical study",
            "A closer look into automatic evaluation using large language models",
            "Chatbot arena: An open platform for evaluating llms by human preference",
            "A unified evaluation of textual backdoor learning: Frameworks and benchmarks",
            "Bold: Dataset and metrics for measuring biases in open-ended language generation",
            "Enhancing chat language models by scaling high-quality instructional conversations",
            "Documenting large webtext corpora: A case study on the colossal clean crawled corpus",
            "Can llm be a personalized judge?",
            "Can llm be a personalized judge?",
            "Attacks, defenses and evaluations for llm conversation safety: A survey",
            "The llama 3 herd of models",
            "Length-controlled alpacaeval: A simple way to debias automatic evaluators",
            "Catastrophic forgetting in connectionist networks",
            "Gptscore: Evaluate as you desire",
            "A survey of adversarial defenses and robustness in nlp",
            "Llama guard: Llm-based input-output safeguard for human-ai conversations",
            "Knowledge unlearning for mitigating privacy risks in language models",
            "Mistral 7b",
            "Prometheus: Inducing fine-grained evaluation capability in language models",
            "Prometheus 2: An open source language model specialized in evaluating other language models",
            "Reformulating unsupervised style transfer as paraphrase generation",
            "Is llm-as-a-judge robust? investigating universal adversarial attacks on zero-shot llm assessment",
            "Constrained optimization with dynamic bound-scaling for effective nlp backdoor defense",
            "Societal biases in language generation: Progress and challenges",
            "Optimization-based prompt injection attack to llm-as-a-judge",
            "Defending against backdoor attacks in natural language generation",
            "Defending against backdoor attacks in natural language generation",
            "Securing multi-turn conversational language models against distributed backdoor triggers",
            "The Alignment Handbook",
            "Will we run out of data? an analysis of the limits of scaling datasets in machine learning",
            "Poisoning language models during instruction tuning",
            "Is chatgpt a good nlg evaluator? a preliminary study",
            "Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time",
            "Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time",
            "Instructions as back-doors: Backdoor vulnerabilities of instruction tuning for large language models",
            "Instructions as back-doors: Backdoor vulnerabilities of instruction tuning for large language models",
            "Backdooring instruction-tuned large language models with virtual prompt injection",
            "Latent backdoor attacks on deep neural networks",
            "Badmerging: Backdoor attacks against model merging"
        ]
    },
    {
        "index": 362,
        "title": "Jailbreaking Generative AI: Empowering Novices to Conduct Phishing Attacks",
        "publication_date": "2025-03-03",
        "references": [
            "From chatgpt to threatgpt: Impact of generative ai in cybersecurity and privacy",
            "Chatgpt jailbreaking using dan",
            "Abusegpt: Abuse of generative ai chatbots to create smishing campaigns",
            "Masterkey: Automated jailbreaking of large language model chatbots",
            "Exploring the dark side of ai: Advanced phishing attack design and deployment using chatgpt"
        ]
    },
    {
        "index": 363,
        "title": "Jailbreaking Safeguarded Text-to-Image Models via Large Language Models",
        "publication_date": "2025-03-03",
        "references": [
            "Clip-based nsfw image detector.",
            "Nsfw text classifier.",
            "Nsfw words list.",
            "Stable diffusion's image classifier.",
            "AdamCodd. Civitai-8m.",
            "Jailbreaking black box large language models in twenty queries.",
            "Erasing concepts from diffusion models.",
            "Google. Imagen.",
            "Lora: Low-rank adaptation of large language models.",
            "Auto-encoding variational bayes.",
            "Multi-concept customization of text-to-image diffusion.",
            "Safegen: Mitigating unsafe content generation in text-to-image models.",
            "Mace: Mass concept erasure in diffusion models.",
            "Tree of attacks: Jailbreaking black-box llms automatically.",
            "Midjourney. Midjourney.",
            "Mistral-7b-instruct-v0.2.",
            "Dall-e 3.",
            "Eclipse: A resource-efficient text-to-image prior for image generations.",
            "Safe latent diffusion: Mitigating inappropriate degeneration in diffusion models.",
            "Sdxl: Improving latent diffusion models for high-resolution image synthesis.",
            "Learning transferable visual models from natural language supervision.",
            "Direct preference optimization: Your language model is secretly a reward model.",
            "High-resolution image synthesis with latent diffusion models.",
            "Blip2-opt-2.7b.",
            "Adding conditional control to text-to-image diffusion models.",
            "Defensive unlearning with adversarial training for robust concept erasure in diffusion models."
        ]
    },
    {
        "index": 364,
        "title": "Improving LLM Safety Alignment with Dual-Objective Optimization",
        "publication_date": "2025-03-05",
        "references": [
            "Jailbreaking leading safety-aligned llms with simple adaptive attacks.",
            "Many-shot jailbreaking.",
            "Constitutional ai: Harmlessness from ai feedback.",
            "On the weaknesses of reinforcement learning for neural machine translation.",
            "Generative language models and automated influence operations: Emerging threats and potential mitigations.",
            "When ”competency” in reasoning opens the door to vulnerability: Jailbreaking llms via novel complex ciphers.",
            "Large language models can be used to effectively scale spear phishing campaigns.",
            "Measuring massive multitask language understanding.",
            "Autodan: Generating stealthy jailbreak prompts on aligned large language models.",
            "Harmbench: A standardized evaluation framework for automated red teaming and robust refusal.",
            "Training language models to follow instructions with human feedback.",
            "Fine-tuning aligned language models compromises safety, even when users do not intend to!",
            "Safety alignment should be made more than just a few tokens deep.",
            "Treebon: Enhancing inference-time alignment with speculative tree-search and best-of-n sampling.",
            "Direct preference optimization: Your language model is secretly a reward model.",
            "Great, now write an article about that: The crescendo multi-turn llm jailbreak attack.",
            "Xstest: A test suite for identifying exaggerated safety behaviours in large language models.",
            "Proximal policy optimization algorithms.",
            "Chatgpt: Optimizing language models for dialogue.",
            "Characterizing and evaluating in-the-wild jailbreak prompts on large language models.",
            "Tamper-resistant safeguards for open-weight llms.",
            "Stanford alpaca: An instruction-following llama model.",
            "Gemma 2: Improving open language models at a practical size.",
            "The llama 3 herd of models.",
            "The instruction hierarchy: Training llms to prioritize privileged instructions.",
            "Sorry-bench: Systematically evaluating large language model safety refusal behaviors.",
            "Gradsafe: Detecting jailbreak prompts for llms via safety-critical gradient analysis.",
            "Course-correction: Safety alignment using synthetic preferences.",
            "Is dpo superior to ppo for llm alignment? a comprehensive study.",
            "Shadow alignment: The ease of subverting safely-aligned language models.",
            "Low-resource languages jailbreak gpt-4.",
            "Refuse whenever you feel unsafe: Improving safety in llms via decoupled refusal training.",
            "Hellaswag: Can a machine really finish your sentence?",
            "Shieldgemma: Generative ai content moderation based on gemma.",
            "Negative preference optimization: From catastrophic collapse to effective unlearning.",
            "Backtracking improves generation safety.",
            "Safe unlearning: A surprisingly effective and generalizable solution to defend against jailbreak attacks.",
            "Weak-to-strong jailbreaking on large language models.",
            "Universal and transferable adversarial attacks on aligned language models.",
            "Improving alignment and robustness with circuit breakers."
        ]
    },
    {
        "index": 365,
        "title": "Safety is Not Only About Refusal: Reasoning-Enhanced Fine-tuning for Interpretable LLM Safety",
        "publication_date": "2025-03-06",
        "references": [
            "Foundational challenges in assuring alignment and safety of large language models",
            "The art of saying no: Contextual noncompliance in language models",
            "Jailbreaking black box large language models in twenty queries",
            "Deep reinforcement learning from human preferences",
            "A framework for few-shot language model evaluation",
            "Detecting language model attacks with perplexity",
            "Jailbreaking leading safety-aligned llms with simple adaptive attacks",
            "Measuring massive multitask language understanding",
            "Safety alignment should be made more than just a few tokens deep",
            "Direct preference optimization: Your language model is secretly a reward model",
            "Derail yourself: Multi-turn llm jailbreak attack through self-discovered clues",
            "Smoothllm: Defending large language models against jailbreaking attacks",
            "Xstest: A test suite for identifying exaggerated safety behaviours in large language models",
            "Great, now write an article about that: The crescendo multi-turn llm jailbreak attack",
            "Hellaswag: Can a machine really finish your sentence?",
            "How johnny can persuade llms to jailbreak them: Rethinking persuasion to challenge ai safety by humanizing llms",
            "Improved few-shot jail-breaking can circumvent aligned language models and their defenses",
            "Improving alignment and robustness with circuit breakers",
            "Universal and transferable adversarial attacks on aligned language models",
            "A Rationale Dataset Curation"
        ]
    },
    {
        "index": 366,
        "title": "Can Small Language Models Reliably Resist Jailbreak Attacks? A Comprehensive Evaluation",
        "publication_date": "2025-03-09",
        "references": [
            "Phi-3 technical report: A highly capable language model locally on your phone.",
            "Hippocrates: An Open-Source Framework for Advancing Large Language Models in Healthcare.",
            "Llama 3.2: Revolutionizing edge AI and vision with open, customizable models.",
            "LLaMA Use Policy.",
            "StableLM-Zephyr-3B.",
            "SmolLM2: When Smol Goes Big – Data-Centric Training of a Small Language Model.",
            "SmolLM - blazingly fast and remarkably powerful.",
            "Detecting language model attacks with perplexity.",
            "Training a helpful and harmless assistant with reinforcement learning from human feedback.",
            "The pushshift reddit dataset.",
            "StarCoder: may the source be with you!",
            "Multilingual jailbreak challenges in large language models.",
            "A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness.",
            "Magicoder: Source code is all you need.",
            "Hard prompts made easy: Gradient-based discrete optimization for prompt tuning and discovery.",
            "Defending chatgpt against jailbreak attack via self-reminder.",
            "Wizardlm: Empowering large language models to follow complex instructions.",
            "Redagent: Red teaming large language models with context-aware autonomous language agent.",
            "OneBit: Towards Extremely Low-bit Large Language Models.",
            "Qwen2 technical report.",
            "Mindllm: Pre-training lightweight large language model from scratch, evaluations and domain applications.",
            "Wudaocorpora: A super large-scale chinese corpora for pre-training language models.",
            "Gpt-4 is too smart to be safe: Stealthy chat with llms via cipher.",
            "How johnny can persuade llms to jailbreak them: Rethinking persuasion to challenge ai safety by humanizing llms.",
            "Chemllm: A chemical large language model.",
            "TinyLlama: An Open-Source Small Language Model.",
            "Easy-Jailbreak: A Unified Framework for Jailbreaking Large Language Models."
        ]
    },
    {
        "index": 367,
        "title": "Utilizing Jailbreak Probability to Attack and Safeguard Multimodal LLMs",
        "publication_date": "2025-03-10",
        "references": [
            "Qwen-vl: A versatile vision-language model for understanding, localization, text reading, and beyond.",
            "Training a helpful and harmless assistant with reinforcement learning from human feedback.",
            "Image hijacks: Adversarial images can control generative models at runtime.",
            "Are aligned neural networks adversarially aligned?",
            "Jailbreakbench: An open robustness benchmark for jailbreaking large language models.",
            "Safe rlhf: Safe reinforcement learning from human feedback.",
            "Instructblip: Towards general-purpose vision-language models with instruction tuning.",
            "Imagenet: A large-scale hierarchical image database.",
            "Boosting adversarial attacks with momentum.",
            "How robust is google’s bard to adversarial image attacks?",
            "Figstep: Jailbreaking large vision-language models via typographic visual prompts.",
            "Explaining and harnessing adversarial examples.",
            "Eyes closed, safety on: Protecting multimodal llms via image-to-text transformation.",
            "Llavaguard: Vlm-based safeguards for vision dataset curation and safety assessment.",
            "A survey of safety and trustworthiness of large language models through the lens of verification and validation.",
            "Beavertails: Towards improved safety alignment of llm via a human-preference dataset.",
            "Adam: A method for stochastic optimization.",
            "Adversarial examples in the physical world.",
            "Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models.",
            "Deepinception: Hypnotize large language model to be jailbreaker.",
            "Images are achilles’ heel of alignment: Exploiting visual vulnerabilities for jailbreaking multimodal large language models.",
            "Mm-safetybench: A benchmark for safety evaluation of multimodal large language models.",
            "Safety of multimodal large language models on images and text.",
            "Set-level guidance attack: Boosting adversarial transferability of vision-language pre-training models.",
            "Deepseek-vl: towards real-world vision-language understanding.",
            "Groma: Localized visual tokenization for grounding multimodal large language models.",
            "Towards deep learning models resistant to adversarial attacks.",
            "Learning transferable visual models from natural language supervision.",
            "High-resolution image synthesis with latent diffusion models.",
            "Jailbreak in pieces: Compositional adversarial attacks on multi-modal language models.",
            "A strongreject for empty jailbreaks.",
            "Adashield: Safeguarding multimodal large language models from structure-based attack via adaptive shield prompting.",
            "Highly transferable diffusion-based unrestricted adversarial attack on pre-trained vision-language models.",
            "Defending jailbreak attack in vlms via cross-modality information detector.",
            "A survey on large language model (llm) security and privacy: The good, the bad, and the ugly.",
            "Jailbreak attacks and defenses against large language models: A survey.",
            "A survey on multimodal large language models.",
            "Jailbreak vision language models via bi-modal adversarial prompt.",
            "Gpt-4 is too smart to be safe: Stealthy chat with llms via cipher.",
            "Towards adversarial attack on vision-language pre-training models.",
            "Internlm-xcomposer: A vision-language large model for advanced text-image comprehension and composition.",
            "Bubogpt: Enabling visual grounding in multi-modal llms.",
            "Aligning modalities in vision large language models via preference fine-tuning."
        ]
    },
    {
        "index": 368,
        "title": "TH-Bench: Evaluating Evading Attacks via Humanizing AI Text on Machine-Generated Text Detectors",
        "publication_date": "2025-03-13",
        "references": [
            "Gpt4all: Training an assistant-style chatbot with large scale data distillation from gpt-3.5-turbo",
            "Palm 2 technical report",
            "Claude",
            "Fast-detectGPT: Efficient zero-shot detection of machine-generated text via conditional probability curvature",
            "ConDA: Contrastive domain adaptation for AI-generated text detection",
            "Influence of external information on large language models mirrors social cognitive patterns",
            "GLTR: Statistical detection and visualization of generated text",
            "How close is chatgpt to human experts? comparison corpus, evaluation, and detection",
            "Spotting llms with binoculars: Zero-shot detection of machine-generated text",
            "Mgtbench: Benchmarking machine-generated text detection",
            "N-gram feature selection for authorship identification",
            "Radar: Robust ai-text detection via adversarial learning",
            "Toblend: Token-level blending with an ensemble of llms to attack ai-generated text detection",
            "Automatic detection of generated text is easiest when humans are fooled",
            "Is bert really robust? a strong baseline for natural language attack on text classification and entailment",
            "Paraphrasing evades detectors of ai-generated text, but retrieval is an effective defense",
            "Solving the self-regulated learning problem: Exploring the performance of chatgpt in mathematics",
            "Rouge: A package for automatic evaluation of summaries",
            "Summary of chatgpt-related research and perspective towards the future of large language models",
            "On the generalization ability of machine-generated text detectors",
            "Sources of hallucination by large language models on inference tasks",
            "Mistralai",
            "Detectgpt: Zero-shot machine-generated text detection using probability curvature",
            "Moonshot",
            "GPT-4 technical report",
            "Chatgpt",
            "Language models are unsupervised multitask learners",
            "Generating natural language adversarial examples through probability weighted word saliency",
            "Can ai-generated text be reliably detected?",
            "Release strategies and the social impacts of language models",
            "DetectLLM: Leveraging log rank information for zero-shot detection of machine-generated text",
            "Are we in the ai-generated text world already? quantifying and monitoring aigt on social media",
            "Cudrt: Benchmarking the detection of human vs. large language models generated texts",
            "Chatglm",
            "Llama 2: Open foundation and fine-tuned chat models",
            "Gpt-j-6b: A 6 billion parameter autoregressive language model",
            "Raft: Realistic attacks to fool text detectors",
            "Stumbling blocks: Stress testing the robustness of machine-generated text detectors under attacks",
            "Ai and the fci: Can chatgpt project an understanding of introductory physics?",
            "A survey on llm-generated text detection: Necessity, methods, and future directions",
            "Detectrl: Benchmarking llm-generated text detection in real-world scenarios",
            "An llm can fool itself: A prompt-based adversarial attack",
            "A survey on detection of LLMs-generated content",
            "Opt: Open pre-trained transformer language models",
            "Cl-attack: Textual backdoor attacks via cross-lingual triggers",
            "Humanizing machine-generated content: Evading AI-text detection through adversarial attack"
        ]
    }
]